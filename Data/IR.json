{
    "Information Retrieval": {
        "http://arxiv.org/abs/2110.09749v5": {
            "Paper Title": "Importance Estimation from Multiple Perspectives for Keyphrase\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.16669v3": {
            "Paper Title": "An In-depth Analysis of Passage-Level Label Transfer for Contextual\n  Document Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15114v2": {
            "Paper Title": "UltraGCN: Ultra Simplification of Graph Convolutional Networks for\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "devise LightGCN\nfor recommendation by removing nonlinear activation and feature\ntransformation too. However, its efficiency is still limited by the\ntime-consuming message passing. Qiu et al. ",
                    "Citation Text": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.\nNetwork Embedding as Matrix Factorization: Unifying Deepwalk, LINE, PTE,\nand Node2vec. In Proceedings of the Eleventh ACM International Conference on\nWeb Search and Data Mining (WSDM) . 459\u2013467.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.02971",
                        "Citation Paper Title": "Title:Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                        "Citation Paper Abstract": "Abstract:Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                        "Citation Paper Authors": "Authors:Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "find the\nnon-necessity of nonlinear activation and feature transformation\nin GCN, proposing a simplified GCN (SGCN) model by removing\nthese two parts. Inspired by SGC, He et al. ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-\nwork for Recommendation. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval (SIGIR) . 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "propose UCMF that sim-\nplifies GCN for the node classification task. Wu et al. ",
                    "Citation Text": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying Graph Convolutional Networks. In International\nConference on Machine Learning (ICML) . 6861\u20136871.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "extend fixed-point theory on GNN for bet-\nter representation learning. Liu et al. ",
                    "Citation Text": "Qiang Liu, Haoli Zhang, and Zhaocheng Liu. 2020. Simplification of Graph Con-\nvolutional Networks: A Matrix Factorization-based Perspective. arXiv preprint\narXiv:2007.09036 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.09036",
                        "Citation Paper Title": "Title:Simplification of Graph Convolutional Networks: A Matrix Factorization-based Perspective",
                        "Citation Paper Abstract": "Abstract:In recent years, substantial progress has been made on Graph Convolutional Networks (GCNs). However, the computing of GCN usually requires a large memory space for keeping the entire graph. In consequence, GCN is not flexible enough, especially for large scale graphs in complex real-world applications. Fortunately, methods based on Matrix Factorization (MF) naturally support constructing mini-batches, and thus are more friendly to distributed computing compared with GCN. Accordingly, in this paper, we analyze the connections between GCN and MF, and simplify GCN as matrix factorization with unitization and co-training. Furthermore, under the guidance of our analysis, we propose an alternative model to GCN named Unitized and Co-training Matrix Factorization (UCMF). Extensive experiments have been conducted on several real-world datasets. On the task of semi-supervised node classification, the experimental results illustrate that UCMF achieves similar or superior performances compared with GCN. Meanwhile, distributed UCMF significantly outperforms distributed GCN methods, which shows that UCMF can greatly benefit large scale and complex real-world applications. Moreover, we have also conducted experiments on a typical task of graph embedding, i.e., community detection, and the proposed UCMF model outperforms several representative graph embedding models.",
                        "Citation Paper Authors": "Authors:Qiang Liu, Haoli Zhang, Zhaocheng Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12613v3": {
            "Paper Title": "SimpleX: A Simple and Strong Baseline for Collaborative Filtering",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "which is a simple linear model\nthat combines the advantages of neighborhood- and model-based\nCF approaches, MLPs-based NeuMF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International\nConference on World Wide Web (WWW) . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "which models the uncertainty in the user-item graph with bayesian\ngraph neural networks, DGCF ",
                    "Citation Text": "Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.\n2020. Disentangled Graph Collaborative Filtering. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR) . 1001\u20131010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.01764",
                        "Citation Paper Title": "Title:Disentangled Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning informative representations of users and items from the interaction data is of crucial importance to collaborative filtering (CF). Present embedding functions exploit user-item relationships to enrich the representations, evolving from a single user-item instance to the holistic interaction graph. Nevertheless, they largely model the relationships in a uniform manner, while neglecting the diversity of user intents on adopting the items, which could be to pass time, for interest, or shopping for others like families. Such uniform approach to model user interests easily results in suboptimal representations, failing to model diverse relationships and disentangle user intents in representations.\nIn this work, we pay special attention to user-item relationships at the finer granularity of user intents. We hence devise a new model, Disentangled Graph Collaborative Filtering (DGCF), to disentangle these factors and yield disentangled representations. Specifically, by modeling a distribution over intents for each user-item interaction, we iteratively refine the intent-aware interaction graphs and representations. Meanwhile, we encourage independence of different intents. This leads to disentangled representations, effectively distilling information pertinent to each intent. We conduct extensive experiments on three benchmark datasets, and DGCF achieves significant improvements over several state-of-the-art models like NGCF, DisenGCN, and MacridVAE. Further analyses offer insights into the advantages of DGCF on the disentanglement of user intents and interpretability of representations. Our codes are available in this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "that explicitly encoded the collaborative signals as high-order con-\nnectivities by performing embedding propagation. He et al. pro-\nposed LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-\nwork for Recommendation. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval (SIGIR) . 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "to model the\nitem-item relationships for Pinterest. Wang et al. devised NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR) . 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", which\napplied variational autoencoder (VAE) for CF. Ma et al. proposed\nMacridVAE ",
                    "Citation Text": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-\ning Disentangled Representations for Recommendation. In Advances in Neural\nInformation Processing Systems (NeurIPS) . 5711\u20135722.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14238",
                        "Citation Paper Title": "Title:Learning Disentangled Representations for Recommendation",
                        "Citation Paper Abstract": "Abstract:User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",
                        "Citation Paper Authors": "Authors:Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.05794v5": {
            "Paper Title": "BARS-CTR: Open Benchmarking for Click-Through Rate Prediction",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "learns the weights of feature interactions via atten-\ntional networks. Different from FwFM, AFM adjusts the weights\ndynamically according to the input data sample.\n\u2022DeepFM . DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\nInternational Joint Conference on Artificial Intelligence (IJCAI) . 1725\u20131731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ".\nRecently, deep learning has become a popular technique in rec-\nommender systems ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based\nRecommender System: A Survey and New Perspectives. ACM Comput. Surv. 52,\n1 (2019), 5:1\u20135:38.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "is an exten-\nsion of FM that considers field information for feature interac-\ntions. It was a winner model in several Kaggle contests on CTR\nprediction.\n\u2022HOFM . Since FM only captures second-order feature interac-\ntions, HOFM ",
                    "Citation Text": "Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016.\nHigher-Order Factorization Machines. In Annual Conference on Neural Informa-\ntion Processing Systems (NeurIPS) . 3351\u20133359.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.07195",
                        "Citation Paper Title": "Title:Higher-Order Factorization Machines",
                        "Citation Paper Abstract": "Abstract:Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.",
                        "Citation Paper Authors": "Authors:Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "proposes a Bi-interaction layer\nthat pools the pairwise feature interactions to a vector and then\nfeed it to a DNN for CTR prediction.\n\u2022AFM . Instead of treating all feature interactions equally as in\nFM, AFM ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional Factorization Machines: Learning the Weight of Feature Inter-\nactions via Attention Networks. In Proceedings of the Twenty-Sixth International\nJoint Conference on Artificial Intelligence, (IJCAI) . 3119\u20133125.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "aims to extend FM to higher-order factorization\nmachines. However, it results in exponential feature combina-\ntions that consume huge memory and take a long running time.\n\u2022FwFM . Recently, Pan et al. ",
                    "Citation Text": "Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,\nand Quan Lu. 2018. Field-weighted Factorization Machines for Click-Through\nRate Prediction in Display Advertising. In Proceedings of the 2018 World Wide\nWeb Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018 .\n1349\u20131357.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03514",
                        "Citation Paper Title": "Title:Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is a critical task in online display advertising. The data involved in CTR prediction are typically multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. One of the interesting characteristics of such data is that features from one field often interact differently with features from different other fields. Recently, Field-aware Factorization Machines (FFMs) have been among the best performing models for CTR prediction by explicitly modeling such difference. However, the number of parameters in FFMs is in the order of feature number times field number, which is unacceptable in the real-world production systems. In this paper, we propose Field-weighted Factorization Machines (FwFMs) to model the different feature interactions between different fields in a much more memory-efficient way. Our experimental evaluations show that FwFMs can achieve competitive prediction performance with only as few as 4% parameters of FFMs. When using the same number of parameters, FwFMs can bring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.",
                        "Citation Paper Authors": "Authors:Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, Quan Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.14201v2": {
            "Paper Title": "CNTLS: A Benchmark Dataset for Abstractive or Extractive Chinese\n  Timeline Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07353v4": {
            "Paper Title": "Nucleus I: Adjunction spectra in recommender systems and descent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05606v2": {
            "Paper Title": "Sentiment Analysis Using Averaged Weighted Word Vector Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08614v8": {
            "Paper Title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "for an overview). U NIQORN\ncannot outperform powerful SoTA models in this retriever-\nreader space like P ATHRETRIEVER ",
                    "Citation Text": "Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., Xiong, C.,\n2020. Learning to retrieve reasoning paths over wikipedia graph\nfor question answering, in: ICLR.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.10470",
                        "Citation Paper Title": "Title:Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
                        "Citation Paper Abstract": "Abstract:Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.",
                        "Citation Paper Authors": "Authors:Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "7.1. QA over heterogeneous sources\nMethods for heterogeneous QA can be broadly grouped\nas adopting one of the three following means ",
                    "Citation Text": "Roy, R.S., Anand, A., 2022. Question Answering for the Curated\nWeb: Tasks and Methods in QA over Knowledge Bases and Text\nCollections. Springer.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11980",
                        "Citation Paper Title": "Title:Question Answering over Curated and Open Web Sources",
                        "Citation Paper Abstract": "Abstract:The last few years have seen an explosion of research on the topic of automated question answering (QA), spanning the communities of information retrieval, natural language processing, and artificial intelligence. This tutorial would cover the highlights of this really active period of growth for QA to give the audience a grasp over the families of algorithms that are currently being used. We partition research contributions by the underlying source from where answers are retrieved: curated knowledge graphs, unstructured text, or hybrid corpora. We choose this dimension of partitioning as it is the most discriminative when it comes to algorithm design. Other key dimensions are covered within each sub-topic: like the complexity of questions addressed, and degrees of explainability and interactivity introduced in the systems. We would conclude the tutorial with the most promising emerging trends in the expanse of QA, that would help new entrants into this field make the best decisions to take the community forward. Much has changed in the community since the last tutorial on QA in SIGIR 2016, and we believe that this timely overview will indeed benefit a large number of conference participants.",
                        "Citation Paper Authors": "Authors:Rishiraj Saha Roy, Avishek Anand"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.02605v3": {
            "Paper Title": "GraphFormers: GNN-nested Transformers for Representation Learning on\n  Textual Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05641v3": {
            "Paper Title": "SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10085v2": {
            "Paper Title": "D-HAN: Dynamic News Recommendation with Hierarchical Attention Network",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nWith the remarkable success of the Transformer model Vaswani et al. ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, I. Polosukhin,\nAttention is all you need, in: Advances in neural information processing systems, 2017, pp.\n5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", it has been widely em-\nployed in news recommendation research Guo et al. ",
                    "Citation Text": "T. Guo, L. Yu, B. Shihada, X. Zhang, Few-shot news recommendation via cross-lingual transfer, in:\nProceedings of the ACM Web Conference 2023, WWW \u201923, Association for Computing Machinery,\nNew York, NY, USA, 2023, p. 1130\u20131140. URL: https://doi.org/10.1145/3543507.3583383 .\ndoi:10.1145/3543507.3583383 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.14370",
                        "Citation Paper Title": "Title:Few-shot News Recommendation via Cross-lingual Transfer",
                        "Citation Paper Abstract": "Abstract:The cold-start problem has been commonly recognized in recommendation systems and studied by following a general idea to leverage the abundant interaction records of warm users to infer the preference of cold users. However, the performance of these solutions is limited by the amount of records available from warm users to use. Thus, building a recommendation system based on few interaction records from a few users still remains a challenging problem for unpopular or early-stage recommendation platforms. This paper focuses on solving the few-shot recommendation problem for news recommendation based on two observations. First, news at different platforms (even in different languages) may share similar topics. Second, the user preference over these topics is transferable across different platforms. Therefore, we propose to solve the few-shot news recommendation problem by transferring the user-news preference from a many-shot source domain to a few-shot target domain. To bridge two domains that are even in different languages and without any overlapping users and news, we propose a novel unsupervised cross-lingual transfer model as the news encoder that aligns semantically similar news in two domains. A user encoder is constructed on top of the aligned news encoding and transfers the user preference from the source to target domain. Experimental results on two real-world news recommendation datasets show the superior performance of our proposed method on addressing few-shot news recommendation, comparing to the baselines.",
                        "Citation Paper Authors": "Authors:Taicheng Guo, Lu Yu, Basem Shihada, Xiangliang Zhang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". In recent studies, attention networks have been applied to model user\nrepresentations in news recommendation Wang et al. ",
                    "Citation Text": "H. Wang, F. Zhang, X. Xie, M. Guo, DKN: deep knowledge-aware network for news recommen-\ndation, in: WWW, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.01600v3": {
            "Paper Title": "Sparseness-constrained Nonnegative Tensor Factorization for Detecting\n  Topics at Different Time Scales",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08055v2": {
            "Paper Title": "Multi-Purchase Behavior: Modeling, Estimation and Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07536v2": {
            "Paper Title": "Tackling Query-Focused Summarization as A Knowledge-Intensive Task: A\n  Pilot Study",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "is an improved\nencoder-decoder architecture which has achieved state-of-art per-\nformance in some knowledge-intensive tasks ",
                    "Citation Text": "Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022. Evidentiality-Guided\nGeneration for Knowledge-Intensive NLP Tasks. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United\nStates, July 10-15, 2022 . Association for Computational Linguistics, 2226\u20132243.\nhttps://doi.org/10.18653/v1/2022.naacl-main.162",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.08688",
                        "Citation Paper Title": "Title:Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks",
                        "Citation Paper Abstract": "Abstract:Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open question answering and fact verification. These models are trained to generate the final output given the retrieved passages, which can be irrelevant to the original query, leading to learning spurious cues or answer memorization. This work introduces a method to incorporate the evidentiality of passages -- whether a passage contains correct evidence to support the output -- into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the evidentiality of each passage, leveraging a new task-agnostic method to obtain silver evidentiality labels for supervision. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart with the same-size model and advances the state of the art on FaVIQ-Ambig. We attribute these improvements to both the auxiliary multi-task learning and silver evidentiality mining techniques.",
                        "Citation Paper Authors": "Authors:Akari Asai, Matt Gardner, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". The closest KI task is called long-form question\nanswering (LFQA) [8, 35], where outputs are informative answers\nto given questions. However, existing LFQA tasks either suffer\nfrom poor answer grounding ",
                    "Citation Text": "Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to Progress in Long-\nform Question Answering. In Proceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021 , Kristina Toutanova, Anna\nRumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard,\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for\nComputational Linguistics, 4940\u20134957. https://doi.org/10.18653/v1/2021.naacl-\nmain.393",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.06332",
                        "Citation Paper Title": "Title:Hurdles to Progress in Long-form Question Answering",
                        "Citation Paper Abstract": "Abstract:The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.",
                        "Citation Paper Authors": "Authors:Kalpesh Krishna, Aurko Roy, Mohit Iyyer"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ".\nAnother line of research aims to alleviate data scarcity by creating\nlarger QFS datasets [ 19,30]. For instance, Kulkarni et al . ",
                    "Citation Text": "Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020. Aqua-\nmuse: Automatically generating datasets for query-based multi-document sum-\nmarization. arXiv preprint arXiv:2010.12694 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12694",
                        "Citation Paper Title": "Title:AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization",
                        "Citation Paper Abstract": "Abstract:Summarization is the task of compressing source document(s) into coherent and succinct passages. This is a valuable tool to present users with concise and accurate sketch of the top ranked documents related to their queries. Query-based multi-document summarization (qMDS) addresses this pervasive need, but the research is severely limited due to lack of training and evaluation datasets as existing single-document and multi-document summarization datasets are inadequate in form and scale. We propose a scalable approach called AQuaMuSe to automatically mine qMDS examples from question answering datasets and large document corpora. Our approach is unique in the sense that it can general a dual dataset -- for extractive and abstractive summaries both. We publicly release a specific instance of an AQuaMuSe dataset with 5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl. Extensive evaluation of the dataset along with baseline summarization model experiments are provided.",
                        "Citation Paper Authors": "Authors:Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12509v4": {
            "Paper Title": "Deep Exploration for Recommendation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14226v2": {
            "Paper Title": "Sequential Recommendation with Graph Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11876v4": {
            "Paper Title": "Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior\n  Implicit Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "encodes multiple relational structures via exploring the cross-behavior\ncollaborative information as well as within-behavior collaborative signals. MB-GMN ",
                    "Citation Text": "Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo. 2021. Graph meta network for multi-behavior\nrecommendation. In Proceedings of the International ACM SIGIR Conference on Research & Development in Information\nRetrieval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.03969",
                        "Citation Paper Title": "Title:Graph Meta Network for Multi-Behavior Recommendation",
                        "Citation Paper Abstract": "Abstract:Modern recommender systems often embed users and items into low-dimensional latent representations, based on their observed interactions. In practical recommendation scenarios, users often exhibit various intents which drive them to interact with items with multiple behavior types (e.g., click, tag-as-favorite, purchase). However, the diversity of user behaviors is ignored in most of the existing approaches, which makes them difficult to capture heterogeneous relational structures across different types of interactive behaviors. Exploring multi-typed behavior patterns is of great importance to recommendation systems, yet is very challenging because of two aspects: i) The complex dependencies across different types of user-item interactions; ii) Diversity of such multi-behavior patterns may vary by users due to their personalized preference. To tackle the above challenges, we propose a Multi-Behavior recommendation framework with Graph Meta Network to incorporate the multi-behavior pattern modeling into a meta-learning paradigm. Our developed MB-GMN empowers the user-item interaction learning with the capability of uncovering type-dependent behavior representations, which automatically distills the behavior heterogeneity and interaction diversity for recommendations. Extensive experiments on three real-world datasets show the effectiveness of MB-GMN by significantly boosting the recommendation performance as compared to various state-of-the-art baselines. The source code is available athttps://github.com/akaxlh/MB-GMN.",
                        "Citation Paper Authors": "Authors:Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, Liefeng Bo"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "explores the relationships among various types of behaviors using the message passing module.\nHMG-CR ",
                    "Citation Text": "Haoran Yang, Hongxu Chen, Lin Li, S Yu Philip, and Guandong Xu. 2021. Hyper Meta-Path Contrastive Learning for\nMulti-Behavior Recommendation. In Proceedings of the International Conference on Data Mining .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.02859",
                        "Citation Paper Title": "Title:Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation",
                        "Citation Paper Abstract": "Abstract:User purchasing prediction with multi-behavior information remains a challenging problem for current recommendation systems. Various methods have been proposed to address it via leveraging the advantages of graph neural networks (GNNs) or multi-task learning. However, most existing works do not take the complex dependencies among different behaviors of users into consideration. They utilize simple and fixed schemes, like neighborhood information aggregation or mathematical calculation of vectors, to fuse the embeddings of different user behaviors to obtain a unified embedding to represent a user's behavioral patterns which will be used in downstream recommendation tasks. To tackle the challenge, in this paper, we first propose the concept of hyper meta-path to construct hyper meta-paths or hyper meta-graphs to explicitly illustrate the dependencies among different behaviors of a user. How to obtain a unified embedding for a user from hyper meta-paths and avoid the previously mentioned limitations simultaneously is critical. Thanks to the recent success of graph contrastive learning, we leverage it to learn embeddings of user behavior patterns adaptively instead of assigning a fixed scheme to understand the dependencies among different behaviors. A new graph contrastive learning based framework is proposed by coupling with hyper meta-paths, namely HMG-CR, which consistently and significantly outperforms all baselines in extensive comparison experiments.",
                        "Citation Paper Authors": "Authors:Haoran Yang, Hongxu Chen, Lin Li, Philip S. Yu, Guandong Xu"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "combines\nthe exploration of multi-behavior relationships with the meta-learning paradigm. GNMR ",
                    "Citation Text": "Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Mengyin Lu, and Liefeng Bo. 2021. Multi-Behavior Enhanced\nRecommendation with Cross-Interaction Collaborative Relation Modeling. In Proceedings of the IEEE Conference on\nData Engineering .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02307",
                        "Citation Paper Title": "Title:Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative Relation Modeling",
                        "Citation Paper Abstract": "Abstract:Many previous studies aim to augment collaborative filtering with deep neural network techniques, so as to achieve better recommendation performance. However, most existing deep learning-based recommender systems are designed for modeling singular type of user-item interaction behavior, which can hardly distill the heterogeneous relations between user and item. In practical recommendation scenarios, there exist multityped user behaviors, such as browse and purchase. Due to the overlook of user's multi-behavioral patterns over different items, existing recommendation methods are insufficient to capture heterogeneous collaborative signals from user multi-behavior data. Inspired by the strength of graph neural networks for structured data modeling, this work proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework which explicitly models the dependencies between different types of user-item interactions under a graph-based message passing architecture. GNMR devises a relation aggregation network to model interaction heterogeneity, and recursively performs embedding propagation between neighboring nodes over the user-item interaction graph. Experiments on real-world recommendation datasets show that our GNMR consistently outperforms state-of-the-art methods. The source code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Mengyin Lu, Liefeng Bo"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "builds upon a message-passing\narchitecture and designs an item-to-item embedding updating manner for exploring item-to-item\nsimilarity. MATN ",
                    "Citation Text": "Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, and Liefeng Bo. 2020. Multiplex behavioral relation learning\nfor recommendation via memory augmented transformer network. In Proceedings of the International ACM SIGIR\nConference on Research & Development in Information Retrieval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.04002",
                        "Citation Paper Title": "Title:Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network",
                        "Citation Paper Abstract": "Abstract:Capturing users' precise preferences is of great importance in various recommender systems (eg., e-commerce platforms), which is the basis of how to present personalized interesting product lists to individual users. In spite of significant progress has been made to consider relations between users and items, most of the existing recommendation techniques solely focus on singular type of user-item interactions. However, user-item interactive behavior is often exhibited with multi-type (e.g., page view, add-to-favorite and purchase) and inter-dependent in nature. The overlook of multiplex behavior relations can hardly recognize the multi-modal contextual signals across different types of interactions, which limit the feasibility of current recommendation methods. To tackle the above challenge, this work proposes a Memory-Augmented Transformer Networks (MATN), to enable the recommendation with multiplex behavioral relational information, and joint modeling of type-specific behavioral context and type-wise behavior inter-dependencies, in a fully automatic manner. In our MATN framework, we first develop a transformer-based multi-behavior relation encoder, to make the learned interaction representations be reflective of the cross-type behavior relations. Furthermore, a memory attention network is proposed to supercharge MATN capturing the contextual signals of different types of behavior into the category-specific latent embedding space. Finally, a cross-behavior aggregation component is introduced to promote the comprehensive collaboration across type-aware interaction behavior representations, and discriminate their inherent contributions in assisting recommendations. Extensive experiments on two benchmark datasets and a real-world e-commence user behavior data demonstrate significant improvements obtained by MATN over baselines. Codes are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, Liefeng Bo"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "substitutes the inner product with\na neural network capable of learning an arbitrary function from input, combining Generalized\nMatrix Factorization (GMF) and Multi-Layer Perceptron (MLP). On the basis of MF, ExpoMF ",
                    "Citation Text": "Dawen Liang, Laurent Charlin, James Mcinerney, and David M Blei. 2016. Modeling User Exposure in Recommendation.\nInProceedings of the Web Conference .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.07025",
                        "Citation Paper Title": "Title:Modeling User Exposure in Recommendation",
                        "Citation Paper Abstract": "Abstract:Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis, the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model, and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". Early works include eALS [ 9,12] which exploits auxiliary interactions with\nan improved negative sampler. Recently, multi-task learning frameworks have been widely adopted\nfor multi-behavior recommendation. For example, ESM2 ",
                    "Citation Text": "Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire space multi-task\nmodeling via post-click behavior decomposition for conversion rate prediction. In Proceedings of the International ACM\nSIGIR Conference on Research & Development in Information Retrieval . 2377\u20132386.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07099",
                        "Citation Paper Title": "Title:Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Recommender system, as an essential part of modern e-commerce, consists of two fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate (CVR) prediction. While CVR has a direct impact on the purchasing volume, its prediction is well-known challenging due to the Sample Selection Bias (SSB) and Data Sparsity (DS) issues. Although existing methods, typically built on the user sequential behavior path ``impression$\\to$click$\\to$purchase'', is effective for dealing with SSB issue, they still struggle to address the DS issue due to rare purchase training samples. Observing that users always take several purchase-related actions after clicking, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph ``impression$\\to$click$\\to$D(O)Action$\\to$purchase''. Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervised signals from D(O)Action, which will effectively address the SSB and DS issues together. To this end, we devise a novel deep recommendation model named Elaborated Entire Space Supervised Multi-task Model ($ESM^{2}$). According to the conditional probability rule defined on the graph, it employs multi-task learning to predict some decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR. Extensive experiments on both offline and online environments demonstrate the superiority of $ESM^{2}$ over state-of-the-art models. The source code and dataset will be released.",
                        "Citation Paper Authors": "Authors:Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, Keping Yang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "considers all unobserved interactions to be negative and leverages item popularity to weight them.\nMult-VAE ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative\nfiltering. In Proceedings of the Web Conference .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.15498v2": {
            "Paper Title": "Classification of Consumer Belief Statements From Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12887v4": {
            "Paper Title": "ICPE: An Item Cluster-Wise Pareto-Efficient Framework for Recommendation\n  Debiasing",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", we need a debiased\ntest set for a fair evaluation. To this end, we follow the\ndata preprocessing methods of previous approaches ",
                    "Citation Text": "T. Wei, F. Feng, J. Chen, Z. Wu, J. Yi, and X. He, \u201cModel-\nagnostic counterfactual reasoning for eliminating popularity bias\nin recommender system,\u201d in Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining , 2021, pp. 1791\u2013\n1800.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.15363",
                        "Citation Paper Title": "Title:Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System",
                        "Citation Paper Abstract": "Abstract:The general aim of the recommender system is to provide personalized suggestions to users, which is opposed to suggesting popular items. However, the normal training paradigm, i.e., fitting a recommender model to recover the user behavior data with pointwise or pairwise loss, makes the model biased towards popular items. This results in the terrible Matthew effect, making popular items be more frequently recommended and become even more popular. Existing work addresses this issue with Inverse Propensity Weighting (IPW), which decreases the impact of popular items on the training and increases the impact of long-tail items. Although theoretically sound, IPW methods are highly sensitive to the weighting strategy, which is notoriously difficult to tune. In this work, we explore the popularity bias issue from a novel and fundamental perspective -- cause-effect. We identify that popularity bias lies in the direct effect from the item node to the ranking score, such that an item's intrinsic property is the cause of mistakenly assigning it a higher ranking score. To eliminate popularity bias, it is essential to answer the counterfactual question that what the ranking score would be if the model only uses item property. To this end, we formulate a causal graph to describe the important cause-effect relations in the recommendation process. During training, we perform multi-task learning to achieve the contribution of each cause; during testing, we perform counterfactual inference to remove the effect of item popularity. Remarkably, our solution amends the learning process of recommendation which is agnostic to a wide range of models -- it can be easily implemented in existing methods. We demonstrate it on Matrix Factorization (MF) and LightGCN [20]. Experiments on five real-world datasets demonstrate the effectiveness of our method.",
                        "Citation Paper Authors": "Authors:Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, Xiangnan He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2003.10699v2": {
            "Paper Title": "Utilizing Human Memory Processes to Model Genre Preferences for\n  Personalized Music Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04106v2": {
            "Paper Title": "Ensemble Knowledge Distillation for CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "propose an adversarial distillation approach to learn\nfrom external knowledge base for collaborative filtering. Xu et\nal. ",
                    "Citation Text": "Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Hanx-\niao Sun, and Wenwu Ou. 2019. Privileged Features Distillation for E-Commerce\nRecommendations. CoRR abs/1907.05171 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05171",
                        "Citation Paper Title": "Title:Privileged Features Distillation at Taobao Recommendations",
                        "Citation Paper Abstract": "Abstract:Features play an important role in the prediction tasks of e-commerce recommendations. To guarantee the consistency of off-line training and on-line serving, we usually utilize the same features that are both available. However, the consistency in turn neglects some discriminative features. For example, when estimating the conversion rate (CVR), i.e., the probability that a user would purchase the item if she clicked it, features like dwell time on the item detailed page are informative. However, CVR prediction should be conducted for on-line ranking before the click happens. Thus we cannot get such post-event features during serving.\nWe define the features that are discriminative but only available during training as the privileged features. Inspired by the distillation techniques which bridge the gap between training and inference, in this work, we propose privileged features distillation (PFD). We train two models, i.e., a student model that is the same as the original one and a teacher model that additionally utilizes the privileged features. Knowledge distilled from the more accurate teacher is transferred to the student to improve its accuracy. During serving, only the student part is extracted and it relies on no privileged features. We conduct experiments on two fundamental prediction tasks at Taobao recommendations, i.e., click-through rate (CTR) at coarse-grained ranking and CVR at fine-grained ranking. By distilling the interacted features that are prohibited during serving for CTR and the post-event features for CVR, we achieve significant improvements over their strong baselines. During the on-line A/B tests, the click metric is improved by +5.0% in the CTR task. And the conversion metric is improved by +2.3% in the CVR task. Besides, by addressing several issues of training PFD, we obtain comparable training speed as the baselines without any distillation.",
                        "Citation Paper Authors": "Authors:Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu, Hanxiao Sun, Wenwu Ou"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ") to learn high-order\nfeature interactions. Some other studies focus on modeling evolu-\ntionary user interests in CTR prediction (e.g., DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,\nYanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for\nClick-Through Rate Prediction. In Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (KDD) . 1059\u20131068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.08316v4": {
            "Paper Title": "A Trio Neural Model for Dynamic Entity Relatedness Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.07890v3": {
            "Paper Title": "Multiple Models for Recommending Temporal Aspects of Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06367v2": {
            "Paper Title": "Multi-Objective Recommendations: A Tutorial",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.09646v2": {
            "Paper Title": "A Systematic Review of Automated Query Reformulations in Source Code\n  Search",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "Query reformulation for feature location using Genetic algorithm using\nexpansion, reduction, and selection as a mutation operation, and textual\nsimilarity as a fitness function\nS70 Da Silva et al. ",
                    "Citation Text": "R. F. G. Da Silva, C. K. Roy, M. M. Rahman, K. Schneider, K. Paixo, and M. Maia. 2019. Recom-\nmending Comprehensive Solutions for Programming Tasks by Mining Crowd Knowledge. In\nProc. ICPC . 358\u2013368.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07662",
                        "Citation Paper Title": "Title:Recommending Comprehensive Solutions for Programming Tasks by Mining Crowd Knowledge",
                        "Citation Paper Abstract": "Abstract:Developers often search for relevant code examples on the web for their programming tasks. Unfortunately, they face two major problems. First, the search is impaired due to a lexical gap between their query (task description) and the information associated with the solution. Second, the retrieved solution may not be comprehensive, i.e., the code segment might miss a succinct explanation. These problems make the developers browse dozens of documents in order to synthesize an appropriate solution. To address these two problems, we propose CROKAGE (Crowd Knowledge Answer Generator), a tool that takes the description of a programming task (the query) and provides a comprehensive solution for the task. Our solutions contain not only relevant code examples but also their succinct explanations. Our proposed approach expands the task description with relevant API classes from Stack Overflow Q&A threads and then mitigates the lexical gap problems. Furthermore, we perform natural language processing on the top quality answers and then return such programming solutions containing code examples and code explanations unlike earlier studies. We evaluate our approach using 48 programming queries and show that it outperforms six baselines including the state-of-art by a statistically significant margin. Furthermore, our evaluation with 29 developers using 24 tasks (queries) confirms the superiority of CROKAGE over the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality (code + explanation).",
                        "Citation Paper Authors": "Authors:Rodrigo F. G. Silva, Chanchal K. Roy, Mohammad Masudur Rahman, Kevin A. Schneider, Klerisson Paixao, Marcelo de Almeida Maia"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "Query reformulation with API method signatures using sentence parse\ntree extraction, uni-gram, and probabilistic context free grammar (PCFG)\nS47 Kimmig et al. ",
                    "Citation Text": "M. Kimmig, M. Monperrus, and M. Mezini. 2011. Querying source code with natural language.\nInProc. ASE . 376\u2013379.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.6361",
                        "Citation Paper Title": "Title:Querying Source Code with Natural Language",
                        "Citation Paper Abstract": "Abstract:One common task of developing or maintaining software is searching the source code for information like specific method calls or write accesses to certain fields. This kind of information is required to correctly implement new features and to solve bugs. This paper presents an approach for querying source code with natural language.",
                        "Citation Paper Authors": "Authors:Markus Kimmig, Martin Monperrus (INRIA Lille - Nord Europe, LIFL), Mira Mezini"
                    }
                },
                {
                    "Sentence ID": 148,
                    "Sentence": ", construct a term-term\nco-occurrence matrix from the past queries, and then suggest frequently co-occurred terms from the\nmatrix to expand a given query. Raghothaman et al . ",
                    "Citation Text": "M. Raghothaman, Y. Wei, and Y. Hamadi. 2016. SWIM: Synthesizing What I Mean: Code\nSearch and Idiomatic Snippet Synthesis. In Proc. ICSE . 357\u2013367.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.08497",
                        "Citation Paper Title": "Title:SWIM: Synthesizing What I Mean",
                        "Citation Paper Abstract": "Abstract:Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as \"generate md5 hash code\".\nWe translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce \\emph{structured call sequences} to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis.\nWe evaluated SWIM with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.",
                        "Citation Paper Authors": "Authors:Mukund Raghothaman, Yi Wei, Youssef Hamadi"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "Fault localization No 1977\u20132014 331 No Underlying technology,\nsubject programs,\nevaluation metrics, and\ncritical aspects\nLiu et al. ",
                    "Citation Text": "C. Liu, X. Xia, D. Lo, C. Gao, X. Yang, and J. C. Grundy. 2020. Opportunities and Challenges\nin Code Search Tools. CoRR abs/2011.02297 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.02297",
                        "Citation Paper Title": "Title:Opportunities and Challenges in Code Search Tools",
                        "Citation Paper Abstract": "Abstract:Code search is a core software engineering task. Effective code search tools can help developers substantially improve their software development efficiency and effectiveness. In recent years, many code search studies have leveraged different techniques, such as deep learning and information retrieval approaches, to retrieve expected code from a large-scale codebase. However, there is a lack of a comprehensive comparative summary of existing code search approaches. To understand the research trends in existing code search studies, we systematically reviewed 81 relevant studies. We investigated the publication trends of code search studies, analyzed key components, such as codebase, query, and modeling technique used to build code search tools, and classified existing tools into focusing on supporting seven different search tasks. Based on our findings, we identified a set of outstanding challenges in existing studies and a research roadmap for future code search research.",
                        "Citation Paper Authors": "Authors:Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, John Grundy"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "learn the word\nembeddings from 1.40 million Q&A threads of Stack Overflow using Skip-gram algorithm ",
                    "Citation Text": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. 2016. Enriching Word Vectors with\nSubword Information. arXiv preprint arXiv:1607.04606 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.08079v4": {
            "Paper Title": "Context-NER : Contextual Phrase Generation at Scale",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "build on the previously mentioned\nframework and uses a joint learning framework and a flexible global loss function to capture the\ninteractions of the entities and their relationships. McCann et al. ",
                    "Citation Text": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The Natural\nLanguage Decathlon: Multitask Learning as Question Answering . 2018. arXiv: 1806.08730\n[cs.CL] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08730",
                        "Citation Paper Title": "Title:The Natural Language Decathlon: Multitask Learning as Question Answering",
                        "Citation Paper Abstract": "Abstract:Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.",
                        "Citation Paper Authors": "Authors:Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "used a zero-shot approach to train MRC\nmodel on templatized questions and inferenced it on unseen relations. Li et al. ",
                    "Citation Text": "Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and\nJiwei Li. \u201cEntity-relation extraction as multi-turn question answering\u201d. In: arXiv preprint\narXiv:1905.05529 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.05529",
                        "Citation Paper Title": "Title:Entity-Relation Extraction as Multi-Turn Question Answering",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and relations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.4 (+1.0), 60.2 (+0.6) and 68.9 (+2.1), respectively. Additionally, we construct a newly developed dataset RESUME in Chinese, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",
                        "Citation Paper Authors": "Authors:Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, Jiwei Li"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "introduced a schemaless approach for extracting facts from text, focusing on\nrelation extraction using OpenIE. However, this approach assumes relations between two entities,\nwhich poses challenges for financial data. Levy et al. ",
                    "Citation Text": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. \u201cZero-shot relation extraction\nvia reading comprehension\u201d. In: arXiv preprint arXiv:1706.04115 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04115",
                        "Citation Paper Title": "Title:Zero-Shot Relation Extraction via Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.",
                        "Citation Paper Authors": "Authors:Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2002.11844v4": {
            "Paper Title": "The hypergeometric test performs comparably to TF-IDF on standard text\n  analysis tasks",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": ". We ana-\nlyzed a preprocessed version of the dataset, consisting of a subset of some\n18,846 posts, that is included in the Python library sklearn 1.2.2 ",
                    "Citation Text": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,\nGrisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vander-\nplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay,\nE.: Scikit-learn: Machine learning in Python. Journal of Machine Learning\nResearch 12, 2825\u20132830 (2011)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ", consti-\ntutes the state-of-the-art when it comes to generating features from text ",
                    "Citation Text": "von der Mosel, J., Trautsch, A., Herbold, S.: On the validity of pre-trained\ntransformers for natural language processing in the software engineering\ndomain. IEEE Transactions on Software Engineering 49(4), 1487\u20131507\n(2023). https://doi.org/10.1109/TSE.2022.3178469",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.04738",
                        "Citation Paper Title": "Title:On the validity of pre-trained transformers for natural language processing in the software engineering domain",
                        "Citation Paper Abstract": "Abstract:Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",
                        "Citation Paper Authors": "Authors:Julian von der Mosel, Alexander Trautsch, Steffen Herbold"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.05446v3": {
            "Paper Title": "Efficient-FedRec: Efficient Federated Learning Framework for\n  Privacy-Preserving News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04726v3": {
            "Paper Title": "AutoTriggER: Label-Efficient and Robust Named Entity Recognition with\n  Auxiliary Trigger Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03207v2": {
            "Paper Title": "Learning to Rank under Multinomial Logit Choice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02151v3": {
            "Paper Title": "Vertex Nomination in Richly Attributed Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00309v2": {
            "Paper Title": "Sparse Bayesian Learning Approach for Discrete Signal Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11073v5": {
            "Paper Title": "RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender\n  System",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "first popularizes semi-simulated RS datasets and simulates envi-\nronments using adversarial learning. Virtual-Taobao ",
                    "Citation Text": "J. Shi, Y. Yu, Q. Da, S.-Y. Chen, and A. Zeng. Virtual-taobao: Virtualizing real-world\nonline retail environment for reinforcement learning. In AAAI , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10000",
                        "Citation Paper Title": "Title:Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Applying reinforcement learning in physical-world tasks is extremely challenging. It is commonly infeasible to sample a large number of trials, as required by current reinforcement learning methods, in a physical environment. This paper reports our project on using reinforcement learning for better commodity search in Taobao, one of the largest online retail platforms and meanwhile a physical environment with a high sampling cost. Instead of training reinforcement learning in Taobao directly, we present our approach: first we build Virtual Taobao, a simulator learned from historical customer behavior data through the proposed GAN-SD (GAN for Simulating Distributions) and MAIL (multi-agent adversarial imitation learning), and then we train policies in Virtual Taobao with no physical costs in which ANC (Action Norm Constraint) strategy is proposed to reduce over-fitting. In experiments, Virtual Taobao is trained from hundreds of millions of customers' records, and its properties are compared with the real environment. The results disclose that Virtual Taobao faithfully recovers important properties of the real environment. We also show that the policies trained in Virtual Taobao can have significantly superior online performance to the traditional supervised approaches. We hope our work could shed some light on reinforcement learning applications in complex physical environments.",
                        "Citation Paper Authors": "Authors:Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, An-Xiang Zeng"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "first apply deep RL to recommenda-\ntion problems and inspire a series of subsequent works. SlateQ ",
                    "Citation Text": "E. Ie, V. Jain, J. Wang, S. Narvekar, R. Agarwal, R. Wu, H.-T. Cheng, T. Chandra,\nand C. Boutilier. Slateq: A tractable decomposition for reinforcement learning\nwith recommendation sets. In IJCAI , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12767",
                        "Citation Paper Title": "Title:Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology",
                        "Citation Paper Abstract": "Abstract:Most practical recommender systems focus on estimating immediate user engagement without considering the long-term effects of recommendations on user behavior. Reinforcement learning (RL) methods offer the potential to optimize recommendations for long-term user engagement. However, since users are often presented with slates of multiple items - which may have interacting effects on user choice - methods are required to deal with the combinatorics of the RL action space. In this work, we address the challenge of making slate-based recommendations to optimize long-term value using RL. Our contributions are three-fold. (i) We develop SLATEQ, a decomposition of value-based temporal-difference and Q-learning that renders RL tractable with slates. Under mild assumptions on user choice behavior, we show that the long-term value (LTV) of a slate can be decomposed into a tractable function of its component item-wise LTVs. (ii) We outline a methodology that leverages existing myopic learning-based recommenders to quickly develop a recommender that handles LTV. (iii) We demonstrate our methods in simulation, and validate the scalability of decomposed TD-learning using SLATEQ in live experiments on YouTube.",
                        "Citation Paper Authors": "Authors:Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Morgane Lustman, Vince Gatto, Paul Covington, Jim McFadden, Tushar Chandra, Craig Boutilier"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ",\nand continuous-action baselines combining policy gradients with\na K-NN search, including PG, A2C, DDPG ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and\nD. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". When only sorting a few\ncandidate items, we can choose to create a variant of DQN called\nParametric-Action DQN ",
                    "Citation Text": "J. Gauci, E. Conti, Y. Liang, K. Virochsiri, Y. R. He, Z. Kaden, V. Narayanan, X. Ye,\nand S. Fujimoto. Horizon: Facebook\u2019s open source applied reinforcement learning\nplatform. ArXiv , abs/1811.00260, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00260",
                        "Citation Paper Title": "Title:Horizon: Facebook's Open Source Applied Reinforcement Learning Platform",
                        "Citation Paper Abstract": "Abstract:In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.",
                        "Citation Paper Authors": "Authors:Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden, Vivek Narayanan, Xiaohui Ye, Zhengxing Chen, Scott Fujimoto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.09439v2": {
            "Paper Title": "Vec2GC -- A Graph Based Clustering Method for Text Representations",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "represented text doc-\numents as hierarchical document-graphs to extract frequent sub-\ngraphs for generating sense-based document clusters. Wang et. al. ",
                    "Citation Text": "Lili Wang, Chongyang Gao, Jason Wei, Weicheng Ma, Ruibo Liu, and Soroush\nVosoughi. 2020. An Empirical Survey of Unsupervised Text Representation\nMethods on Twitter Data. In Proceedings of the Sixth Workshop on Noisy User-\ngenerated Text (W-NUT 2020) . Association for Computational Linguistics, Online,\n209\u2013214. https://doi.org/10.18653/v1/2020.wnut-1.27",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03468",
                        "Citation Paper Title": "Title:An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data",
                        "Citation Paper Abstract": "Abstract:The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, however, unclear whether these improvements translate to noisy user-generated text, such as tweets. In this paper, we present an experimental survey of a wide range of well-known text representation techniques for the task of text clustering on noisy Twitter data. Our results indicate that the more advanced models do not necessarily work best on tweets and that more exploration in this area is needed.",
                        "Citation Paper Authors": "Authors:Lili Wang, Chongyang Gao, Jason Wei, Weicheng Ma, Ruibo Liu, Soroush Vosoughi"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". It computes dense vector representations for docu-\nments, such that similar document embeddings are close in vector\nspace using pretrained language models on transformer networks\nlike BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.13747v6": {
            "Paper Title": "MOEF: Modeling Occasion Evolution in Frequency Domain for\n  Promotion-Aware Click-Through Rate Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06400v2": {
            "Paper Title": "Improving Query Representations for Dense Retrieval with Pseudo\n  Relevance Feedback: A Reproducibility Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14129v6": {
            "Paper Title": "COMET: Convolutional Dimension Interaction for Collaborative Filtering",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "developed a recommendation model called HOSVD that utilizes tensor factorization for user-tag-\nitem triplet data. More recently, Yu et al. proposed a tensor factorization model called DCFA ",
                    "Citation Text": "Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin. 2018. Aesthetic-based clothing recommendation. In Proceedings of the\n27th International World Wide Web Conference . 649\u2013658.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.05822",
                        "Citation Paper Title": "Title:Aesthetic-based Clothing Recommendation",
                        "Citation Paper Abstract": "Abstract:Recently, product images have gained increasing attention in clothing recommendation since the visual appearance of clothing products has a significant impact on consumers' decision. Most existing methods rely on conventional features to represent an image, such as the visual features extracted by convolutional neural networks (CNN features) and the scale-invariant feature transform algorithm (SIFT features), color histograms, and so on. Nevertheless, one important type of features, the \\emph{aesthetic features}, is seldom considered. It plays a vital role in clothing recommendation since a users' decision depends largely on whether the clothing is in line with her aesthetics, however the conventional image features cannot portray this directly. To bridge this gap, we propose to introduce the aesthetic information, which is highly relevant with user preference, into clothing recommender systems. To achieve this, we first present the aesthetic features extracted by a pre-trained neural network, which is a brain-inspired deep structure trained for the aesthetic assessment task. Considering that the aesthetic preference varies significantly from user to user and by time, we then propose a new tensor factorization model to incorporate the aesthetic features in a personalized manner. We conduct extensive experiments on real-world datasets, which demonstrate that our approach can capture the aesthetic preference of users and significantly outperform several state-of-the-art recommendation methods.",
                        "Citation Paper Authors": "Authors:Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, Zheng Qin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.14233v2": {
            "Paper Title": "A Next Basket Recommendation Reality Check",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11248v2": {
            "Paper Title": "Learning to Recommend Using Non-Uniform Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10086v4": {
            "Paper Title": "Meta Matrix Factorization for Federated Rating Predictions",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "propose hypernetworks, which employ a\nnetwork to generate the weights of another network. Krueger et al . ",
                    "Citation Text": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre La-\ncoste, and Aaron Courville. 2017. Bayesian Hypernetworks. arXiv preprint\narXiv:1710.04759 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.04759",
                        "Citation Paper Title": "Title:Bayesian Hypernetworks",
                        "Citation Paper Abstract": "Abstract:We study Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork $\\h$ is a neural network which learns to transform a simple noise distribution, $p(\\vec\\epsilon) = \\N(\\vec 0,\\mat I)$, to a distribution $q(\\pp) := q(h(\\vec\\epsilon))$ over the parameters $\\pp$ of another neural network (the \"primary network\")\\@. We train $q$ with variational inference, using an invertible $\\h$ to enable efficient estimation of the variational lower bound on the posterior $p(\\pp | \\D)$ via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of~$q(\\pp)$. In practice, Bayesian hypernets can provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.",
                        "Citation Paper Authors": "Authors:David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "introduce a model to predict the\nparameters of a pupil network from a single exemplar for one-shot\nlearning. Ha et al . ",
                    "Citation Text": "David Ha, Andrew Dai, and Quoc V Le. 2016. Hypernetworks. arXiv preprint\narXiv:1609.09106 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09106",
                        "Citation Paper Title": "Title:HyperNetworks",
                        "Citation Paper Abstract": "Abstract:This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.",
                        "Citation Paper Authors": "Authors:David Ha, Andrew Dai, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "propose a network to dynamically generate filters\nforCNN s. Bertinetto et al . ",
                    "Citation Text": "Luca Bertinetto, Jo\u00e3o F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi.\n2016. Learning Feed-forward One-shot Learners. In NeurIPS . 523\u2013531.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05233",
                        "Citation Paper Title": "Title:Learning feed-forward one-shot learners",
                        "Citation Paper Abstract": "Abstract:One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.",
                        "Citation Paper Authors": "Authors:Luca Bertinetto, Jo\u00e3o F. Henriques, Jack Valmadre, Philip H. S. Torr, Andrea Vedaldi"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "present the deep matrix\nfactorization ( DMF ) which enhances NCF by considering both ex-\nplicit and implicit feedback. He et al . ",
                    "Citation Text": "Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang, and Tat Seng\nChua. 2018. Outer Product-based Neural Collaborative Filtering. In IJCAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.03912",
                        "Citation Paper Title": "Title:Outer Product-based Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In this work, we contribute a new multi-layer neural network architecture named ONCF to perform collaborative filtering. The idea is to use an outer product to explicitly model the pairwise correlations between the dimensions of the embedding space. In contrast to existing neural recommender models that combine user embedding and item embedding via a simple concatenation or element-wise product, our proposal of using outer product above the embedding layer results in a two-dimensional interaction map that is more expressive and semantically plausible. Above the interaction map obtained by outer product, we propose to employ a convolutional neural network to learn high-order correlations among embedding dimensions. Extensive experiments on two public implicit feedback data demonstrate the effectiveness of our proposed ONCF framework, in particular, the positive effect of using outer product to model the correlations between embedding dimensions in the low level of multi-layer neural recommender model. The experiment codes are available at: this https URL",
                        "Citation Paper Authors": "Authors:Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "enhance AutoRec by incorporating side information into a denois-\ning autoencoder. He et al . ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In WWW . ACM, 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "into\nthe realm of recommendation, which provides a way to realize\nfederated recommender systems. Chen et al . ",
                    "Citation Text": "Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. 2018. Federated Meta-\nlearning for Recommendation. arXiv preprint arXiv:1802.07876 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07876",
                        "Citation Paper Title": "Title:Federated Meta-Learning with Fast Convergence and Efficient Communication",
                        "Citation Paper Abstract": "Abstract:Statistical and systematic challenges in collaboratively training machine learning models across distributed networks of mobile devices have been the bottlenecks in the real-world application of federated learning. In this work, we show that meta-learning is a natural choice to handle these issues, and propose a federated meta-learning framework FedMeta, where a parameterized algorithm (or meta-learner) is shared, instead of a global model in previous approaches. We conduct an extensive empirical evaluation on LEAF datasets and a real-world production dataset, and demonstrate that FedMeta achieves a reduction in required communication cost by 2.82-4.33 times with faster convergence, and an increase in accuracy by 3.23%-14.84% as compared to Federated Averaging (FedAvg) which is a leading optimization algorithm in federated learning. Moreover, FedMeta preserves user privacy since only the parameterized algorithm is transmitted between mobile devices and central servers, and no raw data is collected onto the servers.",
                        "Citation Paper Authors": "Authors:Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "further present a\nuser-based random walk approach with CFacross devices to pre-\ndict ratings. Wang et al . ",
                    "Citation Text": "Zhangyang Wang, Xianming Liu, Shiyu Chang, Jiayu Zhou, Guo-Jun Qi, and\nThomas S. Huang. 2015. Decentralized Recommender Systems. arXiv preprint\narXiv:1503.01647 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.01647",
                        "Citation Paper Title": "Title:Decentralized Recommender Systems",
                        "Citation Paper Abstract": "Abstract:This paper proposes a decentralized recommender system by formulating the popular collaborative filleting (CF) model into a decentralized matrix completion form over a set of users. In such a way, data storages and computations are fully distributed. Each user could exchange limited information with its local neighborhood, and thus it avoids the centralized fusion. Advantages of the proposed system include a protection on user privacy, as well as better scalability and robustness. We compare our proposed algorithm with several state-of-the-art algorithms on the FlickerUserFavor dataset, and demonstrate that the decentralized algorithm can gain a competitive performance to others.",
                        "Citation Paper Authors": "Authors:Zhangyang Wang, Xianming Liu, Shiyu Chang, Jiayu Zhou, Guo-Jun Qi, Thomas S. Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12769v5": {
            "Paper Title": "Heterogeneous Treatment Effect Estimation using machine learning for\n  Healthcare application: tutorial and benchmark",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "Welcome to econml\u2019 s documentation! \u2014 econml 0.12.0b5 documentation, (n.d.). \nhttps://econml.azurewebsites.net  (accessed August 4, 2021). ",
                    "Citation Text": "S. Athey, S. Wager, Estimating Treatment Effects with Causal Forests: An Application, \nObservational Studies. 5 (2019) 37 \u201351.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07409",
                        "Citation Paper Title": "Title:Estimating Treatment Effects with Causal Forests: An Application",
                        "Citation Paper Abstract": "Abstract:We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and consider resulting practical and conceptual challenges. In particular, we discuss how causal forests use estimated propensity scores to be more robust to confounding, and how they handle data with clustered errors.",
                        "Citation Paper Authors": "Authors:Susan Athey, Stefan Wager"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "S. Athey, G. Imbens, Recursive partitioning for heterogeneous causal effects, Proc. Natl. Acad. Sci. \nU. S. A. 113 (2016) 7353 \u20137360. ",
                    "Citation Text": "S. Wager, S. Athey, Estimation and Inference of Heterogeneous Treatment Effects using Random \nForests, J. Am. Stat. Assoc. 113 (2018) 1228 \u20131242.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.04342",
                        "Citation Paper Title": "Title:Estimation and Inference of Heterogeneous Treatment Effects using Random Forests",
                        "Citation Paper Abstract": "Abstract:Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.",
                        "Citation Paper Authors": "Authors:Stefan Wager, Susan Athey"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "F. Johansson, U. Shalit, D. Sontag, Learning Representations for Counterfactual Inference, in: \nInternational Conference on Machine Learning, PMLR, 2016: pp. 3020 \u20133029. ",
                    "Citation Text": "S. Athey, G. Imbens, Recursive partitioning for heterogeneous causal effects, Proc. Natl. Acad. Sci. \nU. S. A. 113 (2016) 7353 \u20137360.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.01132",
                        "Citation Paper Title": "Title:Recursive Partitioning for Heterogeneous Causal Effects",
                        "Citation Paper Abstract": "Abstract:In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the \"ground truth\" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.",
                        "Citation Paper Authors": "Authors:Susan Athey, Guido Imbens"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "C. Shi, D.M. Blei, V. Veitch, Adapting Neural Networks for the Estimation of Treatment Effec ts, \narXiv [stat.ML]. (2019). http://arxiv.org/abs/1906.02120 . ",
                    "Citation Text": "F. Johansson, U. Shalit, D. Sontag, Learning Representations for Counterfactual Inference, in: \nInternational Conference on Machine Learning, PMLR, 2016: pp. 3020 \u20133029.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03661",
                        "Citation Paper Title": "Title:Learning Representations for Counterfactual Inference",
                        "Citation Paper Abstract": "Abstract:Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:Fredrik D. Johansson, Uri Shalit, David Sontag"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "I. Bica, J. Jordon, M. van der Schaar, Estimatin g the Effects of Continuous -valued Interventions \nusing Generative Adversarial Networks, (2020). http://arxiv.org/abs/2002.12326  (accessed July 21, \n2021). ",
                    "Citation Text": "C. Shi, D.M. Blei, V. Veitch, Adapting Neural Networks for the Estimation of Treatment Effec ts, \narXiv [stat.ML]. (2019). http://arxiv.org/abs/1906.02120 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02120",
                        "Citation Paper Title": "Title:Adapting Neural Networks for the Estimation of Treatment Effects",
                        "Citation Paper Abstract": "Abstract:This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at this http URL.",
                        "Citation Paper Authors": "Authors:Claudia Shi, David M. Blei, Victor Veitch"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W.K. Newey, V. Chernozhukov, Double machine \nlearning for treatment and causal parameters, (2016). https://doi.org/ 10.1920/wp.cem.2016.4916. ",
                    "Citation Text": "F.D. Johansson, U. Shalit, D. Sontag, Learning Representations for Counterfactua l Inference, (2016). \nhttp://arxiv.org/abs/1605.03661  (accessed July 21, 2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03661",
                        "Citation Paper Title": "Title:Learning Representations for Counterfactual Inference",
                        "Citation Paper Abstract": "Abstract:Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:Fredrik D. Johansson, Uri Shalit, David Sontag"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "A. Curth, M. van der Schaar, Nonparametric Estimation of Heterogeneous Treatment Effects: From  \nTheory to Learning Algorithms, in: A. Banerjee, K. Fukumizu (Eds.), Proceedings of The 24th \nInternational Conference on Artificial Intelligence and Statistics, PMLR, 2021: pp. 1810 \u20131818. ",
                    "Citation Text": "A. Curth, M. van der Schaar, On Inductive Biases for Heterogeneous Treatment Effect Estimation, \narXiv [stat.ML]. (2021). http://arxiv.org/abs/2106.03765 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03765",
                        "Citation Paper Title": "Title:On Inductive Biases for Heterogeneous Treatment Effect Estimation",
                        "Citation Paper Abstract": "Abstract:We investigate how to exploit structural similarities of an individual's potential outcomes (POs) under different treatments to obtain better estimates of conditional average treatment effects in finite samples. Especially when it is unknown whether a treatment has an effect at all, it is natural to hypothesize that the POs are similar - yet, some existing strategies for treatment effect estimation employ regularization schemes that implicitly encourage heterogeneity even when it does not exist and fail to fully make use of shared structure. In this paper, we investigate and compare three end-to-end learning strategies to overcome this problem - based on regularization, reparametrization and a flexible multi-task architecture - each encoding inductive bias favoring shared behavior across POs. To build understanding of their relative strengths, we implement all strategies using neural networks and conduct a wide range of semi-synthetic experiments. We observe that all three approaches can lead to substantial improvements upon numerous baselines and gain insight into performance differences across various experimental settings.",
                        "Citation Paper Authors": "Authors:Alicia Curth, Mihaela van der Schaar"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "A. Curth, D. Svensson, J. Weatherall, M. van der Schaar, Really Doing Great at Estimating CATE? \nA Critical Loo k at ML Benchmarking Practices in Treatment Effect Estimation, (2021). \nhttps://openreview.net/pdf?id=FQLzQqGEAH  (accessed December 1, 2021). ",
                    "Citation Text": "A. Curth, M. van der Schaar, Nonparametric Estimation of Heterogeneous Treatment Effects: From  \nTheory to Learning Algorithms, in: A. Banerjee, K. Fukumizu (Eds.), Proceedings of The 24th \nInternational Conference on Artificial Intelligence and Statistics, PMLR, 2021: pp. 1810 \u20131818.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.10943",
                        "Citation Paper Title": "Title:Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes.",
                        "Citation Paper Authors": "Authors:Alicia Curth, Mihaela van der Schaar"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "V. Che rnozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, J. Robins, \nDouble/Debiased Machine Learning for Treatment and Causal Parameters, (2016). \nhttp://arxiv .org/abs/1608.00060  (accessed October 1, 2021). ",
                    "Citation Text": "U. Shalit, F.D. Johansson,  D. Sontag, Estimating individual treatment effect: generalization bounds \nand algorithms, (2016). http://arxiv.org/abs/1606.03976  (accessed July 21, 2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03976",
                        "Citation Paper Title": "Title:Estimating individual treatment effect: generalization bounds and algorithms",
                        "Citation Paper Abstract": "Abstract:There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Uri Shalit, Fredrik D. Johansson, David Sontag"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.03104v2": {
            "Paper Title": "HTMOT : Hierarchical Topic Modelling Over Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.05571v6": {
            "Paper Title": "TINKER: A framework for Open source Cyberthreat Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12843v3": {
            "Paper Title": "A Survey of Graph Neural Networks for Recommender Systems: Challenges,\n  Methods, and Directions",
            "Sentences": [
                {
                    "Sentence ID": 173,
                    "Sentence": "conducts GAT on\na directed session graph to assign different weights to different items. SHARE ",
                    "Citation Text": "Jianling Wang, Kaize Ding, Ziwei Zhu, and James Caverlee. 2021. Session-based Recommendation with Hypergraph\nAttention Networks. In Proceedings of the 2021 SIAM International Conference on Data Mining . SIAM, 82\u201390.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.14266",
                        "Citation Paper Title": "Title:Session-based Recommendation with Hypergraph Attention Networks",
                        "Citation Paper Abstract": "Abstract:Session-based recommender systems aim to improve recommendations in short-term sessions that can be found across many platforms. A critical challenge is to accurately model user intent with only limited evidence in these short sessions. For example, is a flower bouquet being viewed meant as part of a wedding purchase or for home decoration? Such different perspectives greatly impact what should be recommended next. Hence, this paper proposes a novel session-based recommendation system empowered by hypergraph attention networks. Three unique properties of the proposed approach are: (i) it constructs a hypergraph for each session to model the item correlations defined by various contextual windows in the session simultaneously, to uncover item meanings; (ii) it is equipped with hypergraph attention layers to generate item embeddings by flexibly aggregating the contextual information from correlated items in the session; and (iii) it aggregates the dynamic item representations for each session to infer the general purpose and current need, which is decoded to infer the next interesting item in the session. Through experiments on three benchmark datasets, we find the proposed model is effective in generating informative dynamic item embeddings and providing more accurate recommendations compared to the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Jianling Wang, Kaize Ding, Ziwei Zhu, James Caverlee"
                    }
                },
                {
                    "Sentence ID": 177,
                    "Sentence": "perform GAT on both the session graph and global graph to\ncapture the local and global information, respectively. MGNN-SPred ",
                    "Citation Text": "Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, and Hongyuan Zha. 2020. Beyond clicks: Modeling\nmulti-relational item graph for session-based target behavior prediction. In Proceedings of The Web Conference 2020 .\n3056\u20133062.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.07993",
                        "Citation Paper Title": "Title:Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based Target Behavior Prediction",
                        "Citation Paper Abstract": "Abstract:Session-based target behavior prediction aims to predict the next item to be interacted with specific behavior types (e.g., clicking). Although existing methods for session-based behavior prediction leverage powerful representation learning approaches to encode items' sequential relevance in a low-dimensional space, they suffer from several limitations. Firstly, they focus on only utilizing the same type of user behavior for prediction, but ignore the potential of taking other behavior data as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally in one behavior sequence, and they lack a principled way to globally encode these relations more effectively. To overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. Specifically, we build a Multi-Relational Item Graph (MRIG) based on all behavior sequences from all sessions, involving target and auxiliary behavior types. Based on MRIG, MGNN-SPred learns global item-to-item relations and further obtains user preferences w.r.t. current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse user representations for predicting next item interacted with target behavior. The extensive experiments on two real-world datasets demonstrate the superiority of MGNN-SPred by comparing with state-of-the-art session-based prediction methods, validating the benefits of leveraging auxiliary behavior and learning item-to-item relations over MRIG.",
                        "Citation Paper Authors": "Authors:Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, Hongyuan Zha"
                    }
                },
                {
                    "Sentence ID": 187,
                    "Sentence": "performs\nGAT on session hypergraphs to capture the high-order contextual relations among items. GCE-\nGNN ",
                    "Citation Text": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui Qiu. 2020. Global context enhanced graph\nneural networks for session-based recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . 169\u2013178.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05081",
                        "Citation Paper Title": "Title:Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
                        "Citation Paper Authors": "Authors:Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.11803v2": {
            "Paper Title": "Content-driven Music Recommendation: Evolution, State of the Art, and\n  Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02473v3": {
            "Paper Title": "A Robust Cybersecurity Topic Classification Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09727v2": {
            "Paper Title": "Rank4Class: A Ranking Formulation for Multiclass Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.12430v2": {
            "Paper Title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "to develop an probabilistic adversarial\ncontamination model in the context of VN as well as regularization schemes to counter the\nadversary. The results of [33, 1] were further extended to the richly featured network setting\n4in ",
                    "Citation Text": "K. Levin, C. E. Priebe, and V. Lyzinski. On the role of features in vertex nomination:\nContent and context together are better (sometimes). arXiv preprint arXiv:2005.02151 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.02151",
                        "Citation Paper Title": "Title:Vertex Nomination in Richly Attributed Networks",
                        "Citation Paper Abstract": "Abstract:Vertex nomination is a lightly-supervised network information retrieval task in which vertices of interest in one graph are used to query a second graph to discover vertices of interest in the second graph. Similar to other information retrieval tasks, the output of a vertex nomination scheme is a ranked list of the vertices in the second graph, with the heretofore unknown vertices of interest ideally concentrating at the top of the list. Vertex nomination schemes provide a useful suite of tools for efficiently mining complex networks for pertinent information. In this paper, we explore, both theoretically and practically, the dual roles of content (i.e., edge and vertex attributes) and context (i.e., network topology) in vertex nomination. We provide necessary and sufficient conditions under which vertex nomination schemes that leverage both content and context outperform schemes that leverage only content or context separately. While the joint utility of both content and context has been demonstrated empirically in the literature, the framework presented in this paper provides a novel theoretical basis for understanding the potential complementary roles of network features and topology.",
                        "Citation Paper Authors": "Authors:Keith Levin, Carey E. Priebe, Vince Lyzinski"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "to cluster the graph we are nominating from\ninto 8 sub-graphs. After that, we use two di\u000berent methods to infer similarity. The \frst\napproximates \u0001via the value of the non-parametric test statistic of ",
                    "Citation Text": "M. Tang, A. Athreya, D. L. Sussman, V. Lyzinski, and C. E. Priebe. A nonparametric\ntwo-sample hypothesis testing for random dot product graphs. Bernoulli , 23:1599{1630,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.2344",
                        "Citation Paper Title": "Title:A nonparametric two-sample hypothesis testing problem for random dot product graphs",
                        "Citation Paper Abstract": "Abstract:We consider the problem of testing whether two finite-dimensional random dot product graphs have generating latent positions that are independently drawn from the same distribution, or distributions that are related via scaling or projection. We propose a test statistic that is a kernel-based function of the adjacency spectral embedding for each graph. We obtain a limiting distribution for our test statistic under the null and we show that our test procedure is consistent across a broad range of alternatives.",
                        "Citation Paper Authors": "Authors:Minh Tang, Avanti Athreya, Daniel L. Sussman, Vince Lyzinski, Carey E. Priebe"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ",\n1\u0000minP2\u0005(n)kAi\u0000PAjPTk2\nF\n1\nn!P\nP2\u0005(n)kAi\u0000PAjPTk2\nF\nwhere either Ai(the induced subgraph of community i) orAjhas been appropriately padded\nas in ",
                    "Citation Text": "D. E. Fishkind, S. Adali, H. G. Patsolic, L. Meng, D. Singh, V. Lyzinski, and C. E.\nPriebe. Seeded graph matching. Pattern recognition , 87:203{215, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1209.0367",
                        "Citation Paper Title": "Title:Seeded Graph Matching",
                        "Citation Paper Abstract": "Abstract:Given two graphs, the graph matching problem is to align the two vertex sets so as to minimize the number of adjacency disagreements between the two graphs. The seeded graph matching problem is the graph matching problem when we are first given a partial alignment that we are tasked with completing. In this paper, we modify the state-of-the-art approximate graph matching algorithm \"FAQ\" of Vogelstein et al. (2015) to make it a fast approximate seeded graph matching algorithm, adapt its applicability to include graphs with differently sized vertex sets, and extend the algorithm so as to provide, for each individual vertex, a nomination list of likely matches. We demonstrate the effectiveness of our algorithm via simulation and real data experiments; indeed, knowledge of even a few seeds can be extremely effective when our seeded graph matching algorithm is used to recover a naturally existing alignment that is only partially observed.",
                        "Citation Paper Authors": "Authors:Donniell E. Fishkind, Sancar Adali, Heather G. Patsolic, Lingyao Meng, Digvijay Singh, Vince Lyzinski, Carey E. Priebe"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ", in which the (potentially) complementary roles of features and network structure are\nexplored in the VN task.\nBeyond the development of novel VN algorithms ",
                    "Citation Text": "J. Yoder, L. Chen, H. Pao, E. Bridgeford, K. Levin, D. E. Fishkind, C. E. Priebe, and\nV. Lyzinski. Vertex nomination: The canonical sampling and the extended spectral\nnomination schemes. Computational Statistics & Data Analysis , 145:106916, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04960",
                        "Citation Paper Title": "Title:Vertex nomination: The canonical sampling and the extended spectral nomination schemes",
                        "Citation Paper Abstract": "Abstract:Suppose that one particular block in a stochastic block model is of interest, but block labels are only observed for a few of the vertices in the network. Utilizing a graph realized from the model and the observed block labels, the vertex nomination task is to order the vertices with unobserved block labels into a ranked nomination list with the goal of having an abundance of interesting vertices near the top of the list. There are vertex nomination schemes in the literature, including the optimally precise canonical nomination scheme~$\\mathcal{L}^C$ and the consistent spectral partitioning nomination scheme~$\\mathcal{L}^P$. While the canonical nomination scheme $\\mathcal{L}^C$ is provably optimally precise, it is computationally intractable, being impractical to implement even on modestly sized graphs.\nWith this in mind, an approximation of the canonical scheme---denoted the {\\it canonical sampling nomination scheme} $\\mathcal{L}^{CS}$---is introduced; $\\mathcal{L}^{CS}$ relies on a scalable, Markov chain Monte Carlo-based approximation of $\\mathcal{L}^{C}$, and converges to $\\mathcal{L}^{C}$ as the amount of sampling goes to infinity. The spectral partitioning nomination scheme is also extended to the {\\it extended spectral partitioning nomination scheme}, $\\mathcal{L}^{EP}$, which introduces a novel semisupervised clustering framework to improve upon the precision of $\\mathcal{L}^P$. Real-data and simulation experiments are employed to illustrate the precision of these vertex nomination schemes, as well as their empirical computational complexity.\nKeywords: vertex nomination, Markov chain Monte Carlo, spectral partitioning, Mclust\nMSC[2010]: 60J22, 65C40, 62H30, 62H25",
                        "Citation Paper Authors": "Authors:Jordan Yoder, Li Chen, Henry Pao, Eric Bridgeford, Keith Levin, Donniell Fishkind, Carey Priebe, Vince Lyzinski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.11800v3": {
            "Paper Title": "STEREO: Scientific Text Reuse in Open Access Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00944v2": {
            "Paper Title": "Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07374v4": {
            "Paper Title": "Network-based Topic Interaction Map for Big Data Mining of COVID-19\n  Biomedical Literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12460v2": {
            "Paper Title": "Extractive Explanations for Interpretable Text Ranking",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". The\nBERT-CLS baseline also works based on that assumption. However, recent strategies ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable &amp; Time-Budget-Constrained\nContextualization for Re-Ranking. https://doi.org/10.48550/ARXIV.2002.01854",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.01854",
                        "Citation Paper Title": "Title:Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking",
                        "Citation Paper Abstract": "Abstract:Search engines operate under a strict time constraint as a fast response is paramount to user satisfaction. Thus, neural re-ranking models have a limited time-budget to re-rank documents. Given the same amount of time, a faster re-ranking model can incorporate more documents than a less efficient one, leading to a higher effectiveness. To utilize this property, we propose TK (Transformer-Kernel): a neural re-ranking model for ad-hoc search using an efficient contextualization mechanism. TK employs a very small number of Transformer layers (up to three) to contextualize query and document word embeddings. To score individual term interactions, we use a document-length enhanced kernel-pooling, which enables users to gain insight into the model. TK offers an optimal ratio between effectiveness and efficiency: under realistic time constraints (max. 200 ms per query) TK achieves the highest effectiveness in comparison to BERT and other re-ranking models. We demonstrate this on three large-scale ranking collections: MSMARCO-Passage, MSMARCO-Document, and TREC CAR. In addition, to gain insight into TK, we perform a clustered query analysis of TK's results, highlighting its strengths and weaknesses on queries with different types of information need and we show how to interpret the cause of ranking differences of two documents by comparing their internal scores.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Markus Zlabinger, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "models.\nMore recently, neural models have entered the field of IR. Common approaches include semantic\nrepresentation learning [ 29,64,65], query-document cross-interactions [ 20,54,55,57,86] or the\nexploitation of positional information [ 30,31,50]. ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations\nof Text for Web Search. In Proceedings of the 26th International Conference on World Wide Web (Perth, Australia) (WWW\n\u201917). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 1291\u20131299.\nhttps://doi.org/10.1145/3038912.3052579",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05194v8": {
            "Paper Title": "DIGRAC: Digraph Clustering Based on Flow Imbalance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11294v5": {
            "Paper Title": "Scaling Law for Recommendation Models: Towards General-purpose User\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.07597v3": {
            "Paper Title": "Variational Bandwidth Auto-encoder for Hybrid Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08374v3": {
            "Paper Title": "Literature-Augmented Clinical Outcome Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13855v3": {
            "Paper Title": "Actionable Entities Recognition Benchmark for Interactive Fiction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07410v3": {
            "Paper Title": "Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked\n  Claims in a Document",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06918v3": {
            "Paper Title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a\n  Sparse One?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13340v3": {
            "Paper Title": "Decentralized Multi-Target Cross-Domain Recommendation for\n  Multi-Organization Collaborations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.02168v3": {
            "Paper Title": "Graph Neural Networks for Nomination and Representation Learning of Web\n  Elements",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "library. To gauge the impact of textual features on the other models,\nwe perform an experiment in Section 6.2 where we augment the\nfeatures with pre-trained embeddings computed using the Universal\nSentence Encoder4 ",
                    "Citation Text": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni\nSt. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian\nStrope, and Ray Kurzweil. Universal sentence encoder for English. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations , pages 169\u2013174, Brussels, Belgium, November 2018. Asso-\nciation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.11175",
                        "Citation Paper Title": "Title:Universal Sentence Encoder",
                        "Citation Paper Abstract": "Abstract:We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.",
                        "Citation Paper Authors": "Authors:Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ",\nfocuses on attribute value extraction and is, therefore, less relevant\nfor learning representations of action elements such as buttons or\nmore abstract elements which do not contain text.\nFinally, DOM-Q-Net ",
                    "Citation Text": "Sheng Jia, Jamie Ryan Kiros, and Jimmy Ba. DOM-Q-NET: Grounded RL on\nstructured language. In International Conference on Learning Representations ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07257",
                        "Citation Paper Title": "Title:DOM-Q-NET: Grounded RL on Structured Language",
                        "Citation Paper Abstract": "Abstract:Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.",
                        "Citation Paper Authors": "Authors:Sheng Jia, Jamie Kiros, Jimmy Ba"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". In this\npaper, we also explore a method called Child-Sum Tree-LSTM ",
                    "Citation Text": "Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic\nrepresentations from tree-structured long short-term memory networks. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 1556\u20131566, July 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.00075",
                        "Citation Paper Title": "Title:Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
                        "Citation Paper Abstract": "Abstract:Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
                        "Citation Paper Authors": "Authors:Kai Sheng Tai, Richard Socher, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "uniformly samples a fixed number of neighbours around\neach node. PinSage ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. Graph convolutional neural networks for web-scale recom-\nmender systems. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining , pages 974\u2013983, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "samples a\npredetermined number of nodes for each convolution layer. Graph-\nSage ",
                    "Citation Text": "William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation\nlearning on large graphs. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems , NIPS\u201917, page 1025\u20131035, Red Hook, NY,\nUSA, 2017. Curran Associates Inc.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "architecture that usually is used\nto model sequential data. Here we apply the sequential LSTM in\na top-down fashion on the path between the root and the target\nnode. A similar top-down tree method was proposed by ",
                    "Citation Text": "Xingxing Zhang, Liang Lu, and Mirella Lapata. Top-down tree long short-term\nmemory networks. In Proceedings of the 2016 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , pages 310\u2013320, San Diego, California, June 2016. Association for\nComputational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.00060",
                        "Citation Paper Title": "Title:Top-down Tree Long Short-Term Memory Networks",
                        "Citation Paper Abstract": "Abstract:Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.",
                        "Citation Paper Authors": "Authors:Xingxing Zhang, Liang Lu, Mirella Lapata"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "on nodes retrieved from truncated random walks. Some\nrecent papers have focused on making the GCN efficient for sizable\ngraphs with large node neighborhoods. FastGCN ",
                    "Citation Text": "Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph con-\nvolutional networks via importance sampling. In International Conference on\nLearning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.10247",
                        "Citation Paper Title": "Title:FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
                        "Citation Paper Abstract": "Abstract:The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.",
                        "Citation Paper Authors": "Authors:Jie Chen, Tengfei Ma, Cao Xiao"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nDeepWalk constructs graph representations using the SkipGramGraph Neural Networks for Nomination and Representation Learning of Web Elements\nmodel ",
                    "Citation Text": "Tom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of\nword representations in vector space. In Yoshua Bengio and Yann LeCun, editors,\n1st International Conference on Learning Representations, ICLR 2013, Scottsdale,\nArizona, USA, May 2-4, 2013, Workshop Track Proceedings , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.00627v4": {
            "Paper Title": "On the Relationship between Explanation and Recommendation: Learning to\n  Rank Explanations for Improved Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05223v3": {
            "Paper Title": "A quantitative and qualitative open citation analysis of retracted\n  articles in the humanities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.13971v3": {
            "Paper Title": "SMLSOM: The shrinking maximum likelihood self-organizing map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00676v5": {
            "Paper Title": "Motifs-based Recommender System via Hypergraph Convolution and\n  Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ").\nA self-supervised hypergraph learning framework for group recommendation was proposed by\nZhang et al. ",
                    "Citation Text": "Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hy-\npergraph Learning for Group Recommendation. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 2557\u20132567.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.04200",
                        "Citation Paper Title": "Title:Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation",
                        "Citation Paper Abstract": "Abstract:With the prevalence of social media, there has recently been a proliferation of recommenders that shift their focus from individual modeling to group recommendation. Since the group preference is a mixture of various predilections from group members, the fundamental challenge of group recommendation is to model the correlations among members. Existing methods mostly adopt heuristic or attention-based preference aggregation strategies to synthesize group preferences. However, these models mainly focus on the pairwise connections of users and ignore the complex high-order interactions within and beyond groups. Besides, group recommendation suffers seriously from the problem of data sparsity due to severely sparse group-item interactions. In this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intra- and inter-group interactions among users; (2) alleviating the data sparsity issue with the raw data itself. Technically, for (1), a hierarchical hypergraph convolutional network based on the user- and group-level hypergraphs is developed to model the complex tuplewise correlations among users within and beyond groups. For (2), we design a double-scale node dropout strategy to create self-supervision signals that can regularize user representations with different granularities against the sparsity issue. The experimental analysis on multiple benchmark datasets demonstrates the superiority of the proposed model and also elucidates the rationality of the hypergraph modeling and the double-scale self-supervision.",
                        "Citation Paper Authors": "Authors:Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, Hongzhi Yin"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". SSL was first applied to the field\nof image processing, such as image restoration, image denoising ",
                    "Citation Text": "Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. 2020. Noise2Same: Optimizing A Self-Supervised Bound for Image\nDenoising. In Advances in Neural Information Processing Systems , Vol. 33. 20320\u201320330.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11971",
                        "Citation Paper Title": "Title:Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising",
                        "Citation Paper Abstract": "Abstract:Self-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J-invariant. However, our analyses indicate that the current theory and the J-invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J-invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yaochen Xie, Zhengyang Wang, Shuiwang Ji"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.09480v6": {
            "Paper Title": "Learned Sorted Table Search and Static Indexes in Small Model Space",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ".\n1.1.1 Core Methods and Benchmarking Platform\nThe Recursive Model Index ",
                    "Citation Text": "T. Kraska, A. Beutel, E. H Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489\u2013504. ACM, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01208",
                        "Citation Paper Title": "Title:The Case for Learned Index Structures",
                        "Citation Paper Abstract": "Abstract:Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.",
                        "Citation Paper Authors": "Authors:Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", extended with several\nvariants in [11,34,43] and more in-depth analysed by Fumagalli et al. ",
                    "Citation Text": "G. Fumagalli, D. Raimondi, R. Giancarlo, D. Malchiodi, and M. Frasca. On the choice of general\npurpose classi\ufb01ers in learned bloom \ufb01lters: An initial analysis within basic \ufb01lters. In Proceedings of\nthe 11th International Conference on Pattern Recognition Applications and Methods (ICPRAM) , pages\n675\u2013682, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.06563",
                        "Citation Paper Title": "Title:On the Choice of General Purpose Classifiers in Learned Bloom Filters: An Initial Analysis Within Basic Filters",
                        "Citation Paper Abstract": "Abstract:Bloom Filters are a fundamental and pervasive data structure. Within the growing area of Learned Data Structures, several Learned versions of Bloom Filters have been considered, yielding advantages over classic Filters. Each of them uses a classifier, which is the Learned part of the data structure. Although it has a central role in those new filters, and its space footprint as well as classification time may affect the performance of the Learned Filter, no systematic study of which specific classifier to use in which circumstances is available. We report progress in this area here, providing also initial guidelines on which classifier to choose among five classic classification paradigms.",
                        "Citation Paper Authors": "Authors:Giacomo Fumagalli, Davide Raimondi, Raffaele Giancarlo, Dario Malchiodi, Marco Frasca"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". As a consequence, Learned Indexes can also make improvements in various related applications.\nIn particular, they are widely used for Databases, providing new challenges and opportunities ",
                    "Citation Text": "W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C. Ooi, and K. Tan. Database meets deep learning:\nChallenges and opportunities. SIGMOD Rec. , 45(2):17\u201322, sep 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08986",
                        "Citation Paper Title": "Title:Database Meets Deep Learning: Challenges and Opportunities",
                        "Citation Paper Abstract": "Abstract:Deep learning has recently become very popular on account of its incredible success in many complex data-driven applications, such as image classification and speech recognition. The database community has worked on data-driven applications for many years, and therefore should be playing a lead role in supporting this new wave. However, databases and deep learning are different in terms of both techniques and applications. In this paper, we discuss research problems at the intersection of the two fields. In particular, we discuss possible improvements for deep learning systems from a database perspective, and analyze database applications that may benefit from deep learning techniques.",
                        "Citation Paper Authors": "Authors:Wei Wang, Meihui Zhang, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Kian-Lee Tan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00720v5": {
            "Paper Title": "LightNER: A Lightweight Tuning Paradigm for Low-resource NER via\n  Pluggable Prompting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08166v3": {
            "Paper Title": "Query Embedding on Hyper-relational Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14774v4": {
            "Paper Title": "Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User\n  MIMO Communications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09118v4": {
            "Paper Title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03663v3": {
            "Paper Title": "Quantifying the Suicidal Tendency on Social Media: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "Given the filtered data from Reddit\nsocial medida platform for depres-\nsion detection taskDepression [80, 111] \u2713\nSina\nWeiboCao et. al., ",
                    "Citation Text": "Lei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin Wang, Ningyun Li, and Xiaohao He. 2019. Latent suicide risk detection on microblog via\nsuicide-oriented word embeddings and layered attention. arXiv preprint arXiv:1910.12038 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.12038",
                        "Citation Paper Title": "Title:Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention",
                        "Citation Paper Abstract": "Abstract:Despite detection of suicidal ideation on social media has made great progress in recent years, people's implicitly and anti-real contrarily expressed posts still remain as an obstacle, constraining the detectors to acquire higher satisfactory performance. Enlightened by the hidden \"tree holes\" phenomenon on microblog, where people at suicide risk tend to disclose their inner real feelings and thoughts to the microblog space whose authors have committed suicide, we explore the use of tree holes to enhance microblog-based suicide risk detection from the following two perspectives. (1) We build suicide-oriented word embeddings based on tree hole contents to strength the sensibility of suicide-related lexicons and context based on tree hole contents. (2) A two-layered attention mechanism is deployed to grasp intermittently changing points from individual's open blog streams, revealing one's inner emotional world more or less. Our experimental results show that with suicide-oriented word embeddings and attention, microblog-based suicide risk detection can achieve over 91\\% accuracy. A large-scale well-labelled suicide data set is also reported in the paper.",
                        "Citation Paper Authors": "Authors:Lei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin Wang, Ningyun Li, Xiaohao He"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": ". Simultaneously, the dual attention mechanism was introduced for multimodal approaches in a study ",
                    "Citation Text": "Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017. Dual attention networks for multimodal reasoning and matching. In Proceedings of the\nIEEE conference on computer vision and pattern recognition . 299\u2013307.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00471",
                        "Citation Paper Title": "Title:Dual Attention Networks for Multimodal Reasoning and Matching",
                        "Citation Paper Abstract": "Abstract:We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.",
                        "Citation Paper Authors": "Authors:Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Consists of 190K Reddit posts of 5\ndifferent categoriesStress [41, 116] \u2713\nGoEmotion Demszky et. al., ",
                    "Citation Text": "Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020. Goemotions: A dataset of\nfine-grained emotions. arXiv preprint arXiv:2005.00547 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00547",
                        "Citation Paper Title": "Title:GoEmotions: A Dataset of Fine-Grained Emotions",
                        "Citation Paper Abstract": "Abstract:Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.",
                        "Citation Paper Authors": "Authors:Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, Sujith Ravi"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Reddit dataset of 9210 users in de-\npression and 1,07,274 users in con-\ntrol groupDepression [17, 107,\n129, 133]S\nSMHD Cohen et. al., ",
                    "Citation Text": "Arman Cohan, Bart Desmet, Andrew Yates, Luca Soldaini, Sean MacAvaney, and Nazli Goharian. 2018. SMHD: a large-scale resource for exploring\nonline language usage for multiple mental health conditions. arXiv preprint arXiv:1806.05258 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05258",
                        "Citation Paper Title": "Title:SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions",
                        "Citation Paper Abstract": "Abstract:Mental health is a significant and growing public health concern. As language usage can be leveraged to obtain crucial insights into mental health conditions, there is a need for large-scale, labeled, mental health-related datasets of users who have been diagnosed with one or more of such conditions. In this paper, we investigate the creation of high-precision patterns to identify self-reported diagnoses of nine different mental health conditions, and obtain high-quality labeled data without the need for manual labelling. We introduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it available. SMHD is a novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users. We examine distinctions in users' language, as measured by linguistic and psychological variables. We further explore text classification methods to identify individuals with mental conditions through their language.",
                        "Citation Paper Authors": "Authors:Arman Cohan, Bart Desmet, Andrew Yates, Luca Soldaini, Sean MacAvaney, Nazli Goharian"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", the GloVe [ 10,107], and the Fasttext were used for feature vector representation. To handle the longer\ntext like phrase, sentence or paragraph, the authors used BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 131,
                    "Sentence": ". Recently, the information\nabout emotion models is summarized due to evolving era of \u2018Emotional Artificial Intelligence\u2019 for affective computing ",
                    "Citation Text": "Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, and Qiang Ji. 2019. Affective computing for large-scale heterogeneous\nmultimedia data: A survey. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 15, 3s (2019), 1\u201332.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05609",
                        "Citation Paper Title": "Title:Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey",
                        "Citation Paper Abstract": "Abstract:The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., image, music, and video), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.",
                        "Citation Paper Authors": "Authors:Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, Qiang Ji"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.01119v3": {
            "Paper Title": "Cloud-Cluster Architecture for Detection in Intermittently Connected\n  Sensor Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05009v6": {
            "Paper Title": "A Gumbel-based Rating Prediction Framework for Imbalanced Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". To tackle the limitations of rating-based methods, a\nnumber of studies derive representations of users and items from\nuser textual reviews [ 21,34,38]. For example, Liu et al .propose a\nmodel that encodes user reviews with neural transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You\nNeed. In Proceedings of the 31st International Conference on Neural InformationA Gumbel-based Rating Prediction Framework for Imbalanced Recommendation CIKM \u201922, October 17\u201321, 2022, Atlanta, GA, USA\nProcessing Systems (Long Beach, California, USA) (NIPS\u201917) . Curran Associates\nInc., Red Hook, NY , USA, 6000\u20136010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". MF and PMF\ndecompose the observed rating matrix into a product of two low-rank\nlatent feature matrices. Neural Matrix Factorization (NeuMF) ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th Interna-\ntional Conference on World Wide Web (Perth, Australia) (WWW \u201917) . International\nWorld Wide Web Conferences Steering Committee, Republic and Canton of\nGeneva, CHE, 173\u2013182. https://doi.org/10.1145/3038912.3052569",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05260v2": {
            "Paper Title": "Sirius: Visualization of Mixed Features as a Mutual Information Network\n  Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04232v2": {
            "Paper Title": "Learning Topic Models: Identifiability and Finite-Sample Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02951v3": {
            "Paper Title": "Multi-FR: A Multi-objective Optimization Framework for Multi-stakeholder\n  Fairness-aware Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "across con-\nsumers. Some works focus on the individual fairness that ensures similar individuals are treated\nsimilarly ",
                    "Citation Text": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. 2012. Fairness through awareness.\nInITCS . ACM, 214\u2013226.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1104.3913",
                        "Citation Paper Title": "Title:Fairness Through Awareness",
                        "Citation Paper Abstract": "Abstract:We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.",
                        "Citation Paper Authors": "Authors:Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Rich Zemel"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": "combine the adversarial training with the\ngraph representation learning together to protect sensitive features of consumers. They introduce\nan adversarial framework to enforce fairness on graph embeddings. Similarly, Wu et al . ",
                    "Citation Text": "Le Wu, Lei Chen, Pengyang Shao, Richang Hong, Xiting Wang, and Meng Wang. 2021. Learning Fair Representations\nfor Recommendation: A Graph-based Perspective. In WWW . ACM / IW3C2, 2198\u20132208.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.09140",
                        "Citation Paper Title": "Title:Learning Fair Representations for Recommendation: A Graph-based Perspective",
                        "Citation Paper Abstract": "Abstract:As a key application of artificial intelligence, recommender systems are among the most pervasive computer aided systems to help users find potential items of interests. Recently, researchers paid considerable attention to fairness issues for artificial intelligence applications. Most of these approaches assumed independence of instances, and designed sophisticated models to eliminate the sensitive information to facilitate fairness. However, recommender systems differ greatly from these approaches as users and items naturally form a user-item bipartite graph, and are collaboratively correlated in the graph structure. In this paper, we propose a novel graph based technique for ensuring fairness of any recommendation models. Here, the fairness requirements refer to not exposing sensitive feature set in the user modeling process. Specifically, given the original embeddings from any recommendation models, we learn a composition of filters that transform each user's and each item's original embeddings into a filtered embedding space based on the sensitive feature set. For each user, this transformation is achieved under the adversarial learning of a user-centric graph, in order to obfuscate each sensitive feature between both the filtered user embedding and the sub graph structures of this user. Finally, extensive experimental results clearly show the effectiveness of our proposed model for fair recommendation. We publish the source code at this https URL.",
                        "Citation Paper Authors": "Authors:Le Wu, Lei Chen, Pengyang Shao, Richang Hong, Xiting Wang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". Both fairness is treated at an individual level. Wu\net al. ",
                    "Citation Text": "Yao Wu, Jian Cao, Guandong Xu, and Yudong Tan. 2021. TFROM: A Two-sided Fairness-Aware Recommendation\nModel for Both Customers and Providers. In SIGIR . ACM, 1013\u20131022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.09024",
                        "Citation Paper Title": "Title:TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
                        "Citation Paper Abstract": "Abstract:At present, most research on the fairness of recommender systems is conducted either from the perspective of customers or from the perspective of product (or service) providers. However, such a practice ignores the fact that when fairness is guaranteed to one side, the fairness and rights of the other side are likely to reduce. In this paper, we consider recommendation scenarios from the perspective of two sides (customers and providers). From the perspective of providers, we consider the fairness of the providers' exposure in recommender system. For customers, we consider the fairness of the reduced quality of recommendation results due to the introduction of fairness measures. We theoretically analyzed the relationship between recommendation quality, customers fairness, and provider fairness, and design a two-sided fairness-aware recommendation model (TFROM) for both customers and providers. Specifically, we design two versions of TFROM for offline and online recommendation. The effectiveness of the model is verified on three real-world data sets. The experimental results show that TFROM provides better two-sided fairness while still maintaining a higher level of personalization than the baseline algorithms.",
                        "Citation Paper Authors": "Authors:Yao Wu, Jian Cao, Guandong Xu, Yudong Tan"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "find that standard\nrecommendation algorithms may result in certain demographic groups of producers being over- or\nunder-represented in recommendation decisions. Beutel et al . ",
                    "Citation Text": "Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi,\nand Cristos Goodrow. 2019. Fairness in Recommendation Ranking through Pairwise Comparisons. In KDD . ACM,\n2212\u20132220.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.00780",
                        "Citation Paper Title": "Title:Fairness in Recommendation Ranking through Pairwise Comparisons",
                        "Citation Paper Abstract": "Abstract:Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information. As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them? In this paper we offer a set of novel metrics for evaluating algorithmic fairness concerns in recommender systems. In particular we show how measuring fairness based on pairwise comparisons from randomized experiments provides a tractable means to reason about fairness in rankings from recommender systems. Building on this metric, we offer a new regularizer to encourage improving this metric during model training and thus improve fairness in the resulting rankings. We apply this pairwise regularization to a large-scale, production recommender system and show that we are able to significantly improve the system's pairwise fairness.",
                        "Citation Paper Authors": "Authors:Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, Cristos Goodrow"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.03848v3": {
            "Paper Title": "Knowledge mining of unstructured information: application to\n  cyber-domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03226v2": {
            "Paper Title": "Graphing else matters: exploiting aspect opinions and ratings in\n  explainable graph-based recommendations",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ",\nwhich incorporate sequential information through a recurrent neural network, and the recent KGAT ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 950\u2013958.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.03385v2": {
            "Paper Title": "Rating and aspect-based opinion graph embeddings for explainable\n  recommendations",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", which incorporates sequential information through a recur-\nrent neural network. Another recent work based on embeddings\nis KGAT ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:\nKnowledge graph attention network for recommendation. In Proceedings of the\n25th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining . 950\u2013958.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.00693v2": {
            "Paper Title": "Multimodal Entity Tagging with Multimodal Knowledge Base",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07352v2": {
            "Paper Title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02274v4": {
            "Paper Title": "Self-supervised Graph Learning for Occasional Group Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00689v2": {
            "Paper Title": "CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch\n  Attribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06207v8": {
            "Paper Title": "Contrastive Triple Extraction with Generative Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04476v4": {
            "Paper Title": "Twitter Big Data as a Resource for Exoskeleton Research: A Large-Scale\n  Dataset of about 140,000 Tweets and 100 Research Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12369v3": {
            "Paper Title": "Neural content-aware collaborative filtering for cold-start music\n  recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08816v2": {
            "Paper Title": "Deep Hash Distillation for Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01999v3": {
            "Paper Title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "to FINT in paradigm because it also\nretains the feature boundary in linear feature interaction and\nexploits nonlinear high-order feature interaction. However,\nit is based on the Transformer model ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,and Illia Polosukhin, \u201cAttention is all you need,\u201d in Ad-\nvances in neural information processing systems , 2017,\npp. 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.00436v3": {
            "Paper Title": "Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "introduce MDR and IRRR, state-of-the-art systems\nthat assume no explicit link structure. Instead, they use an iterative retrieval paradigm\u2014akin to that\nintroduced by Das et al. ",
                    "Citation Text": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. Multi-step\nretriever-reader interaction for scalable open-domain question answering. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net, 2019. URL https://openreview.net/forum?id=HkfPSh05K7 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.05733",
                        "Citation Paper Title": "Title:Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
                        "Citation Paper Abstract": "Abstract:This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures DrQA and BiDAF on various large open-domain datasets --- TriviaQA-unfiltered, QuasarT, SearchQA, and SQuAD-Open.",
                        "Citation Paper Authors": "Authors:Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Andrew McCallum"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "), softening the retrieval\nchallenges discussed in \u00a71. Very recently, Jiang et al. ",
                    "Citation Text": "Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal.\nHoVer: A dataset for many-hop fact extraction and claim veri\ufb01cation. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020 , pages 3441\u20133460, Online, 2020.\n11Association for Computational Linguistics. doi: 10.18653/v1/2020.\ufb01ndings-emnlp.309. URL\nhttps://aclanthology.org/2020.findings-emnlp.309 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03088",
                        "Citation Paper Title": "Title:HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification",
                        "Citation Paper Abstract": "Abstract:We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification. We make the HoVer dataset publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, Mohit Bansal"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "), question generation (e.g., with Jeopardy as in Lewis et al. ",
                    "Citation Text": "Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian\nRiedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP\ntasks. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n6b493230205f780e1bc26945df7481e5-Abstract.html .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11401",
                        "Citation Paper Title": "Title:Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                        "Citation Paper Abstract": "Abstract:Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                        "Citation Paper Authors": "Authors:Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "for knowledge-intensive tasks. Among\nmodels for these tasks, the most relevant to our work are OpenQA models that include learned\nretrieval components. These include ORQA ",
                    "Citation Text": "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly su-\npervised open domain question answering. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics , pages 6086\u20136096, Florence, Italy, 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https:\n//aclanthology.org/P19-1612 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00300",
                        "Citation Paper Title": "Title:Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
                        "Citation Paper Authors": "Authors:Kenton Lee, Ming-Wei Chang, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "), among others. Many of these datasets are\ncompiled in the recently-introduced KILT benchmark ",
                    "Citation Text": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao,\nJames Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim\nRockt\u00e4schel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language\ntasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies , pages 2523\u20132544, Online,\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL\nhttps://aclanthology.org/2021.naacl-main.200 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.02252",
                        "Citation Paper Title": "Title:KILT: a Benchmark for Knowledge Intensive Language Tasks",
                        "Citation Paper Abstract": "Abstract:Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "), and\nopen dialogue (e.g., Wizard of Wikipedia; Dinan et al. ",
                    "Citation Text": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.\nWizard of wikipedia: Knowledge-powered conversational agents. In 7th International Con-\nference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net, 2019. URL https://openreview.net/forum?id=r1l73iRqKm .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.01241",
                        "Citation Paper Title": "Title:Wizard of Wikipedia: Knowledge-Powered Conversational agents",
                        "Citation Paper Abstract": "Abstract:In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.",
                        "Citation Paper Authors": "Authors:Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "), which extends the well-studied machine reading\ncomprehension (MRC) problem over supplied question\u2013passage pairs to answering questions given\nonly a large text corpus like Wikipedia. Other open-domain tasks span claim veri\ufb01cation (e.g.,\nFEVER; Thorne et al. ",
                    "Citation Text": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERi\ufb01cation. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) , pages 809\u2013819, New Orleans, Louisiana,\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https:\n//aclanthology.org/N18-1074 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05355",
                        "Citation Paper Title": "Title:FEVER: a large-scale dataset for Fact Extraction and VERification",
                        "Citation Paper Abstract": "Abstract:In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
                        "Citation Paper Authors": "Authors:James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "Open-domain reasoning There is signi\ufb01cant recent interest in NLP models that can solve tasks\nby retrieving evidence from a large corpus. The most popular such task is arguably open-domain\nquestion answering (OpenQA; Chen et al. ",
                    "Citation Text": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer\nopen-domain questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 1870\u20131879, Vancouver, Canada,\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\n//aclanthology.org/P17-1171 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00051",
                        "Citation Paper Title": "Title:Reading Wikipedia to Answer Open-Domain Questions",
                        "Citation Paper Abstract": "Abstract:This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                        "Citation Paper Authors": "Authors:Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01488v3": {
            "Paper Title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08547v2": {
            "Paper Title": "Learning Rich Representation of Keyphrases from Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06460v4": {
            "Paper Title": "Sequential Recommendation with Bidirectional Chronological Augmentation\n  of Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13827v3": {
            "Paper Title": "Natural Language Processing in-and-for Design Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07620v3": {
            "Paper Title": "Tree-based Focused Web Crawling with Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.11044v2": {
            "Paper Title": "Pseudo Relevance Feedback with Deep Language Models and Dense\n  Retrievers: Successes and Pitfalls",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "as representative first stage dense retrievers because they achieve state-of-\nthe-art effectiveness in previous work on MS MARCO. We note that a host of alternative first stage dense retrievers\nhave been recently proposed, including stronger ones like RocketQA ",
                    "Citation Text": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. RocketQA: An Optimized\nTraining Approach to Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint arXiv:2010.08191 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.08191",
                        "Citation Paper Title": "Title:RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                        "Citation Paper Authors": "Authors:Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12755v2": {
            "Paper Title": "Auto Response Generation in Online Medical Chat Services",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00999v2": {
            "Paper Title": "Contrastive Cross-domain Recommendation in Matching",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ",\nsequential recommendation [ 32,33,37]. For graph-based CL, Wu\net al. ",
                    "Citation Text": "Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,\nand Xing Xie. 2021. Self-supervised graph learning for recommendation. In\nProceedings of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10783",
                        "Citation Paper Title": "Title:Self-supervised Graph Learning for Recommendation",
                        "Citation Paper Abstract": "Abstract:Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.\nIn this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -- node dropout, edge dropout, and random walk -- that change the graph structure in different manners. We term this new learning paradigm as \\textit{Self-supervised Graph Learning} (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "adopts CL between behaviors and models in\nmulti-behavior recommendation. Moreover, CL has also been used\nin disentangled recommendation ",
                    "Citation Text": "Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021.\nContrastive Learning for Debiased Candidate Generation in Large-Scale Recom-\nmender Systems. In Proceedings of KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12964",
                        "Citation Paper Title": "Title:Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Deep candidate generation (DCG) that narrows down the collection of relevant items from billions to hundreds via representation learning has become prevalent in industrial recommender systems. Standard approaches approximate maximum likelihood estimation (MLE) through sampling for better scalability and address the problem of DCG in a way similar to language modeling. However, live recommender systems face severe exposure bias and have a vocabulary several orders of magnitude larger than that of natural language, implying that MLE will preserve and even exacerbate the exposure bias in the long run in order to faithfully fit the observed samples. In this paper, we theoretically prove that a popular choice of contrastive loss is equivalent to reducing the exposure bias via inverse propensity weighting, which provides a new perspective for understanding the effectiveness of contrastive learning. Based on the theoretical discovery, we design CLRec, a contrastive learning method to improve DCG in terms of fairness, effectiveness and efficiency in recommender systems with extremely large candidate size. We further improve upon CLRec and propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our methods have been successfully deployed in Taobao, where at least four-month online A/B tests and offline analyses demonstrate its substantial improvements, including a dramatic reduction in the Matthew effect.",
                        "Citation Paper Authors": "Authors:Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, Hongxia Yang"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "builds contrastive learn-\ning tasks among items, attributes, sentences, and sub-sentences\nin sequential recommendation. UPRec focuses on user-aware SSL ",
                    "Citation Text": "Chaojun Xiao, Ruobing Xie, Yuan Yao, Zhiyuan Liu, Maosong Sun, Xu Zhang,\nand Leyu Lin. 2021. UPRec: User-Aware Pre-training for Recommender Systems.\narXiv preprint arXiv:2102.10989 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.10989",
                        "Citation Paper Title": "Title:UPRec: User-Aware Pre-training for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Existing sequential recommendation methods rely on large amounts of training data and usually suffer from the data sparsity problem. To tackle this, the pre-training mechanism has been widely adopted, which attempts to leverage large-scale data to perform self-supervised learning and transfer the pre-trained parameters to downstream tasks. However, previous pre-trained models for recommendation focus on leverage universal sequence patterns from user behaviour sequences and item information, whereas ignore capturing personalized interests with the heterogeneous user information, which has been shown effective in contributing to personalized recommendation. In this paper, we propose a method to enhance pre-trained models with heterogeneous user information, called User-aware Pre-training for Recommendation (UPRec). Specifically, UPRec leverages the user attributes andstructured social graphs to construct self-supervised objectives in the pre-training stage and proposes two user-aware pre-training tasks. Comprehensive experimental results on several real-world large-scale recommendation datasets demonstrate that UPRec can effectively integrate user information into pre-trained models and thus provide more appropriate recommendations for users.",
                        "Citation Paper Authors": "Authors:Chaojun Xiao, Ruobing Xie, Yuan Yao, Zhiyuan Liu, Maosong Sun, Xu Zhang, Leyu Lin"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "designs a simple con-\ntrastive learning framework with a composition of data augmenta-\ntions and projectors for CL. BYOL ",
                    "Citation Text": "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Daniel Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own\nlatent: A new approach to self-supervised learning. arXiv:2006.07733 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.07733",
                        "Citation Paper Title": "Title:Bootstrap your own latent: A new approach to self-supervised Learning",
                        "Citation Paper Abstract": "Abstract:We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
                        "Citation Paper Authors": "Authors:Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.09533v2": {
            "Paper Title": "Author Clustering and Topic Estimation for Short Texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14403v3": {
            "Paper Title": "Re-evaluating Word Mover's Distance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08096v2": {
            "Paper Title": "Learning to Limit Data Collection via Scaling Laws: A Computational\n  Interpretation for the Legal Principle of Data Minimization",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "and contrast these to the three\nphases of data collection identified by Hestness et al . ",
                    "Citation Text": "J Hestness, S Narang, N Ardalani, G Diamos, H Jun, H Kianinejad, M Patwary, Y\nYang, and Y Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. arXiv\npreprint arXiv:1712.00409 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00409",
                        "Citation Paper Title": "Title:Deep Learning Scaling is Predictable, Empirically",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.\nThis paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",
                        "Citation Paper Authors": "Authors:Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "study breadth-based data mini-\nmization and propose an audit method that uses feature imputation\nto identify whether the features used for a given model are neces-\nsary to preserve predictive performance to a pre-specified degree.\nGoldsteen et al . ",
                    "Citation Text": "Abigail Goldsteen, Gilad Ezov, Ron Shmelkin, Micha Moffie, and Ariel Farkash.\n2021. Data minimization for GDPR Compliance in machine learning models. AI\nand Ethics (2021), 1\u201315.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.04113",
                        "Citation Paper Title": "Title:Data Minimization for GDPR Compliance in Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:The EU General Data Protection Regulation (GDPR) mandates the principle of data minimization, which requires that only data necessary to fulfill a certain purpose be collected. However, it can often be difficult to determine the minimal amount of data required, especially in complex machine learning models such as neural networks. We present a first-of-a-kind method to reduce the amount of personal data needed to perform predictions with a machine learning model, by removing or generalizing some of the input features. Our method makes use of the knowledge encoded within the model to produce a generalization that has little to no impact on its accuracy. This enables the creators and users of machine learning models to acheive data minimization, in a provable manner.",
                        "Citation Paper Authors": "Authors:Abigail Goldsteen, Gilad Ezov, Ron Shmelkin, Micha Moffie, Ariel Farkash"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.08357v2": {
            "Paper Title": "Design Challenges for a Multi-Perspective Search Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11317v2": {
            "Paper Title": "Privacy-Preserving Clustering of Unstructured Big Data for Cloud-Based\n  Enterprise Search Solutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05988v2": {
            "Paper Title": "Cross-language Information Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 208,
                    "Sentence": "built a small manually-curated in-domain dataset consisting of pop-\nular search queries, augmenting it with large amounts of parallel out -of-domain\nsentences to train a NMT model. As a di\ufb00erent approach, Yao et al. ",
                    "Citation Text": "L. Yao, B. Yang, H. Zhang, W. Luo, and B. Chen. Exploiting neu ral\nquery translation into cross lingual information retrieval. arXiv preprint\narXiv:2010.13659 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.13659",
                        "Citation Paper Title": "Title:Exploiting Neural Query Translation into Cross Lingual Information Retrieval",
                        "Citation Paper Abstract": "Abstract:As a crucial role in cross-language information retrieval (CLIR), query translation has three main challenges: 1) the adequacy of translation; 2) the lack of in-domain parallel training data; and 3) the requisite of low latency. To this end, existing CLIR systems mainly exploit statistical-based machine translation (SMT) rather than the advanced neural machine translation (NMT), limiting the further improvements on both translation and retrieval quality. In this paper, we investigate how to exploit neural query translation model into CLIR system. Specifically, we propose a novel data augmentation method that extracts query translation pairs according to user clickthrough data, thus to alleviate the problem of domain-adaptation in NMT. Then, we introduce an asynchronous strategy which is able to leverage the advantages of the real-time in SMT and the veracity in NMT. Experimental results reveal that the proposed approach yields better retrieval quality than strong baselines and can be well applied into a real-world CLIR system, i.e. Aliexpress e-Commerce search engine. Readers can examine and test their cases on our website: this https URL .",
                        "Citation Paper Authors": "Authors:Liang Yao, Baosong Yang, Haibo Zhang, Weihua Luo, Boxing Chen"
                    }
                },
                {
                    "Sentence ID": 206,
                    "Sentence": "P. Yang, H. Fang, and J. Lin. Anserini: Reproducible ranking ba selines\nusing Lucene. J. Data and Information Quality , 10(4), Oct. 2018. ISSN\n1936-1955. ",
                    "Citation Text": "Y. Yang, D. Cer, A. Ahmad, M. Guo, J. Law, N. Constant, G. H ernan-\ndez Abrego, S. Yuan, C. Tar, Y.-h. Sung, B. Strope, and R. Kurzw eil. Mul-\ntilingual universal sentence encoder for semantic retrieval. In Proceedings\nof the 58th Annual Meeting of the Association for Computatio nal Linguis-\ntics: System Demonstrations , pages 87\u201394, Online, July 2020. Association\nfor Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.04307",
                        "Citation Paper Title": "Title:Multilingual Universal Sentence Encoder for Semantic Retrieval",
                        "Citation Paper Abstract": "Abstract:We introduce two pre-trained retrieval focused multilingual sentence encoding models, respectively based on the Transformer and CNN model architectures. The models embed text from 16 languages into a single semantic space using a multi-task trained dual-encoder that learns tied representations using translation based bridge tasks (Chidambaram al., 2018). The models provide performance that is competitive with the state-of-the-art on: semantic retrieval (SR), translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On English transfer learning tasks, our sentence-level embeddings approach, and in some cases exceed, the performance of monolingual, English only, sentence embedding models. Our models are made available for download on TensorFlow Hub.",
                        "Citation Paper Authors": "Authors:Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil"
                    }
                },
                {
                    "Sentence ID": 178,
                    "Sentence": "M. Schuster and K. Nakajima. Japanese and Korean voice sea rch. In2012\nIEEE International Conference on Acoustics, Speech and Sig nal Processing\n(ICASSP) , pages 5149\u20135152, 2012. ",
                    "Citation Text": "R. Sennrich, B. Haddow, and A. Birch. Neural machine transla tion of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: L ong Papers) ,\npages 1715\u20131725, Berlin, Germany, Aug. 2016. ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.07909",
                        "Citation Paper Title": "Title:Neural Machine Translation of Rare Words with Subword Units",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                },
                {
                    "Sentence ID": 154,
                    "Sentence": "C. Peters, M. Braschler, and P. Clough. Multilingual Information Re-\ntrieval: From Research To Practice . Computer Science. Springer Berlin\nHeidelberg, 2012. ",
                    "Citation Text": "M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. L ee, and\nL. Zettlemoyer. Deep contextualized word representations. In Proceedings\nof the 2018 Conference of the North American Chapter of the As sociation\nfor Computational Linguistics: Human Language Technologi es, Volume 1\n(Long Papers) , pages2227\u20132237,NewOrleans,Louisiana,June2018.ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 119,
                    "Sentence": "M. Lupu, A. Fujii, D. W. Oard, M. Iwayama, and N. Kando. Patent-\nRelated Tasks at NTCIR , pages 77\u2013111. Springer Berlin Heidelberg, 2017.\nISBN 978-3-662-53817-3. ",
                    "Citation Text": "S. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goh arian, and\nO. Frieder. E\ufb03cient document re-ranking for transformers by pr ecomput-\ning term representations. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in Informatio n Retrieval ,\nSIGIR \u201920, page 49\u201358, New York, NY, USA, 2020. Association for C om-\nputing Machinery. ISBN 9781450380164.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14255",
                        "Citation Paper Title": "Title:Efficient Document Re-Ranking for Transformers by Precomputing Term Representations",
                        "Citation Paper Abstract": "Abstract:Deep pretrained transformer networks are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their computational expenses deem them cost-prohibitive in practice. Our proposed approach, called PreTTR (Precomputing Transformer Term Representations), considerably reduces the query-time latency of deep transformer networks (up to a 42x speedup on web document ranking) making these networks more practical to use in a real-time ranking scenario. Specifically, we precompute part of the document term representations at indexing time (without a query), and merge them with the query representation at query time to compute the final ranking score. Due to the large size of the token representations, we also propose an effective approach to reduce the storage requirement by training a compression layer to match attention scores. Our compression technique reduces the storage required up to 95% and it can be applied without a substantial degradation in ranking performance.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, Ophir Frieder"
                    }
                },
                {
                    "Sentence ID": 117,
                    "Sentence": "2020 around 5k Q/A pairs perlanguage 2-6k articles per language English, Arabic, German, Spanish,Hindi, Vietnamese, Chinese CC-BY-SA 3.033\nMKQA: Multilingual Knowledge Questions& Answers ",
                    "Citation Text": "S. Longpre, Y. Lu, and J. Daiber. MKQA: A linguistically diverse\nbenchmark for multilingual open domain question answering. CoRR,\nabs/2007.15207, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15207",
                        "Citation Paper Title": "Title:MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on a heavily curated, language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state-of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages",
                        "Citation Paper Authors": "Authors:Shayne Longpre, Yi Lu, Joachim Daiber"
                    }
                },
                {
                    "Sentence ID": 115,
                    "Sentence": "J. Liu, Y. Lin, Z. Liu, and M. Sun. XQA: A cross-lingual open-do main\nquestion answering dataset. In Proceedings of ACL , pages 2358\u20132368,Flo-\nrence, Italy, July 2019. ACL. ",
                    "Citation Text": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. L ewis,\nL. Zettlemoyer, and V. Stoyanov. RoBERTa: A robustly optimized B ERT\npretraining approach. arXiv preprint arXiv:1907.11692 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 111,
                    "Sentence": "used\na transfer learning approach, applying a retrieval model trained o n a large col-\nlection in English to retrieve documents from collections in other langu ages.\nLitschko et al. ",
                    "Citation Text": "R. Litschko, I. Vuli\u00b4 c, and G. Glava\u02c7 s. Parameter-e\ufb03cient n eural\nreranking for cross-lingual and multilingual retrieval, 2022. URL\nhttps://arxiv.org/abs/2204.02292 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02292",
                        "Citation Paper Title": "Title:Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
                        "Citation Paper Abstract": "Abstract:State-of-the-art neural (re)rankers are notoriously data-hungry which -- given the lack of large-scale training data in languages other than English -- makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore commonly transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all parameters of pretrained massively multilingual Transformers (MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy them in the target language(s). In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the (re)ranking adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. We carry out a large scale evaluation on the CLEF-2003 and HC4 benchmarks and additionally, as another contribution, extend the former with queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while being more modular and reducing training times. The gains are particularly pronounced for low-resource languages, where our approaches also substantially outperform the competitive machine translation-based rankers.",
                        "Citation Paper Authors": "Authors:Robert Litschko, Ivan Vuli\u0107, Goran Glava\u0161"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "R. Litschko, I. Vuli\u00b4 c, S. P. Ponzetto, and G. Glava\u02c7 s. On cr oss-lingual\nretrievalwithmultilingualtextencoders. arXiv preprint arXiv:2112.11031 ,\n2021. ",
                    "Citation Text": "R. Litschko, I. Vuli\u00b4 c, S. P. Ponzetto, and G. Glava\u02c7 s. Evalu ating multilin-\ngual text encoders for unsupervised cross-lingual retrieval. In European\nConference on Information Retrieval , pages 342\u2013358. Springer, 2021.40 P. Galu\u02c7 s\u02c7 c\u00b4 akov\u00b4 a et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08370",
                        "Citation Paper Title": "Title:Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval",
                        "Citation Paper Abstract": "Abstract:Pretrained multilingual text encoders based on neural Transformer architectures, such as multilingual BERT (mBERT) and XLM, have achieved strong performance on a myriad of language understanding tasks. Consequently, they have been adopted as a go-to paradigm for multilingual and cross-lingual representation learning and transfer, rendering cross-lingual word embeddings (CLWEs) effectively obsolete. However, questions remain to which extent this finding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual IR (CLIR) tasks. Therefore, in this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a large number of language pairs. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR -- a setup with no relevance judgments for IR-specific fine-tuning -- pretrained encoders fail to significantly outperform models based on CLWEs. For sentence-level CLIR, we demonstrate that state-of-the-art performance can be achieved. However, the peak performance is not met using the general-purpose multilingual text encoders `off-the-shelf', but rather relying on their variants that have been further specialized for sentence understanding tasks.",
                        "Citation Paper Authors": "Authors:Robert Litschko, Ivan Vuli\u0107, Simone Paolo Ponzetto, Goran Glava\u0161"
                    }
                },
                {
                    "Sentence ID": 105,
                    "Sentence": ").\nOne important caveat when using these collections is that older test col-\nlections may not include judgments for relevant documents that ar e found by\nnewer systems ",
                    "Citation Text": "J. Lin, R. Nogueira, and A. Yates. Pretrained transformers for text rank-\ning: BERT and beyond, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.06467",
                        "Citation Paper Title": "Title:Pretrained Transformers for Text Ranking: BERT and Beyond",
                        "Citation Paper Abstract": "Abstract:The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.",
                        "Citation Paper Authors": "Authors:Jimmy Lin, Rodrigo Nogueira, Andrew Yates"
                    }
                },
                {
                    "Sentence ID": 100,
                    "Sentence": "compared\nearly and late fusion using two approaches (one with language model context,\none without), \ufb01nding similar improvements over the best single syste m from\nboth approaches. Finally, Li et al. ",
                    "Citation Text": "J. Li, C. Liu, J. Wang, L. Bing, H. Li, X. Liu, D. Zhao, and R. Yan. Cross-\nlingual low-resource set-to-description retrieval for global e-c ommerce.\nInThe Thirty-Fourth AAAI Conference on Arti\ufb01cial Intelligen ce, AAAI\n2020, The Thirty-Second Innovative Applications of Arti\ufb01c ial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium on Educatio nal Ad-\nvances in Arti\ufb01cial Intelligence, EAAI 2020, New York, NY, U SA, Febru-\nary 7-12, 2020 , pages 8212\u20138219. AAAI Press, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.08188",
                        "Citation Paper Title": "Title:Cross-Lingual Low-Resource Set-to-Description Retrieval for Global E-Commerce",
                        "Citation Paper Abstract": "Abstract:With the prosperous of cross-border e-commerce, there is an urgent demand for designing intelligent approaches for assisting e-commerce sellers to offer local products for consumers from all over the world. In this paper, we explore a new task of cross-lingual information retrieval, i.e., cross-lingual set-to-description retrieval in cross-border e-commerce, which involves matching product attribute sets in the source language with persuasive product descriptions in the target language. We manually collect a new and high-quality paired dataset, where each pair contains an unordered product attribute set in the source language and an informative product description in the target language. As the dataset construction process is both time-consuming and costly, the new dataset only comprises of 13.5k pairs, which is a low-resource setting and can be viewed as a challenging testbed for model development and evaluation in cross-border e-commerce. To tackle this cross-lingual set-to-description retrieval task, we propose a novel cross-lingual matching network (CLMN) with the enhancement of context-dependent cross-lingual mapping upon the pre-trained monolingual BERT representations. Experimental results indicate that our proposed CLMN yields impressive results on the challenging task and the context-dependent cross-lingual mapping on BERT yields noticeable improvement over the pre-trained multi-lingual BERT model.",
                        "Citation Paper Authors": "Authors:Juntao Li, Chang Liu, Jian Wang, Lidong Bing, Hongsong Li, Xiaozhong Liu, Dongyan Zhao, Rui Yan"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "2019 50/50/166 (depends on theyear) queries 1.1M Czech, English, French, German,Hungarian, Polish, Spanish, Swedish CC BY-NC 4.032\nMLQA ",
                    "Citation Text": "P. Lewis, B. O\u02d8 guz, R. Rinott, S. Riedel, and H. Schwenk. MLQA:\nEvaluating cross-lingual extractive question answering. arXiv preprint\narXiv:1910.07475 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07475",
                        "Citation Paper Title": "Title:MLQA: Evaluating Cross-lingual Extractive Question Answering",
                        "Citation Paper Abstract": "Abstract:Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.",
                        "Citation Paper Authors": "Authors:Patrick Lewis, Barlas O\u011fuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "extends these full-inte raction-\nbased approaches to CLIR. These models are initialized with fastTex t bilingual\nembeddings aligned using a dictionary ",
                    "Citation Text": "A. Joulin, P. Bojanowski, T. Mikolov, H. J\u00b4 egou, and E. Grave. L oss in\ntranslation: Learning bilingual word mapping with a retrieval criterio n.\nInProceedings of the 2018 Conference on Empirical Methods in N atural\nLanguage Processing , pages 2979\u20132984, Brussels, Belgium, 2018. ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07745",
                        "Citation Paper Title": "Title:Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion",
                        "Citation Paper Abstract": "Abstract:Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a least-square regression problem to learn a rotation aligning a small bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.",
                        "Citation Paper Authors": "Authors:Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Herve Jegou, Edouard Grave"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "extende d the cascadere-\nranking approach to the cross-language setting and found similar im provements\nas in monolingual retrieval, especially on low-resource languages. Jia ng et al. ",
                    "Citation Text": "Z. Jiang, A. El-Jaroudi, W. Hartmann, D. Karakos, and L. Zhao . Cross-\nlingual information retrieval with BERT. In Proceedings of the work-\nshop on Cross-Language Search and Summarization of Text and Speech\n(CLSSTS2020) , pages 26\u201331, Marseille, France, May 2020. European Lan-\nguage Resources Association. ISBN 979-10-95546-55-9.Cross-language Information Retrieval 37",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13005",
                        "Citation Paper Title": "Title:Cross-lingual Information Retrieval with BERT",
                        "Citation Paper Abstract": "Abstract:Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.",
                        "Citation Paper Authors": "Authors:Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos, Lingjun Zhao"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "2020 1190 Q/A pairs 240 paragraphs 11 languages CC-BY-SA 4.035\nXTREME ",
                    "Citation Text": "J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johns on.\nXtreme: A massively multilingual multi-task benchmark for evaluating\ncross-lingual generalization, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11080",
                        "Citation Paper Title": "Title:XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
                        "Citation Paper Abstract": "Abstract:Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",
                        "Citation Paper Authors": "Authors:Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ", and the model learns interactions be tween those\nrepresentations to maximize a relevance objective (DRMM ",
                    "Citation Text": "J. Guo, Y. Fan, Q. Ai, and W. B. Croft. A deep relevance matchin g\nmodel for ad-hoc retrieval. In Proceedings of the 25th ACM International36 P. Galu\u02c7 s\u02c7 c\u00b4 akov\u00b4 a et al.\non Conference on Information and Knowledge Management , pages 55\u201364,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "C. Cieri, D. Gra\ufb00, M. Liberman, N. Martey, and S. Strassel. The TDT-2\ntext andspeechcorpus. Proceedings of DARPA Broadcast News Workshop ,\n08 2000. ",
                    "Citation Text": "K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. ELECTRA: Pr e-\ntraining text encoders as discriminators rather than generators .arXiv\npreprint arXiv:2003.10555 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10555",
                        "Citation Paper Title": "Title:ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
                        "Citation Paper Abstract": "Abstract:Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", used the translations of relevant passages in MS MARCO ",
                    "Citation Text": "L. H. Bonifacio, I. Campiotti, V. Jeronymo, R. Lotufo, and R. N ogueira.\nmmarco: A multilingual version of the MS MARCO passage ranking\ndataset. arXiv preprint arXiv:2108.13897 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.13897",
                        "Citation Paper Title": "Title:mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
                        "Citation Paper Abstract": "Abstract:The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by finetuning monolingual and multilingual reranking models, as well as a multilingual dense retrieval model on this dataset. We also evaluated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our experiments also show that a distilled multilingual reranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and finetuned models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto Lotufo, Rodrigo Nogueira"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "M. Bagdouri, D. W. Oard, and V. Castelli. CLIR for informal cont ent\nin Arabic forum posts. In Proceedings of the 23rd ACM International\nConference on Conference on Information and Knowledge Mana gement,\npages 1811\u20131814, 2014. ",
                    "Citation Text": "D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translatio n by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "2020 differs by task differs by task 40 languages Apache License v2.036\nXOR QA ",
                    "Citation Text": "A. Asai, J. Kasai, J. H. Clark, K. Lee, E. Choi, and H. Hajishirzi. XO R\nQA: Cross-lingual open-retrieval question answering, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11856",
                        "Citation Paper Title": "Title:XOR QA: Cross-lingual Open-Retrieval Question Answering",
                        "Citation Paper Abstract": "Abstract:Multilingual question answering tasks typically assume answers exist in the same language as the question. Yet in practice, many languages face both information scarcity -- where languages have few reference articles -- and information asymmetry -- where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on questions from TyDi QA lacking same-language answers. Our task formulation, called Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k information-seeking questions from across 7 diverse non-English languages. Based on this dataset, we introduce three new tasks that involve cross-lingual document retrieval using multi-lingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "2020 10k Q/A pair per language - 26 languages CC BY-SA34\nXQuAD ",
                    "Citation Text": "M. Artetxe, S. Ruder, and D. Yogatama. On the cross-lingual t ransfer-\nability of monolingual representations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pages 4623\u2013\n4637, Online, July 2020. ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11856",
                        "Citation Paper Title": "Title:On the Cross-lingual Transferability of Monolingual Representations",
                        "Citation Paper Abstract": "Abstract:State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
                        "Citation Paper Authors": "Authors:Mikel Artetxe, Sebastian Ruder, Dani Yogatama"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "M. Artetxe and H. Schwenk. Massively Multilingual Sentence Embe ddings\nfor Zero-Shot Cross-Lingual Transfer and Beyond. Transactions of the\nAssociation for Computational Linguistics ,7:597\u2013610,092019. ISSN2307-\n387X. ",
                    "Citation Text": "M. Artetxe, G. Labaka, and E. Agirre. A robust self-learning me thod for\nfully unsupervised cross-lingual mappings of word embeddings. In Pro-\nceedings of the 56th ACL , pages 789\u2013798, Melbourne, Australia, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06297",
                        "Citation Paper Title": "Title:A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
                        "Citation Paper Abstract": "Abstract:Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at this https URL",
                        "Citation Paper Authors": "Authors:Mikel Artetxe, Gorka Labaka, Eneko Agirre"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "J. Allan and et al. Research frontiers in information retrieval: Rep ort from\nthe third strategic workshop on information retrieval in Lorne (SW IRL\n2018).SIGIR Forum , 52(1):34\u201390, Aug. 2018. ",
                    "Citation Text": "W. Ammar, G. Mulcaire, Y. Tsvetkov, G. Lample, C. Dyer, and N. A .\nSmith. Massively multilingual word embeddings. arXiv:1602.01925 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.01925",
                        "Citation Paper Title": "Title:Massively Multilingual Word Embeddings",
                        "Citation Paper Abstract": "Abstract:We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC-CCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.",
                        "Citation Paper Authors": "Authors:Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, Noah A. Smith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.06286v2": {
            "Paper Title": "Reinforcement learning based recommender systems: A survey",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "and the problem of recommending the best items to a user is\nnot only a prediction problem, but a sequential decision problem ",
                    "Citation Text": "Guy Shani, David Heckerman, and Ronen I Brafman. An mdp-based recommender system. Journal of Machine Learning Research , 6:1265\u20131295,\n2005.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.0600",
                        "Citation Paper Title": "Title:An MDP-based Recommender System",
                        "Citation Paper Abstract": "Abstract:Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.",
                        "Citation Paper Authors": "Authors:Guy Shani, Ronen I. Brafman, David Heckerman"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "is another actor-critic algorithm used by RLRSs. In fact, PPO is an\nimproved version of trust region policy optimization (TRPO) ",
                    "Citation Text": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML\u201915 , pages 1889\u20131897,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05477",
                        "Citation Paper Title": "Title:Trust Region Policy Optimization",
                        "Citation Paper Abstract": "Abstract:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                        "Citation Paper Authors": "Authors:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ". Double DQN ( DDQN )\nwas proposed to alleviate this problem ",
                    "Citation Text": "Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. arXiv preprint arXiv:1509.06461 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.06461",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning with Double Q-learning",
                        "Citation Paper Abstract": "Abstract:The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.",
                        "Citation Paper Authors": "Authors:Hado van Hasselt, Arthur Guez, David Silver"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", to name a few. There are also some published survey papers on topics\nclosely related to RLRSs [ 14,40\u201342]. Perhaps two closest survey papers to ours are [ 14,40]. Ref. ",
                    "Citation Text": "Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. Deep reinforcement learning for search, recommendation, and online advertising: a survey.\nACM SIGWEB Newsletter , pages 1\u201315, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07127",
                        "Citation Paper Title": "Title:Deep reinforcement learning for search, recommendation, and online advertising: a survey",
                        "Citation Paper Abstract": "Abstract:Search, recommendation, and online advertising are the three most important information-providing mechanisms on the web. These information seeking techniques, satisfying users' information needs by suggesting users personalized objects (information or services) at the appropriate time and place, play a crucial role in mitigating the information overload problem. With recent great advances in deep reinforcement learning (DRL), there have been increasing interests in developing DRL based information seeking techniques. These DRL based techniques have two key advantages -- (1) they are able to continuously update information seeking strategies according to users' real-time feedback, and (2) they can maximize the expected cumulative long-term reward from users where reward has different definitions according to information seeking applications such as click-through rate, revenue, user satisfaction and engagement. In this paper, we give an overview of deep reinforcement learning for search, recommendation, and online advertising from methodologies to applications, review representative algorithms, and discuss some appealing research directions.",
                        "Citation Paper Authors": "Authors:Xiangyu Zhao, Long Xia, Jiliang Tang, Dawei Yin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00527v3": {
            "Paper Title": "Boosting Search Engines with Interactive Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13292v3": {
            "Paper Title": "Understanding and Predicting Characteristics of Test Collections in\n  Information Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "report on TREC Million Query track and conclude that WaS\njudging produces a more reliable evaluation of IR systems than NaD judging.\nKutlu et al. ",
                    "Citation Text": "Kutlu, M., Elsayed, T., Lease, M.: Intelligent topic selection for low-cost\ninformation retrieval evaluation: A New perspective on deep vs. shallow\njudging. Information Processing & Management 54(1), 37{59 (Jan 2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.07810",
                        "Citation Paper Title": "Title:Intelligent Topic Selection for Low-Cost Information Retrieval Evaluation: A New Perspective on Deep vs. Shallow Judging",
                        "Citation Paper Abstract": "Abstract:While test collections provide the cornerstone for Cranfield-based evaluation of information retrieval (IR) systems, it has become practically infeasible to rely on traditional pooling techniques to construct test collections at the scale of today's massive document collections. In this paper, we propose a new intelligent topic selection method which reduces the number of search topics needed for reliable IR evaluation. To rigorously assess our method, we integrate previously disparate lines of research on intelligent topic selection and deep vs. shallow judging. While prior work on intelligent topic selection has never been evaluated against shallow judging baselines, prior work on deep vs. shallow judging has largely argued for shallowed judging, but assuming random topic selection. We argue that for evaluating any topic selection method, ultimately one must ask whether it is actually useful to select topics, or should one simply perform shallow judging over many topics? In seeking a rigorous answer to this over-arching question, we conduct a comprehensive investigation over a set of relevant factors never previously studied together 1) topic selection method 2) the effect of topic familiarity on human judging speed and 3) how different topic generation processes impact (i) budget utilization and (ii) the resultant quality of judgments. Experiments on NIST TREC Robust 2003 and Robust 2004 test collections show that not only can we reliably evaluate IR systems with fewer topics, but also that 1) when topics are intelligently selected, deep judging is often more cost-effective than shallow judging in evaluation reliability and 2) topic familiarity and topic generation costs greatly impact the evaluation cost vs. reliability trade-off. Our findings challenge conventional wisdom in showing that deep judging is often preferable to shallow judging when topics are selected intelligently.",
                        "Citation Paper Authors": "Authors:Mucahid Kutlu, Tamer Elsayed, Matthew Lease"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09190v2": {
            "Paper Title": "Supervised Contrastive Learning for Interpretable Long-Form Document\n  Matching",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "layer for aggregation which uses a\nunique \u2018multi-headed chunkwise attention\u2019 component (described below).\nMulti-headed Chunkwise Attention .To get chunk level similarity scores, we introduce a multi-\nheaded chunkwise attention layer. This is a self-attention layer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems . 6000\u20136010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "uses a self-supervised loss function for learning universal sentence embeddings without labels.\nSimCSE ",
                    "Citation Text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings.\narXiv preprint arXiv:2104.08821 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08821",
                        "Citation Paper Title": "Title:SimCSE: Simple Contrastive Learning of Sentence Embeddings",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
                        "Citation Paper Authors": "Authors:Tianyu Gao, Xingcheng Yao, Danqi Chen"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "use self-supervised and supervised contrastive learning frameworks,\nrespectively, to learn meaningful image representations that help in downstream tasks. DeCLUTR ",
                    "Citation Text": "John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised\nTextual Representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . 879\u2013895.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.03659",
                        "Citation Paper Title": "Title:DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations",
                        "Citation Paper Abstract": "Abstract:Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text.",
                        "Citation Paper Authors": "Authors:John Giorgi, Osvald Nitski, Bo Wang, Gary Bader"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "presents a framework that uses\nmulti-step reasoning over key sentences to overcome the insufficient long-range attentions in BERT\nby using text blocks for rehearsal and decay. Transformer-XL ",
                    "Citation Text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL:\nAttentive Language Models beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics . 2978\u20132988.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02860",
                        "Citation Paper Title": "Title:Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                        "Citation Paper Abstract": "Abstract:Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
                        "Citation Paper Authors": "Authors:Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "propose a multi-depth attention based hierarchical recurrent neural network (SMASH)\nfor long-document matching. However, Yang et al. ",
                    "Citation Text": "Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, and Marc Najork. 2020. Beyond 512 Tokens: Siamese\nMulti-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching. In Proceedings of the 29th\nACM International Conference on Information & Knowledge Management . 1725\u20131734.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.12297",
                        "Citation Paper Title": "Title:Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching",
                        "Citation Paper Abstract": "Abstract:Many natural language processing and information retrieval problems can be formalized as the task of semantic matching. Existing work in this area has been largely focused on matching between short texts (e.g., question answering), or between a short and a long text (e.g., ad-hoc retrieval). Semantic matching between long-form documents, which has many important applications like news recommendation, related article recommendation and document clustering, is relatively less explored and needs more research effort. In recent years, self-attention based models like Transformers and BERT have achieved state-of-the-art performance in the task of text matching. These models, however, are still limited to short text like a few sentences or one paragraph due to the quadratic computational complexity of self-attention with respect to input text length. In this paper, we address the issue by proposing the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. Our model contains several innovations to adapt self-attention models for longer text input. In order to better capture sentence level semantic relations within a document, we pre-train the model with a novel masked sentence block language modeling task in addition to the masked word language modeling task used by BERT. Our experimental results on several benchmark datasets for long-form document matching show that our proposed SMITH model outperforms the previous state-of-the-art models including hierarchical attention, multi-depth attention-based hierarchical recurrent neural network, and BERT. Comparing to BERT based baselines, our model is able to increase maximum input text length from 512 to 2048. We will open source a Wikipedia based benchmark dataset, code and a pre-trained checkpoint to accelerate future research on long-form document matching.",
                        "Citation Paper Authors": "Authors:Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "propose a hierarchical attention network for\ndocument classification whereas Adhikari et al. ",
                    "Citation Text": "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin. 2019. Docbert: Bert for document classification.\narXiv preprint arXiv:1904.08398 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08398",
                        "Citation Paper Title": "Title:DocBERT: BERT for Document Classification",
                        "Citation Paper Abstract": "Abstract:We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",
                        "Citation Paper Authors": "Authors:Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "propose a joint deep learning based architecture\nfor ad-hoc retrieval when comparing documents. Several works have also used convolutional\nnetworks [ 18,30,43], with weighting mechanism ",
                    "Citation Text": "Liu Yang, Qingyao Ai, Jiafeng Guo, and W Bruce Croft. 2016. aNMM: Ranking short answer texts with attention-based\nneural matching model. In Proceedings of the 25th ACM international on conference on information and knowledge\nmanagement . 287\u2013296.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01641",
                        "Citation Paper Title": "Title:aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model",
                        "Citation Paper Abstract": "Abstract:As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models that are combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.",
                        "Citation Paper Authors": "Authors:Liu Yang, Qingyao Ai, Jiafeng Guo, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". Section 3\npresents the proposed framework, CoLDE, in detail. Our experimental methodology is discussed in\nSection 4 and Section 5 concludes our work.\n3ACM TKDD, May, 2022 Jha et al.\n2 RELATED WORK\nLong Document Matching. Guo et al. ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval.\nInProceedings of the 25th ACM international on conference on information and knowledge management . 55\u201364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.09177v5": {
            "Paper Title": "Compositional Coding Capsule Network with K-Means Routing for Text\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15609v3": {
            "Paper Title": "BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "pre-trained on a large-scale text-image dataset as a back-\nbone. Frozen uses a transformer-based video model ",
                    "Citation Text": "G. Bertasius, H. Wang, and L. Torresani, \u201cIs space-time attention all you\nneed for video understanding,\u201d arXiv preprint arXiv:2102.05095 , vol. 2,\nno. 3, p. 4, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.05095",
                        "Citation Paper Title": "Title:Is Space-Time Attention All You Need for Video Understanding?",
                        "Citation Paper Abstract": "Abstract:We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Gedas Bertasius, Heng Wang, Lorenzo Torresani"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "to improve the per-\nformance of cross-modal retrieval. Lately, Transformer-based\nworks ",
                    "Citation Text": "H. Luo, L. Ji, M. Zhong, Y . Chen, W. Lei, N. Duan, and T. Li, \u201cClip4clip:\nAn empirical study of clip for end to end video clip retrieval,\u201d arXiv\npreprint arXiv:2104.08860 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08860",
                        "Citation Paper Title": "Title:CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval",
                        "Citation Paper Abstract": "Abstract:Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pre-training), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at this https URL.",
                        "Citation Paper Authors": "Authors:Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "is adopted to extract 1536 -D 2D\nfeatures and the Kinetics ",
                    "Citation Text": "W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\nnarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al. , \u201cThe kinetics\nhuman action video dataset,\u201d arXiv preprint arXiv:1705.06950 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06950",
                        "Citation Paper Title": "Title:The Kinetics Human Action Video Dataset",
                        "Citation Paper Abstract": "Abstract:We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
                        "Citation Paper Authors": "Authors:Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "construct different\nsemantic correlation graphs for videos and learn \ufb01ne-grained\nsemantic relations for text-video retrieval. Some works ",
                    "Citation Text": "S. Chen, Y . Zhao, Q. Jin, and Q. Wu, \u201cFine-grained video-text retrieval\nwith hierarchical graph reasoning,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, (CVPR) , 2020,\npp. 10 635\u201310 644.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00392",
                        "Citation Paper Title": "Title:Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning",
                        "Citation Paper Abstract": "Abstract:Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.",
                        "Citation Paper Authors": "Authors:Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "to extract visual relation\ngraph features. Massive graph construction and graph feature\nextraction are hand-crafted, complex, and time-consuming.\nRecently, the transformer ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances\non Neural Information Processing Systems, (NeurIPS) , 2017, pp. 6000\u2013\n6010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.13537v3": {
            "Paper Title": "Neural Topic Model via Optimal Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11136v2": {
            "Paper Title": "Adversarial Gradient Driven Exploration for Deep Click-Through Rate\n  Prediction",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "adopted the gradient-based neural-UCB and neural Thompson\nSampling for uncertainty estimation. Du et al . ",
                    "Citation Text": "Chao Du, Zhifeng Gao, Shuo Yuan, Lining Gao, Ziyan Li, Yifan Zeng, Xiaoqiang\nZhu, Jian Xu, Kun Gai, and Kuang-Chih Lee. 2021. Exploration in Online Adver-\ntising Systems with Deep Uncertainty-Aware Learning. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2792\u20132801.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02298",
                        "Citation Paper Title": "Title:Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning",
                        "Citation Paper Abstract": "Abstract:Modern online advertising systems inevitably rely on personalization methods, such as click-through rate (CTR) prediction. Recent progress in CTR prediction enjoys the rich representation capabilities of deep learning and achieves great success in large-scale industrial applications. However, these methods can suffer from lack of exploration. Another line of prior work addresses the exploration-exploitation trade-off problem with contextual bandit methods, which are recently less studied in the industry due to the difficulty in extending their flexibility with deep models. In this paper, we propose a novel Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on Gaussian processes, which can provide predictive uncertainty estimations while maintaining the flexibility of deep neural networks. DUAL can be easily implemented on existing models and deployed in real-time systems with minimal extra computational overhead. By linking the predictive uncertainty estimation ability of DUAL to well-known bandit algorithms, we further present DUAL-based Ad-ranking strategies to boost up long-term utilities such as the social welfare in advertising systems. Experimental results on several public datasets demonstrate the effectiveness of our methods. Remarkably, an online A/B test deployed in the Alibaba display advertising platform shows an 8.2% social welfare improvement and an 8.0% revenue lift.",
                        "Citation Paper Authors": "Authors:Chao Du, Zhifeng Gao, Shuo Yuan, Lining Gao, Ziyan Li, Yifan Zeng, Xiaoqiang Zhu, Jian Xu, Kun Gai, Kuang-chih Lee"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", and ap-\nproximate\u00ae\ud835\udc54through one-step update (Equation 8). To improve the\nestimation performance, we further utilize the Project Gradient\nDescent (PGD) ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. 2017. Towards Deep\nLearning Models Resistant to Adversarial Attacks. (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.05025v2": {
            "Paper Title": "Historical Credibility for Movie Reviews and Its Application to Weakly\n  Supervised Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09924v2": {
            "Paper Title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large\n  Web Corpus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10508v4": {
            "Paper Title": "Polar Deconvolution of Mixed Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02275v4": {
            "Paper Title": "A Multi-Strategy based Pre-Training Method for Cold-Start Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "reconstructs graph\nstructure and node feature using side information. GCN-P/COM-P ",
                    "Citation Text": "Zaiqiao Meng, Siwei Liu, Craig Macdonald, and Iadh Ounis. 2021. Graph Neural Pre-training for Enhancing Recommendations using\nSide Information. CoRR abs/2107.03936 (2021). https://arxiv.org/abs/2107.03936",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.03936",
                        "Citation Paper Title": "Title:Graph Neural Pre-training for Enhancing Recommendations using Side Information",
                        "Citation Paper Abstract": "Abstract:Leveraging the side information associated with entities (i.e. users and items) to enhance the performance of recommendation systems has been widely recognized as an important modelling dimension. While many existing approaches focus on the integration scheme to incorporate entity side information -- by combining the recommendation loss function with an extra side information-aware loss -- in this paper, we propose instead a novel pre-training scheme for leveraging the side information. In particular, we first pre-train a representation model using the side information of the entities, and then fine-tune it using an existing general representation-based recommendation model. Specifically, we propose two pre-training models, named GCN-P and COM-P, by considering the entities and their relations constructed from side information as two different types of graphs respectively, to pre-train entity embeddings. For the GCN-P model, two single-relational graphs are constructed from all the users' and items' side information respectively, to pre-train entity representations by using the Graph Convolutional Networks. For the COM-P model, two multi-relational graphs are constructed to pre-train the entity representations by using the Composition-based Graph Convolutional Networks. An extensive evaluation of our pre-training models fine-tuned under four general representation-based recommender models, i.e. MF, NCF, NGCF and LightGCN, shows that effectively pre-training embeddings with both the user's and item's side information can significantly improve these original models in terms of both effectiveness and stability.",
                        "Citation Paper Authors": "Authors:Zaiqiao Meng, Siwei Liu, Craig Macdonald, Iadh Ounis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.05018v4": {
            "Paper Title": "Are Neural Ranking Models Robust?",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "proposed cross domain\nregularization on the ranking models to improve their performance on unforeseen domains. They\nadopted the adversarial learning by using an adversarial discriminator and the gradient reversal\nlayers ",
                    "Citation Text": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario\nMarchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural networks. The journal of machine\nlearning research 17, 1 (2016), 2096\u20132030.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.07818",
                        "Citation Paper Title": "Title:Domain-Adversarial Training of Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
                        "Citation Paper Authors": "Authors:Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Without any special supervision, the ranking\nmodels may come at the cost of poor generalization and performance on examples not observed\nduring training ",
                    "Citation Text": "Daniel Cohen, Bhaskar Mitra, Katja Hofmann, and W Bruce Croft. 2018. Cross domain regularization for neural\nranking models using adversarial learning. In The 41st International ACM SIGIR Conference on Research & Development\nin Information Retrieval . 1025\u20131028.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03403",
                        "Citation Paper Title": "Title:Cross Domain Regularization for Neural Ranking Models Using Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:Unlike traditional learning to rank models that depend on hand-crafted features, neural representation learning models learn higher level features for the ranking task by training on large datasets. Their ability to learn new features directly from the data, however, may come at a price. Without any special supervision, these models learn relationships that may hold only in the domain from which the training data is sampled, and generalize poorly to domains not observed during training. We study the effectiveness of adversarial learning as a cross domain regularizer in the context of the ranking task. We use an adversarial discriminator and train our neural ranking model on a small set of domains. The discriminator provides a negative feedback signal to discourage the model from learning domain specific representations. Our experiments show consistently better performance on held out domains in the presence of the adversarial discriminator---sometimes up to 30% on precision@1.",
                        "Citation Paper Authors": "Authors:Daniel Cohen, Bhaskar Mitra, Katja Hofmann, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "is another popular interaction-focused deep matching\nmodel, which models n-gram soft matches for ad-hoc retrieval based on convolutional\nneural networks (CNN) and kernel-pooling.\n\u2022Hybrid Deep Matching Models include,\n\u2013 Duet : Duet ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using local and distributed representations\nof text for web search. In Proceedings of the 26th International Conference on World Wide Web . 1291\u20131299.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "is a representative-focused deep matching model designed for Web\nsearch, which contains a letter n-gram based word hashing layer, two non-linear hidden\nlayers and an output layer.\n\u2022Interaction-focused Deep Matching Models include,\n\u2013 DRMM : DRMM ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval.\nInProceedings of the 25th ACM International on Conference on Information and Knowledge Management . 55\u201364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". We set the filter size as 10 in both local model and distributed\nmodel, and the hidden size as 20 in the fully-connected layer on MQ2007 dataset following ",
                    "Citation Text": "Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Chengxiang Zhai, and Xueqi Cheng. 2018. Modeling diverse relevance\npatterns in ad-hoc retrieval. In The 41st international ACM SIGIR conference on research & development in information\nretrieval . 375\u2013384.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.05737",
                        "Citation Paper Title": "Title:Modeling Diverse Relevance Patterns in Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:Assessing relevance between a query and a document is challenging in ad-hoc retrieval due to its diverse patterns, i.e., a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' need. Such diverse relevance patterns require an ideal retrieval model to be able to assess relevance in the right granularity adaptively. Unfortunately, most existing retrieval models compute relevance at a single granularity, either document-wide or passage-level, or use fixed combination strategy, restricting their ability in capturing diverse relevance patterns. In this work, we propose a data-driven method to allow relevance signals at different granularities to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals by modeling the semantic matching between a query and each passage of a document. The global decision layer accumulates local signals into different granularities and allows them to compete with each other to decide the final relevance score. Experimental results demonstrate that our HiNT model outperforms existing state-of-the-art retrieval models significantly on benchmark ad-hoc retrieval datasets.",
                        "Citation Paper Authors": "Authors:Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Chengxiang Zhai, Xueqi Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.08735v2": {
            "Paper Title": "SiReN: Sign-Aware Recommendation Using Graph Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07708v2": {
            "Paper Title": "Learning to Retrieve Passages without Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07423v2": {
            "Paper Title": "The Role of Context in Detecting Previously Fact-Checked Claims",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08679v4": {
            "Paper Title": "Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "leveraged hyper-\ngraph to model recommendation data, and proposed to contrast\ndifferent hypergraph structures for representation regularization. In\naddition to the data sparsity problem, Zhou et al. ",
                    "Citation Text": "Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021.\nContrastive learning for debiased candidate generation in large-scale recom-\nmender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining . 3985\u20133995.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12964",
                        "Citation Paper Title": "Title:Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Deep candidate generation (DCG) that narrows down the collection of relevant items from billions to hundreds via representation learning has become prevalent in industrial recommender systems. Standard approaches approximate maximum likelihood estimation (MLE) through sampling for better scalability and address the problem of DCG in a way similar to language modeling. However, live recommender systems face severe exposure bias and have a vocabulary several orders of magnitude larger than that of natural language, implying that MLE will preserve and even exacerbate the exposure bias in the long run in order to faithfully fit the observed samples. In this paper, we theoretically prove that a popular choice of contrastive loss is equivalent to reducing the exposure bias via inverse propensity weighting, which provides a new perspective for understanding the effectiveness of contrastive learning. Based on the theoretical discovery, we design CLRec, a contrastive learning method to improve DCG in terms of fairness, effectiveness and efficiency in recommender systems with extremely large candidate size. We further improve upon CLRec and propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our methods have been successfully deployed in Taobao, where at least four-month online A/B tests and offline analyses demonstrate its substantial improvements, including a dramatic reduction in the Matthew effect.",
                        "Citation Paper Authors": "Authors:Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, Hongxia Yang"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "proposes\nto reorder and crop item segments for sequential data augmentation.\nYuet al. ",
                    "Citation Text": "Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung,\nand Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convo-\nlutional Network for Social Recommendation. In Proceedings of the Web Confer-\nence 2021 . 413\u2013424.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06448",
                        "Citation Paper Title": "Title:Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation",
                        "Citation Paper Abstract": "Abstract:Social relations are often used to improve recommendation quality when user-item interaction data is sparse in recommender systems. Most existing social recommendation models exploit pairwise relations to mine potential user preferences. However, real-life interactions among users are very complicated and user relations can be high-order. Hypergraph provides a natural way to model complex high-order relations, while its potentials for improving social recommendation are under-explored. In this paper, we fill this gap and propose a multi-channel hypergraph convolutional network to enhance social recommendation by leveraging high-order user relations. Technically, each channel in the network encodes a hypergraph that depicts a common high-order user relation pattern via hypergraph convolution. By aggregating the embeddings learned through multiple channels, we obtain comprehensive user representations to generate recommendation results. However, the aggregation operation might also obscure the inherent characteristics of different types of high-order connectivity information. To compensate for the aggregating loss, we innovatively integrate self-supervised learning into the training of the hypergraph convolutional network to regain the connectivity information with hierarchical mutual information maximization. The experimental results on multiple real-world datasets show that the proposed model outperforms the SOTA methods, and the ablation study verifies the effectiveness of the multi-channel setting and the self-supervised task. The implementation of our model is available via this https URL.",
                        "Citation Paper Authors": "Authors:Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, Xiangliang Zhang"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "further propose to mine multiple positive samples with semi-\nsupervised learning on the perturbed graph for social/session-based\nrecommendation. In addition to the dropout, CL4Rec ",
                    "Citation Text": "Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin\nCui. 2020. Contrastive Learning for Sequential Recommendation. arXiv preprint\narXiv:2010.14395 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.14395",
                        "Citation Paper Title": "Title:Contrastive Learning for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:Sequential recommendation methods play a crucial role in modern recommender systems because of their ability to capture a user's dynamic interest from her/his historical interactions. Despite their success, we argue that these approaches usually rely on the sequential prediction task to optimize the huge amounts of parameters. They usually suffer from the data sparsity problem, which makes it difficult for them to learn high-quality user representations. To tackle that, inspired by recent advances of contrastive learning techniques in the computer version, we propose a novel multi-task model called \\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential \\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of the traditional next item prediction task but also utilizes the contrastive learning framework to derive self-supervision signals from the original user behavior sequences. Therefore, it can extract more meaningful user patterns and further encode the user representation effectively. In addition, we propose three data augmentation approaches to construct self-supervision signals. Extensive experiments on four public datasets demonstrate that CL4SRec achieves state-of-the-art performance over existing baselines by inferring better user representations.",
                        "Citation Paper Authors": "Authors:Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, Bin Cui"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "reformulated the cold-start item rep-\nresentation learning from an information-theoretic standpoint and\nmaximized the mutual dependencies between item content and col-\nlaborative signals to alleviate the data sparsity issue. Similar ideas\nare also found in ",
                    "Citation Text": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya\nMenon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al .2021. Self-supervised\nLearning for Large-scale Item Recommendations. In Proceedings of the 30th ACM\nInternational Conference on Information & Knowledge Management . 4321\u20134330.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12865",
                        "Citation Paper Title": "Title:Self-supervised Learning for Large-scale Item Recommendations",
                        "Citation Paper Abstract": "Abstract:Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.\nInspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.\nWe evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",
                        "Citation Paper Authors": "Authors:Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "adopted random\nmasking on attributes and items to create sequence augmenta-\ntions for sequential model pretraining with mutual information\nmaximization. Wei et al. ",
                    "Citation Text": "Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng\nChua. 2021. Contrastive learning for cold-start recommendation. In Proceedings\nof the 29th ACM International Conference on Multimedia . 5382\u20135390.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.05315",
                        "Citation Paper Title": "Title:Contrastive Learning for Cold-Start Recommendation",
                        "Citation Paper Abstract": "Abstract:Recommending cold-start items is a long-standing and fundamental challenge in recommender systems. Without any historical interaction on cold-start items, CF scheme fails to use collaborative signals to infer user preference on these items. To solve this problem, extensive studies have been conducted to incorporate side information into the CF scheme. Specifically, they employ modern neural network techniques (e.g., dropout, consistency constraint) to discover and exploit the coalition effect of content features and collaborative representations. However, we argue that these works less explore the mutual dependencies between content features and collaborative representations and lack sufficient theoretical supports, thus resulting in unsatisfactory performance. In this work, we reformulate the cold-start item representation learning from an information-theoretic standpoint. It aims to maximize the mutual dependencies between item content and collaborative signals. Specifically, the representation learning is theoretically lower-bounded by the integration of two terms: mutual information between collaborative embeddings of users and items, and mutual information between collaborative embeddings and feature representations of items. To model such a learning process, we devise a new objective function founded upon contrastive learning and develop a simple yet effective Contrastive Learning-based Cold-start Recommendation framework(CLCRec). In particular, CLCRec consists of three components: contrastive pair organization, contrastive embedding, and contrastive optimization modules. It allows us to preserve collaborative signals in the content representations for both warm and cold-start items. Through extensive experiments on four publicly accessible datasets, we observe that CLCRec achieves significant improvements over state-of-the-art approaches in both warm- and cold-start scenarios.",
                        "Citation Paper Authors": "Authors:Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ".\n5.2 Contrastive Learning in Recommendation\nAs CL works in a self-supervised manner ",
                    "Citation Text": "Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2022.\nSelf-Supervised Learning for Recommender Systems: A Survey. arXiv preprint\narXiv:2203.15876 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.15876",
                        "Citation Paper Title": "Title:Self-Supervised Learning for Recommender Systems: A Survey",
                        "Citation Paper Abstract": "Abstract:In recent years, neural architecture-based recommender systems have achieved tremendous success, but they still fall short of expectation when dealing with highly sparse data. Self-supervised learning (SSL), as an emerging technique for learning from unlabeled data, has attracted considerable attention as a potential solution to this issue. This survey paper presents a systematic and timely review of research efforts on self-supervised recommendation (SSR). Specifically, we propose an exclusive definition of SSR, on top of which we develop a comprehensive taxonomy to divide existing SSR methods into four categories: contrastive, generative, predictive, and hybrid. For each category, we elucidate its concept and formulation, the involved methods, as well as its pros and cons. Furthermore, to facilitate empirical comparison, we release an open-source library SELFRec (this https URL), which incorporates a wide range of SSR models and benchmark datasets. Through rigorous experiments using this library, we derive and report some significant findings regarding the selection of self-supervised signals for enhancing recommendation. Finally, we shed light on the limitations in the current research and outline the future research directions.",
                        "Citation Paper Authors": "Authors:Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, Zi Huang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", it removes the redundant operations including transforma-\ntion matrices and nonlinear activation functions. Such a design\nis proved efficient and effective, and inspires a lot of follow-up\nCL-based recommendation models like SGL ",
                    "Citation Text": "Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,\nand Xing Xie. 2021. Self-supervised graph learning for recommendation. In\nProceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 726\u2013735.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10783",
                        "Citation Paper Title": "Title:Self-supervised Graph Learning for Recommendation",
                        "Citation Paper Abstract": "Abstract:Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.\nIn this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -- node dropout, edge dropout, and random walk -- that change the graph structure in different manners. We term this new learning paradigm as \\textit{Self-supervised Graph Learning} (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Among these methods, LightGCN is the most popularone due to its simple structure and decent performance. Following ",
                    "Citation Text": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying graph convolutional networks. In International\nconference on machine learning . PMLR, 6861\u20136871.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", as the most prevalent\nvariant of GNNs, further fuels the development of the graph neural\nrecommendation models like GCMC ",
                    "Citation Text": "Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\ntional matrix completion. arXiv preprint arXiv:1706.02263 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.04640v4": {
            "Paper Title": "Rethinking the Optimization of Average Precision: Only Penalizing\n  Negative Instances before Positive Ones is Enough",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08366v3": {
            "Paper Title": "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific\n  Document Similarity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07619v3": {
            "Paper Title": "What Makes a Good and Useful Summary? Incorporating Users in Automatic\n  Summarization Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09791v4": {
            "Paper Title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction\n  for Ad-hoc Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "5.1 Pre-trained Language Models\nPre-training Transformer-based language models on large-scale\nunlabeled text corpora and then fine-tuning on downstream tasks\nhave achieved remarkable success in NLP field [ 4,12,30,37,39]. A\nnotable example is BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . The 58th Annual\nMeeting of the Association for Computational Linguistics, Stroudsburg, PA, USA,\n4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.15814v2": {
            "Paper Title": "Discovering Collaborative Signals for Next POI Recommendation with\n  Iterative Seq2Graph Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07428v2": {
            "Paper Title": "Obtaining Calibrated Probabilities with Personalized Ranking Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06354v3": {
            "Paper Title": "Tell Me How to Survey: Literature Review Made Simple with Automatic\n  Reading Path Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07577v3": {
            "Paper Title": "GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of\n  Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13853v3": {
            "Paper Title": "Pre-training Methods in Information Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00696v2": {
            "Paper Title": "Full-privacy secured search engine empowered by efficient genome-mapping\n  algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08147v2": {
            "Paper Title": "Attacking Black-box Recommendations via Copying Cross-domain User\n  Profiles",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". The popular GNNs model in\nrecommendations, PinSage, for item recommendations ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scalerecommender systems. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . ACM, 974\u2013983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.05552v2": {
            "Paper Title": "Graph Trend Filtering Networks for Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07295v6": {
            "Paper Title": "Towards Resolving Propensity Contradiction in Offline Recommender\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12702v2": {
            "Paper Title": "Extracting and Inferring Personal Attributes from Dialogue",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01746v2": {
            "Paper Title": "Effects of Multi-Aspect Online Reviews with Unobserved Confounders:\n  Estimation and Implication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.06912v2": {
            "Paper Title": "QAConv: Question Answering on Informative Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05819v2": {
            "Paper Title": "Query Obfuscation Semantic Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08564v2": {
            "Paper Title": "Slot Filling for Biomedical Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07701v3": {
            "Paper Title": "Exposing Query Identification for Search Transparency",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", extensively used to compare neu-\nral methods for retrieval, including in the TREC Deep Learning\ntrack ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview\nof the TREC 2019 deep learning track. In TREC .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07820",
                        "Citation Paper Title": "Title:Overview of the TREC 2019 deep learning track",
                        "Citation Paper Abstract": "Abstract:The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the first track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs significantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M. Voorhees"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "develop a framework for suggesting queries\nwhich might retrieve more relevant results for a given query. Nogueira\net al. ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nexpansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ".\nModel-oriented techniques include modifying models to be more\ninterpretable and transparent ",
                    "Citation Text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky,\nYonghui Wu, Petr Mitrichev, Ethan Sterling, Nathan Bell, Walker Ravina, and Hai\nQian. 2020. Interpretable Learning-to-Rank with Generalized Additive Models.\narXiv preprint arXiv:2005.02553 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.02553",
                        "Citation Paper Title": "Title:Interpretable Learning-to-Rank with Generalized Additive Models",
                        "Citation Paper Abstract": "Abstract:Interpretability of learning-to-rank models is a crucial yet relatively under-examined research area. Recent progress on interpretable ranking models largely focuses on generating post-hoc explanations for existing black-box ranking models, whereas the alternative option of building an intrinsically interpretable ranking model with transparent and self-explainable structure remains unexplored. Developing fully-understandable ranking models is necessary in some scenarios (e.g., due to legal or policy constraints) where post-hoc methods cannot provide sufficiently accurate explanations. In this paper, we lay the groundwork for intrinsically interpretable learning-to-rank by introducing generalized additive models (GAMs) into ranking tasks. Generalized additive models (GAMs) are intrinsically interpretable machine learning models and have been extensively studied on regression and classification tasks. We study how to extend GAMs into ranking models which can handle both item-level and list-level features and propose a novel formulation of ranking GAMs. To instantiate ranking GAMs, we employ neural networks instead of traditional splines or regression trees. We also show that our neural ranking GAMs can be distilled into a set of simple and compact piece-wise linear functions that are much more efficient to evaluate with little accuracy loss. We conduct experiments on three data sets and show that our proposed neural ranking GAMs can achieve significantly better performance than other traditional GAM baselines while maintaining similar interpretability.",
                        "Citation Paper Authors": "Authors:Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan Sterling, Nathan Bell, Walker Ravina, Hai Qian"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.07173v2": {
            "Paper Title": "Scene-adaptive Knowledge Distillation for Sequential Recommendation via\n  Differentiable Architecture Search",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "proposed SSE-PT, a\npersonalized transformer model which applies stochastic shared\nembeddings (SSE) regularization to achieve personalized user\nrepresentations. With the advancement on Graph Neural Networks\n(GNN), GNN-based sequential models, such as SR-GNN ",
                    "Citation Text": "S. Wu, Y . Tang, Y . Zhu, L. Wang, X. Xie, and T. Tan, \u201cSession-based\nrecommendation with graph neural networks,\u201d in AAAI , 2019, pp. 346\u2013\n353.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "utilized the popular self-attention mechanism to model long-\nterm sequential semantics by encoding user\u2019s historical behaviors.\nInspired by the great success of BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "regarding its advancement in deep\nlearning (DL).\nDeep neural networks have achieved superior recommenda-\ntion accuracy in SRS tasks. In general, these models could be\nclassi\ufb01ed into three categories, namely RNN-based, CNN-based\nand self-attention based methods. Speci\ufb01cally, Hidasi et al. ",
                    "Citation Text": "B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, \u201cSession-based\nrecommendations with recurrent neural networks,\u201d ICLR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.15352v2": {
            "Paper Title": "Intention Adaptive Graph Neural Network for Category-aware Session-based\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02445v3": {
            "Paper Title": "Overlapping Spaces for Compact Graph Representations",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", and EuCore dataset generated using email data from a large\nEuropean research institution ",
                    "Citation Text": "Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. 2007. Graph evolution: Densi\ufb01cation\nand shrinking diameters. ACM transactions on Knowledge Discovery from Data (TKDD) 1, 1\n(2007), 2\u2013es.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:physics/0603229",
                        "Citation Paper Title": "Title:Graph Evolution: Densification and Shrinking Diameters",
                        "Citation Paper Abstract": "Abstract:  How do real graphs evolve over time? What are ``normal'' growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.\nHere we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).\nExisting graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a ``forest fire'' spreading process, that has a simple, intuitive justification, requires very few parameters (like the ``flammability'' of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.\nWe also notice that the ``forest fire'' model exhibits a sharp transition between sparse graphs and graphs that are densifying. Graphs with decreasing distance between the nodes are generated around this transition point.",
                        "Citation Paper Authors": "Authors:Jure Leskovec, Jon Kleinberg, Christos Faloutsos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13419v2": {
            "Paper Title": "KazNERD: Kazakh Named Entity Recognition Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.15285v2": {
            "Paper Title": "Improving Neural Ranking via Lossless Knowledge Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08597v9": {
            "Paper Title": "Beyond NED: Fast and Effective Search Space Reduction for Complex\n  Question Answering over Knowledge Bases",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed a framework Rel\nfor end-to-end entity linking, building on state-of-the-art neural\ncomponents. Elq ",
                    "Citation Text": "Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih.\n2020. Efficient One-Pass End-to-End Entity Linking for Questions. In EMNLP .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02413",
                        "Citation Paper Title": "Title:Efficient One-Pass End-to-End Entity Linking for Questions",
                        "Citation Paper Abstract": "Abstract:We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever (arXiv:1911.03868). Code and data available at this https URL",
                        "Citation Paper Authors": "Authors:Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, Wen-tau Yih"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ", a mention-entity graph is established, and the mentions\nare disambiguated jointly by approximating the densest subgraph.\nMore recently, van Hulst et al. ",
                    "Citation Text": "Johannes M van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, and\nArjen P de Vries. 2020. REL: An Entity Linker Standing on the Shoulders of\nGiants. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.01969",
                        "Citation Paper Title": "Title:REL: An Entity Linker Standing on the Shoulders of Giants",
                        "Citation Paper Abstract": "Abstract:Entity linking is a standard component in modern retrieval system that is often performed by third-party toolkits. Despite the plethora of open source options, it is difficult to find a single system that has a modular architecture where certain components may be replaced, does not depend on external sources, can easily be updated to newer Wikipedia versions, and, most important of all, has state-of-the-art performance. The REL system presented in this paper aims to fill that gap. Building on state-of-the-art neural components from natural language processing research, it is provided as a Python package as well as a web API. We also report on an experimental comparison against both well-established systems and the current state-of-the-art on standard entity linking benchmarks.",
                        "Citation Paper Authors": "Authors:Johannes M. van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, Arjen P. de Vries"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10922v4": {
            "Paper Title": "OntoED: Low-resource Event Detection with Ontology Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.11059v2": {
            "Paper Title": "Exploring Heterogeneous Metadata for Video Recommendation with Two-tower\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02260v4": {
            "Paper Title": "Graph Neural Networks in Recommender Systems: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04387v5": {
            "Paper Title": "Recent Advances in Deep Learning Based Dialogue Systems: A Systematic\n  Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06712v3": {
            "Paper Title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural\n  Language Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07323v2": {
            "Paper Title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04016v2": {
            "Paper Title": "DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance\n  Improves Out-Of-Distribution Face Identification",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": ").\nIn contrast, here, we demonstrate a substantial improvement\nin FI using EMD without re-training the feature extractors.\nConcurrent to our work, Zhao et al. ",
                    "Citation Text": "Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, and\nJie Zhou. Towards interpretable deep metric learning with\nstructural matching. In ICCV , 2021. 2, 3, 4, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.05889",
                        "Citation Paper Title": "Title:Towards Interpretable Deep Metric Learning with Structural Matching",
                        "Citation Paper Abstract": "Abstract:How do the neural networks distinguish two images? It is of critical importance to understand the matching mechanism of deep models for developing reliable intelligent systems for many risky visual applications such as surveillance and access control. However, most existing deep metric learning methods match the images by comparing feature vectors, which ignores the spatial structure of images and thus lacks interpretability. In this paper, we present a deep interpretable metric learning (DIML) method for more transparent embedding learning. Unlike conventional metric learning methods based on feature vector comparison, we propose a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. Our method enables deep models to learn metrics in a more human-friendly way, where the similarity of two images can be decomposed to several part-wise similarities and their contributions to the overall similarity. Our method is model-agnostic, which can be applied to off-the-shelf backbone networks and metric learning methods. We evaluate our method on three major benchmarks of deep metric learning including CUB200-2011, Cars196, and Stanford Online Products, and achieve substantial improvements over popular metric learning methods with better interpretability. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, Jie Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09663v2": {
            "Paper Title": "EILEEN: A recommendation system for scientific publications and grants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07404v2": {
            "Paper Title": "Two Birds with One Stone: Unified Model Learning for Both Recall and\n  Ranking in News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07407v2": {
            "Paper Title": "MM-Rec: Multimodal News Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ", learning news representations with two parallel\nCNN networks from news title and entities; (4) NAML ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\nand Xing Xie. 2019. Neural News Recommendation with Attentive Multi-View\nLearning. In IJCAI . 3863\u20133869.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", a pre-trained\nlanguage empowered approach for news recommendation. We use\nBERT ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering\nNews Recommendation with Pre-trained Language Models. In SIGIR . 1652\u20131656.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07413",
                        "Citation Paper Title": "Title:Empowering News Recommendation with Pre-trained Language Models",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is an essential technique for online news services. News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation. Existing news recommendation methods mainly model news texts based on traditional text modeling methods, which is not optimal for mining the deep semantic information in news texts. Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling. However, there is no public report that show PLMs have been applied to news recommendation. In this paper, we report our work on exploiting pre-trained language models to empower news recommendation. Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation. Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", using multi-head self-attention networks to learn news\nrepresentations; (6) GERL ",
                    "Citation Text": "Suyu Ge, Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2020. Graph\nenhanced representation learning for news recommendation. In WWW . 2863\u2013\n2869.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14292",
                        "Citation Paper Title": "Title:Graph Enhanced Representation Learning for News Recommendation",
                        "Citation Paper Abstract": "Abstract:With the explosion of online news, personalized news recommendation becomes increasingly important for online news platforms to help their users find interesting information. Existing news recommendation methods achieve personalization by building accurate news representations from news content and user representations from their direct interactions with news (e.g., click), while ignoring the high-order relatedness between users and news. Here we propose a news recommendation method which can enhance the representation learning of users and news by modeling their relatedness in a graph setting. In our method, users and news are both viewed as nodes in a bipartite graph constructed from historical user click behaviors. For news representations, a transformer architecture is first exploited to build news semantic representations. Then we combine it with the information from neighbor news in the graph via a graph attention network. For user representations, we not only represent users from their historically clicked news, but also attentively incorporate the representations of their neighbor users in the graph. Improved performances on a large-scale real-world dataset validate the effectiveness of our proposed method.",
                        "Citation Paper Authors": "Authors:Suyu Ge, Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ", learning news embeddings via\nautoencoder and user embeddings with a GRU network; (2) DKN ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nKnowledge-Aware Network for News Recommendation. In WWW . 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.10780v3": {
            "Paper Title": "An Open Natural Language Processing Development Framework for EHR-based\n  Clinical Research: A case demonstration using the National COVID Cohort\n  Collaborative (N3C)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00197v2": {
            "Paper Title": "Task-adaptive Asymmetric Deep Cross-modal Hashing",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "proposes a novel multi-task learning scheme that leverages multi-view atten-\ntion mechanism to interact with different tasks and learn more comprehensive sentence\nrepresentations. Multi-Task Field-weighted Factorization Machine (MT-FwFM) ",
                    "Citation Text": "J. Pan, Y . Mao, A. L. Ruiz, Y . Sun, A. Flores, Predicting different types of conver-\nsions with multi-task learning in online advertising, in: Proceedings of the ACM\nInternational Conference on Knowledge Discovery and Data Mining (SIGKDD),\n2019, pp. 2689\u20132697.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10235",
                        "Citation Paper Title": "Title:Predicting Different Types of Conversions with Multi-Task Learning in Online Advertising",
                        "Citation Paper Abstract": "Abstract:Conversion prediction plays an important role in online advertising since Cost-Per-Action (CPA) has become one of the primary campaign performance objectives in the industry. Unlike click prediction, conversions have different types in nature, and each type may be associated with different decisive factors. In this paper, we formulate conversion prediction as a multi-task learning problem, so that the prediction models for different types of conversions can be learned together. These models share feature representations, but have their specific parameters, providing the benefit of information-sharing across all tasks. We then propose Multi-Task Field-weighted Factorization Machine (MT-FwFM) to solve these tasks jointly. Our experiment results show that, compared with two state-of-the-art models, MT-FwFM improve the AUC by 0.74% and 0.84% on two conversion types, and the weighted AUC across all conversion types is also improved by 0.50%.",
                        "Citation Paper Authors": "Authors:Junwei Pan, Yizhi Mao, Alfonso Lobos Ruiz, Yu Sun, Aaron Flores"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.06008v2": {
            "Paper Title": "Cross-Domain Multi-Task Learning for Sequential Sentence Classification\n  in Research Papers",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "Biomedicine 1,000 abstracts Background, Intervention, Study, Population, Outcome, Other\nCSABSTRUCT ",
                    "Citation Text": "Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Daniel S. Weld. 2019.\nPretrained Language Models for Sequential Sentence Classification. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019 , Kentaro Inui, Jing Jiang,\nVincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics,\n3691\u20133697. https://doi.org/10.18653/v1/D19-1383",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04054",
                        "Citation Paper Title": "Title:Pretrained Language Models for Sequential Sentence Classification",
                        "Citation Paper Abstract": "Abstract:As a step toward better document-level understanding, we explore classification of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful models for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four datasets, including a new dataset of structured scientific abstracts.",
                        "Citation Paper Authors": "Authors:Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Daniel S. Weld"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "utilise a multi-task model for entity recognition and\nrelation extraction on one dataset in the non-academic domain.\nChangpinyo et al . ",
                    "Citation Text": "Soravit Changpinyo, Hexiang Hu, and Fei Sha. 2018. Multi-Task Learning for\nSequence Tagging: An Empirical Study. In Proceedings of the 27th International\nConference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico,\nUSA, August 20-26, 2018 , Emily M. Bender, Leon Derczynski, and Pierre Isabelle\n(Eds.). Association for Computational Linguistics, 2965\u20132977. https://www.\naclweb.org/anthology/C18-1251/",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04151",
                        "Citation Paper Title": "Title:Multi-Task Learning for Sequence Tagging: An Empirical Study",
                        "Citation Paper Abstract": "Abstract:We study three general multi-task learning (MTL) approaches on 11 sequence tagging tasks. Our extensive empirical results show that in about 50% of the cases, jointly learning all 11 tasks improves upon either independent or pairwise learning of the tasks. We also show that pairwise MTL can inform us what tasks can benefit others or what tasks can be benefited if they are learned jointly. In particular, we identify tasks that can always benefit others as well as tasks that can always be harmed by others. Interestingly, one of our MTL approaches yields embeddings of the tasks that reveal the natural clustering of semantic and syntactic tasks. Our inquiries have opened the doors to further utilization of MTL in NLP.",
                        "Citation Paper Authors": "Authors:Soravit Changpinyo, Hexiang Hu, Fei Sha"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "train a model on three tasks (coreference resolution,\nentity and relation extraction) using one dataset of research papers.\nWei et al . ",
                    "Citation Text": "Zhepei Wei, Yantao Jia, Yuan Tian, Mohammad Javad Hosseini, Mark Steedman,\nand Yi Chang. 2019. Joint Extraction of Entities and Relations with a Hierarchical\nMulti-task Tagging Model. CoRR abs/1908.08672 (2019). arXiv:1908.08672 http:\n//arxiv.org/abs/1908.08672",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.08672",
                        "Citation Paper Title": "Title:Jointly Modeling Hierarchical and Horizontal Features for Relational Triple Extraction",
                        "Citation Paper Abstract": "Abstract:Recent works on relational triple extraction have shown the superiority of jointly extracting entities and relations over the pipelined extraction manner. However, most existing joint models fail to balance the modeling of entity features and the joint decoding strategy, and thus the interactions between the entity level and triple level are not fully investigated. In this work, we first introduce the hierarchical dependency and horizontal commonality between the two levels, and then propose an entity-enhanced dual tagging framework that enables the triple extraction (TE) task to utilize such interactions with self-learned entity features through an auxiliary entity extraction (EE) task, without breaking the joint decoding of relational triples. Specifically, we align the EE and TE tasks in a position-wise manner by formulating them as two sequence labeling problems with identical encoder-decoder structure. Moreover, the two tasks are organized in a carefully designed parameter sharing setting so that the learned entity features could be naturally shared via multi-task learning. Empirical experiments on the NYT benchmark demonstrate the effectiveness of the proposed framework compared to the state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Zhepei Wei, Yantao Jia, Yuan Tian, Mohammad Javad Hosseini, Sujian Li, Mark Steedman, Yi Chang"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "utilise a multi-task learning with two scaffold tasks to\ndetect contribution sentences in full papers, however, only in one\ndomain and with limited sentence context.\nSeveral approaches also exist to train multiple tasks jointly : Luan\net al. ",
                    "Citation Text": "Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-Task\nIdentification of Entities, Relations, and Coreference for Scientific Knowledge\nGraph Construction. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 ,\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (Eds.). Associa-\ntion for Computational Linguistics, 3219\u20133232. https://doi.org/10.18653/v1/d18-\n1360",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.09602",
                        "Citation Paper Title": "Title:Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
                        "Citation Paper Abstract": "Abstract:We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SciIE) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",
                        "Citation Paper Authors": "Authors:Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "apply sequential\ntransfer learning from the medical to the computer science domain\nfor discourse classification, however, only for two domains and on\nabstracts, whereas Spangher et al . ",
                    "Citation Text": "Alexander Spangher, Jonathan May, Sz-Rung Shiang, and Lingjia Deng. 2021.\nMultitask Semi-Supervised Learning for Class-Imbalanced Discourse Classifi-\ncation. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,\n7-11 November, 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih (Eds.). Association for Computational Linguistics, 498\u2013517.\nhttps://aclanthology.org/2021.emnlp-main.40",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.00389",
                        "Citation Paper Title": "Title:Multitask Learning for Class-Imbalanced Discourse Classification",
                        "Citation Paper Abstract": "Abstract:Small class-imbalanced datasets, common in many high-level semantic tasks like discourse analysis, present a particular challenge to current deep-learning architectures. In this work, we perform an extensive analysis on sentence-level classification approaches for the News Discourse dataset, one of the largest high-level semantic discourse datasets recently published. We show that a multitask approach can improve 7% Micro F1-score upon current state-of-the-art benchmarks, due in part to label corrections across tasks, which improve performance for underrepresented classes. We also offer a comparative review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in such a setting.",
                        "Citation Paper Authors": "Authors:Alexander Spangher, Jonathan May, Sz-rung Shiang, Lingjia Deng"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "evaluate\nmulti-task learning on scientific texts, however, only on one dataset\nwith different annotation layers. Banerjee et al . ",
                    "Citation Text": "Soumya Banerjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay, Plaban Ku-\nmar Bhowmick, and Partha Pratim Das. 2020. Segmenting Scientific Abstracts\ninto Discourse Categories: A Deep Learning-Based Approach for Sparse Labeled\nData. In JCDL \u201920: Proceedings of the ACM/IEEE Joint Conference on Digital Li-\nbraries in 2020, Virtual Event, China, August 1-5, 2020 , Ruhua Huang, Dan Wu,\nGary Marchionini, Daqing He, Sally Jo Cunningham, and Preben Hansen (Eds.).\nACM, 429\u2013432. https://doi.org/10.1145/3383583.3398598",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.05414",
                        "Citation Paper Title": "Title:Segmenting Scientific Abstracts into Discourse Categories: A Deep Learning-Based Approach for Sparse Labeled Data",
                        "Citation Paper Abstract": "Abstract:The abstract of a scientific paper distills the contents of the paper into a short paragraph. In the biomedical literature, it is customary to structure an abstract into discourse categories like BACKGROUND, OBJECTIVE, METHOD, RESULT, and CONCLUSION, but this segmentation is uncommon in other fields like computer science. Explicit categories could be helpful for more granular, that is, discourse-level search and recommendation. The sparsity of labeled data makes it challenging to construct supervised machine learning solutions for automatic discourse-level segmentation of abstracts in non-bio domains. In this paper, we address this problem using transfer learning. In particular, we define three discourse categories BACKGROUND, TECHNIQUE, OBSERVATION-for an abstract because these three categories are the most common. We train a deep neural network on structured abstracts from PubMed, then fine-tune it on a small hand-labeled corpus of computer science papers. We observe an accuracy of 75% on the test corpus. We perform an ablation study to highlight the roles of the different parts of the model. Our method appears to be a promising solution to the automatic segmentation of abstracts, where the labeled data is sparse.",
                        "Citation Paper Authors": "Authors:Soumya Banerjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay, Plaban Kumar Bhowmick, Parthapratim Das"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "apply sequential transfer learning and\nutilise a large dataset from the general domain to improve models\nfor a small dataset in the scientific domain.\nForsentence classification , Mou et al . ",
                    "Citation Text": "Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016.\nHow Transferable are Neural Networks in NLP Applications?. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2016, Austin, Texas, USA, November 1-4, 2016 , Jian Su, Xavier Carreras,\nand Kevin Duh (Eds.). The Association for Computational Linguistics, 479\u2013489.\nhttps://doi.org/10.18653/v1/d16-1046",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06111",
                        "Citation Paper Title": "Title:How Transferable are Neural Networks in NLP Applications?",
                        "Citation Paper Abstract": "Abstract:Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.",
                        "Citation Paper Authors": "Authors:Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "successfully transfer pre-trained parameters from a big\ndataset to a small dataset in the biological domain. For coreference\nresolution , Brack et al . ",
                    "Citation Text": "Arthur Brack, Daniel Uwe M\u00fcller, Anett Hoppe, and Ralph Ewerth. 2021. Coref-\nerence Resolution in Research Papers from Multiple Domains. In Advances in\nInformation Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual\nEvent, March 28 - April 1, 2021, Proceedings, Part I (Lecture Notes in Computer\nScience, Vol. 12656) , Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe,\nRaffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer, 79\u201397.\nhttps://doi.org/10.1007/978-3-030-72113-8_6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.00884",
                        "Citation Paper Title": "Title:Coreference Resolution in Research Papers from Multiple Domains",
                        "Citation Paper Abstract": "Abstract:Coreference resolution is essential for automatic text understanding to facilitate high-level information retrieval tasks such as text summarisation or question answering. Previous work indicates that the performance of state-of-the-art approaches (e.g. based on BERT) noticeably declines when applied to scientific papers. In this paper, we investigate the task of coreference resolution in research papers and subsequent knowledge graph population. We present the following contributions: (1) We annotate a corpus for coreference resolution that comprises 10 different scientific disciplines from Science, Technology, and Medicine (STM); (2) We propose transfer learning for automatic coreference resolution in research papers; (3) We analyse the impact of coreference resolution on knowledge graph (KG) population; (4) We release a research KG that is automatically populated from 55,485 papers in 10 STM domains. Comprehensive experiments show the usefulness of the proposed approach. Our transfer learning approach considerably outperforms state-of-the-art baselines on our corpus with an F1 score of 61.4 (+11.0), while the evaluation against a gold standard KG shows that coreference resolution improves the quality of the populated KG significantly with an F1 score of 63.5 (+21.8).",
                        "Citation Paper Authors": "Authors:Arthur Brack, Daniel Uwe M\u00fcller, Anett Hoppe, Ralph Ewerth"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "investigate multi-task learn-\ning in the general non-academic domain with a small and a big\ndataset. Schulz et al . ",
                    "Citation Text": "Claudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse, and Iryna\nGurevych. 2018. Multi-Task Learning for Argumentation Mining in Low-Resource\nSettings. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Pa-\npers) , Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for\nComputational Linguistics, 35\u201341. https://doi.org/10.18653/v1/n18-2006",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04083",
                        "Citation Paper Title": "Title:Multi-Task Learning for Argumentation Mining in Low-Resource Settings",
                        "Citation Paper Abstract": "Abstract:We investigate whether and where multi-task learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.",
                        "Citation Paper Authors": "Authors:Claudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse, Iryna Gurevych"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.06884v2": {
            "Paper Title": "Towards Equivalent Transformation of User Preferences in Cross Domain\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01206v3": {
            "Paper Title": "Local Citation Recommendation with Hierarchical-Attention Text Encoder\n  and SciBERT-based Reranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00370v3": {
            "Paper Title": "Axiomatic Explanations for Visual Search, Retrieval, and Similarity\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06732v3": {
            "Paper Title": "Efficient Transformers: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.11303v2": {
            "Paper Title": "CancerBERT: a BERT model for Extracting Breast Cancer Phenotypes from\n  Electronic Health Records",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05205v2": {
            "Paper Title": "Contrastive Quantization with Code Memory for Unsupervised Image\n  Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08087v6": {
            "Paper Title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04833v3": {
            "Paper Title": "Towards Open-World Recommendation: An Inductive Model-based\n  Collaborative Filtering Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08186v4": {
            "Paper Title": "Fast-Slow Transformer for Visually Grounding Speech",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nSemantic\" ABX#\nModel Syn. Lib. W-C W-O A-C A-O ",
                    "Citation Text": "B. v. Niekerk, L. Nortje, M. Baas, and H. Kamper, \u201cAnalyzing\nspeaker information in self-supervised models to improve zero-\nresource speech processing,\u201d in INTERSPEECH , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00917",
                        "Citation Paper Title": "Title:Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
                        "Citation Paper Abstract": "Abstract:Contrastive predictive coding (CPC) aims to learn representations of speech by distinguishing future observations from a set of negative examples. Previous work has shown that linear classifiers trained on CPC features can accurately predict speaker and phone labels. However, it is unclear how the features actually capture speaker and phonetic information, and whether it is possible to normalize out the irrelevant details (depending on the downstream task). In this paper, we first show that the per-utterance mean of CPC features captures speaker information to a large extent. Concretely, we find that comparing means performs well on a speaker verification task. Next, probing experiments show that standardizing the features effectively removes speaker information. Based on this observation, we propose a speaker normalization step to improve acoustic unit discovery using K-means clustering of CPC features. Finally, we show that a language model trained on the resulting units achieves some of the best results in the ZeroSpeech2021~Challenge.",
                        "Citation Paper Authors": "Authors:Benjamin van Niekerk, Leanne Nortje, Matthew Baas, Herman Kamper"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "dataset and each\nimage has a single corresponding audio caption that was\nrecorded spontaneously by a human as they viewed the\nimage. Both SpokenCOCO and FACC were produced by\nhaving humans speak aloud the text captions accompany-\ning the MSCOCO ",
                    "Citation Text": "T.-Y . Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common\nobjects in context,\u201d in ECCV , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00062v2": {
            "Paper Title": "Shallow pooling for sparse labels",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "2.1 MS MARCO\nA recent perspective paper by Craswell et al. ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. MS MARCO: Bench-\nmarking ranking models in the large-data regime. In 44th Annual International ACM SIGIR Conference\non Research and Development in Infomation Retrieval , pages 1566{1576, Montreal, Canada, July 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04021",
                        "Citation Paper Title": "Title:MS MARCO: Benchmarking Ranking Models in the Large-Data Regime",
                        "Citation Paper Abstract": "Abstract:Evaluation efforts such as TREC, CLEF, NTCIR and FIRE, alongside public leaderboard such as MS MARCO, are intended to encourage research and track our progress, addressing big questions in our field. However, the goal is not simply to identify which run is \"best\", achieving the top score. The goal is to move the field forward by developing new robust techniques, that work in many different settings, and are adopted in research and practice. This paper uses the MS MARCO and TREC Deep Learning Track as our case study, comparing it to the case of TREC ad hoc ranking in the 1990s. We show how the design of the evaluation effort can encourage or discourage certain outcomes, and raising questions about internal and external validity of results. We provide some analysis of certain pitfalls, and a statement of best practices for avoiding such pitfalls. We summarize the progress of the effort so far, and describe our desired end state of \"robust usefulness\", along with steps that might be required to get us there.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.06852v5": {
            "Paper Title": "Self-Supervised Hypergraph Convolutional Networks for Session-based\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06448v4": {
            "Paper Title": "Self-Supervised Multi-Channel Hypergraph Convolutional Network for\n  Social Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08934v3": {
            "Paper Title": "Personalized News Recommendation: Methods and Challenges",
            "Sentences": [
                {
                    "Sentence ID": 159,
                    "Sentence": ". For\nexample, since many news recommendation methods are learned on private user data, it is impor-\ntant to protect user privacy in recommendation model training and online serving ",
                    "Citation Text": "Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. 2021. Uni-FedRec: A Unified Privacy-Preserving\nNews Recommendation Framework for Model Training and Online Serving. In EMNLP: Findings . 1438\u20131448.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05236",
                        "Citation Paper Title": "Title:Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving",
                        "Citation Paper Abstract": "Abstract:News recommendation is important for personalized online news services. Most existing news recommendation methods rely on centrally stored user behavior data to both train models offline and provide online recommendation services. However, user data is usually highly privacy-sensitive, and centrally storing them may raise privacy concerns and risks. In this paper, we propose a unified news recommendation framework, which can utilize user data locally stored in user clients to train models and serve users in a privacy-preserving way. Following a widely used paradigm in real-world recommender systems, our framework contains two stages. The first one is for candidate news generation (i.e., recall) and the second one is for candidate news ranking (i.e., ranking). At the recall stage, each client locally learns multiple interest representations from clicked news to comprehensively model user interests. These representations are uploaded to the server to recall candidate news from a large news pool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy.",
                        "Citation Paper Authors": "Authors:Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 107,
                    "Sentence": ", which may better\nindicate the real performance of the recommender systems. To fill the gaps between offline and\nonline experiments, one prior study ",
                    "Citation Text": "Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. 2011. Unbiased offline evaluation of contextual-bandit-based\nnews article recommendation algorithms. In WSDM . 297\u2013306.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.5956",
                        "Citation Paper Title": "Title:Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms",
                        "Citation Paper Abstract": "Abstract:Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. \\emph{Offline} evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their \"partial-label\" nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a \\emph{replay} methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.",
                        "Citation Paper Authors": "Authors:Lihong Li, Wei Chu, John Langford, Xuanhui Wang"
                    }
                },
                {
                    "Sentence ID": 214,
                    "Sentence": "proposed to learn news representations\nfrom news titles via a combination of multi-head self-attention and additive attention networks.\nWu et al. ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering News Recommendation with Pre-trained\nLanguage Models. In SIGIR . 1652\u20131656.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07413",
                        "Citation Paper Title": "Title:Empowering News Recommendation with Pre-trained Language Models",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is an essential technique for online news services. News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation. Existing news recommendation methods mainly model news texts based on traditional text modeling methods, which is not optimal for mining the deep semantic information in news texts. Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling. However, there is no public report that show PLMs have been applied to news recommendation. In this paper, we report our work on exploiting pre-trained language models to empower news recommendation. Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation. Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang"
                    }
                },
                {
                    "Sentence ID": 204,
                    "Sentence": "proposed to use a GRU network to learn user\nrepresentations from clicked news. Wu et al. ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. Npa: Neural news\nrecommendation with personalized attention. In KDD . 2576\u20132584.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05559",
                        "Citation Paper Title": "Title:NPA: Neural News Recommendation with Personalized Attention",
                        "Citation Paper Abstract": "Abstract:News recommendation is very important to help users find interested news and alleviate information overload. Different users usually have different interests and the same user may have various interests. Thus, different users may click the same news article with attention on different aspects. In this paper, we propose a neural news recommendation model with personalized attention (NPA). The core of our approach is a news representation model and a user representation model. In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. In the user representation model we learn the representations of users based on the representations of their clicked news articles. Since different words and different news articles may have different informativeness for representing news and users, we propose to apply both word- and news-level attention mechanism to help our model attend to important words and news articles. In addition, the same news article and the same word may have different informativeness for different users. Thus, we propose a personalized attention network which exploits the embedding of user ID to generate the query vector for the word- and news-level attentions. Extensive experiments are conducted on a real-world news recommendation dataset collected from MSN news, and the results validate the effectiveness of our approach on news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 197,
                    "Sentence": "proposed to use autoencoders to learn news representations from news content. Wang et al. ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep knowledge-aware network for news\nrecommendation. In WWW . 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                },
                {
                    "Sentence ID": 106,
                    "Sentence": ", which may cause the \u201cfilter bubble\u201d problem.\nA few methods use reinforcement learning for personalized ranking. Li et al. ",
                    "Citation Text": "Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized news\narticle recommendation. In WWW . 661\u2013670.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.0146",
                        "Citation Paper Title": "Title:A Contextual-Bandit Approach to Personalized News Article Recommendation",
                        "Citation Paper Abstract": "Abstract:Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\nIn this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.\nThe contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.",
                        "Citation Paper Authors": "Authors:Lihong Li, Wei Chu, John Langford, Robert E. Schapire"
                    }
                },
                {
                    "Sentence ID": 160,
                    "Sentence": "proposed a personalized attention network to\nlearn user representations from clicked news in a personalized manner. Qi et al. ",
                    "Citation Text": "Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, and Yongfeng Huang. 2021. HieRec: Hierarchical\nUser Interest Modeling for Personalized News Recommendation. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.04408",
                        "Citation Paper Title": "Title:HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation",
                        "Citation Paper Abstract": "Abstract:User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.",
                        "Citation Paper Authors": "Authors:Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, Yongfeng Huang"
                    }
                },
                {
                    "Sentence ID": 207,
                    "Sentence": "proposed to use a knowledge-aware convolutional neural network (CNN) to learn news representa-\ntions from news titles and their entities. Wu et al. ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural News Recommendation\nwith Multi-Head Self-Attention. In EMNLP-IJCNLP . 6390\u20136395.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.01649v4": {
            "Paper Title": "Heterogeneous Graph based Deep Learning for Biomedical Network Link\n  Prediction",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", mean-pooling and attention mechanisms are used as A(\u0001), whereas GRU ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv\npreprint arXiv:1511.05493 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.14282v3": {
            "Paper Title": "Active Learning for Human-in-the-Loop Customs Inspection",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ". Recent research has also focused\non the concurrent inclusion of uncertainty and diversity\naspects ",
                    "Citation Text": "J. T. Ash, C. Zhang, A. Krishnamurthy, J. Langford, and A. Agar-\nwal, \u201cDeep batch active learning by diverse, uncertain gradient\nlower bounds,\u201d in ICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03671",
                        "Citation Paper Title": "Title:Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",
                        "Citation Paper Abstract": "Abstract:We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a versatile option for practical active learning problems.",
                        "Citation Paper Authors": "Authors:Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". It has been utilized in training models to\ndeal with high-dimensional data ",
                    "Citation Text": "Y. Gal, R. Islam, and Z. Ghahramani, \u201cDeep bayesian active\nlearning with image data,\u201d in ICML , 2017, pp. 1183\u20131192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02910",
                        "Citation Paper Title": "Title:Deep Bayesian Active Learning with Image Data",
                        "Citation Paper Abstract": "Abstract:Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",
                        "Citation Paper Authors": "Authors:Yarin Gal, Riashat Islam, Zoubin Ghahramani"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Voting schemes have also been applied to manage\nbase classi\ufb01ers by adding them to ensembles ",
                    "Citation Text": "Y. Xu, R. Xu, W. Yan, and P . A. Ardis, \u201cConcept drift learning with\nalternating learners,\u201d in IJCNN , 2017, p. 2104\u20132111.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06940",
                        "Citation Paper Title": "Title:Concept Drift Learning with Alternating Learners",
                        "Citation Paper Abstract": "Abstract:Data-driven predictive analytics are in use today across a number of industrial applications, but further integration is hindered by the requirement of similarity among model training and test data distributions. This paper addresses the need of learning from possibly nonstationary data streams, or under concept drift, a commonly seen phenomenon in practical applications. A simple dual-learner ensemble strategy, alternating learners framework, is proposed. A long-memory model learns stable concepts from a long relevant time window, while a short-memory model learns transient concepts from a small recent window. The difference in prediction performance of these two models is monitored and induces an alternating policy to select, update and reset the two models. The method features an online updating mechanism to maintain the ensemble accuracy, and a concept-dependent trigger to focus on relevant data. Through empirical studies the method demonstrates effective tracking and prediction when the steaming data carry abrupt and/or gradual changes.",
                        "Citation Paper Authors": "Authors:Yunwen Xu, Rui Xu, Weizhong Yan, Paul Ardis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.08535v3": {
            "Paper Title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02160v2": {
            "Paper Title": "Urban Fire Station Location Planning using Predicted Demand and Service\n  Quality Index",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09520v2": {
            "Paper Title": "NETR-Tree: An Eifficient Framework for Social-Based Time-Aware Spatial\n  Keyword Query",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "exploits the random walk algorithm to generate\nsequences of instances to obtain the embedding result vectors\nof nodes. LINE ",
                    "Citation Text": "J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \u201cLine:\nLarge-scale information network embedding,\u201d in ACM International\nConference on World Wide Web (WWW) , 2015, pp. 1067\u20131077.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "has made major strides. Lots of embedding\nlearning models have been proposed to learn the embedding\nvectors of nodes by predicting nodes\u2019 neighborhoods. Deep-\nWalk ",
                    "Citation Text": "B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning of\nsocial representations,\u201d in ACM Knowledge discovery and data mining\n(KDD) , 2014, pp. 701\u2013710.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.02011v2": {
            "Paper Title": "Neural Loop Combiner: Neural Network Models for Assessing the\n  Compatibility of Loops",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04214v2": {
            "Paper Title": "Learning music audio representations via weak language supervision",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "). In both ta-\nbles we also include a fully supervised approach trained end-to-end:\nHCNN ",
                    "Citation Text": "M. Won, A. Ferraro, D. Bogdanov, and X. Serra, \u201cEvalua-\ntion of CNN-based Automatic Music Tagging Models,\u201d in\nProceedings of the 17th Sound and Music Computing Con-\nference , 2020, pp. 331\u2013337.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.00751",
                        "Citation Paper Title": "Title:Evaluation of CNN-based Automatic Music Tagging Models",
                        "Citation Paper Abstract": "Abstract:Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.",
                        "Citation Paper Authors": "Authors:Minz Won, Andres Ferraro, Dmitry Bogdanov, Xavier Serra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13057v3": {
            "Paper Title": "Evaluating the Robustness of Retrieval Pipelines with Query Variation\n  Generators",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". For\nBackTranslation we use the facebook/m2m100_418M pre-trained\n7https://github.com/terrierteam/pyterrier_t5Woodstock \u201918, June 03\u201305, 2018, Woodstock, NY Gustavo Penha, Arthur C\u00e2mara, and Claudia Hauff\nmodel from the transformers library8. For all other methods, we\nuse the implementations from the TextAttack ",
                    "Citation Text": "John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020.\nTextAttack: A Framework for Adversarial Attacks, Data Augmentation, and\nAdversarial Training in NLP. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations . 119\u2013126.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.05909",
                        "Citation Paper Title": "Title:TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",
                        "Citation Paper Abstract": "Abstract:While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at this https URL.",
                        "Citation Paper Authors": "Authors:John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "by the original authors of monoT5.\n4.3 Query Generators Implementation\nAs for our methods of generating query variations, for T5DescToTitle\nandT5QQP we rely on pre-trained T5 models ( t5-base ) and we fine-\ntune them using the Huggingface transformers library ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational\nLinguistics, Online, 38\u201345. https://www.aclweb.org/anthology/2020.emnlp-\ndemos.6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "on several IR benchmarks\u2014a comprehen-\nsive account of the effectiveness gains can be found in ",
                    "Citation Text": "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020. Pretrained transformers\nfor text ranking: Bert and beyond. arXiv preprint arXiv:2010.06467 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.06467",
                        "Citation Paper Title": "Title:Pretrained Transformers for Text Ranking: BERT and Beyond",
                        "Citation Paper Abstract": "Abstract:The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.",
                        "Citation Paper Authors": "Authors:Jimmy Lin, Rodrigo Nogueira, Andrew Yates"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "showed that BERT does not adhere to\nIR axioms, i.e., heuristics that a reasonable IR model should fulfill,\nthrough the use of diagnostic datasets. MacAvaney et al . ",
                    "Citation Text": "Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman\nCohan. 2020. ABNIRML: Analyzing the Behavior of Neural IR Models. arXiv\npreprint arXiv:2011.00696 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.00696",
                        "Citation Paper Title": "Title:ABNIRML: Analyzing the Behavior of Neural IR Models",
                        "Citation Paper Abstract": "Abstract:Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics -- such as writing styles, factuality, sensitivity to paraphrasing and word order -- that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, like that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer linguistic information, evidenced by their higher sensitivity to word and sentence order. Other results are more surprising, such as that some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts. Further, some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, Arman Cohan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11656v2": {
            "Paper Title": "AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification\n  with Multi-modal Explanations",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "(extracts an important subgraph for a particular node classification\nprediction), cannot be used since they only learn local explanation\nmodels. We use the recently proposed methods of concept learning ",
                    "Citation Text": "Chih-Kuan Yeh, Been Kim, Sercan O Arik, Chun-Liang Li, Tomas Pfister, and\nPradeep Ravikumar. 2019. On completeness-aware concept-based explanations\nin deep neural networks. arXiv preprint arXiv:1910.07969 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07969",
                        "Citation Paper Title": "Title:On Completeness-aware Concept-Based Explanations in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior based on the assumption that complete concept scores are sufficient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To define an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose ConceptSHAP. Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in finding concepts that are both complete in explaining the decisions and interpretable. (The code is released at this https URL)",
                        "Citation Paper Authors": "Authors:Chih-Kuan Yeh, Been Kim, Sercan O. Arik, Chun-Liang Li, Tomas Pfister, Pradeep Ravikumar"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "uses memory networks gen-\nerate aspect representations influenced by other aspects in the\nsame sentence. ",
                    "Citation Text": "Chen Zhang, Qiuchi Li, and Dawei Song. 2019. Aspect-based Sentiment Classifi-\ncation with Aspect-specific Graph Convolutional Networks. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\n4560\u20134570.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.03477",
                        "Citation Paper Title": "Title:Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.",
                        "Citation Paper Authors": "Authors:Chen Zhang, Qiuchi Li, Dawei Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.11570v2": {
            "Paper Title": "MIC: Model-agnostic Integrated Cross-channel Recommenders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09059v2": {
            "Paper Title": "Context-aware Reranking with Utility Maximization for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "methods.\nRNN-based methods apply RNN to model the item interaction\nand implicitly extract positional information from the initial rank-\ning. DLCM ",
                    "Citation Text": "Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Croft. 2018. Learning a Deep Listwise\nContext Model for Ranking Refinement. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05936",
                        "Citation Paper Title": "Title:Learning a Deep Listwise Context Model for Ranking Refinement",
                        "Citation Paper Abstract": "Abstract:Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the \\textit{local ranking context}, can provide important information about the query's characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.",
                        "Citation Paper Authors": "Authors:Qingyao Ai, Keping Bi, Jiafeng Guo, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "applies GRU to encode the whole ranking list into\nthe representation of items. Seq2Slate ",
                    "Citation Text": "Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi, Elad Eban,\nXiyang Luo, Alan Mackey, and Ofer Meshi. 2019. Seq2Slate: Re-ranking and Slate\nOptimization with RNNs. arXiv:1810.02019 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.02019",
                        "Citation Paper Title": "Title:Seq2Slate: Re-ranking and Slate Optimization with RNNs",
                        "Citation Paper Abstract": "Abstract:Ranking is a central task in machine learning and information retrieval. In this task, it is especially important to present the user with a slate of items that is appealing as a whole. This in turn requires taking into account interactions between items, since intuitively, placing an item on the slate affects the decision of which other items should be placed alongside it. In this work, we propose a sequence-to-sequence model for ranking called seq2slate. At each step, the model predicts the next `best' item to place on the slate given the items already selected. The sequential nature of the model allows complex dependencies between the items to be captured directly in a flexible and scalable way. We show how to learn the model end-to-end from weak supervision in the form of easily obtained click-through data. We further demonstrate the usefulness of our approach in experiments on standard ranking benchmarks as well as in a real-world recommendation system.",
                        "Citation Paper Authors": "Authors:Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi, Elad Eban, Xiyang Luo, Alan Mackey, Ofer Meshi"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "uses pointer network\nwith a decoder to directly generates the ranked list. PRS ",
                    "Citation Text": "Yufei Feng, Yu Gong, Fei Sun, Qingwen Liu, and Wenwu Ou. 2021. Revisit\nRecommender System in the Permutation Prospective. arXiv:2102.12057 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12057",
                        "Citation Paper Title": "Title:Revisit Recommender System in the Permutation Prospective",
                        "Citation Paper Abstract": "Abstract:Recommender systems (RS) work effective at alleviating information overload and matching user interests in various web-scale applications. Most RS retrieve the user's favorite candidates and then rank them by the rating scores in the greedy manner. In the permutation prospective, however, current RS come to reveal the following two limitations: 1) They neglect addressing the permutation-variant influence within the recommended results; 2) Permutation consideration extends the latent solution space exponentially, and current RS lack the ability to evaluate the permutations. Both drive RS away from the permutation-optimal recommended results and better user experience. To approximate the permutation-optimal recommended results effectively and efficiently, we propose a novel permutation-wise framework PRS in the re-ranking stage of RS, which consists of Permutation-Matching (PMatch) and Permutation-Ranking (PRank) stages successively. Specifically, the PMatch stage is designed to obtain the candidate list set, where we propose the FPSA algorithm to generate multiple candidate lists via the permutation-wise and goal-oriented beam search algorithm. Afterwards, for the candidate list set, the PRank stage provides a unified permutation-wise ranking criterion named LR metric, which is calculated by the rating scores of elaborately designed permutation-wise model DPWN. Finally, the list with the highest LR score is recommended to the user. Empirical results show that PRS consistently and significantly outperforms state-of-the-art methods. Moreover, PRS has achieved a performance improvement of 11.0% on PV metric and 8.7% on IPV metric after the successful deployment in one popular recommendation scenario of Taobao application.",
                        "Citation Paper Authors": "Authors:Yufei Feng, Yu Gong, Fei Sun, Junfeng Ge, Wenwu Ou"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "adopts it to encode the\nmutual influences between items. SetRank ",
                    "Citation Text": "Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Ji-Rong Wen.\n2020. SetRank: Learning a Permutation-Invariant Ranking Model for Information\nRetrieval. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.05891",
                        "Citation Paper Title": "Title:SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:In learning-to-rank for information retrieval, a ranking model is automatically learned from the data and then utilized to rank the sets of retrieved documents. Therefore, an ideal ranking model would be a mapping from a document set to a permutation on the set, and should satisfy two critical requirements: (1)~it should have the ability to model cross-document interactions so as to capture local context information in a query; (2)~it should be permutation-invariant, which means that any permutation of the inputted documents would not change the output ranking. Previous studies on learning-to-rank either design uni-variate scoring functions that score each document separately, and thus failed to model the cross-document interactions; or construct multivariate scoring functions that score documents sequentially, which inevitably sacrifice the permutation invariance requirement. In this paper, we propose a neural learning-to-rank model called SetRank which directly learns a permutation-invariant ranking model defined on document sets of any size. SetRank employs a stack of (induced) multi-head self attention blocks as its key component for learning the embeddings for all of the retrieved documents jointly. The self-attention mechanism not only helps SetRank to capture the local context information from cross-document interactions, but also to learn permutation-equivariant representations for the inputted documents, which therefore achieving a permutation-invariant ranking model. Experimental results on three large scale benchmarks showed that the SetRank significantly outperformed the baselines include the traditional learning-to-rank models and state-of-the-art Neural IR models.",
                        "Citation Paper Authors": "Authors:Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, Jirong Wen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.12794v2": {
            "Paper Title": "X-Class: Text Classification with Extremely Weak Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11775v2": {
            "Paper Title": "Multiple Choice Questions based Multi-Interest Policy Learning for\n  Conversational Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.09689v2": {
            "Paper Title": "The Case for Claim Difficulty Assessment in Automatic Fact Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.04903v2": {
            "Paper Title": "FeedRec: News Feed Recommendation with Various User Feedbacks",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "pro-\nposed a news recommendation method with personalized attention\nnetwork that selects informative clicked news for user modeling\naccording to the embeddings of user IDs. Wu et al. ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.\n2019. Neural News Recommendation with Multi-Head Self-Attention. In EMNLP-\nIJCNLP . 6390\u20136395.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed to use a candidate-aware\nattention network to learn user representations from clicked news\nbased on their relevance to candidate news. Wu et al. ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and\nXing Xie. 2019. Npa: Neural news recommendation with personalized attention.\nInKDD . 2576\u20132584.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05559",
                        "Citation Paper Title": "Title:NPA: Neural News Recommendation with Personalized Attention",
                        "Citation Paper Abstract": "Abstract:News recommendation is very important to help users find interested news and alleviate information overload. Different users usually have different interests and the same user may have various interests. Thus, different users may click the same news article with attention on different aspects. In this paper, we propose a neural news recommendation model with personalized attention (NPA). The core of our approach is a news representation model and a user representation model. In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. In the user representation model we learn the representations of users based on the representations of their clicked news articles. Since different words and different news articles may have different informativeness for representing news and users, we propose to apply both word- and news-level attention mechanism to help our model attend to important words and news articles. In addition, the same news article and the same word may have different informativeness for different users. Thus, we propose a personalized attention network which exploits the embedding of user ID to generate the query vector for the word- and news-level attentions. Extensive experiments are conducted on a real-world news recommendation dataset collected from MSN news, and the results validate the effectiveness of our approach on news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "proposed\nan embedding-based news recommendation method that uses a\nGRU network to capture user interests from the representations of\nclicked news. Wang et al. ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nKnowledge-Aware Network for News Recommendation. In WWW . 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11308v3": {
            "Paper Title": "Breaking BERT: Understanding its Vulnerabilities for Named Entity\n  Recognition through Adversarial Attack",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05669v3": {
            "Paper Title": "Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author\n  Discovery",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". The problem of identifying connections\nbetween mentions of tasks, methods or resources across scientific\npapers is very much an open one ",
                    "Citation Text": "Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug Downey,\nand Tom Hope. 2021. SciCo: Hierarchical Cross-Document Coreference for\nScientific Concepts. arXiv preprint arXiv:2104.08809 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08809",
                        "Citation Paper Title": "Title:SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts",
                        "Citation Paper Abstract": "Abstract:Determining coreference of concept mentions across multiple documents is a fundamental task in natural language understanding. Previous work on cross-document coreference resolution (CDCR) typically considers mentions of events in the news, which seldom involve abstract technical concepts that are prevalent in science and technology. These complex concepts take diverse or ambiguous forms and have many hierarchical levels of granularity (e.g., tasks and subtasks), posing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference clusters and hierarchy between them. We create SciCo, an expert-annotated dataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+ resource. We study strong baseline models that we customize for H-CDCR, and highlight challenges for future work.",
                        "Citation Paper Authors": "Authors:Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug Downey, Tom Hope"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ", and fine-grained\nspans of text referring to methods, tasks and resources automatically\nextracted from paper \ud835\udc56with a scientific named entity recognition\nmodel ",
                    "Citation Text": "David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity,\nRelation, and Event Extraction with Contextualized Span Representations. In\nEMNLP/IJCNLP . https://doi.org/10.18653/v1/D19-1585",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.03546",
                        "Citation Paper Title": "Title:Entity, Relation, and Event Extraction with Contextualized Span Representations",
                        "Citation Paper Abstract": "Abstract:We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at this https URL and can be easily adapted for new tasks or datasets.",
                        "Citation Paper Authors": "Authors:David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10019v2": {
            "Paper Title": "Enhancing Cross-Sectional Currency Strategies by Context-Aware Learning\n  to Rank with Self-Attention",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "for a comprehensive overview.\nGiven the widespread adoption of LTR across numerous applica-\ntions such as search engines, e-commerce ",
                    "Citation Text": "Shubhra Kanti Karmaker Santu, Parikshit Sondhi, and ChengXiang Zhai. 2017.\nOn Application of Learning to Rank for E-Commerce Search. Proceedings of\nthe 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (Aug. 2017), 475\u2013484. https://doi.org/10.1145/3077136.\n3080838 arXiv:1903.04263",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.04263",
                        "Citation Paper Title": "Title:On Application of Learning to Rank for E-Commerce Search",
                        "Citation Paper Abstract": "Abstract:E-Commerce (E-Com) search is an emerging important new application of information retrieval. Learning to Rank (LETOR) is a general effective strategy for optimizing search engines, and is thus also a key technology for E-Com search. While the use of LETOR for web search has been well studied, its use for E-Com search has not yet been well explored. In this paper, we discuss the practical challenges in applying learning to rank methods to E-Com search, including the challenges in feature representation, obtaining reliable relevance judgments, and optimally exploiting multiple user feedback signals such as click rates, add-to-cart ratios, order rates, and revenue. We study these new challenges using experiments on industry data sets and report several interesting findings that can provide guidance on how to optimally apply LETOR to E-Com search: First, popularity-based features defined solely on product items are very useful and LETOR methods were able to effectively optimize their combination with relevance-based features. Second, query attribute sparsity raises challenges for LETOR, and selecting features to reduce/avoid sparsity is beneficial. Third, while crowdsourcing is often useful for obtaining relevance judgments for Web search, it does not work as well for E-Com search due to difficulty in eliciting sufficiently fine grained relevance judgments. Finally, among the multiple feedback signals, the order rate is found to be the most robust training objective, followed by click rate, while add-to-cart ratio seems least robust, suggesting that an effective practical strategy may be to initially use click rates for training and gradually shift to using order rates as they become available.",
                        "Citation Paper Authors": "Authors:Shubhra Kanti Karmaker Santu, Parikshit Sondhi, ChengXiang Zhai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.08949v2": {
            "Paper Title": "Inductive Conformal Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14164v2": {
            "Paper Title": "Don't read, just look: Main content extraction from web pages using\n  visual features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.02191v2": {
            "Paper Title": "Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf\n  DLRM Model : 1000$\\times$ Compression and 3.1$\\times$ Faster Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.01044v2": {
            "Paper Title": "Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13751v3": {
            "Paper Title": "A Search Engine for Discovery of Scientific Challenges and Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12353v2": {
            "Paper Title": "Private Recommender Systems: How Can Users Build Their Own Fair\n  Recommender Systems without Log Data?",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "studied book recommendations\nand found that some recommendation algorithms arebiased toward books written by men authors. Beutel et\nal. ",
                    "Citation Text": "A. Beutel, J. Chen, T. Doshi, H. Qian, L. Wei,\nY. Wu, L. Heldt, Z. Zhao, L. Hong, E. H.\nChi, and C. Goodrow ,Fairness in recommendation\nranking through pairwise comparisons , in KDD, 2019,\npp. 2212\u20132220.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.00780",
                        "Citation Paper Title": "Title:Fairness in Recommendation Ranking through Pairwise Comparisons",
                        "Citation Paper Abstract": "Abstract:Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information. As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them? In this paper we offer a set of novel metrics for evaluating algorithmic fairness concerns in recommender systems. In particular we show how measuring fairness based on pairwise comparisons from randomized experiments provides a tractable means to reason about fairness in rankings from recommender systems. Building on this metric, we offer a new regularizer to encourage improving this metric during model training and thus improve fairness in the resulting rankings. We apply this pairwise regularization to a large-scale, production recommender system and show that we are able to significantly improve the system's pairwise fairness.",
                        "Citation Paper Authors": "Authors:Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, Cristos Goodrow"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "cannot be\napplied in this setting. Examples of previous studies on\nP-fairness are as follows. Geyik et al. ",
                    "Citation Text": "S. C. Geyik, S. Ambler, and K. Kenthapadi ,\nFairness-aware ranking in search & recommendation\nsystems with application to linkedin talent search , in\nKDD, 2019, pp. 2221\u20132231.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01989",
                        "Citation Paper Title": "Title:Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search",
                        "Citation Paper Abstract": "Abstract:We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.",
                        "Citation Paper Authors": "Authors:Sahin Cem Geyik, Stuart Ambler, Krishnaram Kenthapadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01853v4": {
            "Paper Title": "Recursive Bayesian Networks: Generalising and Unifying Probabilistic\n  Context-Free Grammars and Dynamic Bayesian Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05235v2": {
            "Paper Title": "Position-enhanced and Time-aware Graph Convolutional Network for\n  Sequential Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02812v3": {
            "Paper Title": "User behavior understanding in real world settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14979v3": {
            "Paper Title": "On component interactions in two-stage recommender systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02787v2": {
            "Paper Title": "Gumble Softmax For User Behavior Modeling",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "meth-\nods consider all the variable interaction information which not only\nimprove the recommendation results but also achieve good results\neven when the data is sparse. With the success of deep learning in\ncomputer vision and natural language processing ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based\nrecommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1\u201338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.09730v3": {
            "Paper Title": "Trends in Publishing Blockchain Surveys: A Bibliometric Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11866v3": {
            "Paper Title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "0.8056 0.4463 0.7722 0.3880 0.8421 0.3831 +0.0062 -0.0214\nGraphFM (ours) 0.8087 0.4428 0.7765 0.3816 0.8535 0.3716 +0.0000 -0.0000\n5.1.3 Implementation Details\nWe implement the method using Tensor\ufb02ow ",
                    "Citation Text": "M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, et al. Tensor\ufb02ow: A system for\nlarge-scale machine learning. In OSDI , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "(C) utilizes DNN with residual\nconnections to model non-linear feature interactions\nin an implicit manner.\n\u000fCrossNet ",
                    "Citation Text": "R. Wang, B. Fu, G. Fu, and M. Wang. Deep & cross network for ad\nclick predictions. In ADKDD , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "(C) devises a bi-Interaction layer to model\nsecond-order interactions and uses DNN to intro-\nduce non-linearity and model high-order interac-\ntions.\n\u000fHOFM (C) ",
                    "Citation Text": "M. Blondel, A. Fujino, N. Ueda, and M. Ishihata. Higher-order\nfactorization machines. In NIPS , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.07195",
                        "Citation Paper Title": "Title:Higher-Order Factorization Machines",
                        "Citation Paper Abstract": "Abstract:Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.",
                        "Citation Paper Authors": "Authors:Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "(B) is the of\ufb01cial implementation for FM,\nwhich can only model second-order interactions.\n\u000fAFM ",
                    "Citation Text": "J. Xiao, H. Ye, X. He, H. Zhang, F. Wu, and T.-S. Chua. Attentional\nfactorization machines: Learning the weight of feature interactions\nvia attention networks. arXiv preprint arXiv:1708.04617 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.09506v3": {
            "Paper Title": "Ekar: An Explainable Method for Knowledge Aware Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01516v3": {
            "Paper Title": "Introducing Self-Attention to Target Attentive Graph Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "mechanism over multiple heads achieving state of the art\nresults. ",
                    "Citation Text": "C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar,\n\u201cAre transformers universal approximators of sequence-to-sequence\nfunctions?\u201d arXiv preprint arXiv:1912.10077 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10077",
                        "Citation Paper Title": "Title:Are Transformers universal approximators of sequence-to-sequence functions?",
                        "Citation Paper Abstract": "Abstract:Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to self-attention layers and empirically evaluate them.",
                        "Citation Paper Authors": "Authors:Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ".\nB. Proposed method - TAGNN++\nWe observe that a simple attention model is unable to\ncapture both local and global context ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, \u201cAttention is all you\nneed,\u201d in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/\ufb01le/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Lastly, each node has an incoming weight and an outgoing\nweight associated to it. We effectively capture item transitions\nand connections using a gated GNN ",
                    "Citation Text": "Y . Li, D. Tarlow, M. Brockschmidt, and R. Zemel, \u201cGated graph\nsequence neural networks,\u201d arXiv preprint arXiv:1511.05493 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08581v2": {
            "Paper Title": "Query Interpretations from Entity-Linked Segmentations",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": "suggested to combine word\u2013entity\nembeddings with graph-based approaches in a learning-to-rank\nsetup. Also the RadboudEL tool ",
                    "Citation Text": "J. M. van Hulst, F. Hasibi, K. Dercksen, K. Balog, and A. P. de Vries. 2020. REL:\nAn entity linker standing on the shoulders of giants. SIGIR 2020 . 2197\u20132200.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.01969",
                        "Citation Paper Title": "Title:REL: An Entity Linker Standing on the Shoulders of Giants",
                        "Citation Paper Abstract": "Abstract:Entity linking is a standard component in modern retrieval system that is often performed by third-party toolkits. Despite the plethora of open source options, it is difficult to find a single system that has a modular architecture where certain components may be replaced, does not depend on external sources, can easily be updated to newer Wikipedia versions, and, most important of all, has state-of-the-art performance. The REL system presented in this paper aims to fill that gap. Building on state-of-the-art neural components from natural language processing research, it is provided as a Python package as well as a web API. We also report on an experimental comparison against both well-established systems and the current state-of-the-art on standard entity linking benchmarks.",
                        "Citation Paper Authors": "Authors:Johannes M. van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, Arjen P. de Vries"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "who first disambiguate low ambiguity mentions and\nthen use these to disambiguate the rest. With the rise of neural\napproaches, Yamada et al . ",
                    "Citation Text": "I. Yamada, H. Shindo, H. Takeda, and Y. Takefuji. 2016. Joint learning of the\nembedding of words and entities for named entity disambiguation. CoNLL 2016 .\n250\u2013259.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01343",
                        "Citation Paper Title": "Title:Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation",
                        "Citation Paper Abstract": "Abstract:Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.",
                        "Citation Paper Authors": "Authors:Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "to detect and\nrank candidates; later extended to also identify entity types ",
                    "Citation Text": "D. Garigliotti, F. Hasibi, and K. Balog. 2018. Identifying and exploiting target\nentity type information for ad hoc entity retrieval. Inf. Retr. J. 22, 3\u20134 (2019),\n285\u2013323.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06056",
                        "Citation Paper Title": "Title:Target Type Identification for Entity-Bearing Queries",
                        "Citation Paper Abstract": "Abstract:Identifying the target types of entity-bearing queries can help improve retrieval performance as well as the overall search experience. In this work, we address the problem of automatically detecting the target types of a query with respect to a type taxonomy. We propose a supervised learning approach with a rich variety of features. Using a purpose-built test collection, we show that our approach outperforms existing methods by a remarkable margin. This is an extended version of the article published with the same title in the Proceedings of SIGIR'17.",
                        "Citation Paper Authors": "Authors:Dar\u00edo Garigliotti, Faegheh Hasibi, Krisztian Balog"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04475v2": {
            "Paper Title": "Localized Graph Collaborative Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11294v2": {
            "Paper Title": "Extending CLIP for Category-to-image Retrieval in E-commerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11336v2": {
            "Paper Title": "CODE: Contrastive Pre-training with Adversarial Fine-tuning for\n  Zero-shot Expert Linking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02046v2": {
            "Paper Title": "Attentive Knowledge-aware Graph Convolutional Networks with\n  Collaborative Guidance for Personalized Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06468v3": {
            "Paper Title": "Modeling Scale-free Graphs with Hyperbolic Geometry for Knowledge-aware\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "with structural, tex tual\nand visual information to subsume matrix factorization und er a\nuni\ufb01ed Bayesian framework.\n\u2022KGAT ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Se ng Chua. 2019.\nKgat: Knowledge graph attention network for recommendatio n. In International\nConference on Knowledge Discovery & Data Mining (SIGKDD) . 950\u2013958.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.15575v1": {
            "Paper Title": "Fast Learning of MNL Model from General Partial Rankings with\n  Application to Network Formation Modeling",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "breaks\nthe general partial rankings into maximal-ordered partitions but\nhas to discard a part of hard-to-evaluate data to confine the time\ncomplexity. Though the method proposed by Ma et al. ",
                    "Citation Text": "Jiaqi Ma, Xinyang Yi, Weijing Tang, Zhe Zhao, Lichan Hong, Ed H. Chi, and\nQiaozhu Mei. 2021. Learning-to-Rank with Partitioned Preference: Fast Estima-\ntion for the Plackett-Luce Model. arXiv:2006.05067 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05067",
                        "Citation Paper Title": "Title:Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model",
                        "Citation Paper Abstract": "Abstract:We investigate the Plackett-Luce (PL) model based listwise learning-to-rank (LTR) on data with partitioned preference, where a set of items are sliced into ordered and disjoint partitions, but the ranking of items within a partition is unknown. Given $N$ items with $M$ partitions, calculating the likelihood of data with partitioned preference under the PL model has a time complexity of $O(N+S!)$, where $S$ is the maximum size of the top $M-1$ partitions. This computational challenge restrains most existing PL-based listwise LTR methods to a special case of partitioned preference, top-$K$ ranking, where the exact order of the top $K$ items is known. In this paper, we exploit a random utility model formulation of the PL model, and propose an efficient numerical integration approach for calculating the likelihood and its gradients with a time complexity $O(N+S^3)$. We demonstrate that the proposed method outperforms well-known LTR baselines and remains scalable through both simulation experiments and applications to real-world eXtreme Multi-Label classification tasks.",
                        "Citation Paper Authors": "Authors:Jiaqi Ma, Xinyang Yi, Weijing Tang, Zhe Zhao, Lichan Hong, Ed H. Chi, Qiaozhu Mei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.15328v1": {
            "Paper Title": "Temporal aware Multi-Interest Graph Neural Network For Session-based\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "converts session sequences into directed unweighted\ngraphs and utilizes a GGNN layer ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated\ngraph sequence neural networks. arXiv preprint arXiv:1511.05493 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Session-based Recommendation. Following the development\nof deep learning, many neural network based approaches have\nbeen proposed for SBR. Due to well sequence modeling capabil-\nity of RNNs, RNN-based methods have been widely used for SBR\n[8,10,13,19,22]. For instance, GRU4Rec ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.00014v1": {
            "Paper Title": "Exploiting Bi-directional Global Transition Patterns and Personal\n  Preferences for Missing POI Category Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14958v1": {
            "Paper Title": "A Benchmark Dataset for Micro-video Thumbnail Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14921v1": {
            "Paper Title": "Retrieving Black-box Optimal Images from External Databases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14861v1": {
            "Paper Title": "Using word clouds for fast identification of papers' subject domain and\n  reviewers' competences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03240v2": {
            "Paper Title": "Bias and Debias in Recommender System: A Survey and Future Directions",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": "assumes an interaction can be estimated by embedding inner product, while He et al. ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW .\nACM, 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 217,
                    "Sentence": ", and geo locations. 4)\nPopular items are more likely to be seen by users. Hence, such \u201cpopularity bias\u201d is another form of \u201cexposure\nbias\u201d ",
                    "Citation Text": "Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling User Interest and Conformity for\nRecommendation with Causal Embedding. In Proceedings of the Web Conference 2021 . 2980\u20132991.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11011",
                        "Citation Paper Title": "Title:Disentangling User Interest and Conformity for Recommendation with Causal Embedding",
                        "Citation Paper Abstract": "Abstract:Recommendation models are usually trained on observational interaction data. However, observational interaction data could result from users' conformity towards popular items, which entangles users' real interest. Existing methods tracks this problem as eliminating popularity bias, e.g., by re-weighting training samples or leveraging a small fraction of unbiased data. However, the variety of user conformity is ignored by these approaches, and different causes of an interaction are bundled together as unified representations, hence robustness and interpretability are not guaranteed when underlying causes are changing. In this paper, we present DICE, a general framework that learns representations where interest and conformity are structurally disentangled, and various backbone recommendation models could be smoothly integrated. We assign users and items with separate embeddings for interest and conformity, and make each embedding capture only one cause by training with cause-specific data which is obtained according to the colliding effect of causal inference. Our proposed methodology outperforms state-of-the-art baselines with remarkable improvements on two real-world datasets on top of various backbone models. We further demonstrate that the learned embeddings successfully capture the desired causes, and show that DICE guarantees the robustness and interpretability of recommendation.",
                        "Citation Paper Authors": "Authors:Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Depeng Jin, Yong Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.14444v1": {
            "Paper Title": "Literature Review of the Pioneering Approaches in Cloud-based Search\n  Engines Powered by LETOR Techniques",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ", is used for\ncomparison by changing the number of negative samples J. Overall comparative\nresults can be found in Figure 4.\nPaper Review: A Deep Relevance Matching Model for Ad-hoc Re-\ntrieval ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM inter-\nnational on conference on information and knowledge management , pages\n55{64, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.14370v1": {
            "Paper Title": "On the Overlooked Significance of Underutilized Contextual Features in\n  Recent News Recommendation Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14318v1": {
            "Paper Title": "Mirror Matching: Document Matching Approach in Seed-driven Document\n  Ranking for Medical Systematic Reviews",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "uses semantic matching as well as syntactic match-\ning in character \ud835\udc5b-graph interactions between two documents.\n\u2022MV-LSTM ",
                    "Citation Text": "Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng.\n2016. A deep architecture for semantic matching with multiple positional sen-\ntence representations. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.08277",
                        "Citation Paper Title": "Title:A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations",
                        "Citation Paper Abstract": "Abstract:Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.",
                        "Citation Paper Authors": "Authors:Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Furthermore, the ordered elements are explicitly\nindicated with headings in some abstracts.\nOur hypothesis is that PICO words appear in patterns in med-\nical scientific abstracts. We study the positional distributions of\nPICO elements using EBM-NLP dataset ",
                    "Citation Text": "Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain Marshall, Ani\nNenkova, and Byron Wallace. 2018. A Corpus with Multi-Level Annotations\nof Patients, Interventions and Outcomes to Support Language Processing for\nMedical Literature. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04185",
                        "Citation Paper Title": "Title:A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature",
                        "Citation Paper Abstract": "Abstract:We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the `PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.",
                        "Citation Paper Authors": "Authors:Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain J. Marshall, Ani Nenkova, Byron C. Wallace"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.14279v1": {
            "Paper Title": "Query Suggestion for Click-Absent Queries in Enterprise Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14070v1": {
            "Paper Title": "Intelligent Document Processing -- Methods and Tools in the real world",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05914v2": {
            "Paper Title": "Leaping Through Time with Gradient-based Adaptation for Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00688v1": {
            "Paper Title": "Automatic Pharma News Categorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14423v2": {
            "Paper Title": "Enumerating Fair Packages for Group Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13742v1": {
            "Paper Title": "Hamtajoo: A Persian Plagiarism Checker for Academic Manuscripts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13556v1": {
            "Paper Title": "Towards Personalized Answer Generation in E-Commerce via\n  Multi-Perspective Preference Modeling",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "propose a multi-style abstractive summarization model for generative QA with multi-task\nlearning. Bi et al . ",
                    "Citation Text": "Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, and Chenliang Li. 2019. Incorporating External Knowledge into\nMachine Reading for Generative Question Answering. In EMNLP-IJCNLP . 2521\u20132530.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02745",
                        "Citation Paper Title": "Title:Incorporating External Knowledge into Machine Reading for Generative Question Answering",
                        "Citation Paper Abstract": "Abstract:Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context.\nIn this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.",
                        "Citation Paper Authors": "Authors:Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, Chenliang Li"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "propose an adversarial learning framework\nto identify whether the generated answer matches the facts, which is further enhanced by a\nreview-reasoning module and prototype-editing ",
                    "Citation Text": "Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, and Rui Yan. 2021. Meaningful Answer Generation of\nE-Commerce Question-Answering. ACM Trans. Inf. Syst. (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.07307",
                        "Citation Paper Title": "Title:Meaningful Answer Generation of E-Commerce Question-Answering",
                        "Citation Paper Abstract": "Abstract:In e-commerce portals, generating answers for product-related questions has become a crucial task. In this paper, we focus on the task of product-aware answer generation, which learns to generate an accurate and complete answer from large-scale unlabeled e-commerce reviews and product attributes. However, safe answer problems pose significant challenges to text generation tasks, and e-commerce question-answering task is no exception. To generate more meaningful answers, in this paper, we propose a novel generative neural model, called the Meaningful Product Answer Generator (MPAG), which alleviates the safe answer problem by taking product reviews, product attributes, and a prototype answer into consideration. Product reviews and product attributes are used to provide meaningful content, while the prototype answer can yield a more diverse answer pattern. To this end, we propose a novel answer generator with a review reasoning module and a prototype answer reader. Our key idea is to obtain the correct question-aware information from a large scale collection of reviews and learn how to write a coherent and meaningful answer from an existing prototype answer. To be more specific, we propose a read-and-write memory consisting of selective writing units to conduct reasoning among these reviews. We then employ a prototype reader consisting of comprehensive matching to extract the answer skeleton from the prototype answer. Finally, we propose an answer editor to generate the final answer by taking the question and the above parts as input. Conducted on a real-world dataset collected from an e-commerce platform, extensive experimental results show that our model achieves state-of-the-art performance in terms of both automatic metrics and human evaluations. Human evaluation also demonstrates that our model can consistently generate specific and proper answers.",
                        "Citation Paper Authors": "Authors:Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, Rui Yan"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "propose a gated attention mechanism to capture the question-related information from the reviews\nfor denoising in answer generation. Gao et al . ",
                    "Citation Text": "Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, and Rui Yan. 2019. Product-Aware Answer\nGeneration in E-Commerce Question-Answering. In WSDM . 429\u2013437.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.07696",
                        "Citation Paper Title": "Title:Product-Aware Answer Generation in E-Commerce Question-Answering",
                        "Citation Paper Abstract": "Abstract:In e-commerce portals, generating answers for product-related questions has become a crucial task. In this paper, we propose the task of product-aware answer generation, which tends to generate an accurate and complete answer from large-scale unlabeled e-commerce reviews and product attributes. Unlike existing question-answering problems, answer generation in e-commerce confronts three main challenges: (1) Reviews are informal and noisy; (2) joint modeling of reviews and key-value product attributes is challenging; (3) traditional methods easily generate meaningless answers. To tackle above challenges, we propose an adversarial learning based model, named PAAG, which is composed of three components: a question-aware review representation module, a key-value memory network encoding attributes, and a recurrent neural network as a sequence generator. Specifically, we employ a convolutional discriminator to distinguish whether our generated answer matches the facts. To extract the salience part of reviews, an attention-based review reader is proposed to capture the most relevant words given the question. Conducted on a large-scale real-world e-commerce dataset, our extensive experiments verify the effectiveness of each module in our proposed model. Moreover, our experiments show that our model achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",
                        "Citation Paper Authors": "Authors:Shen Gao, Zhaochun Ren, Yihong Eric Zhao, Dongyan Zhao, Dawei Yin, Rui Yan"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "exploit\nproduct reviews as external knowledge to enhance the answer ranking in PQA. Besides, Zhang\net al. ",
                    "Citation Text": "Wenxuan Zhang, Wai Lam, Yang Deng, and Jing Ma. 2020. Review-guided Helpful Answer Identification in E-commerce.\nInWWW . 2620\u20132626.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06209",
                        "Citation Paper Title": "Title:Review-guided Helpful Answer Identification in E-commerce",
                        "Citation Paper Abstract": "Abstract:Product-specific community question answering platforms can greatly help address the concerns of potential customers. However, the user-provided answers on such platforms often vary a lot in their qualities. Helpfulness votes from the community can indicate the overall quality of the answer, but they are often missing. Accurately predicting the helpfulness of an answer to a given question and thus identifying helpful answers is becoming a demanding need. Since the helpfulness of an answer depends on multiple perspectives instead of only topical relevance investigated in typical QA tasks, common answer selection algorithms are insufficient for tackling this task. In this paper, we propose the Review-guided Answer Helpfulness Prediction (RAHP) model that not only considers the interactions between QA pairs but also investigates the opinion coherence between the answer and crowds' opinions reflected in the reviews, which is another important factor to identify helpful answers. Moreover, we tackle the task of determining opinion coherence as a language inference problem and explore the utilization of pre-training strategy to transfer the textual inference knowledge obtained from a specifically designed trained network. Extensive experiments conducted on real-world data across seven product categories show that our proposed model achieves superior performance on the prediction task.",
                        "Citation Paper Authors": "Authors:Wenxuan Zhang, Wai Lam, Yang Deng, Jing Ma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.13510v1": {
            "Paper Title": "Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical\n  Knowledge Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13323v1": {
            "Paper Title": "Airphant: Cloud-oriented Document Indexing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13237v1": {
            "Paper Title": "CABACE: Injecting Character Sequence Information and Domain Knowledge\n  for Enhanced Acronym and Long-Form Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06206v5": {
            "Paper Title": "On Robustness and Bias Analysis of BERT-based Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00695v1": {
            "Paper Title": "An Efficient Combinatorial Optimization Model Using Learning-to-Rank\n  Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12996v1": {
            "Paper Title": "Distinguishing Transformative from Incremental Clinical Evidence: A\n  Classifier of Clinical Research using Textual features from Abstracts and\n  Citing Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01946v2": {
            "Paper Title": "Machine Learning Applications in the Routing in Computer Networks",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "Centralized \nReinforcement \nLearning SARSA \nReinforcement \nLearning Fast convergence; \nMinimize the \nsignaling delay Need to improve \nthe scalability \nDRL-based \nCognitive \nRouting ",
                    "Citation Text": "Jiawei Wu, Jianxue Li., Yang Xiao, and Jun Liu, \u201cTowards Cognitive Routing \nbased on Deep Reinforcement Learning,\u201darXiv:2003.12439, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12439",
                        "Citation Paper Title": "Title:Towards Cognitive Routing based on Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Routing is one of the key functions for stable operation of network infrastructure. Nowadays, the rapid growth of network traffic volume and changing of service requirements call for more intelligent routing methods than before. Towards this end, we propose a definition of cognitive routing and an implementation approach based on Deep Reinforcement Learning (DRL). To facilitate the research of DRL-based cognitive routing, we introduce a simulator named RL4Net for DRL-based routing algorithm development and simulation. Then, we design and implement a DDPG-based routing algorithm. The simulation results on an example network topology show that the DDPG-based routing algorithm achieves better performance than OSPF and random weight algorithms. It demonstrate the preliminary feasibility and potential advantage of cognitive routing for future network.",
                        "Citation Paper Authors": "Authors:Jiawei Wu, Jianxue Li, Yang Xiao, Jun Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.07265v2": {
            "Paper Title": "Linear, or Non-Linear, That is the Question!",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". More-\nover, we test with various options of where to put them. First, HM-\nLET(Front) focuses on the fact that GCNs are highly influenced by\nclose neighborhood, i.e.,in the first and second layers ",
                    "Citation Text": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\n2015. Line: Large-scale information network embedding. In Proc. of The Web\nConference (WWW) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". These MF-based methods simply learn\nrelationships between users and items via dot products. There-\nfore, they have limitations in considering potentially complex re-\nlationships between users and items inherent in user-item inter-\nactions ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-seng Chua.\n2017. Neural Collaborative Filtering. In Proc. of The Web Conference (WWW) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.06924v1": {
            "Paper Title": "A Synthetic Prediction Market for Estimating Confidence in Published\n  Work",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12802v1": {
            "Paper Title": "Biased or Not?: The Story of Two Search Engines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.12478v2": {
            "Paper Title": "Deep Asymmetric Hashing with Dual Semantic Regression and Class\n  Structure Quantization",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nThe proposed DSAH is implemented with deep learning toolbox MatConvNet ",
                    "Citation Text": "A. Vedaldi, K. Lenc, MatConvNet: convolutional neural networks for MATLAB, in:\nProceedings of the ACM International Conference on Multimedia, 2015, pp. 689-692.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.4564",
                        "Citation Paper Title": "Title:MatConvNet - Convolutional Neural Networks for MATLAB",
                        "Citation Paper Abstract": "Abstract:MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.",
                        "Citation Paper Authors": "Authors:Andrea Vedaldi, Karel Lenc"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.12353v1": {
            "Paper Title": "LAME: Layout Aware Metadata Extraction Approach for Research Articles",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". Afterwar d, various successful cases using bidirectional encoder representations from transformers \n(BERT) ",
                    "Citation Text": "J. Devlin, M.  W. Chang, K. Lee and K. Toutanova, \"BERT: Pre -training of Deep Bidirectional Transformers \nfor Language Understanding ,\" in Proc.  2019 Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Techno logies , vol. 1, pp. 4171 -4186,  2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "pretrained with a large -scale corpus were introduced in the field of NLP. In the studies by ",
                    "Citation Text": "A. Adhikari, A. Ram, R. Tang, and J. Lin, \u2018\u2018DocBERT: BERT for document classification,\u2019\u2019 arXiv preprint  \narXiv:1904.08398 , 2019,  [Online]. Available: http://arxiv.org/abs/1904.08398",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08398",
                        "Citation Paper Title": "Title:DocBERT: BERT for Document Classification",
                        "Citation Paper Abstract": "Abstract:We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",
                        "Citation Paper Authors": "Authors:Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04976v2": {
            "Paper Title": "Deep Pairwise Learning To Rank For Search Autocomplete",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.07201v2": {
            "Paper Title": "Evaluating the effectiveness of Phishing Reports on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11736v1": {
            "Paper Title": "An Efficient Pruning Process with Locality Aware Exploration and Dynamic\n  Graph Editing for Subgraph Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00692v1": {
            "Paper Title": "Validation and Transparency in AI systems for pharmacovigilance: a case\n  study applied to the medical literature monitoring of adverse events",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11134v1": {
            "Paper Title": "FedPOIRec: Privacy Preserving Federated POI Recommendation with Social\n  Influence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11031v1": {
            "Paper Title": "On Cross-Lingual Retrieval with Multilingual Text Encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02560v2": {
            "Paper Title": "A comment-driven evidence appraisal approach for decision-making when\n  only uncertain evidence available",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10746v1": {
            "Paper Title": "Learning Semi-Structured Representations of Radiology Reports",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10189v1": {
            "Paper Title": "LUC at ComMA-2021 Shared Task: Multilingual Gender Biased and Communal\n  Language Identification without using linguistic features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11154v3": {
            "Paper Title": "Personalized Transfer of User Preferences for Cross-domain\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09425v1": {
            "Paper Title": "Knowledge graph enhanced recommender system",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "reconstructs user-\nto-user relationships and item-to-item relationships based on KG\nand proposes a novel graph convolutional network to aggregate the\nattribute information from neighbors. KGIN ",
                    "Citation Text": "Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,\nXiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions with\nKnowledge Graph for Recommendation. In Proceedings of the Web Conference\n2021. 878\u2013887.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07057",
                        "Citation Paper Title": "Title:Learning Intents behind Interactions with Knowledge Graph for Recommendation",
                        "Citation Paper Abstract": "Abstract:Knowledge graph (KG) plays an increasingly important role in recommender systems. A recent technical trend is to develop end-to-end models founded on graph neural networks (GNNs). However, existing GNN-based models are coarse-grained in relational modeling, failing to (1) identify user-item relation at a fine-grained level of intents, and (2) exploit relation dependencies to preserve the semantics of long-range connectivity.\nIn this study, we explore intents behind a user-item interaction by using auxiliary item knowledge, and propose a new model, Knowledge Graph-based Intent Network (KGIN). Technically, we model each intent as an attentive combination of KG relations, encouraging the independence of different intents for better model capability and interpretability. Furthermore, we devise a new information aggregation scheme for GNN, which recursively integrates the relation sequences of long-range connectivity (i.e., relational paths). This scheme allows us to distill useful information about user intents and encode them into the representations of users and items. Experimental results on three benchmark datasets show that, KGIN achieves significant improvements over the state-of-the-art methods like KGAT, KGNN-LS, and CKAN. Further analyses show that KGIN offers interpretable explanations for predictions by identifying influential intents and relational paths. The implementations are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ") to model prior\nrepresentations of item, which are used to guide the recommender\nmodel. For example, DKN ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nknowledge-aware network for news recommendation. In Proceedings of the 2018\nworld wide web conference . 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.09424v1": {
            "Paper Title": "Search Strategy Formulation for Systematic Reviews: issues, challenges\n  and opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05068v2": {
            "Paper Title": "Neural News Recommendation with Event Extraction",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "achieves excellent results on multiple benchmarks, including sentiment analysis and text\nclassi\ufb01cation.\n\u2022Wide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,\nGreg Corrado, Wei Chai, and Mustafa Ispir. Wide & deep learning for recommender systems. In Proceedings of\nthe 1st workshop on deep learning for recommender systems , pages 7\u201310, 2016.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Event extraction from news articles is a\ncommonly required prerequisite for various tasks, such as article summarization, article clustering, or news aggregation ",
                    "Citation Text": "Felix Hamborg, Corinna Breitinger, and Bela Gipp. Giveme5w1h: A universal system for extracting main events\nfrom news articles. arXiv preprint arXiv:1909.02766 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02766",
                        "Citation Paper Title": "Title:Giveme5W1H: A Universal System for Extracting Main Events from News Articles",
                        "Citation Paper Abstract": "Abstract:Event extraction from news articles is a commonly required prerequisite for various tasks, such as article summarization, article clustering, and news aggregation. Due to the lack of universally applicable and publicly available methods tailored to news datasets, many researchers redundantly implement event extraction methods for their own projects. The journalistic 5W1H questions are capable of describing the main event of an article, i.e., by answering who did what, when, where, why, and how. We provide an in-depth description of an improved version of Giveme5W1H, a system that uses syntactic and domain-specific rules to automatically extract the relevant phrases from English news articles to provide answers to these 5W1H questions. Given the answers to these questions, the system determines an article's main event. In an expert evaluation with three assessors and 120 articles, we determined an overall precision of p=0.73, and p=0.82 for answering the first four W questions, which alone can sufficiently summarize the main event reported on in a news article. We recently made our system publicly available, and it remains the only universal open-source 5W1H extractor capable of being applied to a wide range of use cases in news analysis.",
                        "Citation Paper Authors": "Authors:Felix Hamborg, Corinna Breitinger, Bela Gipp"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.08745v1": {
            "Paper Title": "Knowledge-enhanced Session-based Recommendation with Temporal\n  Transformer",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "proposed\nGated-GSNN for sequential learning in graph structure. Wu et al . ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "represents the interactions\nbetween users and items as a bipartite graph. Li et al . ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated\ngraph sequence neural networks. arXiv:1511.05493 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "use a\ngated recurrent unit model to model the user\u2019s behaviors in each\nsingle session. Li et al. ",
                    "Citation Text": "Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural attentive session-based recommendation. In CIKM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.04725",
                        "Citation Paper Title": "Title:Neural Attentive Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Given e-commerce scenarios that user profiles are invisible, session-based recommendation is proposed to generate recommendation results from short sessions. Previous work only considers the user's sequential behavior in the current session, whereas the user's main purpose in the current session is not emphasized. In this paper, we propose a novel neural networks framework, i.e., Neural Attentive Recommendation Machine (NARM), to tackle this problem. Specifically, we explore a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session, which are combined as a unified session representation later. We then compute the recommendation scores for each candidate item with a bi-linear matching scheme based on this unified session representation. We train NARM by jointly learning the item and session representations as well as their matchings. We carried out extensive experiments on two benchmark datasets. Our experimental results show that NARM outperforms state-of-the-art baselines on both datasets. Furthermore, we also find that NARM achieves a significant improvement on long sessions, which demonstrates its advantages in modeling the user's sequential behavior and main purpose simultaneously.",
                        "Citation Paper Authors": "Authors:Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "4.1 Session-based Recommendation\nSession-based recommendation, which is an emerging topic in\nthe recommendation system, has attracted interests of many re-\nsearchers from both academia and industry. Hidasi et al . ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks.\narXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.08717v1": {
            "Paper Title": "GIMIRec: Global Interaction Information Aware Multi-Interest Framework\n  for Sequential Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08663v1": {
            "Paper Title": "MAVE: A Product Dataset for Multi-source Attribute Value Extraction",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "adopt BiLSTM-\nCRF model to tag several product attributes from search queries\nwith hand-crafted features. Furthermore, Zheng et al. ",
                    "Citation Text": "G. Zheng, S. Mukherjee, X. L. Dong, and F. Li. Opentag: Open attribute value\nextraction from product profiles. In SIGKDD , pages 1049\u20131058, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01264",
                        "Citation Paper Title": "Title:OpenTag: Open Attribute Value Extraction from Product Profiles [Deep Learning, Active Learning, Named Entity Recognition]",
                        "Citation Paper Abstract": "Abstract:Extraction of missing attribute values is to find values describing an attribute of interest from a free text input. Most past related work on extraction of missing attribute values work with a closed world assumption with the possible set of values known beforehand, or use dictionaries of values and hand-crafted features. How can we discover new attribute values that we have never seen before? Can we do this with limited human annotation or supervision? We study this problem in the context of product catalogs that often have missing values for many attributes of interest.\nIn this work, we leverage product profile information such as titles and descriptions to discover missing values of product attributes. We develop a novel deep tagging model OpenTag for this extraction problem with the following contributions: (1) we formalize the problem as a sequence tagging task, and propose a joint model exploiting recurrent neural networks (specifically, bidirectional LSTM) to capture context and semantics, and Conditional Random Fields (CRF) to enforce tagging consistency, (2) we develop a novel attention mechanism to provide interpretable explanation for our model's decisions, (3) we propose a novel sampling strategy exploring active learning to reduce the burden of human annotation. OpenTag does not use any dictionary or hand-crafted features as in prior works. Extensive experiments in real-life datasets in different domains show that OpenTag with our active learning strategy discovers new attribute values from as few as 150 annotated samples (reduction in 3.3x amount of annotation effort) with a high F-score of 83%, outperforming state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "to generate the\ncontextual embeddings for the global and long input tokens. ETC\nis an extension of the Transformer ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin. Attention is all you need. In NIPS , pages 5998\u20136008, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "are the first to apply the\nBiLSTM-CRF model to a sequence tagging task. But it employs\nheavy feature engineering to extract character-level features.\nLample et al. ",
                    "Citation Text": "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer. Neural\narchitectures for named entity recognition. In NAACL-HLT , pages 260\u2013270, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01360",
                        "Citation Paper Title": "Title:Neural Architectures for Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.",
                        "Citation Paper Authors": "Authors:Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.08606v1": {
            "Paper Title": "An Empirical Study on Transfer Learning for Privilege Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10612v1": {
            "Paper Title": "Context-Based Music Recommendation Algorithm Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08486v1": {
            "Paper Title": "Text Mining Through Label Induction Grouping Algorithm Based Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08219v1": {
            "Paper Title": "Quantitative analysis of visual representation of sign elements in\n  COVID-19 context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08140v1": {
            "Paper Title": "Improving Conversational Recommendation Systems' Quality with\n  Context-Aware Item Meta Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07899v1": {
            "Paper Title": "Large Dual Encoders Are Generalizable Retrievers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07858v1": {
            "Paper Title": "EDAssistant: Supporting Exploratory Data Analysis in Computational\n  Notebooks with In-Situ Code Search and Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "April Yi Wang, Anant Mittal, Christopher Brooks, and Steve Oney. 2019. How Data Scientists Use Computational\nNotebooks for Real-Time Collaboration. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (nov 2019),\n1\u201330. https://doi.org/10.1145/3359141 ",
                    "Citation Text": "Chenglong Wang, Yu Feng, Rastislav Bodik, Isil Dillig, Alvin Cheung, and Amy J Ko. 2021. Falx: Synthesis-Powered\nVisualization Authoring. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI \u201921) .\nNew York, NY, USA, 1\u201315. https://doi.org/10.1145/3411764.3445249",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.01024",
                        "Citation Paper Title": "Title:Falx: Synthesis-Powered Visualization Authoring",
                        "Citation Paper Abstract": "Abstract:Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation.\nWe propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.",
                        "Citation Paper Authors": "Authors:Chenglong Wang, Yu Feng, Rastislav Bodik, Isil Dillig, Alvin Cheung, Amy J. Ko"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "Mi Feng, Cheng Deng, Evan M. Peck, and Lane Harrison. 2018. The Effects of Adding Search Functionality to Interactive\nVisualizations on the Web. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1\u201313.\nhttps://doi.org/10.1145/3173574.3173711 ",
                    "Citation Text": "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\nDaxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In\nFindings of the Association for Computational Linguistics: EMNLP . https://doi.org/10.18653/v1/2020.findings-emnlp.139",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08155",
                        "Citation Paper Title": "Title:CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
                        "Citation Paper Abstract": "Abstract:We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.",
                        "Citation Paper Authors": "Authors:Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "E. Clarkson, K. Desai, and J. Foley. 2009. ResultMaps: Visualization for Search Interfaces. IEEE Transactions on\nVisualization and Computer Graphics 15, 6 (2009), 1057\u20131064. https://doi.org/10.1109/TVCG.2009.176 ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02353v3": {
            "Paper Title": "Top-K Off-Policy Correction for a REINFORCE Recommender System",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "defined by:\n\u00af\ud835\udf14\ud835\udc5b(\ud835\udc60,\ud835\udc4e)=\ud835\udf14(\ud835\udc60,\ud835\udc4e)\u00cd\n(\ud835\udc60\u2032,\ud835\udc4e\u2032)\u223c\ud835\udefd\ud835\udf14(\ud835\udc60\u2032,\ud835\udc4e\u2032).\nAsE\ud835\udefd[\ud835\udf14(\ud835\udc60,\ud835\udc4e)]=1, the normalizing constant is equal to \ud835\udc5b, the batch\nsize, in expectation. As \ud835\udc5bincreases, the effect of NIS is equivalent\nto tuning down the learning rate.\nTrusted Region Policy Optimization (TRPO). TRPO ",
                    "Citation Text": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.\n2015. Trust region policy optimization. In International Conference on Machine\nLearning . 1889\u20131897.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05477",
                        "Citation Paper Title": "Title:Trust Region Policy Optimization",
                        "Citation Paper Abstract": "Abstract:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                        "Citation Paper Authors": "Authors:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", have been successfully applied to news recommenda-\ntions and display advertising. Contextual bandits offer a context-\naware refinement of the basic on-line learning approaches and tailor\nthe recommendation toward user interests ",
                    "Citation Text": "Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-\nbandit approach to personalized news article recommendation. In Proceedings of\nthe 19th international conference on World wide web . ACM, 661\u2013670.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.0146",
                        "Citation Paper Title": "Title:A Contextual-Bandit Approach to Personalized News Article Recommendation",
                        "Citation Paper Abstract": "Abstract:Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\nIn this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.\nThe contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.",
                        "Citation Paper Authors": "Authors:Lihong Li, Wei Chu, John Langford, Robert E. Schapire"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Often, extensive\nhyper-parameter tuning is required to achieve stable behavior for\nthese approaches. Despite the practical success of many value-based\napproaches such as deep Q-learning ",
                    "Citation Text": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep\nreinforcement learning. arXiv preprint arXiv:1312.5602 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.5602",
                        "Citation Paper Title": "Title:Playing Atari with Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
                        "Citation Paper Authors": "Authors:Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". A general comparison\nof modern RL approaches can be found in ",
                    "Citation Text": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-\nothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous methods for deep reinforcement learning. In International conference\non machine learning . 1928\u20131937.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.01783",
                        "Citation Paper Title": "Title:Asynchronous Methods for Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
                        "Citation Paper Authors": "Authors:Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07771v1": {
            "Paper Title": "Boosted Dense Retriever",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.06813v4": {
            "Paper Title": "A cost-benefit analysis of cross-lingual transfer methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07209v1": {
            "Paper Title": "ACE-BERT: Adversarial Cross-modal Enhanced BERT for E-commerce Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". In detail, these algorithms use RoI method which\nextracts RoIs from images as image tokens. FashionBERT ",
                    "Citation Text": "Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao\nWang. 2020. Fashionbert: Text and image matching with adaptive loss for cross-\nmodal retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . 2251\u20132260.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.09801",
                        "Citation Paper Title": "Title:FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the text and image matching in cross-modal retrieval of the fashion industry. Different from the matching in the general domain, the fashion matching is required to pay much more attention to the fine-grained information in the fashion images and texts. Pioneer approaches detect the region of interests (i.e., RoIs) from images and use the RoI embeddings as image representations. In general, RoIs tend to represent the \"object-level\" information in the fashion images, while fashion texts are prone to describe more detailed information, e.g. styles, attributes. RoIs are thus not fine-grained enough for fashion text and image matching. To this end, we propose FashionBERT, which leverages patches as image features. With the pre-trained BERT model as the backbone network, FashionBERT learns high level representations of texts and images. Meanwhile, we propose an adaptive loss to trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text and image matching and cross-modal retrieval) are incorporated to evaluate FashionBERT. On the public dataset, experiments demonstrate FashionBERT achieves significant improvements in performances than the baseline and state-of-the-art approaches. In practice, FashionBERT is applied in a concrete cross-modal retrieval application. We provide the detailed matching performance and inference efficiency analysis.",
                        "Citation Paper Authors": "Authors:Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, Hao Wang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nThe development of the pre-training in Natural Language Pro-\ncessing (NLP) is behind in CV. In recent years, there are research\nworks[ 20,29] on pre-training the generic representation. BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "classification, most of the researchers\nsubsequently found that the CNN-based neural networks[ 22,26]\npre-trained on the large-scale image corpus can server well as\ngeneric feature representation for a variety of down-stream tasks ",
                    "Citation Text": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng,\nand Trevor Darrell. 2014. Decaf: A deep convolutional activation feature for\ngeneric visual recognition. In International conference on machine learning . PMLR,\n647\u2013655.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.1531",
                        "Citation Paper Title": "Title:DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
                        "Citation Paper Abstract": "Abstract:We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
                        "Citation Paper Authors": "Authors:Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07191v1": {
            "Paper Title": "An Adaptive Graph Pre-training Framework for Localized Collaborative\n  Filtering",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "is proposed\nto use two graph attention networks to learn user embeddings and item embeddings, and the user embedding is learned\nfrom both the social graph and interaction graph. KGAT ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 950\u2013958.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "proposes to encode high-order connectivity among user-item interaction graphs into user/item\nembeddings. Specifically, it designs propagation layers to aggregate information from connected nodes. The\nrecommendation prediction is made based on the refined user and item embeddings.\n\u2022LightGCN : LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution\nnetwork for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval .\n639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "has not been\nincluded because it also requires node attributes and cannot be applied across multiple graphs. The details of these\nbaselines are presented as follows:\n\u2022NGCF : NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd\ninternational ACM SIGIR conference on Research and development in Information Retrieval . 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "because there are no node attributes or graph labels in our scenarios. Similarly, GPT-GNN ",
                    "Citation Text": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020. Gpt-gnn: Generative pre-training of graph neural networks. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1857\u20131867.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15437",
                        "Citation Paper Title": "Title:GPT-GNN: Generative Pre-Training of Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
                        "Citation Paper Authors": "Authors:Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "proposes a learnable way to better integrate non-selected nodes, which can better preserve\nthe information of both the important nodes and normal nodes .In addition, EigenPooling ",
                    "Citation Text": "Yao Ma, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019. Graph convolutional networks with eigenpooling. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining . 723\u2013731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.13107",
                        "Citation Paper Title": "Title:Graph Convolutional Networks with EigenPooling",
                        "Citation Paper Abstract": "Abstract:Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the \\textit{graph representation} from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $\\pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $\\m$ for graph classification. Theoretical analysis is provided to understand $\\pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.",
                        "Citation Paper Authors": "Authors:Yao Ma, Suhang Wang, Charu C. Aggarwal, Jiliang Tang"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". Besides,\nthere are increasing number of hierarchical pooling methods proposed to learn graph representation hierarchically,\nwhich are believed to be able to better capture the graph structure information. Specifically, DiffPool ",
                    "Citation Text": "Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with\ndifferentiable pooling. In Advances in Neural Information Processing Systems . 4800\u20134810.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08804",
                        "Citation Paper Title": "Title:Hierarchical Graph Representation Learning with Differentiable Pooling",
                        "Citation Paper Abstract": "Abstract:Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",
                        "Citation Paper Authors": "Authors:Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "or adding a virtual node that\nconnects to all the nodes in the graph and then taking its node representation as the graph representation ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493\n(2015).\nManuscript submitted to ACM22 Wang and Li, et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". In addition, there emerge some work trying to\nfurther explore the rationale behind GNNs, such as Ma et al ",
                    "Citation Text": "Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2020. A unified view on graph neural networks as graph signal denoising.\narXiv preprint arXiv:2010.01777 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01777",
                        "Citation Paper Title": "Title:A Unified View on Graph Neural Networks as Graph Signal Denoising",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN.",
                        "Citation Paper Authors": "Authors:Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, Neil Shah"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ", the first graph neural network is proposed from the spatial perspective to solve both\ngraph and node level tasks, which aggregates information from neighboring nodes for each node in every layer.\nSubsequently, Bruna et al. ",
                    "Citation Text": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral networks and locally connected networks on graphs. arXiv preprint\narXiv:1312.6203 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6203",
                        "Citation Paper Title": "Title:Spectral Networks and Locally Connected Networks on Graphs",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
                        "Citation Paper Authors": "Authors:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07165v1": {
            "Paper Title": "Discovering Explanatory Sentences in Legal Case Decisions Using\n  Pre-trained Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06348v2": {
            "Paper Title": "MedGraph: An experimental semantic information retrieval method using\n  knowledge graph embedding for the biomedical citations indexed in PubMed",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06540v1": {
            "Paper Title": "A Study on Token Pruning for ColBERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07622v1": {
            "Paper Title": "ISEEQ: Information Seeking Question Generation using Dynamic\n  Meta-Information Retrieval and Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.16102v3": {
            "Paper Title": "Machine Reading of Hypotheses for Organizational Research Reviews and\n  Pre-trained Models via R Shiny App for Non-Programmers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07615v1": {
            "Paper Title": "Cold Item Integration in Deep Hybrid Recommenders via Tunable Stochastic\n  Gates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07621v1": {
            "Paper Title": "Re-ranking With Constraints on Diversified Exposures for Homepage\n  Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06080v1": {
            "Paper Title": "UPV at TREC Health Misinformation Track 2021 Ranking with SBERT and\n  Quality Estimators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11478v1": {
            "Paper Title": "LSH methods for data deduplication in a Wikipedia artificial dataset",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "in which they performed perfect match after verbatim processing\nto discover an average of 3 percent matches between the C4 dataset and a variety of other validation\ndatasets. A model based on perplexity is presented in ",
                    "Citation Text": "T. Gao, A. Fisch, and D. Chen. Making Pre-trained Language Models Better Few-shot Learners.\npages 3816\u20133830, 2021. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.\norg/2021.acl-long.295 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15723",
                        "Citation Paper Title": "Title:Making Pre-trained Language Models Better Few-shot Learners",
                        "Citation Paper Abstract": "Abstract:The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
                        "Citation Paper Authors": "Authors:Tianyu Gao, Adam Fisch, Danqi Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12104v2": {
            "Paper Title": "GERNERMED -- An Open German Medical NER Model",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "In recent years substantial progress has been in the area of NLP which can\nmostly be attributed to the joint use of large amounts of data and its process-\ning through large language models like BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: pre-training of deep bidirectional transformers for language under-\nstanding. CoRR , abs/1810.04805, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14097v2": {
            "Paper Title": "Automated Evaluation of Web Site Accessibility Using A Dynamic\n  Accessibility Measurement Crawler",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05452v1": {
            "Paper Title": "Improving the Question Answering Quality using Answer Candidate\n  Filtering based on Natural-Language Features",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "represent the current state-of-the-art. The well-known BERT\nstill stays as the landmark model showing one of the best\nresults in many downstream tasks (cf., ",
                    "Citation Text": "C. Sun, X. Qiu, Y . Xu, and X. Huang, \u201cHow to \ufb01ne-tune BERT for text\nclassi\ufb01cation?\u201d ArXiv , vol. abs/1905.05583, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.05583",
                        "Citation Paper Title": "Title:How to Fine-Tune BERT for Text Classification?",
                        "Citation Paper Abstract": "Abstract:Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
                        "Citation Paper Authors": "Authors:Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "applied supervised learning methods.\nThe \ufb01rst utilized the following features: lexical, n-grams, bad-\nanswer speci\ufb01c, named entities, term-frequency vectors, and\nword2vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, \u201cDistributed\nrepresentations of words and phrases and their compositionality,\u201d in\nAdvances in Neural Information Processing Systems 26 (NIPS 2013) ,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04788v2": {
            "Paper Title": "\"What can I cook with these ingredients?\" -- Understanding\n  cooking-related information needs in conversational search",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "which\ncan help reduce complexity. This strategy was also found to be important by Aliannejadi et al. ",
                    "Citation Text": "M. Aliannejadi, H. Zamani, F. Crestani, and W. B. Croft. Asking clarifying questions in open-domain information-\nseeking conversations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , SIGIR\u201919, page 475\u2013484, New York, NY , USA, 2019. Association for\nComputing Machinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.06554",
                        "Citation Paper Title": "Title:Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
                        "Citation Paper Abstract": "Abstract:Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of (often only one) result(s). In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "found that contextually relevant utterances lie at a position in the conversation that is close to\nthe current utterance which informs the design of the memory and context property of such systems. Ren et al. ",
                    "Citation Text": "P. Ren, Z. Chen, Z. Ren, E. Kanoulas, C. Monz, and M. de Rijke. Conversations with search engines. CoRR ,\nabs/2004.14162, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14162",
                        "Citation Paper Title": "Title:Conversations with Search Engines: SERP-based Conversational Response Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of answering complex information needs by conversing conversations with search engines, in the sense that users can express their queries in natural language, and directly receivethe information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs, or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.\nWe pursue two goals in this paper: (1) the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and (2) the development of astate-of-the-art pipeline for conversations with search engines, the Conversations with Search Engines (CaSE), using this dataset. SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and aprior-aware pointer generator, which enables us to generate more accurate responses.\nWe carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.",
                        "Citation Paper Authors": "Authors:Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "identi\ufb01ed patterns resembling Prekop\u2019s when pairs of users conversed\nduring collaborative tasks. She also found that integrating discourse in conversational search models is necessary\nto cope with the complexity of the information-seeking process ",
                    "Citation Text": "J. R. Trippas, D. Spina, P. Thomas, M. Sanderson, H. Joho, and L. Cavedon. Towards a model for spoken\nconversational search. Information Processing & Management , 57(2):102162, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.13166",
                        "Citation Paper Title": "Title:Towards a Model for Spoken Conversational Search",
                        "Citation Paper Abstract": "Abstract:Conversation is the natural mode for information exchange in daily life, a spoken conversational interaction for search input and output is a logical format for information seeking. However, the conceptualisation of user-system interactions or information exchange in spoken conversational search (SCS) has not been explored. The first step in conceptualising SCS is to understand the conversational moves used in an audio-only communication channel for search. This paper explores conversational actions for the task of search. We define a qualitative methodology for creating conversational datasets, propose analysis protocols, and develop the SCSdata. Furthermore, we use the SCSdata to create the first annotation schema for SCS: the SCoSAS, enabling us to investigate interactivity in SCS. We further establish that SCS needs to incorporate interactivity and pro-activity to overcome the complexity that the information seeking process in an audio-only channel poses. In summary, this exploratory study unpacks the breadth of SCS. Our results highlight the need for integrating discourse in future SCS models and contributes the advancement in the formalisation of SCS models and the design of SCS systems.",
                        "Citation Paper Authors": "Authors:Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo Joho, Lawrence Cavedon"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": ". Also, resampling\nwas not conducted as BERT handles imbalanced datasets well ",
                    "Citation Text": "H. Tayyar Madabushi, E. Kochkina, and M. Castelle. Cost-sensitive BERT for generalisable sentence clas-\nsi\ufb01cation on imbalanced data. In Proceedings of the Second Workshop on Natural Language Processing for\nInternet Freedom: Censorship, Disinformation, and Propaganda , pages 125\u2013134, Hong Kong, China, Nov. 2019.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11563",
                        "Citation Paper Title": "Title:Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data",
                        "Citation Paper Abstract": "Abstract:The automatic identification of propaganda has gained significance in recent years due to technological and social changes in the way news is generated and consumed. That this task can be addressed effectively using BERT, a powerful new architecture which can be fine-tuned for text classification tasks, is not surprising. However, propaganda detection, like other tasks that deal with news documents and other forms of decontextualized social communication (e.g. sentiment analysis), inherently deals with data whose categories are simultaneously imbalanced and dissimilar. We show that BERT, while capable of handling imbalanced classes with no additional data augmentation, does not generalise well when the training and test data are sufficiently dissimilar (as is often the case with news sources, whose topics evolve over time). We show how to address this problem by providing a statistical measure of similarity between datasets and a method of incorporating cost-weighting into BERT when the training and test sets are dissimilar. We test these methods on the Propaganda Techniques Corpus (PTC) and achieve the second-highest score on sentence-level propaganda classification.",
                        "Citation Paper Authors": "Authors:Harish Tayyar Madabushi, Elena Kochkina, Michael Castelle"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "showed that information need detection in the cooking domain bene\ufb01ts\nfrom using BERT-based models, we employed these for comparison. BERT was chosen due to its success in natural\nlanguage understanding problems ",
                    "Citation Text": "T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, and\nJ. Brew. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "on a class of so-called open domain dialogues [ 43,2] ormulti-domain dialogues ",
                    "Citation Text": "G. Gordon-Hall, P. J. Gorinski, G. Lampouras, and I. Iacobacci. Show us the way: Learning to manage dialog\nfrom demonstrations. CoRR , abs/2004.08114, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.08114",
                        "Citation Paper Title": "Title:Show Us the Way: Learning to Manage Dialog from Demonstrations",
                        "Citation Paper Abstract": "Abstract:We present our submission to the End-to-End Multi-Domain Dialog Challenge Track of the Eighth Dialog System Technology Challenge. Our proposed dialog system adopts a pipeline architecture, with distinct components for Natural Language Understanding, Dialog State Tracking, Dialog Management and Natural Language Generation. At the core of our system is a reinforcement learning algorithm which uses Deep Q-learning from Demonstrations to learn a dialog policy with the help of expert examples. We find that demonstrations are essential to training an accurate dialog policy where both state and action spaces are large. Evaluation of our Dialog Management component shows that our approach is effective - beating supervised and reinforcement learning baselines.",
                        "Citation Paper Authors": "Authors:Gabriel Gordon-Hall, Philip John Gorinski, Gerasimos Lampouras, Ignacio Iacobacci"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "attempts to capture context with the use of topic\nsequence models in order to suggest topics in open domain dialogues. Qu et al. ",
                    "Citation Text": "C. Qu, L. Yang, W. B. Croft, Y . Zhang, J. R. Trippas, and M. Qiu. User intent prediction in information-seeking\nconversations. In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval , CHIIR\n\u201919, page 25\u201333, New York, NY , USA, 2019. Association for Computing Machinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.03489",
                        "Citation Paper Title": "Title:User Intent Prediction in Information-seeking Conversations",
                        "Citation Paper Abstract": "Abstract:Conversational assistants are being progressively adopted by the general population. However, they are not capable of handling complicated information-seeking tasks that involve multiple turns of information exchange. Due to the limited communication bandwidth in conversational search, it is important for conversational assistants to accurately detect and predict user intent in information-seeking conversations. In this paper, we investigate two aspects of user intent prediction in an information-seeking setting. First, we extract features based on the content, structural, and sentiment characteristics of a given utterance, and use classic machine learning methods to perform user intent prediction. We then conduct an in-depth feature importance analysis to identify key features in this prediction task. We find that structural features contribute most to the prediction performance. Given this finding, we construct neural classifiers to incorporate context information and achieve better performance without feature engineering. Our findings can provide insights into the important factors and effective methods of user intent prediction in information-seeking conversations.",
                        "Citation Paper Authors": "Authors:Chen Qu, Liu Yang, Bruce Croft, Yongfeng Zhang, Johanne R. Trippas, Minghui Qiu"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "focus on the prediction of latent intents in text-based product queries.\nIn addition to predicting user intents, there have been efforts to improve information need prediction. For example,\nAliannejadi et al. ",
                    "Citation Text": "M. Aliannejadi, M. Chakraborty, E. A. R\u00edssola, and F. Crestani. Harnessing evolution of multi-turn conversations\nfor effective answer retrieval. In Proceedings of the 2020 Conference on Human Information Interaction and\nRetrieval , CHIIR \u201920, page 33\u201342, New York, NY , USA, 2020. Association for Computing Machinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10554",
                        "Citation Paper Title": "Title:Harnessing Evolution of Multi-Turn Conversations for Effective Answer Retrieval",
                        "Citation Paper Abstract": "Abstract:With the improvements in speech recognition and voice generation technologies over the last years, a lot of companies have sought to develop conversation understanding systems that run on mobile phones or smart home devices through natural language interfaces. Conversational assistants, such as Google Assistant and Microsoft Cortana, can help users to complete various types of tasks. This requires an accurate understanding of the user's information need as the conversation evolves into multiple turns. Finding relevant context in a conversation's history is challenging because of the complexity of natural language and the evolution of a user's information need. In this work, we present an extensive analysis of language, relevance, dependency of user utterances in a multi-turn information-seeking conversation. To this aim, we have annotated relevant utterances in the conversations released by the TREC CaST 2019 track. The annotation labels determine which of the previous utterances in a conversation can be used to improve the current one. Furthermore, we propose a neural utterance relevance model based on BERT fine-tuning, outperforming competitive baselines. We study and compare the performance of multiple retrieval models, utilizing different strategies to incorporate the user's context. The experimental results on both classification and retrieval tasks show that our proposed approach can effectively identify and incorporate the conversation context. We show that processing the current utterance using the predicted relevant utterance leads to a 38% relative improvement in terms of nDCG@20. Finally, to foster research in this area, we have released the dataset of the annotations.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, Fabio Crestani"
                    }
                },
                {
                    "Sentence ID": 102,
                    "Sentence": "in the sense that\nclarifying questions led to better search results since they help to resolve complex information needs. Zamani et al. ",
                    "Citation Text": "H. Zamani, B. Mitra, E. Chen, G. Lueck, F. Diaz, P. N. Bennett, N. Craswell, and S. T. Dumais. Analyzing and\nlearning from user interactions for search clari\ufb01cation. In Proceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Information Retrieval , SIGIR \u201920, page 1181\u20131190, New York, NY ,\nUSA, 2020. Association for Computing Machinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.00166",
                        "Citation Paper Title": "Title:Analyzing and Learning from User Interactions for Search Clarification",
                        "Citation Paper Abstract": "Abstract:Asking clarifying questions in response to search queries has been recognized as a useful technique for revealing the underlying intent of the query. Clarification has applications in retrieval systems with different interfaces, from the traditional web search interfaces to the limited bandwidth interfaces as in speech-only and small screen devices. Generation and evaluation of clarifying questions have been recently studied in the literature. However, user interaction with clarifying questions is relatively unexplored. In this paper, we conduct a comprehensive study by analyzing large-scale user interactions with clarifying questions in a major web search engine. In more detail, we analyze the user engagements received by clarifying questions based on different properties of search queries, clarifying questions, and their candidate answers. We further study click bias in the data, and show that even though reading clarifying questions and candidate answers does not take significant efforts, there still exist some position and presentation biases in the data. We also propose a model for learning representation for clarifying questions based on the user interaction data as implicit feedback. The model is used for re-ranking a number of automatically generated clarifying questions for a given query. Evaluation on both click data and human labeled data demonstrates the high quality of the proposed method.",
                        "Citation Paper Authors": "Authors:Hamed Zamani, Bhaskar Mitra, Everest Chen, Gord Lueck, Fernando Diaz, Paul N. Bennett, Nick Craswell, Susan T. Dumais"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05197v1": {
            "Paper Title": "Self-Supervised Bot Play for Conversational Recommendation with\n  Justifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05003v1": {
            "Paper Title": "Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's\n  Revision History",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04810v1": {
            "Paper Title": "From Scattered Sources to Comprehensive Technology Landscape: A\n  Recommendation-based Retrieval Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04711v1": {
            "Paper Title": "Feature Modulation to Improve Struggle Detection in Web Search: A\n  Psychological Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04666v1": {
            "Paper Title": "Densifying Sparse Representations for Passage Retrieval by\n  Representational Slicing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04368v1": {
            "Paper Title": "Semantic TrueLearn: Using Semantic Knowledge Graphs in Recommendation\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "Thomas N. Kipf and Max Welling. Semi-supervised classi\fcation with graph con-\nvolutional networks. In International Conference on Learning Representations\n(ICLR) , 2017. ",
                    "Citation Text": "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K\u007f uttler, Mike Lewis, Wen-tau Yih, Tim\nRockt\u007f aschel, et al. Retrieval-augmented generation for knowledge-intensive nlp\ntasks. arXiv preprint arXiv:2005.11401 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11401",
                        "Citation Paper Title": "Title:Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                        "Citation Paper Abstract": "Abstract:Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                        "Citation Paper Authors": "Authors:Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "infer hidden node embeddings ( H`+1) by weighted averaging the embeddings\nof its neighbours using adjacency matrix Aand diagonal D(H`+1=D\u00001\n2AD\u00001\n2H`W`).\nThe popularising attention mechanism ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine transla-\ntion by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun,\neditors, 3rd International Conference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04184v1": {
            "Paper Title": "Zero-Shot Recommendation as Language Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04090v1": {
            "Paper Title": "Seed-driven Document Ranking for Systematic Reviews: A Reproducibility\n  Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01686v2": {
            "Paper Title": "AliCG: Fine-grained and Evolvable Conceptual Graph Construction for\n  Semantic Search at Alibaba",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "treat concept mining as sequence tagging. Nevertheless, the perfor-\nmance of those long-tail concepts degrades significantly. Several\nlow-resource sequence tagging approaches that utilize transfer\nlearning ",
                    "Citation Text": "Yixin Cao, Zikun Hu, Tat-Seng Chua, Zhiyuan Liu, and Heng Ji. 2019. Low-\nResource Name Tagging Learned with Weakly Labeled Data. arXiv preprint\narXiv:1908.09659 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.09659",
                        "Citation Paper Title": "Title:Low-Resource Name Tagging Learned with Weakly Labeled Data",
                        "Citation Paper Abstract": "Abstract:Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency.",
                        "Citation Paper Authors": "Authors:Yixin Cao, Zikun Hu, Tat-Seng Chua, Zhiyuan Liu, Heng Ji"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". The latest methods for concept mining\nrely on phrase quality. ",
                    "Citation Text": "Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, and Jiawei Han.\n2018. Automated phrase mining from massive text corpora. IEEE Transactions on\nKnowledge and Data Engineering 30, 10 (2018), 1825\u20131837.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04457",
                        "Citation Paper Title": "Title:Automated Phrase Mining from Massive Text Corpora",
                        "Citation Paper Abstract": "Abstract:As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. Phrase mining is important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases.\nSince one can easily obtain many quality phrases from public knowledge bases to a scale that is much larger than that produced by human experts, in this paper, we propose a novel framework for automated phrase mining, AutoPhrase, which leverages this large amount of high-quality phrases in an effective way and achieves better performance compared to limited human labeled phrases. In addition, we develop a POS-guided phrasal segmentation model, which incorporates the shallow syntactic information in part-of-speech (POS) tags to further enhance the performance, when a POS tagger is available. Note that, AutoPhrase can support any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, the new method has shown significant improvements in effectiveness on five real-world datasets across different domains and languages.",
                        "Citation Paper Authors": "Authors:Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Fine-graied Concept Acquisition. Conventional concept min-\ning methods are closely related to noun phrase segmentation and\nnamed entity recognition ",
                    "Citation Text": "Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2021. Contrastive triple extraction with generative transformer.\nInIn AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.06207",
                        "Citation Paper Title": "Title:Contrastive Triple Extraction with Generative Transformer",
                        "Citation Paper Abstract": "Abstract:Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines.",
                        "Citation Paper Authors": "Authors:Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, Huajun Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.03667v1": {
            "Paper Title": "Cross-domain User Preference Learning for Cold-start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05625v3": {
            "Paper Title": "CAN: Feature Co-Action for Click-Through Rate Prediction",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "uses a product layer followed by a fully connected\nlayer to explore high-order feature interactions.\n\u2022NCF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In WWW . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is a variant of Factorization Machines\n(FMs) with field aware that can classify large sparse data. DeepFFM\nappends a DNN term to incorporate high order combination in-\nformation implicitly.\n\u2022PNN ",
                    "Citation Text": "Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.\n2016. Product-based neural networks for user response prediction. In ICDM .\n1149\u20131154.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00144",
                        "Citation Paper Title": "Title:Product-based Neural Networks for User Response Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.",
                        "Citation Paper Authors": "Authors:Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "is a representative method in\nthe age of shallow models. In FM, feature interaction is modeled as\nthe inner product of latent vectors of features. However, FM uses\nthe same latent vectors in different types of inter-field interactions.\nDeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepfm: a factorization-machine based neural network for ctr prediction. In\nIJCAI . 2782\u20132788.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "uses sufficient multi-vectors to capture complicated patterns lying\nin the user and items. Moreover, inspired by the success of self-\nattention architecture in the tasks of sequence learning [ 2,21], the\ntransformer is introduced in Feng et al . ",
                    "Citation Text": "Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping\nYang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. In\nIJCAI . 2301\u20132307.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06482",
                        "Citation Paper Title": "Title:Deep Session Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-Through Rate (CTR) prediction plays an important role in many industrial applications, such as online advertising and recommender systems. How to capture users' dynamic and evolving interests from their behavior sequences remains a continuous research topic in the CTR prediction. However, most existing studies overlook the intrinsic structure of the sequences: the sequences are composed of sessions, where sessions are user behaviors separated by their occurring time. We observe that user behaviors are highly homogeneous in each session, and heterogeneous cross sessions. Based on this observation, we propose a novel CTR model named Deep Session Interest Network (DSIN) that leverages users' multiple historical sessions in their behavior sequences. We first use self-attention mechanism with bias encoding to extract users' interests in each session. Then we apply Bi-LSTM to model how users' interests evolve and interact among sessions. Finally, we employ the local activation unit to adaptively learn the influences of various session interests on the target item. Experiments are conducted on both advertising and production recommender datasets and DSIN outperforms other state-of-the-art models on both datasets.",
                        "Citation Paper Authors": "Authors:Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, Keping Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.11879v2": {
            "Paper Title": "Hybrid Autoregressive Inference for Scalable Multi-hop Explanation\n  Regeneration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13922v2": {
            "Paper Title": "New Hybrid Techniques for Business Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02601v1": {
            "Paper Title": "Variational Autoencoder with CCA for Audio-Visual Cross-Modal Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "to extract visual features of 512 dimensions, and a VGGish ",
                    "Citation Text": "S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold\net al. , \u201cCnn architectures for large-scale audio classification,\u201d in 2017 ieee international conference on acoustics, speech\nand signal processing (icassp) . IEEE, 2017, pp. 131\u2013135.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09430",
                        "Citation Paper Title": "Title:CNN Architectures for Large-Scale Audio Classification",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.",
                        "Citation Paper Authors": "Authors:Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02591v1": {
            "Paper Title": "Multiple Interest and Fine Granularity Network for User Modeling",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "highlights that user behaviors\nare highly homogeneous in each session and heterogeneous cross\nsessions and designs a self-attention network with bias encoding\nto get the corresponding interest representation of each session.\nMIND ",
                    "Citation Text": "Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,\nGuoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-Interest\nNetwork with Dynamic Routing for Recommendation at Tmall. In CIKM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08030",
                        "Citation Paper Title": "Title:Multi-Interest Network with Dynamic Routing for Recommendation at Tmall",
                        "Citation Paper Abstract": "Abstract:Industrial recommender systems usually consist of the matching stage and the ranking stage, in order to handle the billion-scale of users and items. The matching stage retrieves candidate items relevant to user interests, while the ranking stage sorts candidate items by user interests. Thus, the most critical ability is to model and represent user interests for either stage. Most of the existing deep learning-based models represent one user as a single vector which is insufficient to capture the varying nature of user's interests. In this paper, we approach this problem from a different view, to represent one user with multiple vectors encoding the different aspects of the user's interests. We propose the Multi-Interest Network with Dynamic routing (MIND) for dealing with user's diverse interests in the matching stage. Specifically, we design a multi-interest extractor layer based on capsule routing mechanism, which is applicable for clustering historical behaviors and extracting diverse interests. Furthermore, we develop a technique named label-aware attention to help learn a user representation with multiple vectors. Through extensive experiments on several public benchmarks and one large-scale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods for recommendation. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.",
                        "Citation Paper Authors": "Authors:Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao, Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "considers the\ntemporal relationship among the historical behaviors and proposes\nto model the evolution of users\u2019 interests with an interest extrac-\ntion layers based on GRU. DSIN ",
                    "Citation Text": "Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping\nYang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. In\nIJCAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06482",
                        "Citation Paper Title": "Title:Deep Session Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-Through Rate (CTR) prediction plays an important role in many industrial applications, such as online advertising and recommender systems. How to capture users' dynamic and evolving interests from their behavior sequences remains a continuous research topic in the CTR prediction. However, most existing studies overlook the intrinsic structure of the sequences: the sequences are composed of sessions, where sessions are user behaviors separated by their occurring time. We observe that user behaviors are highly homogeneous in each session, and heterogeneous cross sessions. Based on this observation, we propose a novel CTR model named Deep Session Interest Network (DSIN) that leverages users' multiple historical sessions in their behavior sequences. We first use self-attention mechanism with bias encoding to extract users' interests in each session. Then we apply Bi-LSTM to model how users' interests evolve and interact among sessions. Finally, we employ the local activation unit to adaptively learn the influences of various session interests on the target item. Experiments are conducted on both advertising and production recommender datasets and DSIN outperforms other state-of-the-art models on both datasets.",
                        "Citation Paper Authors": "Authors:Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, Keping Yang"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "leverages an attention mechanism to capture the diverse inter-\nests of a user on different candidate items. DIEN ",
                    "Citation Text": "Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,\nand Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate\nPrediction. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03672",
                        "Citation Paper Title": "Title:Deep Interest Evolution Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate~(CTR) prediction, whose goal is to estimate the probability of the user clicks, has become one of the core tasks in advertising systems. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, few work consider the changing trend of interest. In this paper, we propose a novel model, named Deep Interest Evolution Network~(DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7\\% improvement on CTR.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "In recent decades, user interest modeling has attracted much atten-\ntion in industrial applications such as recommender systems and\nonline advertising, which concentrates on learning the representa-\ntion of users\u2019 interests from the historical behavior sequences. DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma,\nYanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for\nClick-Through Rate Prediction. In KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02570v1": {
            "Paper Title": "Extracting and Measuring Uncertain Biomedical Knowledge from Scientific\n  Statements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02530v1": {
            "Paper Title": "Exploring and Mitigating Gender Bias in Recommender Systems with\n  Explicit Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05346v2": {
            "Paper Title": "End-to-End Training of Multi-Document Reader and Retriever for\n  Open-Domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02406v1": {
            "Paper Title": "Considering user dynamic preferences for mitigating negative effects of\n  long tail in recommender systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03111v2": {
            "Paper Title": "Sparse Distributed Memory using Spiking Neural Networks on Nengo",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02006v1": {
            "Paper Title": "User-click Modelling for Predicting Purchase Intent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01944v1": {
            "Paper Title": "Towards Low-loss 1-bit Quantization of User-item Representations for\n  Top-K Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "aims to binarize all parameters a nd\nactivationsinneuralnetworks sothattheycanevenbetrain edby\nlogical units of CPUs. Binarized models will dramatically r educe\nthememoryusage.DespitetheprogressofbinarizationCNNs [34,\n35]formultimediaretrieval,thistechniqueisnotadequat elystud-\niedingeometricdeeplearning[3,41,45].Bi-GCN ",
                    "Citation Text": "Junfu Wang, Yunhong Wang, Zhen Yang, Liang Yang, and Yua nfang Guo. 2021.\nBi-gcn:Binarygraph convolutional network.In CVPR.1561\u20131570.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.13149",
                        "Citation Paper Title": "Title:Binary Graph Convolutional Network with Capacity Exploration",
                        "Citation Paper Abstract": "Abstract:The current success of Graph Neural Networks (GNNs) usually relies on loading the entire attributed graph for processing, which may not be satisfied with limited memory resources, especially when the attributed graph is large. This paper pioneers to propose a Binary Graph Convolutional Network (Bi-GCN), which binarizes both the network parameters and input node attributes and exploits binary operations instead of floating-point matrix multiplications for network compression and acceleration. Meanwhile, we also propose a new gradient approximation based back-propagation method to properly train our Bi-GCN. According to the theoretical analysis, our Bi-GCN can reduce the memory consumption by an average of ~31x for both the network parameters and input data, and accelerate the inference speed by an average of ~51x, on three citation networks, i.e., Cora, PubMed, and CiteSeer. Besides, we introduce a general approach to generalize our binarization method to other variants of GNNs, and achieve similar efficiencies. Although the proposed Bi-GCN and Bi-GNNs are simple yet efficient, these compressed networks may also possess a potential capacity problem, i.e., they may not have enough storage capacity to learn adequate representations for specific tasks. To tackle this capacity problem, an Entropy Cover Hypothesis is proposed to predict the lower bound of the width of Bi-GNN hidden layers. Extensive experiments have demonstrated that our Bi-GCN and Bi-GNNs can give comparable performances to the corresponding full-precision baselines on seven node classification datasets and verified the effectiveness of our Entropy Cover Hypothesis for solving the capacity problem.",
                        "Citation Paper Authors": "Authors:Junfu Wang, Yuanfang Guo, Liang Yang, Yunhong Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01810v1": {
            "Paper Title": "Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a\n  New Czech Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00963v2": {
            "Paper Title": "Multi-Domain Transformer-Based Counterfactual Augmentation for Earnings\n  Call Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01160v1": {
            "Paper Title": "Learning Robust Recommender from Noisy Implicit Feedback",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ") \ufb01rst corrupt the interactions\nof user by random noises, and then try to reconstruct\nthe original one with auto-encoders. Besides, adversarial\nlearning ",
                    "Citation Text": "J. Wang, L. Yu, W. Zhang, Y. Gong, Y. Xu, B. Wang, P . Zhang, and\nD. Zhang, \u201cIrgan: A minimax game for unifying generative and\ndiscriminative information retrieval models,\u201d in SIGIR . ACM,\n2017, pp. 515\u2013524.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10513",
                        "Citation Paper Title": "Title:IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models",
                        "Citation Paper Abstract": "Abstract:This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",
                        "Citation Paper Authors": "Authors:Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, Dell Zhang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nOverall, the results are also consistent with the memoriza-\ntion effect ",
                    "Citation Text": "D. Arpit, S. Jastrz\u02db ebski, N. Ballas, D. Krueger, E. Bengio, M. S.\nKanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio et al. , \u201cA\ncloser look at memorization in deep networks,\u201d in ICML . PMLR,\n2017, pp. 233\u2013242.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05394",
                        "Citation Paper Title": "Title:A Closer Look at Memorization in Deep Networks",
                        "Citation Paper Abstract": "Abstract:We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
                        "Citation Paper Authors": "Authors:Devansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00973v1": {
            "Paper Title": "Adversarial Robustness of Deep Reinforcement Learning based Dynamic\n  Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00429v3": {
            "Paper Title": "Enhancing Top-N Item Recommendations by Peer Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00859v1": {
            "Paper Title": "Are Investors Biased Against Women? Analyzing How Gender Affects Startup\n  Funding in Europe",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00160v1": {
            "Paper Title": "Towards Full-Fledged Argument Search: A Framework for Extracting and\n  Clustering Arguments from Unstructured Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00076v1": {
            "Paper Title": "Using Conversational Artificial Intelligence to Support Children's\n  Search in the Classroom",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.14106v2": {
            "Paper Title": "Enhancing Keyphrase Extraction from Academic Articles with their\n  Reference Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12929v2": {
            "Paper Title": "Unbiased Pairwise Learning to Rank in Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11439v1": {
            "Paper Title": "Automated Drug-Related Information Extraction from French Clinical\n  Documents: ReLyfe Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00827v1": {
            "Paper Title": "Changepoint Analysis of Topic Proportions in Temporal Text Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.00870v2": {
            "Paper Title": "Lightweight representation learning for efficient and scalable\n  recommendation",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nAdvances at scale are particularly notable on graph networks.\nA data-efficient Graph Convolutional Network (GCN) combines\nefficient random walks and graph convolutions incorporating both\ngraph structure and node information at the scale of billions of\nusers and items ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (London, United Kingdom)\n(KDD \u201918) . Association for Computing Machinery, New York, NY, USA, 974\u2013983.\nhttps://doi.org/10.1145/3219819.3219890",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", making\nit possible to use fast-KNN retrieval techniques, some ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International\nConference on World Wide Web (Perth, Australia) (WWW \u201917) . International World\nWide Web Conferences Steering Committee, Republic and Canton of Geneva,\nCHE, 173\u2013182. https://doi.org/10.1145/3038912.3052569",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.00627v3": {
            "Paper Title": "Improving Community Detection Performance in Heterogeneous Music Network\n  by Learning Edge-type Usefulness Distribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.14139v1": {
            "Paper Title": "Semantic Code Search for Smart Contracts",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "which is used to\nnormalize the activation value of each layer. Given that the embed-\nding modules for \ud835\udc47,\ud835\udc39and\ud835\udc34differ only in parameters, we here take\nthe embedding process for \ud835\udc47as an example.\nSimilar to Transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "which is used to prevent network degen-\neration, Norm denotes Layer Normalization ",
                    "Citation Text": "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-\ntion. CoRR abs/1607.06450 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06450",
                        "Citation Paper Title": "Title:Layer Normalization",
                        "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                        "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "expands convolution operation from traditional data (e.g. image) to\ngraph data. Its core idea is to perform first-order approximation on\nthe convolution kernel around each node. Graph attention network\n(GAT) ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". NCS merely uses the tokens from the code\ncorpus for word embedding and highly depends on the overlapped\nwords between code and query. UNIF ",
                    "Citation Text": "Jos\u00e9 Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.\n2019. When deep learning met code search. In ESEC/SIGSOFT FSE . ACM, 964\u2013974.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03813",
                        "Citation Paper Title": "Title:When Deep Learning Met Code Search",
                        "Citation Paper Abstract": "Abstract:There have been multiple recent proposals on using deep neural networks for code search using natural language. Common across these proposals is the idea of $\\mathit{embedding}$ code and natural language queries, into real vectors and then using vector distance to approximate semantic correlation between code and the query. Multiple approaches exist for learning these embeddings, including $\\mathit{unsupervised}$ techniques, which rely only on a corpus of code examples, and $\\mathit{supervised}$ techniques, which use an $\\mathit{aligned}$ corpus of paired code and natural language descriptions. The goal of this supervision is to produce embeddings that are more similar for a query and the corresponding desired code snippet. Clearly, there are choices in whether to use supervised techniques at all, and if one does, what sort of network and training to use for supervision. This paper is the first to evaluate these choices systematically. To this end, we assembled implementations of state-of-the-art techniques to run on a common platform, training and evaluation corpora. To explore the design space in network complexity, we also introduced a new design point that is a $\\mathit{minimal}$ supervision extension to an existing unsupervised technique. Our evaluation shows that: 1. adding supervision to an existing unsupervised technique can improve performance, though not necessarily by much; 2. simple networks for supervision can be more effective that more sophisticated sequence-based networks for code search; 3. while it is common to use docstrings to carry out supervision, there is a sizeable gap between the effectiveness of docstrings and a more query-appropriate supervision corpus.\nThe evaluation dataset is now available at arXiv:1908.09804",
                        "Citation Paper Authors": "Authors:Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, Satish Chandra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.10820v2": {
            "Paper Title": "AdnFM: An Attentive DenseNet based Factorization Machine for CTR\n  Prediction",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "transforms embeddings of users\u2019 watching lists into a vector\nof fixed length by an average pooling. The Deep Interest Network\n(DIN) ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In SIGKDD .\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "captures feature interactions of bounded degrees in an explicit\nfashion. Another FiBiNet proposed in ",
                    "Citation Text": "Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: Combining\nFeature Importance and Bilinear feature Interaction for Click-Through Rate\nPrediction. arXiv preprint arXiv:1905.09433 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09433",
                        "Citation Paper Title": "Title:FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).",
                        "Citation Paper Authors": "Authors:Tongwen Huang, Zhiqi Zhang, Junlin Zhang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". Enlightened by the transformer\nmodel, researchers bring attention mechanism into CTR prediction\nand facilitate feature selection automatically, such as AFM ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "uses the attention mechanism to learn the representation\nof the users\u2019 historical behaviors with respect to the target item.\nThe Deep Interest Evolution Network (DIEN) ",
                    "Citation Text": "Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang\nZhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate\nprediction. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03672",
                        "Citation Paper Title": "Title:Deep Interest Evolution Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate~(CTR) prediction, whose goal is to estimate the probability of the user clicks, has become one of the core tasks in advertising systems. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, few work consider the changing trend of interest. In this paper, we propose a novel model, named Deep Interest Evolution Network~(DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7\\% improvement on CTR.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "effectively learns the\nfeature interactions via the bilinear function in the Squeeze-and-\nExcitation block ",
                    "Citation Text": "Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition . 7132\u20137141.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01507",
                        "Citation Paper Title": "Title:Squeeze-and-Excitation Networks",
                        "Citation Paper Abstract": "Abstract:The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "uses pre-trained factorization machines\nfor field embedding to learn high-order feature interactions. Instead\nof using pairwise feature interactions, the Deep & cross network ",
                    "Citation Text": "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network\nfor ad click predictions. In ADKDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "introduces\na FM layer as a wide component and uses a deep component to\nlearn implicit feature interactions. Factorization Machine supported\nNeural Network (FNN) ",
                    "Citation Text": "Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field\ncategorical data. In European conference on information retrieval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.02376",
                        "Citation Paper Title": "Title:Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Weinan Zhang, Tianming Du, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "jointly trains a wide model and a deep\nmodel, which leverages the effectiveness of feature engineering\nand learns implicit feature interactions. DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. arXiv\npreprint arXiv:1703.04247 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "5.1 Deep Learning-Based Models\nSeveral deep learning-based models have been proposed for the CTR\nprediction. Wide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .\n2016. Wide & deep learning for recommender systems. In RecSys .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13957v1": {
            "Paper Title": "Interpreting Dense Retrieval as Mixture of Topics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06731v2": {
            "Paper Title": "PiRank: Scalable Learning To Rank via Differentiable Sorting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08707v2": {
            "Paper Title": "Contextualized Query Embeddings for Conversational Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13576v1": {
            "Paper Title": "Job Recommender Systems: A Review",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nLinkedIn\u2019s job recommender system/search engine is largel y described in [62, 75], respectively. To facilitate\ndi\ufb00erent information needs, LinkedIn encompasses multipl e search indices, which are combined into one to\nfacilitate federated search ",
                    "Citation Text": "Dhruv Arya, Viet Ha-Thuc, and Shakti Sinha. Personalize d federated search at linkedin. In Proceedings\nof the 24th ACM International on Conference on Information and Kn owledge Management , pages\n1699\u20131702, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04924",
                        "Citation Paper Title": "Title:Personalized Federated Search at LinkedIn",
                        "Citation Paper Abstract": "Abstract:LinkedIn has grown to become a platform hosting diverse sources of information ranging from member profiles, jobs, professional groups, slideshows etc. Given the existence of multiple sources, when a member issues a query like \"software engineer\", the member could look for software engineer profiles, jobs or professional groups. To tackle this problem, we exploit a data-driven approach that extracts searcher intents from their profile data and recent activities at a large scale. The intents such as job seeking, hiring, content consuming are used to construct features to personalize federated search experience. We tested the approach on the LinkedIn homepage and A/B tests show significant improvements in member engagement. As of writing this paper, the approach powers all of federated search on LinkedIn homepage.",
                        "Citation Paper Authors": "Authors:Dhruv Arya, Viet Ha-Thuc, Shakti Sinha"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13468v1": {
            "Paper Title": "Emotion Embedding Spaces for Matching Music to Stories",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "transformer\nmodel for text analysis, which is a compact variant of the\npopular BERT transformer model [14, 16]. We use a pre-\ntrained model from the Huggingface library ",
                    "Citation Text": "T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jer-\nnite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, \u201cTransformers: State-of-\nthe-art natural language processing,\u201d in Proc. of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13466v1": {
            "Paper Title": "Streamlining Evaluation with ir-measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15679v2": {
            "Paper Title": "Kernel Density Estimation based Factored Relevance Model for\n  Multi-Contextual Point-of-Interest Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14084v2": {
            "Paper Title": "Building an Application Independent Natural Language Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13122v1": {
            "Paper Title": "GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "CNNs for CBIR. Since the introduction of deep learning models for feature extraction in ",
                    "Citation Text": "Artem Babenko, Anton Slesarev, Alexander Chigorin, and Victor S. Lempitsky. Neural codes for image retrieval.\nInECCV (1) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1404.1777",
                        "Citation Paper Title": "Title:Neural Codes for Image Retrieval",
                        "Citation Paper Abstract": "Abstract:It has been shown that the activations invoked by an image within the top layers of a large convolutional neural network provide a high-level descriptor of the visual content of the image. In this paper, we investigate the use of such descriptors (neural codes) within the image retrieval application. In the experiments with several standard retrieval benchmarks, we establish that neural codes perform competitively even when the convolutional neural network has been trained for an unrelated classification task (e.g.\\ Image-Net). We also evaluate the improvement in the retrieval performance of neural codes, when the network is retrained on a dataset of images that are similar to images encountered at test time.\nWe further evaluate the performance of the compressed neural codes and show that a simple PCA compression provides very good short codes that give state-of-the-art accuracy on a number of datasets. In general, neural codes turn out to be much more resilient to such compression in comparison other state-of-the-art descriptors. Finally, we show that discriminative dimensionality reduction trained on a dataset of pairs of matched photographs improves the performance of PCA-compressed neural codes even further. Overall, our quantitative experiments demonstrate the promise of neural codes as visual descriptors for image retrieval.",
                        "Citation Paper Authors": "Authors:Artem Babenko, Anton Slesarev, Alexandr Chigorin, Victor Lempitsky"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13051v1": {
            "Paper Title": "Ranking by Momentum based on Pareto ordering of entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03686v2": {
            "Paper Title": "Long Short-Term Temporal Meta-learning in Online Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10154v2": {
            "Paper Title": "Small Changes Make Big Differences: Improving Multi-turn Response\n  Selection in Dialogue Systems via Fine-Grained Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11338v3": {
            "Paper Title": "VLDeformer: Vision-Language Decomposed Transformer for Fast Cross-Modal\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "86.9 98.1 99.2 75.5 94.0 96.6 91.7 - - - - - - - 405.3\nOscar-base ",
                    "Citation Text": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In European Conference on Computer\nVision , pages 121\u2013137, 2020. 1, 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06165",
                        "Citation Paper Title": "Title:Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
                        "Citation Paper Abstract": "Abstract:Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.",
                        "Citation Paper Authors": "Authors:Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "VL Transformers Pre-trained VL transformers have\nshown impressive performance for many multimodal tasks.\nThese models learn the cross-modal interaction with an\nearly-interaction data\ufb02ow, where the text and image fea-Type Model T2I I2T Pre-train data Time (s)\n(a) VinVL ",
                    "Citation Text": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\nVinvl: Revisiting visual representations in vision-language\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 5579\u2013\n5588, 2021. 2, 3, 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.00529",
                        "Citation Paper Title": "Title:VinVL: Revisiting Visual Representations in Vision-Language Models",
                        "Citation Paper Abstract": "Abstract:This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \\oscar \\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.",
                        "Citation Paper Authors": "Authors:Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "develop the interaction from global and local views.\nPre-trained Visual-Semantic Embedding Models Pre-\ntraining transformers typically use several millions of text-\nimage pairs. Recently, some researchers have explored pre-\ntraining visual-semantic embedding methods with larger\ntext-image pairs. For example, CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\n9ing transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "calculates attention between detected object features and\nword embedding from both visual and text view, respec-\ntively. The following work SCAN ",
                    "Citation Text": "Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi-\naodong He. Stacked cross attention for image-text matching.\nInProceedings of the European Conference on Computer Vi-\nsion (ECCV) , pages 201\u2013216, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08024",
                        "Citation Paper Title": "Title:Stacked Cross Attention for Image-Text Matching",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: this https URL.",
                        "Citation Paper Authors": "Authors:Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12618v1": {
            "Paper Title": "Group based Personalized Search by Integrating Search Behaviour and\n  Friend Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12614v1": {
            "Paper Title": "PSSL: Self-supervised Learning for Personalized Search with Contrastive\n  Sampling",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "designed several paragraph-level pre-training tasks to enhance the\nrepresentations of queries and documents. Ma et al . ",
                    "Citation Text": "Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng.\n2021. PROP: Pre-training with Representative Words Prediction for Ad-hoc\nRetrieval. In WSDM . ACM, 283\u2013291.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10137",
                        "Citation Paper Title": "Title:PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:Recently pre-trained language representation models such as BERT have shown great success when fine-tuned on downstream tasks including information retrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval have not been well explored. In this paper, we propose Pre-training with Representative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired by the classical statistical language model for IR, specifically the query likelihood model, which assumes that the query is generated as the piece of text representative of the \"ideal\" document. Based on this idea, we construct the representative words prediction (ROP) task for pre-training. Given an input document, we sample a pair of word sets according to the document language model, where the set with higher likelihood is deemed as more representative of the document. We then pre-train the Transformer model to predict the pairwise preference between the two word sets, jointly with the Masked Language Model (MLM) objective. By further fine-tuning on a variety of representative downstream ad-hoc retrieval tasks, PROP achieves significant improvements over baselines without pre-training or with other pre-training methods. We also show that PROP can achieve exciting performance under both the zero- and low-resource IR settings. The code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "used mutual information maximization\nto learn the correlations among attributes, items, and sequences.\nXie et al . ",
                    "Citation Text": "Xu Xie, Fei Sun, Zhaoyang Liu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020.\nContrastive Pre-training for Sequential Recommendation. CoRR abs/2010.14395\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.14395",
                        "Citation Paper Title": "Title:Contrastive Learning for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:Sequential recommendation methods play a crucial role in modern recommender systems because of their ability to capture a user's dynamic interest from her/his historical interactions. Despite their success, we argue that these approaches usually rely on the sequential prediction task to optimize the huge amounts of parameters. They usually suffer from the data sparsity problem, which makes it difficult for them to learn high-quality user representations. To tackle that, inspired by recent advances of contrastive learning techniques in the computer version, we propose a novel multi-task model called \\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential \\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of the traditional next item prediction task but also utilizes the contrastive learning framework to derive self-supervision signals from the original user behavior sequences. Therefore, it can extract more meaningful user patterns and further encode the user representation effectively. In addition, we propose three data augmentation approaches to construct self-supervision signals. Extensive experiments on four public datasets demonstrate that CL4SRec achieves state-of-the-art performance over existing baselines by inferring better user representations.",
                        "Citation Paper Authors": "Authors:Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, Bin Cui"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12421v1": {
            "Paper Title": "Few-shot Named Entity Recognition with Cloze Questions",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "Recently, there is an increasing number of research works about few-shot Named Entity Recognition.\nThey are mainly based on meta-learning [4,9,10,13,18,31],distant andweak supervision [1,11,16,\n17,26,32] and transfer learning [2,23,25]. However, current methods usually rely on additional data\nor annotating efforts ",
                    "Citation Text": "Bill Yuchen Lin, Dong-Ho Lee, Minghan Shen, Ryan Rene Moreno, X. Huang, Prashant\nShiralkar, and X. Ren. Triggerner: Learning with entity triggers as explanations for named\nentity recognition. In ACL, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07493",
                        "Citation Paper Title": "Title:TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce \"entity triggers,\" an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence.\nWe crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.",
                        "Citation Paper Authors": "Authors:Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang, Prashant Shiralkar, Xiang Ren"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12262v1": {
            "Paper Title": "Reinforcement Learning based Path Exploration for Sequential Explainable\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "was \ufb01rst introduced in machine\ntranslation task and the attention weights were later widely\nused in natural language processing tasks as explanations\nin neural networks ",
                    "Citation Text": "K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\nR. Zemel, and Y. Bengio, \u201cShow, attend and tell: Neural image\ncaption generation with visual attention,\u201d in International confer-\nence on machine learning , pp. 2048\u20132057, PMLR, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.03044",
                        "Citation Paper Title": "Title:Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
                        "Citation Paper Abstract": "Abstract:Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
                        "Citation Paper Authors": "Authors:Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ".\n3.7 Discussion of Explanation for Recommendation\nwith Attention Mechanism\nAttention Model ",
                    "Citation Text": "D. Bahdanau, K. H. Cho, and Y. Bengio, \u201cNeural machine transla-\ntion by jointly learning to align and translate,\u201d in 3rd International\nConference on Learning Representations, ICLR 2015 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12202v1": {
            "Paper Title": "Combinations of Jaccard with Numerical Measures for Collaborative\n  Filtering Enhancement: Current Work and Future Proposal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10539v2": {
            "Paper Title": "Edge-Enhanced Global Disentangled Graph Neural Network for Sequential\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11520v1": {
            "Paper Title": "Zero-Shot Open-Book Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11314v1": {
            "Paper Title": "The Generalized Cascade Click Model: A Unified Framework for Estimating\n  Click Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11272v1": {
            "Paper Title": "SOMPS-Net : Attention based social graph framework for early detection\n  of fake health news",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10864v1": {
            "Paper Title": "The Impact of Main Content Extraction on Near-Duplicate Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10549v1": {
            "Paper Title": "An End-to-End Framework for Dynamic Crime Profiling of Places",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10536v1": {
            "Paper Title": "Quaternion-Based Graph Convolution Network for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "combines random\nwalk and graph convolutions to learn the embeddings of nodes.\nGC-MC explores the \ufb01rst-order connectivity between users\nand items by utilizing only one convolution layer over the\nuser-item bipartite graph. NGCF ",
                    "Citation Text": "X. Wang, X. He, M. Wang, F. Feng, and T. Chua, \u201cNeural graph\ncollaborative \ufb01ltering,\u201d in SIGIR . ACM, 2019, pp. 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.10504v1": {
            "Paper Title": "Effects of context, complexity, and clustering on evaluation for math\n  formula retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10501v1": {
            "Paper Title": "Exploring Language Patterns in a Medical Licensure Exam Item Bank",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "on embeddings\ngenerated for the clean item stems using SciBERT ",
                    "Citation Text": "I. Beltagy, K. Lo, and A. Cohan, \u201cScibert: A pretrained language model\nfor scienti\ufb01c text,\u201d arXiv preprint arXiv:1903.10676 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10676",
                        "Citation Paper Title": "Title:SciBERT: A Pretrained Language Model for Scientific Text",
                        "Citation Paper Abstract": "Abstract:Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Iz Beltagy, Kyle Lo, Arman Cohan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.09927v1": {
            "Paper Title": "Quality and Cost Trade-offs in Passage Re-ranking Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09737v1": {
            "Paper Title": "A Bibliometric Analysis of the BPM Conference Using Computational Data\n  Analytics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.15230v2": {
            "Paper Title": "Convolutional Hypercomplex Embeddings for Link Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08739v2": {
            "Paper Title": "GAP Enhancing Semantic Interoperability of Genomic Datasets and\n  Provenance Through Nanopublications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08999v1": {
            "Paper Title": "NLP based grievance redressal system for Indian Railways",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06523v3": {
            "Paper Title": "Recommending POIs for Tourists by User Behavior Modeling and\n  Pseudo-Rating",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08543v1": {
            "Paper Title": "WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ",\nand apply multinomial logistic regression and support vector\nmachine for document classi\ufb01cation [40, 41]. Techniques\non deep learning can generate better representations of\ndocuments, and thus signi\ufb01cantly improve the performance\nof document classi\ufb01cation ",
                    "Citation Text": "S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad,\nM. Chenaghlu, and J. Gao, \u201cDeep learning\u2013based text classi\ufb01-\ncation: A comprehensive review,\u201d ACM Comput. Surv. , vol. 54,\nno. 3, Apr. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03705",
                        "Citation Paper Title": "Title:Deep Learning Based Text Classification: A Comprehensive Review",
                        "Citation Paper Abstract": "Abstract:Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.",
                        "Citation Paper Authors": "Authors:Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12050v2": {
            "Paper Title": "Attention on Global-Local Representation Spaces in Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08268v1": {
            "Paper Title": "Pre-training Graph Neural Network for Cross Domain Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "A. van den Oord, Y . Li, and O. Vinyals, \u201cRepresentation learning with\ncontrastive predictive coding,\u201d CoRR , vol. abs/1807.03748, 2018. ",
                    "Citation Text": "K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, \u201cMomentum contrast\nfor unsupervised visual representation learning,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 9729\u20139738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "L. Babai and L. Kucera, \u201cCanonical labelling of graphs in linear average\ntime,\u201d in 20th Annual Symposium on Foundations of Computer Science,\nSan Juan, Puerto Rico, 29-31 October 1979 . IEEE Computer Society,\n1979, pp. 39\u201346. ",
                    "Citation Text": "A. van den Oord, Y . Li, and O. Vinyals, \u201cRepresentation learning with\ncontrastive predictive coding,\u201d CoRR , vol. abs/1807.03748, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "Z. Liu, L. Zheng, J. Zhang, J. Han, and P. S. Yu, \u201cJSCN: joint spectral\nconvolutional network for cross domain recommendation,\u201d in IEEE\nBigData . IEEE, 2019, pp. 850\u2013859. ",
                    "Citation Text": "K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are\ngraph neural networks?\u201d in 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00826",
                        "Citation Paper Title": "Title:How Powerful are Graph Neural Networks?",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.08229v1": {
            "Paper Title": "QA4PRF: A Question Answering based Framework for Pseudo Relevance\n  Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.07346v1": {
            "Paper Title": "A Study on the Efficient Product Search Service for the Damaged Image\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.07198v1": {
            "Paper Title": "Keyphrase Extraction Using Neighborhood Knowledge Based on Word\n  Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", maximum entropy [24, 25], and\nsupport vector machines [26, 27]. Recently, people begin to\nuse deep neural network to extract the keyphrases to avoid\nthe burden of hand-craft features. ",
                    "Citation Text": "R. Meng, S. Zhao, S. Han, D. He, P. Brusilovsky, and\nY . Chi, \u201cDeep keyphrase generation,\u201d arXiv preprint\narXiv:1704.06879 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06879",
                        "Citation Paper Title": "Title:Deep Keyphrase Generation",
                        "Citation Paper Abstract": "Abstract:Keyphrase provides highly-condensed information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.07154v1": {
            "Paper Title": "Session-aware Item-combination Recommendation with Transformer Network",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", we switch the backbone part into a trans-\nformer ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances\nin Neural Information Processing Systems (NeurIPS) , pp. 5998\u20136008,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2005.00624v3": {
            "Paper Title": "Minimally Supervised Categorization of Text with Metadata",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". However, each of these frameworks focuses on\none specific type of data, while we propose a general framework to\ndeal with various data sources and metadata types. Kim et al. ",
                    "Citation Text": "Jihyeok Kim, Reinald Kim Amplayo, Kyungjae Lee, Sua Sung, Minji Seo, and\nSeung-won Hwang. 2019. Categorical Metadata Representation for Customized\nText Classification. TACL 7 (2019), 201\u2013215.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05196",
                        "Citation Paper Title": "Title:Categorical Metadata Representation for Customized Text Classification",
                        "Citation Paper Abstract": "Abstract:The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. These information have been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose to use basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various datasets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.",
                        "Citation Paper Authors": "Authors:Jihyeok Kim, Reinald Kim Amplayo, Kyungjae Lee, Sua Sung, Minji Seo, Seung-won Hwang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "to incorporate\nsuch keyword information. (2) a small set of labeled documents . For\nexample, ",
                    "Citation Text": "Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Predictive text embedding\nthrough large-scale heterogeneous text networks. In KDD\u201915 . 1165\u20131174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.00200",
                        "Citation Paper Title": "Title:PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \\textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Qiaozhu Mei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.05937v1": {
            "Paper Title": "Recent Advances in Automated Question Answering In Biomedical Domain",
            "Sentences": [
                {
                    "Sentence ID": 124,
                    "Sentence": ". It \fnds embeddings (in complex dense multidimensional\nspace) of the questions using RoBERTa ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\nabs/1907.11692, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 187,
                    "Sentence": "based answer span predictor model was used as the reader in the pioneering method named Dr. QA\n[43, 193]. However, gradually, with the suceess of large pretrained transformer based language models ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, unde\fne-\ndukasz Kaiser, and Illia Polosukhin. Attention is all you need. page 6000{6010, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 195,
                    "Sentence": "Factoid Answer Rule Based Method\nOAQA Factoid Machine Learning\nLSTM based QA model ",
                    "Citation Text": "Georg Wiese, Dirk Weissenborn, and Mariana Neves. Neural domain adaptation for biomedical question\nanswering. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03610",
                        "Citation Paper Title": "Title:Neural Domain Adaptation for Biomedical Question Answering",
                        "Citation Paper Abstract": "Abstract:Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch. For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances. In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques. Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions. In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontologies, parsers or entity taggers, which are expensive to create. Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.",
                        "Citation Paper Authors": "Authors:Georg Wiese, Dirk Weissenborn, Mariana Neves"
                    }
                },
                {
                    "Sentence ID": 155,
                    "Sentence": "have\nbeen developed. EmbedKGQA, similar to KEQA, \fnds embeddings (in complex dense multidimensional space)\n7Table 3: Properties of common datasets used for benchmarking text based QA systems\nName of\nDatasetSize Type Question Type Answer Type Remarks Text Source\nSQuAD 1.0 ",
                    "Citation Text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for\nMachine Comprehension of Text.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 154,
                    "Sentence": "100000+Machine Reading\nComprehensionQuestion and\nRelevant Passage\n(context)Factoid AnswerCan be converted to open\ndomain factoid QA by\nremoving the contextWikipedia\nSQuAD 2.0 ",
                    "Citation Text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Don't Know: Unanswerable Questions\nfor SQuAD.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03822",
                        "Citation Paper Title": "Title:Know What You Don't Know: Unanswerable Questions for SQuAD",
                        "Citation Paper Abstract": "Abstract:Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Robin Jia, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 133,
                    "Sentence": "describes a\nmethod for construction of summary answers using the gold standard abstracts snippets provided along with\nthe question. In order to remove redundant sentences, a sentence similarity score based on word2vec ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\u000b Dean. Distributed representations of\nwords and phrases and their compositionality. 26, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 178,
                    "Sentence": ". This approach,\n(referred to as late fusion ) have limited capacity to use evidence from di\u000berent sources ",
                    "Citation Text": "Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William\nCohen. Open domain question answering using early fusion of knowledge bases and text. pages 4231{\n4242, October-November 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00782",
                        "Citation Paper Title": "Title:Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text",
                        "Citation Paper Abstract": "Abstract:Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting. Source code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, William W. Cohen"
                    }
                },
                {
                    "Sentence ID": 214,
                    "Sentence": ".\nThe GST reduces the search space for the answers signi\fcantly and the \fnal answers are predicted using some\nheuristic criteria like answer type, relevance, presence of node term in question etc.\nDELFT ",
                    "Citation Text": "Chen Zhao, Chenyan Xiong, Xin Qian, and Jordan Boyd-Graber. Complex factoid question answering\nwith a free-text knowledge graph. page 1205{1216, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12876",
                        "Citation Paper Title": "Title:Complex Factoid Question Answering with a Free-Text Knowledge Graph",
                        "Citation Paper Abstract": "Abstract:We introduce DELFT, a factoid question answering system which combines the nuance and depth of knowledge graph question answering approaches with the broader coverage of free-text. DELFT builds a free-text knowledge graph from Wikipedia, with entities as nodes and sentences in which entities co-occur as edges. For each question, DELFT finds the subgraph linking question entity nodes to candidates using text sentences as edges, creating a dense and high coverage semantic graph. A novel graph neural network reasons over the free-text graph-combining evidence on the nodes via information along edge sentences-to select a final answer. Experiments on three question answering datasets show DELFT can answer entity-rich questions better than machine reading based models, bert-based answer ranking and memory networks. DELFT's advantage comes from both the high coverage of its free-text knowledge graph-more than double that of dbpedia relations-and the novel graph neural network which reasons on the rich but noisy free-text evidence.",
                        "Citation Paper Authors": "Authors:Chen Zhao, Chenyan Xiong, Xin Qian, Jordan Boyd-Graber"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": ", are able to answer\nquestions without using any external information. The parameters of the models (numbering up to hundreds\nof billions) are believed to contain the knowledge needed to answer any question, although this \"parametric\nknowledge\" ",
                    "Citation Text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u007f uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u007f aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. 33:9459{9474, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11401",
                        "Citation Paper Title": "Title:Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                        "Citation Paper Abstract": "Abstract:Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                        "Citation Paper Authors": "Authors:Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela"
                    }
                },
                {
                    "Sentence ID": 181,
                    "Sentence": "2180 Open Domain QA Question Only Factoid AnswerUnspeci\fed\nbut focused\non\nWikipedia\nof nodes and relations in the KG using ComplEx ",
                    "Citation Text": "Th\u0013 eo Trouillon, Johannes Welbl, Sebastian Riedel, \u0013Eric Gaussier, and Guillaume Bouchard. Complex\nembeddings for simple link prediction. page 2071{2080, 2016.\n36",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06357",
                        "Citation Paper Title": "Title:Complex Embeddings for Simple Link Prediction",
                        "Citation Paper Abstract": "Abstract:In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "was created, which contains more diverse types of complex questions.\n4. Several other datasets have been described in literature viz, ComQA ",
                    "Citation Text": "Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, and Gerhard Weikum. ComQA: A\ncommunity-sourced dataset for complex factoid question answering with paraphrase clusters. pages 307{\n317, June 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.09528",
                        "Citation Paper Title": "Title:ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters",
                        "Citation Paper Abstract": "Abstract:To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.",
                        "Citation Paper Authors": "Authors:Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "contains the annotated SPARQL queries for\nthe questions in WebQuestions, whereas some unclear and ambiguous questions are removed. A majority\n(84% ",
                    "Citation Text": "Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. A survey on complex question\nanswering over knowledge base: Recent advances and challenges. arXiv preprint arXiv:2007.13069 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.13069",
                        "Citation Paper Title": "Title:A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges",
                        "Citation Paper Abstract": "Abstract:Question Answering (QA) over Knowledge Base (KB) aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge bases. In order to make KBQA more applicable in actual scenarios, researchers have shifted their attention from simple questions to complex questions, which require more KB triples and constraint inference. In this paper, we introduce the recent advances in complex QA. Besides traditional methods relying on templates and rules, the research is categorized into a taxonomy that contains two main branches, namely Information Retrieval-based and Neural Semantic Parsing-based. After describing the methods of these branches, we analyze directions for future research and introduce the models proposed by the Alime team.",
                        "Citation Paper Authors": "Authors:Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Simple and Complex KG (Freebase) No 2100 Factoid and List F1 Score\nSimpleQuestions ",
                    "Citation Text": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering\nwith memory networks. arXiv preprint arXiv:1506.02075 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02075",
                        "Citation Paper Title": "Title:Large-scale Simple Question Answering with Memory Networks",
                        "Citation Paper Abstract": "Abstract:Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.",
                        "Citation Paper Authors": "Authors:Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2005.12423v2": {
            "Paper Title": "Racism is a Virus: Anti-Asian Hate and Counterspeech in Social Media\n  during the COVID-19 Crisis",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "collected tweets containing controversial hashtags like\n#ChineseVirus. However, as we have shown, the presence of\nhashtags is inaccurate to label a tweet as hate. We overcome\nthis by developing a hand-labeled dataset and text-based\nclassi\ufb01er. Finally, contemporaneous work by ",
                    "Citation Text": "B. Vidgen, A. Botelho, D. Broniatowski, E. Guest,\nM. Hall, H. Margetts, R. Tromble, Z. Waseem, and\nS. Hale, \u201cDetecting east asian prejudice on social media,\u201d\narXiv preprint arXiv: 2005.03909 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03909",
                        "Citation Paper Title": "Title:Detecting East Asian Prejudice on Social Media",
                        "Citation Paper Abstract": "Abstract:The outbreak of COVID-19 has transformed societies across the world as governments tackle the health, economic and social costs of the pandemic. It has also raised concerns about the spread of hateful language and prejudice online, especially hostility directed against East Asia. In this paper we report on the creation of a classifier that detects and categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice and a neutral class. The classifier achieves an F1 score of 0.83 across all four classes. We provide our final model (coded in Python), as well as a new 20,000 tweet training dataset used to make the classifier, two analyses of hashtags associated with East Asian prejudice and the annotation codebook. The classifier can be implemented by other researchers, assisting with both online content moderation processes and further research into the dynamics, prevalence and impact of East Asian prejudice online during this global pandemic.",
                        "Citation Paper Authors": "Authors:Bertie Vidgen, Austin Botelho, David Broniatowski, Ella Guest, Matthew Hall, Helen Margetts, Rebekah Tromble, Zeerak Waseem, Scott Hale"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". More recent work has focused\non developing novel counterspeech data sets ",
                    "Citation Text": "B. Mathew, N. Kumar, P. Goyal, A. Mukherjee et al. ,\n\u201cAnalyzing the hate and counter speech accounts on\ntwitter,\u201d arXiv preprint arXiv:1812.02712 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02712",
                        "Citation Paper Title": "Title:Analyzing the hate and counter speech accounts on Twitter",
                        "Citation Paper Abstract": "Abstract:The online hate speech is proliferating with several organization and countries implementing laws to ban such harmful speech. While these restrictions might reduce the amount of such hateful content, it does so by restricting freedom of speech. Thus, an promising alternative supported by several organizations is to counter such hate speech with more speech. In this paper, We analyze hate speech and the corresponding counters (aka counterspeech) on Twitter. We perform several lexical, linguistic and psycholinguistic analysis on these user accounts and obverse that counter speakers employ several strategies depending on the target community. The hateful accounts express more negative sentiments and are more profane. We also find that the hate tweets by verified accounts have much more virality as compared to a tweet by a non-verified account. While the hate users seem to use words more about envy, hate, negative emotion, swearing terms, ugliness, the counter users use more words related to government, law, leader. We also build a supervised model for classifying the hateful and counterspeech accounts on Twitter and obtain an F-score of 0.77. We also make our dataset public to help advance the research on hate speech.",
                        "Citation Paper Authors": "Authors:Binny Mathew, Navish Kumar, Ravina, Pawan Goyal, Animesh Mukherjee"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Hate speech on social media. Detecting hate speech has\nbeen shown to be an important and challenging task for\nsociety ",
                    "Citation Text": "B. Ross, M. Rist, G. Carbonell, B. Cabrera, N. Kurowsky,\nand M. Wojatzki, \u201cMeasuring the reliability of hate\nspeech annotations: The case of the european refugee\ncrisis,\u201d arXiv preprint arXiv:1701.08118 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.08118",
                        "Citation Paper Title": "Title:Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis",
                        "Citation Paper Abstract": "Abstract:Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.",
                        "Citation Paper Authors": "Authors:Bj\u00f6rn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, Michael Wojatzki"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.05736v1": {
            "Paper Title": "Multimodal Approach for Metadata Extraction from German Scientific\n  Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05587v1": {
            "Paper Title": "Design Theory to improve health evidence retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03465v2": {
            "Paper Title": "Spatio-Temporal Urban Knowledge Graph Enabled Mobility Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05564v1": {
            "Paper Title": "Understanding and Mitigating Multi-Sided Exposure Bias in Recommender\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06720v2": {
            "Paper Title": "Fairness in Ranking under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09775v3": {
            "Paper Title": "An Information Retrieval Approach to Building Datasets for Hate Speech\n  Detection",
            "Sentences": [
                {
                    "Sentence ID": 121,
                    "Sentence": ". However, while HateXplain collects rationales only for\nthe overall labeling decision, our collection of rationales for different annotation sub-tasks creates\nintriguing possibilities for dual-supervision ",
                    "Citation Text": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. 2017. Dual\nsupervised learning. In International Conference on Machine Learning . PMLR, 3789\u20133798.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.00415",
                        "Citation Paper Title": "Title:Dual Supervised Learning",
                        "Citation Paper Abstract": "Abstract:Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach \\emph{dual supervised learning}. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.",
                        "Citation Paper Authors": "Authors:Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, Tie-Yan Liu"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "and explanations across different types of evidence\ncontributing to the overall labeling decision ",
                    "Citation Text": "Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C Wallace. 2019. Inferring Which\nMedical Treatments Work from Reports of Clinical Trials. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) . 3705\u20133717.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01606",
                        "Citation Paper Title": "Title:Inferring Which Medical Treatments Work from Reports of Clinical Trials",
                        "Citation Paper Abstract": "Abstract:How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo.\nWe present a new corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. Results using a suite of models --- ranging from heuristic (rule-based) approaches to attentive neural architectures --- demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at this http URL.",
                        "Citation Paper Authors": "Authors:Eric Lehman, Jay DeYoung, Regina Barzilay, Byron C. Wallace"
                    }
                },
                {
                    "Sentence ID": 106,
                    "Sentence": ", our collection of rationales as well as labels creates the potential for explainable\nmodeling ",
                    "Citation Text": "Julia Strout, Ye Zhang, and Raymond Mooney. 2019. Do Human Rationales Improve Machine\nExplanations?. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP . 56\u201362.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.13714",
                        "Citation Paper Title": "Title:Do Human Rationales Improve Machine Explanations?",
                        "Citation Paper Abstract": "Abstract:Work on \"learning with rationales\" shows that humans providing explanations to a machine learning system can improve the system's predictive accuracy. However, this work has not been connected to work in \"explainable AI\" which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine's explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN- based text classification, explanations generated using \"supervised attention\" are judged superior to explanations generated using normal unsupervised attention.",
                        "Citation Paper Authors": "Authors:Julia Strout, Ye Zhang, Raymond J. Mooney"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.11296v1": {
            "Paper Title": "Improving Next-Application Prediction with Deep Personalized-Attention\n  Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04983v1": {
            "Paper Title": "Dynamic Parameterized Network for CTR Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08787v2": {
            "Paper Title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00150v2": {
            "Paper Title": "Capturing Logical Structure of Visually Structured Documents with\n  Multimodal Transition Parser",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11295v1": {
            "Paper Title": "Artificial Intelligence Technology analysis using Artificial\n  Intelligence patent through Deep Learning model and vector space model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12906v3": {
            "Paper Title": "CSFCube -- A Test Collection of Computer Science Research Articles for\n  Faceted Query by Example",
            "Sentences": [
                {
                    "Sentence ID": 68,
                    "Sentence": ". However they represent a somewhat simpler and different task, requiring to match\nsentences from full-text to the citance. In similar vein, our task setup also resembles that of claim\nveri\ufb01cation as proposed in Wadden et al . ",
                    "Citation Text": "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan,\nand Hannaneh Hajishirzi. 2020. Fact or Fiction: Verifying Scienti\ufb01c Claims. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 7534\u20137550. https://doi.org/10.\n18653/v1/2020.emnlp-main.609\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14974",
                        "Citation Paper Title": "Title:Fact or Fiction: Verifying Scientific Claims",
                        "Citation Paper Abstract": "Abstract:We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at this https URL. A leaderboard and COVID-19 fact-checking demo are available at this https URL.",
                        "Citation Paper Authors": "Authors:David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", extensively evaluate a\nrange of methods intended to extract rhetorical structure elements for faceted scienti\ufb01c paper search\nand evaluate it with a small scale dataset of biomedical publications labelled for \ufb01ne grained facet\nsimilarity. Hope et al . ",
                    "Citation Text": "Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric Horvitz, Daniel S Weld,\nMarti A Hearst, and Jevin West. 2020. SciSight: Combining faceted navigation and research\ngroup detection for COVID-19 exploratory scienti\ufb01c search. arXiv preprint arXiv:2005.12668\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12668",
                        "Citation Paper Title": "Title:SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search",
                        "Citation Paper Abstract": "Abstract:The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight has so far served over $15K$ users with over $42K$ page views and $13\\%$ returns.",
                        "Citation Paper Authors": "Authors:Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric Horvitz, Daniel S. Weld, Marti A. Hearst, Jevin West"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.04070v1": {
            "Paper Title": "Em-K Indexing for Approximate Query Matching in Large-scale ER",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04067v1": {
            "Paper Title": "High Performance Out-of-sample Embedding Techniques for Multidimensional\n  Scaling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03910v1": {
            "Paper Title": "FAIR Metadata: A Community-driven Vocabulary Application",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09741v1": {
            "Paper Title": "Patent Sentiment Analysis to Highlight Patent Paragraphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03579v1": {
            "Paper Title": "A Semi-automatic Data Extraction System for Heterogeneous Data Sources:\n  A Case Study from Cotton Industry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03576v1": {
            "Paper Title": "Investigation of Topic Modelling Methods for Understanding the Reports\n  of the Mining Projects in Queensland",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03474v1": {
            "Paper Title": "Supervised Advantage Actor-Critic for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "which is formulated as\n\ud835\udc41\ud835\udc3a\ud835\udc5c\ud835\udc53\ud835\udc53=\u00cd\ud835\udc41\ud835\udc37\ud835\udc36\ud835\udc3a\n\ud835\udefd\u00cd1\n\ud835\udefd. (11)\nIn this implementation, we use the item frequency to approximate\nthe behavior policy \ud835\udefd, which is also adopted in ",
                    "Citation Text": "Alex Strehl, John Langford, Lihong Li, and Sham M. Kakade. 2010. Learning from\nLogged Implicit Exploration Data. In NIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.0120",
                        "Citation Paper Title": "Title:Learning from Logged Implicit Exploration Data",
                        "Citation Paper Abstract": "Abstract:We provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in \"contextual bandit\" or \"partially labeled\" settings where only the value of a chosen action is learned.\nThe primary challenge in a variety of settings is that the exploration policy, in which \"offline\" data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged.\nWe empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.",
                        "Citation Paper Authors": "Authors:Alex Strehl, John Langford, Sham Kakade, Lihong Li"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "Recurrent neural networks (RNN) and convolutional networks\n(CNN) have shown promising results in modeling recommendation\nsequences [ 7,33,39]. Transformer architectures have been proven\nto be highly successful ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", while\nactually recommendations do have an effect on user behavior ",
                    "Citation Text": "David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros\nKaratzoglou. 2018. RecoGym: A Reinforcement Learning Environment for the\nproblem of Product Recommendation in Online Advertising. arXiv preprint\narXiv:1808.00720 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.00720",
                        "Citation Paper Title": "Title:RecoGym: A Reinforcement Learning Environment for the problem of Product Recommendation in Online Advertising",
                        "Citation Paper Abstract": "Abstract:Recommender Systems are becoming ubiquitous in many settings and take many forms, from product recommendation in e-commerce stores, to query suggestions in search engines, to friend recommendation in social networks. Current research directions which are largely based upon supervised learning from historical data appear to be showing diminishing returns with a lot of practitioners report a discrepancy between improvements in offline metrics for supervised learning and the online performance of the newly proposed models. One possible reason is that we are using the wrong paradigm: when looking at the long-term cycle of collecting historical performance data, creating a new version of the recommendation model, A/B testing it and then rolling it out. We see that there a lot of commonalities with the reinforcement learning (RL) setup, where the agent observes the environment and acts upon it in order to change its state towards better states (states with higher rewards). To this end we introduce RecoGym, an RL environment for recommendation, which is defined by a model of user traffic patterns on e-commerce and the users response to recommendations on the publisher websites. We believe that this is an important step forward for the field of recommendation systems research, that could open up an avenue of collaboration between the recommender systems and reinforcement learning communities and lead to better alignment between offline and online performance metrics.",
                        "Citation Paper Authors": "Authors:David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, Alexandros Karatzoglou"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", achieve good performance\nwhen generating content like images and speeches, simulating\nusers\u2019 responses is a much more complex and difficult task ",
                    "Citation Text": "Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. 2019.\nGenerative Adversarial User Model for Reinforcement Learning Based Recom-\nmendation System. In International Conference on Machine Learning . 1052\u20131061.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.10613",
                        "Citation Paper Title": "Title:Generative Adversarial User Model for Reinforcement Learning Based Recommendation System",
                        "Citation Paper Abstract": "Abstract:There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",
                        "Citation Paper Authors": "Authors:Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, Le Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03349v1": {
            "Paper Title": "Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences\n  for Image-Text Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03344v1": {
            "Paper Title": "Inhomogeneous Social Recommendation with Hypergraph Convolutional\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "as follows,\nLoss =X\n(i;j1;j2)2O\u0000ln\u001b(rij1\u0000rij2) +\u0015k\u0002k2\n2; (13)\nwhereO=f(i;j1;j2)j(i;j1)2Y;(i;j2)2Y\u0000gdenotes the\npairwise training data with negative sampling, and Yand\nY\u0000denote the observed and sampled unobserved user-item\ninteraction set, respectively.Thanks to the Automatic Differentiation framework like\nTensorFlow ",
                    "Citation Text": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,\nA. Harp, G. Irving, M. Isard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,\nJ. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV . Vanhoucke, V . Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wat-\ntenberg, M. Wicke, Y . Yu, and X. Zheng, \u201cTensor\ufb02ow: Large-scale\nmachine learning on heterogeneous distributed systems,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.08566v1": {
            "Paper Title": "SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03000v1": {
            "Paper Title": "Reducing the impact of out of vocabulary words in the translation of\n  natural language questions into SPARQL queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.02878v1": {
            "Paper Title": "Unsupervised and Distributional Detection of Machine-Generated Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01455v2": {
            "Paper Title": "FEBR: Expert-Based Recommendation Framework for beneficial and\n  personalized content",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ", which considers a\nrecommendation session as multiple independent state-action pairs,\nChen et al. ",
                    "Citation Text": "Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. 2019.\nGenerative Adversarial User Model for Reinforcement Learning Based Recom-\nmendation System. In International Conference on Machine Learning . 1052\u20131061.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.10613",
                        "Citation Paper Title": "Title:Generative Adversarial User Model for Reinforcement Learning Based Recommendation System",
                        "Citation Paper Abstract": "Abstract:There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",
                        "Citation Paper Authors": "Authors:Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, Le Song"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "proposed a\nmaximum entropy probabilistic framework using IRL, which aims\nto choose a trajectory distribution with maximum entropy that\nmatches the expected expert reward. This approach is further ex-\ntended by Wulfmeier et al. ",
                    "Citation Text": "Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. 2015. Deep Inverse\nReinforcement Learning. CoRR abs/1507.04888 (2015). arXiv:1507.04888 http:\n//arxiv.org/abs/1507.04888",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04888",
                        "Citation Paper Title": "Title:Maximum Entropy Deep Inverse Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",
                        "Citation Paper Authors": "Authors:Markus Wulfmeier, Peter Ondruska, Ingmar Posner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.02100v1": {
            "Paper Title": "Conditional Attention Networks for Distilling Knowledge Graphs in\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "combines collaborative filtering\nwith knowledge graph embedding. Cao et al. ",
                    "Citation Text": "Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019.\nUnifying knowledge graph learning and recommendation: Towards a better\nunderstanding of user preferences. In The world wide web conference . 151\u2013161.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06236",
                        "Citation Paper Title": "Title:Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences",
                        "Citation Paper Abstract": "Abstract:Incorporating knowledge graph (KG) into recommender system is promising in improving the recommendation accuracy and explainability. However, existing methods largely assume that a KG is complete and simply transfer the \"knowledge\" in KG at the shallow level of entity raw data or embeddings. This may lead to suboptimal performance, since a practical KG can hardly be complete, and it is common that a KG has missing facts, relations, and entities. Thus, we argue that it is crucial to consider the incomplete nature of KG when incorporating it into recommender system.\nIn this paper, we jointly learn the model of recommendation and knowledge graph completion. Distinct from previous KG-based recommendation methods, we transfer the relation information in KG, so as to understand the reasons that a user likes an item. As an example, if a user has watched several movies directed by (relation) the same person (entity), we can infer that the director relation plays a critical role when the user makes the decision, thus help to understand the user's preference at a finer granularity.\nTechnically, we contribute a new translation-based recommendation model, which specially accounts for various preferences in translating a user to an item, and then jointly train it with a KG completion model by combining several transfer schemes. Extensive experiments on two benchmark datasets show that our method outperforms state-of-the-art KG-based recommendation methods. Further analysis verifies the positive effect of joint training on both tasks of recommendation and KG completion, and the advantage of our model in understanding user preference. We publish our project at this https URL.",
                        "Citation Paper Authors": "Authors:Yixin Cao, Xiang Wang, Xiangnan He, Zikun hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "introduces\na self-attention strategy to specify different weights to different\nnodes in a neighborhood. However, these works are designed for\nhomogeneous graphs instead of knowledge graphs. Our method isconceptually inspired by GCN. Similarly, Schlichtkrull et al. ",
                    "Citation Text": "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan\nTitov, and Max Welling. 2018. Modeling relational data with graph convolutional\nnetworks. In European Semantic Web Conference . Springer, 593\u2013607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06103",
                        "Citation Paper Title": "Title:Modeling Relational Data with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
                        "Citation Paper Authors": "Authors:Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "samples a fixed-size neighbor-\nhood of each node and then aggregates over it. GAT ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. International Con-\nference on Learning Representations (2018). https://openreview.net/forum?id=\nrJXMpikCZ accepted as poster.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "propose to use a first-order approximation to deal with\nthe complexity issue. Recently, lots of non-spectral GCN, which\ndirectly define convolution on the graph, have been proposed in\nthe spatial domain. GraphSage ",
                    "Citation Text": "William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation\nLearning on Large Graphs. In NIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.02036v1": {
            "Paper Title": "GRCN: Graph-Refined Convolutional Network for Multimedia Recommendation\n  with Implicit Feedback",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "treated the all user\u2019s unobserved item as\nnegative instances and indicated the numerical value of implicit\nfeedback as confidence. Besides, Rendle et al. ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings\nof the conference on Uncertainty in Artificial Intelligence . 452\u2013461.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.06968v3": {
            "Paper Title": "Multi-Interactive Attention Network for Fine-grained Feature Learning in\n  CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "replaces the wide linear part with cross-network, which gen-\nerates explicit feature crossing among low and high level layers.\nDeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. arXiv\npreprint arXiv:1703.04247 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "further con-\nsider the different impact of the fields that a feature belongs to in\norder to improve the performance of CTR prediction. Along this\nline, Attentional Factorization Machines (AFM) ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01663v1": {
            "Paper Title": "Classification of Goods Using Text Descriptions With Sentences Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "are used to \ufb01nd supporting sentences. Unsu-\npervised sentence retrieval can increase the interpretability\nand performance in \ufb01nding answers ",
                    "Citation Text": "Dirk Groeneveld, Tushar Khot, Mausam, and Ashish Sab-\nharwal. A simple yet strong pipeline for hotpotqa. In Proc.\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing , pages 8839\u20138845, 2020.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06753",
                        "Citation Paper Title": "Title:A Simple Yet Strong Pipeline for HotpotQA",
                        "Citation Paper Abstract": "Abstract:State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named Quark, performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.",
                        "Citation Paper Authors": "Authors:Dirk Groeneveld, Tushar Khot, Mausam, Ashish Sabharwal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.11427v5": {
            "Paper Title": "One Model to Serve All: Star Topology Adaptive Recommender for\n  Multi-Domain CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "introduces a product layer to capture interactive\npatterns between inter-field categories. In these models, the user\u2019s\nhistory behaviors are transformed into low-dimensional vectors\nafter the embedding and pooling. DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining . ACM, 1059\u20131068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "propose a method to learn sparse representations\nshared across multiple tasks. In the context of deep learning, multi-\ntask learning is typically done with parameter sharing of hidden\nlayers [ 5,26]. Misra et al . ",
                    "Citation Text": "Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016.\nCross-Stitch Networks for Multi-task Learning. 3994\u20134003.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.03539",
                        "Citation Paper Title": "Title:Cross-stitch Networks for Multi-task Learning",
                        "Citation Paper Abstract": "Abstract:Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
                        "Citation Paper Authors": "Authors:Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "employs the mecha-\nnism of attention to activate historical behaviors locally w.r.t. the\ngiven the target item, and successfully captures the diversity char-\nacteristic of user interest. DIEN ",
                    "Citation Text": "Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,\nand Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate\nPrediction. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence .\nHonolulu, Hawaii, USA, 5941\u20135948.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03672",
                        "Citation Paper Title": "Title:Deep Interest Evolution Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate~(CTR) prediction, whose goal is to estimate the probability of the user clicks, has become one of the core tasks in advertising systems. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, few work consider the changing trend of interest. In this paper, we propose a novel model, named Deep Interest Evolution Network~(DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7\\% improvement on CTR.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "further proposes an auxiliaryloss to capture latent interest from historical behaviors. Addition-\nally, DIEN integrates the attention mechanism with GRU to model\nthe dynamic evolution of user interest. MIND ",
                    "Citation Text": "Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,\nGuoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-Interest\nNetwork with Dynamic Routing for Recommendation at Tmall. In Proceedings of\nthe 28th ACM International Conference on Information and Knowledge Management .\n2615\u20132623.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08030",
                        "Citation Paper Title": "Title:Multi-Interest Network with Dynamic Routing for Recommendation at Tmall",
                        "Citation Paper Abstract": "Abstract:Industrial recommender systems usually consist of the matching stage and the ranking stage, in order to handle the billion-scale of users and items. The matching stage retrieves candidate items relevant to user interests, while the ranking stage sorts candidate items by user interests. Thus, the most critical ability is to model and represent user interests for either stage. Most of the existing deep learning-based models represent one user as a single vector which is insufficient to capture the varying nature of user's interests. In this paper, we approach this problem from a different view, to represent one user with multiple vectors encoding the different aspects of the user's interests. We propose the Multi-Interest Network with Dynamic routing (MIND) for dealing with user's diverse interests in the matching stage. Specifically, we design a multi-interest extractor layer based on capsule routing mechanism, which is applicable for clustering historical behaviors and extracting diverse interests. Furthermore, we develop a technique named label-aware attention to help learn a user representation with multiple vectors. Through extensive experiments on several public benchmarks and one large-scale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods for recommendation. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.",
                        "Citation Paper Authors": "Authors:Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao, Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "combine low-order\nand high-order features to improve the expression power of the\nmodel. PNN ",
                    "Citation Text": "Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.\n2016. Product-based neural networks for user response prediction. In Proceedings\nof the16th International Conference on Data Mining . IEEE, 1149\u20131154.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00144",
                        "Citation Paper Title": "Title:Product-based Neural Networks for User Response Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.",
                        "Citation Paper Authors": "Authors:Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01496v1": {
            "Paper Title": "Quality change: norm or exception? Measurement, Analysis and Detection\n  of Quality Change in Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01314v1": {
            "Paper Title": "Explaining Documents' Relevance to Search Queries",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ". Neural abstractive summarization started by\ngenerating headlines from the first sentence of news articles ",
                    "Citation Text": "Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence\nSummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing . Association\nfor Computational Linguistics, Lisbon, Portugal, 379\u2013389.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.00685",
                        "Citation Paper Title": "Title:A Neural Attention Model for Abstractive Sentence Summarization",
                        "Citation Paper Abstract": "Abstract:Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.",
                        "Citation Paper Authors": "Authors:Alexander M. Rush, Sumit Chopra, Jason Weston"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01013v1": {
            "Paper Title": "Improving Location Recommendation with Urban Knowledge Graph",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "estimate the propensity of popular, and re-weight samples using the\ninverse propensity scores. MACR ",
                    "Citation Text": "Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.\n2021. Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias\nin Recommender System. In Proceedings of the 27th ACM SIGKDD Conference on\nKnowledge Discovery & Data Mining . 1791\u20131800.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.15363",
                        "Citation Paper Title": "Title:Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System",
                        "Citation Paper Abstract": "Abstract:The general aim of the recommender system is to provide personalized suggestions to users, which is opposed to suggesting popular items. However, the normal training paradigm, i.e., fitting a recommender model to recover the user behavior data with pointwise or pairwise loss, makes the model biased towards popular items. This results in the terrible Matthew effect, making popular items be more frequently recommended and become even more popular. Existing work addresses this issue with Inverse Propensity Weighting (IPW), which decreases the impact of popular items on the training and increases the impact of long-tail items. Although theoretically sound, IPW methods are highly sensitive to the weighting strategy, which is notoriously difficult to tune. In this work, we explore the popularity bias issue from a novel and fundamental perspective -- cause-effect. We identify that popularity bias lies in the direct effect from the item node to the ranking score, such that an item's intrinsic property is the cause of mistakenly assigning it a higher ranking score. To eliminate popularity bias, it is essential to answer the counterfactual question that what the ranking score would be if the model only uses item property. To this end, we formulate a causal graph to describe the important cause-effect relations in the recommendation process. During training, we perform multi-task learning to achieve the contribution of each cause; during testing, we perform counterfactual inference to remove the effect of item popularity. Remarkably, our solution amends the learning process of recommendation which is agnostic to a wide range of models -- it can be easily implemented in existing methods. We demonstrate it on Matrix Factorization (MF) and LightGCN [20]. Experiments on five real-world datasets demonstrate the effectiveness of our method.",
                        "Citation Paper Authors": "Authors:Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, Xiangnan He"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "propagates on\nthe composite graph intergrating KG and interactions, adopting\nattention mechanism. In addition to information propagation on\nthe KG, KGIN ",
                    "Citation Text": "Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,\nXiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions\nwith Knowledge Graph for Recommendation. In WWW . 878\u2013887.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07057",
                        "Citation Paper Title": "Title:Learning Intents behind Interactions with Knowledge Graph for Recommendation",
                        "Citation Paper Abstract": "Abstract:Knowledge graph (KG) plays an increasingly important role in recommender systems. A recent technical trend is to develop end-to-end models founded on graph neural networks (GNNs). However, existing GNN-based models are coarse-grained in relational modeling, failing to (1) identify user-item relation at a fine-grained level of intents, and (2) exploit relation dependencies to preserve the semantics of long-range connectivity.\nIn this study, we explore intents behind a user-item interaction by using auxiliary item knowledge, and propose a new model, Knowledge Graph-based Intent Network (KGIN). Technically, we model each intent as an attentive combination of KG relations, encouraging the independence of different intents for better model capability and interpretability. Furthermore, we devise a new information aggregation scheme for GNN, which recursively integrates the relation sequences of long-range connectivity (i.e., relational paths). This scheme allows us to distill useful information about user intents and encode them into the representations of users and items. Experimental results on three benchmark datasets show that, KGIN achieves significant improvements over the state-of-the-art methods like KGAT, KGNN-LS, and CKAN. Further analyses show that KGIN offers interpretable explanations for predictions by identifying influential intents and relational paths. The implementations are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "exploits TransE to obtain\nembeddings of entities in the KG, feeding them into matrix fac-\ntorization. CFKG ",
                    "Citation Text": "Qingyao Ai, Vahid Azizi, Xu Chen, and Yongfeng Zhang. 2018. Learning Hetero-\ngeneous Knowledge Base Embeddings for Explainable Recommendation. Algo-\nrithms 11, 9 (Sep 2018), 137. https://doi.org/10.3390/a11090137",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03352",
                        "Citation Paper Title": "Title:Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation",
                        "Citation Paper Abstract": "Abstract:Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms - especially collaborative filtering (CF)-based approaches with shallow or deep models - usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the research focus on CF approaches. However, structured knowledge exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Qingyao Ai, Vahid Azizi, Xu Chen, Yongfeng Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00974v1": {
            "Paper Title": "Transductive Data Augmentation with Relational Path Rule Mining for\n  Knowledge Graph Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00735v1": {
            "Paper Title": "Calibrating Explore-Exploit Trade-off for Fair Online Learning to Rank",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06426v2": {
            "Paper Title": "A Survey on Extraction of Causal Relations from Natural Language Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01908v1": {
            "Paper Title": "Classifying YouTube Comments Based on Sentiment and Type of Sentence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13522v2": {
            "Paper Title": "Probabilistic Entity Representation Model for Reasoning over Knowledge\n  Graphs",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "provides a method of learning Gaussian densities for words from their distributional semantic\ninformation. In addition, the authors further apply this work to knowledge graphs ",
                    "Citation Text": "Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of\nknowledge graphs with box lattice measures. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pages 263\u2013272,\nMelbourne, Australia, July 2018. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06627",
                        "Citation Paper Title": "Title:Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures",
                        "Citation Paper Abstract": "Abstract:Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.",
                        "Citation Paper Authors": "Authors:Luke Vilnis, Xiang Li, Shikhar Murty, Andrew McCallum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.00309v2": {
            "Paper Title": "PREDICT: Persian Reverse Dictionary",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "also achieved better performance\nusing a unique embedding for each sense of a word. These embeddings were used by Hedderich et al. ",
                    "Citation Text": "Michael A. Hedderich, Andrew Yates, Dietrich Klakow, and Gerard de Melo. Using multi-sense vector embeddings\nfor reverse dictionaries. Proceedings of the 13th International Conference on Computational Semantics - Long\nPapers , pages 247\u2013258, May 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01451",
                        "Citation Paper Title": "Title:Using Multi-Sense Vector Embeddings for Reverse Dictionaries",
                        "Citation Paper Abstract": "Abstract:Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.",
                        "Citation Paper Authors": "Authors:Michael A. Hedderich, Andrew Yates, Dietrich Klakow, Gerard de Melo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00123v1": {
            "Paper Title": "Learning Representations for Zero-Shot Retrieval over Structured Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.13276v3": {
            "Paper Title": "MULTIMODAL ANALYSIS: Informed content estimation and audio source\n  separation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15789v1": {
            "Paper Title": "On the Feasibility of Predicting Questions being Forgotten in Stack\n  Overflow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13047v2": {
            "Paper Title": "Drug Similarity and Link Prediction Using Graph Embeddings on Medical\n  Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01910v1": {
            "Paper Title": "Deep Keyphrase Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.10511v4": {
            "Paper Title": "CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15089v2": {
            "Paper Title": "D2RLIR : an improved and diversified ranking function in interactive\n  recommendation systems based on deep reinforcement learning",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "utilized a Markov decision process (MDP) \nformulation to model sequential decision problems in \nrecommender systems using a predictive n -gram model for the \ninitial MDP. Zhao et al. ",
                    "Citation Text": "X. Zhao, L. Zhang, L. Xia, Z. Ding, D. Yin, and J. Tang, \n\u201cDeep reinforcement learning for list -wise \nrecommendations,\u201d arXiv preprint arXiv:1801.00209, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00209",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning for List-wise Recommendations",
                        "Citation Paper Abstract": "Abstract:Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.",
                        "Citation Paper Authors": "Authors:Xiangyu Zhao, Liang Zhang, Long Xia, Zhuoye Ding, Dawei Yin, Jiliang Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.00685v2": {
            "Paper Title": "Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label\n  Text Classification",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "learns\nMLP layers on several smaller XMC sub-problems induced by hashing tricks on the large label space.\nSimilarly, DeepXML ",
                    "Citation Text": "Kunal Dahiya, Deepak Saini, Anshul Mittal, Ankush Shaw, Kushal Dave, Akshay Soni, Himan-\nshu Jain, Sumeet Agarwal, and Manik Varma. DeepXML: A deep extreme multi-label learning\nframework applied to short text documents. In Proceedings of the 14th ACM International\nConference on Web Search and Data Mining , pages 31\u201339, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06685",
                        "Citation Paper Title": "Title:DeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Documents",
                        "Citation Paper Abstract": "Abstract:Scalability and accuracy are well recognized challenges in deep extreme multi-label learning where the objective is to train architectures for automatically annotating a data point with the most relevant subset of labels from an extremely large label set. This paper develops the DeepXML framework that addresses these challenges by decomposing the deep extreme multi-label task into four simpler sub-tasks each of which can be trained accurately and efficiently. Choosing different components for the four sub-tasks allows DeepXML to generate a family of algorithms with varying trade-offs between accuracy and scalability. In particular, DeepXML yields the Astec algorithm that could be 2-12% more accurate and 5-30x faster to train than leading deep extreme classifiers on publically available short text datasets. Astec could also efficiently train on Bing short text datasets containing up to 62 million labels while making predictions for billions of users and data points per day on commodity hardware. This allowed Astec to be deployed on the Bing search engine for a number of short text applications ranging from matching user queries to advertiser bid phrases to showing personalized ads where it yielded significant gains in click-through-rates, coverage, revenue and other online metrics over state-of-the-art techniques currently in production. DeepXML's code is available at this https URL",
                        "Citation Paper Authors": "Authors:Kunal Dahiya, Deepak Saini, Anshul Mittal, Ankush Shaw, Kushal Dave, Akshay Soni, Himanshu Jain, Sumeet Agarwal, Manik Varma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.15439v1": {
            "Paper Title": "Dense Hierarchical Retrieval for Open-Domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15781v1": {
            "Paper Title": "Two-sided fairness in rankings via Lorenz dominance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15154v1": {
            "Paper Title": "Cross-Batch Negative Sampling for Training Two-Tower Recommenders",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "with CBNS that focuses on reducing\nsampling bias in large-scale recommendation from a view of con-\ntrastive learning ",
                    "Citation Text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen-\ntum contrast for unsupervised visual representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9729\u20139738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Two-Tower Models. The two-tower architecture is a general\nframework of a query encoder along with a candidate encoder,\nwhich has been widely studied in language retrieval tasks such as\nquestion answering ",
                    "Citation Text": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . 6769\u20136781.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04906",
                        "Citation Paper Title": "Title:Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.15097v1": {
            "Paper Title": "Choosing the Best of Both Worlds: Diverse and Novel Recommendations\n  through Multi-Objective Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "Several deep learning-based approaches that model the user inter-\naction sequences effectively have been proposed for RS. Hidasi et al . ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14960v1": {
            "Paper Title": "An AI-based Approach for Tracing Content Requirements in Financial\n  Documents",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". However, these tasks usually focus on analyzing\nsmall pieces of text (e.g., conversations, newsgroups, and weblogs) [34, 40, 44]. Driven by the recent deep\nlearning advances (such as BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL HLT'19 , pages 4171{4186, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ") from the text. The extracted information can be used for tasks like\nquestion answering ",
                    "Citation Text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions\nfor squad. In ACL'18 , pages 784{789, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03822",
                        "Citation Paper Title": "Title:Know What You Don't Know: Unanswerable Questions for SQuAD",
                        "Citation Paper Abstract": "Abstract:Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Robin Jia, Percy Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14925v1": {
            "Paper Title": "Hierarchical User Intent Graph Network forMultimedia Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". The distribution of the neighborhood\nfeatures can provide a strong signal for distilling the\ninformation on the user intents ",
                    "Citation Text": "K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are\ngraph neural networks?\u201d in Proceedings of International Conference\non Learning Representations , 2018, pp. 1\u201317.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00826",
                        "Citation Paper Title": "Title:How Powerful are Graph Neural Networks?",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14844v1": {
            "Paper Title": "From Intrinsic to Counterfactual: On the Explainability of\n  Contextualized Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ": A\nmodified version of the original VBPR by using the neu-\nral network based embeddings along with the contextual\nfeatures.\n\u2022Counterfactual explainable recommendation (CER) ",
                    "Citation Text": "Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang.\n2021. Counterfactual Explainable Recommendation. CoRR abs/2108.10539 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10539",
                        "Citation Paper Title": "Title:Counterfactual Explainable Recommendation",
                        "Citation Paper Abstract": "Abstract:By providing explanations for users and system designers to facilitate better understanding and decision making, explainable recommendation has been an important research problem. In this paper, we propose Counterfactual Explainable Recommendation (CountER), which takes the insights of counterfactual reasoning from causal inference for explainable recommendation. CountER is able to formulate the complexity and the strength of explanations, and it adopts a counterfactual learning framework to seek simple (low complexity) and effective (high strength) explanations for the model decision. Technically, for each item recommended to each user, CountER formulates a joint optimization problem to generate minimal changes on the item aspects so as to create a counterfactual item, such that the recommendation decision on the counterfactual item is reversed. These altered aspects constitute the explanation of why the original item is recommended. The counterfactual explanation helps both the users for better understanding and the system designers for better model debugging. Another contribution of the work is the evaluation of explainable recommendation, which has been a challenging task. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. To measure the explanation quality, we design two types of evaluation metrics, one from user's perspective (i.e. why the user likes the item), and the other from model's perspective (i.e. why the item is recommended by the model). We apply our counterfactual learning algorithm on a black-box recommender system and evaluate the generated explanations on five real-world datasets. Results show that our model generates more accurate and effective explanations than state-of-the-art explainable recommendation models.",
                        "Citation Paper Authors": "Authors:Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, Yongfeng Zhang"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "leverages the adversarial training by adding adversarial perturba-\ntions to target image features. For the second type: IRGAN ",
                    "Citation Text": "Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang,\nPeng Zhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying\nGenerative and Discriminative Information Retrieval Models. In Proceedings of\nthe 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR . ACM, 515\u2013524.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10513",
                        "Citation Paper Title": "Title:IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models",
                        "Citation Paper Abstract": "Abstract:This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",
                        "Citation Paper Authors": "Authors:Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, Dell Zhang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed to add adversarial perturbations on\nuser/item embeddings to improve BPR ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings\nof the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence UAI . 452\u2013\n461.\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "and used the backpropagated ranking\nloss to generate item image\u2019s saliency map as visual explanations;\nAMCF ",
                    "Citation Text": "Deng Pan, Xiangrui Li, Xin Li, and Dongxiao Zhu. 2020. Explainable Recom-\nmendation via Interpretable Feature Mapping and Evaluation of Explainability.\nInProceedings of the Twenty-Ninth International Joint Conference on Artificial\nIntelligence, IJCAI . 2690\u20132696.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06133",
                        "Citation Paper Title": "Title:Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability",
                        "Citation Paper Abstract": "Abstract:Latent factor collaborative filtering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory accuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate the explainability, we propose two new evaluation metrics specifically designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from this https URL.",
                        "Citation Paper Authors": "Authors:Deng Pan, Xiangrui Li, Xin Li, Dongxiao Zhu"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "is an attribute-aware model that uses the\nresidual network to model user-item reviews with joint sentiment\nanalysis; SAERS ",
                    "Citation Text": "Min Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W. Zheng, and Qi Liu. 2019.\nExplainable Fashion Recommendation: A Semantic Attribute Region Guided\nApproach. In Proceedings of the Twenty-Eighth International Joint Conference on\nArtificial Intelligence, IJCAI . 4681\u20134688.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12862",
                        "Citation Paper Title": "Title:Explainable Fashion Recommendation: A Semantic Attribute Region Guided Approach",
                        "Citation Paper Abstract": "Abstract:In fashion recommender systems, each product usually consists of multiple semantic attributes (e.g., sleeves, collar, etc). When making cloth decisions, people usually show preferences for different semantic attributes (e.g., the clothes with v-neck collar). Nevertheless, most previous fashion recommendation models comprehend the clothing images with a global content representation and lack detailed understanding of users' semantic preferences, which usually leads to inferior recommendation performance. To bridge this gap, we propose a novel Semantic Attribute Explainable Recommender System (SAERS). Specifically, we first introduce a fine-grained interpretable semantic space. We then develop a Semantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA) module to project users and items into this space, respectively. With SAERS, we are capable of not only providing cloth recommendations for users, but also explaining the reason why we recommend the cloth through intuitive visual attribute semantic highlights in a personalized manner. Extensive experiments conducted on real-world datasets clearly demonstrate the effectiveness of our approach compared with the state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Min Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W. Zheng, Qi Liu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nHowever, they usually have limited performance and lack the abil-\nity to extend to complicated scenarios. Recently, various types of\nneural network based explainable recommendation models have\nbeen proposed. A2CF ",
                    "Citation Text": "Tong Chen, Hongzhi Yin, Guanhua Ye, Zi Huang, Yang Wang, and Meng Wang.\n2020. Try This Instead: Personalized and Interpretable Substitute Recommenda-\ntion. In Proceedings of the 43rd International ACM SIGIR conference on research\nand development in Information Retrieval, SIGIR . ACM, 891\u2013900.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.09344",
                        "Citation Paper Title": "Title:Try This Instead: Personalized and Interpretable Substitute Recommendation",
                        "Citation Paper Abstract": "Abstract:As a fundamental yet significant process in personalized recommendation, candidate generation and suggestion effectively help users spot the most suitable items for them. Consequently, identifying substitutable items that are interchangeable opens up new opportunities to refine the quality of generated candidates. When a user is browsing a specific type of product (e.g., a laptop) to buy, the accurate recommendation of substitutes (e.g., better equipped laptops) can offer the user more suitable options to choose from, thus substantially increasing the chance of a successful purchase. However, existing methods merely treat this problem as mining pairwise item relationships without the consideration of users' personal preferences. Moreover, the substitutable relationships are implicitly identified through the learned latent representations of items, leading to uninterpretable recommendation results. In this paper, we propose attribute-aware collaborative filtering (A2CF) to perform substitute recommendation by addressing issues from both personalization and interpretability perspectives. Instead of directly modelling user-item interactions, we extract explicit and polarized item attributes from user reviews with sentiment analysis, whereafter the representations of attributes, users, and items are simultaneously learned. Then, by treating attributes as the bridge between users and items, we can thoroughly model the user-item preferences (i.e., personalization) and item-item relationships (i.e., substitution) for recommendation. In addition, A2CF is capable of generating intuitive interpretations by analyzing which attributes a user currently cares the most and comparing the recommended substitutes with her/his currently browsed items at an attribute level. The recommendation effectiveness and interpretation quality of A2CF are demonstrated via extensive experiments on three real datasets.",
                        "Citation Paper Authors": "Authors:Tong Chen, Hongzhi Yin, Guanhua Ye, Zi Huang, Yang Wang, Meng Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.11540v2": {
            "Paper Title": "Wacky Weights in Learned Sparse Representations and the Revenge of\n  Score-at-a-Time Query Evaluation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "before being\nimported into PISA and JASS. In addition, both PISA and JASS\nmade use of document reordering [ 8,26] which has been shown\nto improve compression and accelerate query processing in both\nDaaT andSaaT ",
                    "Citation Text": "J. Mackenzie, M. Petri, and A. Moffat. Anytime ranking on document-ordered\nindexes. ACM Transactions on Information Systems , 40(1):13.1\u201313.32, Jan. 2022.\nTo appear.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08976",
                        "Citation Paper Title": "Title:Anytime Ranking on Document-Ordered Indexes",
                        "Citation Paper Abstract": "Abstract:Inverted indexes continue to be a mainstay of text search engines, allowing efficient querying of large document collections. While there are a number of possible organizations, document-ordered indexes are the most common, since they are amenable to various query types, support index updates, and allow for efficient dynamic pruning operations. One disadvantage with document-ordered indexes is that high-scoring documents can be distributed across the document identifier space, meaning that index traversal algorithms that terminate early might put search effectiveness at risk. The alternative is impact-ordered indexes, which primarily support top-k disjunctions, but also allow for anytime query processing, where the search can be terminated at any time, with search quality improving as processing latency increases. Anytime query processing can be used to effectively reduce high-percentile tail latency which is essential for operational scenarios in which a service level agreement (SLA) imposes response time requirements. In this work, we show how document-ordered indexes can be organized such that they can be queried in an anytime fashion, enabling strict latency control with effective early termination. Our experiments show that processing document-ordered topical segments selected by a simple score estimator outperforms existing anytime algorithms, and allows query runtimes to be accurately limited in order to comply with SLA requirements.",
                        "Citation Paper Authors": "Authors:Joel Mackenzie, Matthias Petri, Alistair Moffat"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "that has lower\ninference costs but appears to be just as effective.\nSPLADEv2 ",
                    "Citation Text": "T. Formal, C. Lassance, B. Piwowarski, and S. Clinchant. SPLADE v2: Sparse\nlexical and expansion model for information retrieval. arXiv:2109.10086 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.10086",
                        "Citation Paper Title": "Title:SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:In neural Information Retrieval (IR), ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning \\emph{sparse} representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. Introduced recently, the SPLADE model provides highly sparse representations and competitive results with respect to state-of-the-art dense and sparse approaches. In this paper, we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation. We also report results on the BEIR benchmark. Overall, SPLADE is considerably improved with more than $9$\\% gains on NDCG@10 on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.",
                        "Citation Paper Authors": "Authors:Thibault Formal, Carlos Lassance, Benjamin Piwowarski, St\u00e9phane Clinchant"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "(uniCOIL-T5 for short) is a sim-\nplified variant of COIL ",
                    "Citation Text": "L. Gao, Z. Dai, T. Chen, Z. Fan, B. V. Durme, and J. Callan. Complementing lexical\nretrieval with semantic residual embedding. In Proceedings of the 43rd European\nConference on Information Retrieval (ECIR 2021), Part I , pages 146\u2013160, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13969",
                        "Citation Paper Title": "Title:Complementing Lexical Retrieval with Semantic Residual Embedding",
                        "Citation Paper Abstract": "Abstract:This paper presents CLEAR, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model. CLEAR explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of CLEAR over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ".\nBM25 w/ doc2query\u2013T5 [34,35] (BM25-T5 for short) augments\npassages in the corpus with query predictions generated by the\nT5 ",
                    "Citation Text": "C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li,\nand P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research , 21(140):1\u201367, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "over bag-of-words representations of the passages in\nthe corpus. We set \ud835\udc581=0.82and\ud835\udc4f=0.68, based on tuning on a\nselection of training instances on the MS MARCO passage ranking\ntest collection ",
                    "Citation Text": "J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: A\nPython toolkit for reproducible information retrieval research with sparse and\ndense representations. In Proceedings of the 44th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2021) ,\npages 2356\u20132362, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.10073",
                        "Citation Paper Title": "Title:Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations",
                        "Citation Paper Abstract": "Abstract:Pyserini is an easy-to-use Python toolkit that supports replicable IR research by providing effective first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. We also describe how our group has built a culture of replicability through shared norms and tools that enable rigorous automated testing.",
                        "Citation Paper Authors": "Authors:Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01911v1": {
            "Paper Title": "Parameterized Explanations for Investor / Company Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13442v2": {
            "Paper Title": "A Purely Regular Approach to Non-Regular Core Spanners",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "which, by presenting a logic that exactly\ncovers core spanners, answers questions on the expressive p ower of core spanners, ",
                    "Citation Text": "Liat Peterfreund, Balder ten Cate, Ronald Fagin, and Be nny Kimelfeld. Recursive programs for docu-\nment spanners. In 22nd International Conference on Database Theory, ICDT 201 9, March 26-28, 2019,\nLisbon, Portugal , pages 13:1\u201313:18, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.08198",
                        "Citation Paper Title": "Title:Recursive Programs for Document Spanners",
                        "Citation Paper Abstract": "Abstract:A document spanner models a program for Information Extraction (IE) as a function that takes as input a text document (string over a finite alphabet) and produces a relation of spans (intervals in the document) over a predefined schema. A well studied language for expressing spanners is that of the regular spanners: relational algebra over regex formulas, which are obtained by adding capture variables to regular expressions. Equivalently, the regular spanners are the ones expressible in non-recursive Datalog over regex formulas (extracting relations that play the role of EDBs from the input document). In this paper, we investigate the expressive power of recursive Datalog over regex formulas. Our main result is that such programs capture precisely the document spanners computable in polynomial time. Additional results compare recursive programs to known formalisms such as the language of core spanners (that extends regular spanners by allowing to test for string equality) and its closure under difference. Finally, we extend our main result to a recently proposed framework that generalizes both the relational model and document spanners.",
                        "Citation Paper Authors": "Authors:Liat Peterfreund, Balder ten Cate, Ronald Fagin, Benny Kimelfeld"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14455v1": {
            "Paper Title": "CBIR using Pre-Trained Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ", ResNet, ResNeXt, SEnet\nin order to use their potential for making low dimensional\nrepresentation of the images. They use kNN for \ufb01nding the\nnearest images to the query image. In this paper, we implement\na similar strategy, utilising InceptionV3 ",
                    "Citation Text": "C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking\nthe inception architecture for computer vision,\u201d in Proceedings of the\nIEEE conference on computer vision and pattern recognition , pp. 2818\u2013\n2826, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.15790v1": {
            "Paper Title": "LSTM-RPA: A Simple but Effective Long Sequence Prediction Algorithm for\n  Music Popularity Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14266v1": {
            "Paper Title": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical\n  Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "proposed DeepPath, which relies on a policy-\nbased agent that learns to reason over multi-hop paths by sampling relations at each step. Also, Das\net al. ",
                    "Citation Text": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay\nKrishnamurthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer:\nReasoning over paths in knowledge bases using reinforcement learning. In ICLR 2018 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05851",
                        "Citation Paper Title": "Title:Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with combinatorially many destinations from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. Empirically, this approach obtains state-of-the-art results on several datasets, significantly outperforming prior methods.",
                        "Citation Paper Authors": "Authors:Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew McCallum"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". Many KB embedding approaches support the same operation performed by our\nrelation-level model, namely relation projection [ 10,43,25,38]. Some KB embedding methods also\nexplicitly learn to follow chains of relations and traverse KGs [ 24,30,13]. Notably, Query2Box ",
                    "Citation Text": "Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs\nin vector space using box embeddings. In ICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05969",
                        "Citation Paper Title": "Title:Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings",
                        "Citation Paper Abstract": "Abstract:Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on three large KGs and show that query2box achieves up to 25% relative improvement over the state of the art.",
                        "Citation Paper Authors": "Authors:Hongyu Ren, Weihua Hu, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14044v1": {
            "Paper Title": "iALS++: Speeding up Matrix Factorization with Subspace Optimization",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "studies a problem of size jSj= 9;617;414 and\njUj+jIj= 289;493, and ",
                    "Citation Text": "He, X., Zhang, H., Kan, M.-Y., and Chua, T.-S. Fast matrix factor-\nization for online recommendation with implicit feedback. In Proceedings\nof the 39th International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval (New York, NY, USA, 2016), SIGIR '16,\nAssociation for Computing Machinery, p. 549{558.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05024",
                        "Citation Paper Title": "Title:Fast Matrix Factorization for Online Recommendation with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our eALS method consistently outperforms state-of-the-art implicit MF methods. Our implementation is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Hanwang Zhang, Min-Yen Kan, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.13971v1": {
            "Paper Title": "Diachronic Text Mining Investigation of Therapeutic Candidates for\n  COVID-19",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13504v1": {
            "Paper Title": "Managing Bias in Human-Annotated Data: Moving Beyond Bias Removal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13016v1": {
            "Paper Title": "Generating artificial texts as substitution or complement of training\n  data",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ").\nIn a similar vein since it only modifies the original examples\nlocally, some neural techniques exploit masked language models\n(such as Bert ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,\n4171\u20134186. https://doi.org/10.18653/v1/N19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "tasks, of course disjoint from the training sets T.\n5.1 First results\nFor our first experiments, we use state-of-the-art neural classifi-\ncation models based on transformers. For the MediaEval data, in\nEnglish, we opt for a Ro bert a ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12946v1": {
            "Paper Title": "Optimal Model Averaging: Towards Personalized Collaborative Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05686v2": {
            "Paper Title": "Yes, BM25 is a Strong Baseline for Legal Case Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "compared TF-IDF, BM25 and Word2Vec\nmodels for jurisprudence retrieval. The results indicated that the\nWord2Vec Skip-Gram model trained on a specialized legal cor pus\nand BM25 yield similar performance. Althammer et al. ",
                    "Citation Text": "Sophia Althammer, Sebastian Hofst\u00e4tter, and Allan Hanb ury. 2020. Cross-\ndomain Retrieval in the Legal and Patent Domains: a Reproduc ibility Study.\narXiv preprint arXiv:2012.11405 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11405",
                        "Citation Paper Title": "Title:Cross-domain Retrieval in the Legal and Patent Domains: a Reproducibility Study",
                        "Citation Paper Abstract": "Abstract:Domain specific search has always been a challenging information retrieval task due to several challenges such as the domain specific language, the unique task setting, as well as the lack of accessible queries and corresponding relevance judgements. In the last years, pretrained language models, such as BERT, revolutionized web and news search. Naturally, the community aims to adapt these advancements to cross-domain transfer of retrieval models for domain specific search. In the context of legal document retrieval, Shao et al. propose the BERT-PLI framework by modeling the Paragraph Level Interactions with the language model BERT. In this paper we reproduce the original experiments, we clarify pre-processing steps, add missing scripts for framework steps and investigate different evaluation approaches, however we are not able to reproduce the evaluation results. Contrary to the original paper, we demonstrate that the domain specific paragraph-level modelling does not appear to help the performance of the BERT-PLI model compared to paragraph-level modelling with the original BERT. In addition to our legal search reproducibility study, we investigate BERT-PLI for document retrieval in the patent domain. We find that the BERT-PLI model does not yet achieve performance improvements for patent document retrieval compared to the BM25 baseline. Furthermore, we evaluate the BERT-PLI model for cross-domain retrieval between the legal and patent domain on individual components, both on a paragraph and document-level. We find that the transfer of the BERT-PLI model on the paragraph-level leads to comparable results between both domains as well as first promising results for the cross-domain transfer on the document-level. For reproducibility and transparency as well as to benefit the community we make our source code and the trained models publicly available.",
                        "Citation Paper Authors": "Authors:Sophia Althammer, Sebastian Hofst\u00e4tter, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". For\nexample, in task 1 of COLIEE 2019, Gain et al. ",
                    "Citation Text": "B. Gain, D. Bandyopadhyay, T. Saikh, and A Ekbal. 2019. II TP@COLIEE 2019:\nlegal information retrieval using BM25 and BERT. Proceedings of the 6th Com-\npetition on Legal Information Extraction/Entailment. COL IEE 2019 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08653",
                        "Citation Paper Title": "Title:IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT",
                        "Citation Paper Abstract": "Abstract:Natural Language Processing (NLP) and Information Retrieval (IR) in the judicial domain is an essential task. With the advent of availability domain-specific data in electronic form and aid of different Artificial intelligence (AI) technologies, automated language processing becomes more comfortable, and hence it becomes feasible for researchers and developers to provide various automated tools to the legal community to reduce human burden. The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in association with the International Conference on Artificial Intelligence and Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to provide few automated systems to the judicial system. The paper presents our working note on the experiments carried out as a part of our participation in all the sub-tasks defined in this shared task. We make use of different Information Retrieval(IR) and deep learning based approaches to tackle these problems. We obtain encouraging results in all these four sub-tasks.",
                        "Citation Paper Authors": "Authors:Baban Gain, Dibyanayan Bandyopadhyay, Tanik Saikh, Asif Ekbal"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Some successful NLP approaches to the legal domain use a com-\nbination of data-driven methods and hand-crafted rules ",
                    "Citation Text": "Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and\nMaosong Sun. 2020. How Does NLP Bene\ufb01t Legal System: A Summar y of\nLegal Arti\ufb01cial Intelligence. arXiv:2004.12158 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.12158",
                        "Citation Paper Title": "Title:How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence",
                        "Citation Paper Abstract": "Abstract:Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from this https URL.",
                        "Citation Paper Authors": "Authors:Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, Maosong Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12681v1": {
            "Paper Title": "Content Filtering Enriched GNN Framework for News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14812v1": {
            "Paper Title": "Differentiable NAS Framework and Application to Ads CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ", recom-\nmender systems have the strictest accuracy requirements. A\nlogloss difference of just 0.001 is practically signi\ufb01cant ",
                    "Citation Text": "Wang, Ruoxi, Bin Fu, Gang Fu, and Mingliang Wang. \u201cDeep & cross\nnetwork for ad click predictions.\u201d In Proceedings of the ADKDD\u201917,\npp. 1-7. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". Some concrete examples of this range can be found\nin ",
                    "Citation Text": "Gupta, Udit, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon\nReagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole-\nJean Wu. \u201cDeeprecsys: A system for optimizing end-to-end at-scale\nneural recommendation inference.\u201d In 2020 ACM/IEEE 47th Annual\nInternational Symposium on Computer Architecture (ISCA), pp. 982-\n995. IEEE, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.02772",
                        "Citation Paper Title": "Title:DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference",
                        "Citation Paper Abstract": "Abstract:Neural personalized recommendation is the corner-stone of a wide collection of cloud services and products, constituting significant compute demand of the cloud infrastructure. Thus, improving the execution efficiency of neural recommendation directly translates into infrastructure capacity saving. In this paper, we devise a novel end-to-end modeling infrastructure, DeepRecInfra, that adopts an algorithm and system co-design methodology to custom-design systems for recommendation use cases. Leveraging the insights from the recommendation characterization, a new dynamic scheduler, DeepRecSched, is proposed to maximize latency-bounded throughput by taking into account characteristics of inference query size and arrival patterns, recommendation model architectures, and underlying hardware systems. By doing so, system throughput is doubled across the eight industry-representative recommendation models. Finally, design, deployment, and evaluation in at-scale production datacenter shows over 30% latency reduction across a wide variety of recommendation models running on hundreds of machines.",
                        "Citation Paper Authors": "Authors:Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, Carole-Jean Wu"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "focuses on designing architectures speci\ufb01c to\nindividual NLP tasks based on a general pre-trained BERT\nmodel. BERT ",
                    "Citation Text": "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\u201cBERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding.\u201d In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, V olume 1 (Long and Short Papers), pp.\n4171-4186. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.11072v2": {
            "Paper Title": "Sequential Modeling with Multiple Attributes for Watchlist\n  Recommendation in E-Commerce",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "allow to include item attributes as an addi-\ntional context that can be utilized for representing items both during\ntraining and prediction time. Such works either extend basic deep-\nsequential models (RNNs [ 2,21] and Transformers ",
                    "Citation Text": "Yotam Eshel, Or Levi, Haggai Roitman, and Alexander Nus. 2021. PreSizE: Pre-\ndicting Size in E-Commerce using Transformers. arXiv:2105.01564 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01564",
                        "Citation Paper Title": "Title:PreSizE: Predicting Size in E-Commerce using Transformers",
                        "Citation Paper Abstract": "Abstract:Recent advances in the e-commerce fashion industry have led to an exploration of novel ways to enhance buyer experience via improved personalization. Predicting a proper size for an item to recommend is an important personalization challenge, and is being studied in this work. Earlier works in this field either focused on modeling explicit buyer fitment feedback or modeling of only a single aspect of the problem (e.g., specific category, brand, etc.). More recent works proposed richer models, either content-based or sequence-based, better accounting for content-based aspects of the problem or better modeling the buyer's online journey. However, both these approaches fail in certain scenarios: either when encountering unseen items (sequence-based models) or when encountering new users (content-based models).\nTo address the aforementioned gaps, we propose PreSizE - a novel deep learning framework which utilizes Transformers for accurate size prediction. PreSizE models the effect of both content-based attributes, such as brand and category, and the buyer's purchase history on her size preferences. Using an extensive set of experiments on a large-scale e-commerce dataset, we demonstrate that PreSizE is capable of achieving superior prediction performance compared to previous state-of-the-art baselines. By encoding item attributes, PreSizE better handles cold-start cases with unseen items, and cases where buyers have little past purchase data. As a proof of concept, we demonstrate that size predictions made by PreSizE can be effectively integrated into an existing production recommender system yielding very effective features and significantly improving recommendations.",
                        "Citation Paper Authors": "Authors:Yotam Eshel, Or Levi, Haggai Roitman, Alexander Nus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.13626v1": {
            "Paper Title": "A Pipeline for Graph-Based Monitoring of the Changes in the Information\n  Space of Russian Social Media during the Lockdown",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01188v3": {
            "Paper Title": "LawSum: A weakly supervised approach for Indian Legal Document\n  Summarization",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". We also used the transformer based approach for extractive text\nsummarization that uses BERT ",
                    "Citation Text": "Y. Liu, M. Lapata, Text summarization with pretrained encoders, 2019. arXiv:1908.08345 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.08345",
                        "Citation Paper Title": "Title:Text Summarization with Pretrained Encoders",
                        "Citation Paper Abstract": "Abstract:Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Yang Liu, Mirella Lapata"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", a systematic com-\nparative study is performed on 17000+ Indian supreme court judgements, using algorithms such\nas LexRank ",
                    "Citation Text": "G. Erkan, D. R. Radev, Lexrank: Graph-based lexical centrality as salience in text summa-\nrization, J. Artif. Intell. Res. 22 (2004) 457{479.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1109.2128",
                        "Citation Paper Title": "Title:LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
                        "Citation Paper Abstract": "Abstract:We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
                        "Citation Paper Authors": "Authors:Gunes Erkan, Dragomir R. Radev"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", LSTM and GRU were used in a hierarchical manner for generating sentence\nrepresentations. With onset state-of-the-art pre-trained language models several systems that used\ntransformer like BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional\ntransformers for language understanding, arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", two language models are used, and shows that by using a product-of-\nexperts criteria is enough for maintaining continuous contextual matching while maintaining output\n\ruency. In ",
                    "Citation Text": "H. Zheng, M. Lapata, Sentence centrality revisited for unsupervised summarization, in:\nProceedings of the 57th Annual Meeting of the Association for Computational Linguis-\ntics, Association for Computational Linguistics, Florence, Italy, 2019, pp. 6236{6247. URL:\nhttps://aclanthology.org/P19-1628 . doi: 10.18653/v1/P19-1628 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03508",
                        "Citation Paper Title": "Title:Sentence Centrality Revisited for Unsupervised Summarization",
                        "Citation Paper Abstract": "Abstract:Single document summarization has enjoyed renewed interests in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a)~we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b)~we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",
                        "Citation Paper Authors": "Authors:Hao Zheng, Mirella Lapata"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "proposes a novel approach for unsupervised\nsentence summarization by mapping the Information Bottleneck principle to a conditional language\nmodelling objective. In ",
                    "Citation Text": "J. Zhou, A. Rush, Simple unsupervised summarization by contextual matching, in: Pro-\nceedings of the 57th Annual Meeting of the Association for Computational Linguistics, As-\nsociation for Computational Linguistics, Florence, Italy, 2019, pp. 5101{5106. URL: https:\n//aclanthology.org/P19-1503 . doi: 10.18653/v1/P19-1503 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.13337",
                        "Citation Paper Title": "Title:Simple Unsupervised Summarization by Contextual Matching",
                        "Citation Paper Abstract": "Abstract:We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.",
                        "Citation Paper Authors": "Authors:Jiawei Zhou, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "etc, were proposed for summarization [34, 35]. Recently,\nGoogle has also released ",
                    "Citation Text": "M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Onta~ n\u0013 on, P. Pham,\nA. Ravula, Q. Wang, L. Yang, A. Ahmed, Big bird: Transformers for longer sequences,\nArXiv abs/2007.14062 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.14062",
                        "Citation Paper Title": "Title:Big Bird: Transformers for Longer Sequences",
                        "Citation Paper Abstract": "Abstract:Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                        "Citation Paper Authors": "Authors:Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ", both CNN and LSTM are used for ranking\nthe sentences. In ",
                    "Citation Text": "R. Nallapati, F. Zhai, B. Zhou, Summarunner: A recurrent neural network based sequence\nmodel for extractive summarization of documents, in: Proceedings of the Thirty-First AAAI\nConference on Arti\fcial Intelligence, AAAI'17, AAAI Press, 2017, p. 3075{3081.\n20",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04230",
                        "Citation Paper Title": "Title:SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
                        "Citation Paper Abstract": "Abstract:We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.",
                        "Citation Paper Authors": "Authors:Ramesh Nallapati, Feifei Zhai, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ", CNN and LSTM are\nwere used for creating sentence representations. In ",
                    "Citation Text": "J. Cheng, M. Lapata, Neural summarization by extracting sentences and words, ArXiv\nabs/1603.07252 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.07252",
                        "Citation Paper Title": "Title:Neural Summarization by Extracting Sentences and Words",
                        "Citation Paper Abstract": "Abstract:Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.",
                        "Citation Paper Authors": "Authors:Jianpeng Cheng, Mirella Lapata"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12141v1": {
            "Paper Title": "Rethinking Neural vs. Matrix-Factorization Collaborative Filtering: the\n  Theoretical Perspectives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11298v1": {
            "Paper Title": "Video and Text Matching with Conditioned Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": ". However, a dedicated matching\nmodel significantly improved the results ",
                    "Citation Text": "Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and\nhierarchical modeling of video and text. In ECCV , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.07212",
                        "Citation Paper Title": "Title:Cross-Modal and Hierarchical Modeling of Video and Text",
                        "Citation Paper Abstract": "Abstract:Visual data and text data are composed of information at multiple granularities. A video can describe a complex scene that is composed of multiple clips or shots, where each depicts a semantically coherent event or action. Similarly, a paragraph may contain sentences with different topics, which collectively conveys a coherent message or story. In this paper, we investigate the modeling techniques for such hierarchical sequential data where there are correspondences across multiple modalities. Specifically, we introduce hierarchical sequence embedding (HSE), a generic model for embedding sequential data of different modalities into hierarchically semantic spaces, with either explicit or implicit correspondence information. We perform empirical studies on large-scale video and paragraph retrieval datasets and demonstrated superior performance by the proposed methods. Furthermore, we examine the effectiveness of our learned embeddings when applied to downstream tasks. We show its utility in zero-shot action recognition and video captioning.",
                        "Citation Paper Authors": "Authors:Bowen Zhang, Hexiang Hu, Fei Sha"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "utilize different experts such as speech, audio, mo-\ntion, OCR, appearance, and face detection. Gabeur et al. ",
                    "Citation Text": "Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid. Multi-modal transformer for video retrieval. ECCV ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.10639",
                        "Citation Paper Title": "Title:Multi-modal Transformer for Video Retrieval",
                        "Citation Paper Abstract": "Abstract:The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at this http URL.",
                        "Citation Paper Authors": "Authors:Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", improved this approach by leveraging transformers net-\nworks. Our approach is much more straightforward and con-\nsiders only the video and the text. Lei et al. ",
                    "Citation Text": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\nvideo-and-language learning via sparse sampling. CVPR ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06183",
                        "Citation Paper Title": "Title:Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling",
                        "Citation Paper Abstract": "Abstract:The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from vision models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video features, it is often difficult (or infeasible) to plug feature extractors directly into existing approaches for easy finetuning. To provide a remedy to this dilemma, we propose a generic framework ClipBERT that enables affordable end-to-end learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each training step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that ClipBERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learning with just a few sparsely sampled clips is often more accurate than using densely extracted offline features from full-length videos, proving the proverbial less-is-more principle. Videos in the datasets are from considerably different domains and lengths, ranging from 3-second generic domain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach. Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, Jingjing Liu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "leverage the hierarchy of a sentence parse\ntree to match a sentence with an image. Chen et al . ",
                    "Citation Text": "Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained\nvideo-text retrieval with hierarchical graph reasoning. In\nCVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00392",
                        "Citation Paper Title": "Title:Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning",
                        "Citation Paper Abstract": "Abstract:Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.",
                        "Citation Paper Authors": "Authors:Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposed a\nco-attention module that attends to visual and textual modal-\nities jointly. A more advanced approach for co-attention\nused a bilinear module that efficiently generates attention\nfor every pair ",
                    "Citation Text": "Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear\nattention networks. In NIPS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07932",
                        "Citation Paper Title": "Title:Bilinear Attention Networks",
                        "Citation Paper Abstract": "Abstract:Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.",
                        "Citation Paper Authors": "Authors:Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". Other approaches improved the vector-\nfusion operator for better interaction modeling [ 9,16,2].\nSome approaches imitate a multi-step reasoning with stacked\nattention modules sequentially ",
                    "Citation Text": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and\nAlex Smola. Stacked attention networks for image question\nanswering. In CVPR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.02274",
                        "Citation Paper Title": "Title:Stacked Attention Networks for Image Question Answering",
                        "Citation Paper Abstract": "Abstract:This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
                        "Citation Paper Authors": "Authors:Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "suggested\nthe use of just a few sparsely sampled clips for matching.\nPatrick et al. ",
                    "Citation Text": "Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze,\nAlexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi.\nSupport-set bottlenecks for video-text representation learning.\nICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02824",
                        "Citation Paper Title": "Title:Support-set bottlenecks for video-text representation learning",
                        "Citation Paper Abstract": "Abstract:The dominant paradigm for learning video-text representations -- noise contrastive learning -- increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related -- for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of other support samples' visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX and ActivityNet, and MSVD for video-to-text and text-to-video retrieval.",
                        "Citation Paper Authors": "Authors:Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, Andrea Vedaldi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.11259v1": {
            "Paper Title": "A scale invariant ranking function for learning-to-rank: a real-world\n  use case",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nConcerning the evaluation of our proposed method, we employ one of the most widely used measure in Learning-To-\nRank, the NDCG ",
                    "Citation Text": "Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. A theoretical analysis of ndcg\nranking measures. In Proceedings ofthe26th annual conference onlearning theory (COLT 2013), volume 8,\npage 6, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1304.6480",
                        "Citation Paper Title": "Title:A Theoretical Analysis of NDCG Type Ranking Measures",
                        "Citation Paper Abstract": "Abstract:A central problem in ranking is to design a ranking measure for evaluation of ranking functions. In this paper we study, from a theoretical perspective, the widely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures. Although there are extensive empirical studies of NDCG, little is known about its theoretical properties. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result is very surprising. It seems to imply that NDCG cannot differentiate good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. In order to have a deeper understanding of ranking measures in general, we propose a notion referred to as consistent distinguishability. This notion captures the intuition that a ranking measure should have such a property: For every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets. We show that NDCG with logarithmic discount has consistent distinguishability although it converges to the same limit for all ranking functions. We next characterize the set of all feasible discount functions for NDCG according to the concept of consistent distinguishability. Specifically we show that whether NDCG has consistent distinguishability depends on how fast the discount decays, and 1/r is a critical point. We then turn to the cut-off version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for various choices of k and the discount functions. Experimental results on real Web search datasets agree well with the theory.",
                        "Citation Paper Authors": "Authors:Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06130v3": {
            "Paper Title": "Semantic Answer Similarity for Evaluating Question Answering Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.10926v1": {
            "Paper Title": "PipAttack: Poisoning Federated Recommender Systems forManipulating Item\n  Promotion",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", a reinforcement learning (RL)-based model is designed\nto generate fake profiles by copying the benign users\u2019 profiles in\nthe source domain, while ",
                    "Citation Text": "Hengtong Zhang, Yaliang Li, Bolin Ding, and Jing Gao. 2020. Practical data\npoisoning attack against next-item recommendation. In WWW . 2458\u20132464.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03728",
                        "Citation Paper Title": "Title:Practical Data Poisoning Attack against Next-Item Recommendation",
                        "Citation Paper Abstract": "Abstract:Online recommendation systems make use of a variety of information sources to provide users the items that users are potentially interested in. However, due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks. Existing attack approaches are either based on simple heuristic rules or designed against specific recommendations approaches. The former often suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we focus on a general next-item recommendation setting and propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted.Given these real-world restrictions, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also propose to use the influence function to efficiently estimate the influence of injected samples on the recommendation results, without re-training the models within the simulator. Extensive experiments on two datasets against four representative recommendation models show that the proposed LOKI achieves better attacking performance than existing methods.",
                        "Citation Paper Authors": "Authors:Hengtong Zhang, Yaliang Li, Bolin Ding, Jing Gao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.10850v1": {
            "Paper Title": "Locality-Sensitive Experience Replay for Online Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". More-\nover, more advance reinforcement learning algorithms could be\nused to replace the DDPG such as soft actor-critic (SAC) ",
                    "Citation Text": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning with a\nstochastic actor. In International conference on machine learning . PMLR, 1861\u2013\n1870.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01290",
                        "Citation Paper Title": "Title:Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                        "Citation Paper Abstract": "Abstract:Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
                        "Citation Paper Authors": "Authors:Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.07752v2": {
            "Paper Title": "Hindsight: Posterior-guided training of retrievers for improved\n  open-ended generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08663v4": {
            "Paper Title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": "(faiss depth = 100) and ColBERT re-ranks by computing the late aggregated\ninteractions. We train a bert-base-uncased model, with maximum sequence length of 300 on the MS\nMARCO dataset for 300K steps.\n(v) Re-ranking model : (a)BM25 + CE ",
                    "Citation Text": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM:\nDeep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.\nInAdvances in Neural Information Processing Systems , volume 33, pages 5776\u20135788. Curran\nAssociates, Inc. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10957",
                        "Citation Paper Title": "Title:MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.",
                        "Citation Paper Authors": "Authors:Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "): \ufb01rst top-k candidates are retrieved using\nANN with faiss ",
                    "Citation Text": "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with\nGPUs. arXiv preprint arXiv:1702.08734 . 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08734",
                        "Citation Paper Title": "Title:Billion-scale similarity search with GPUs",
                        "Citation Paper Abstract": "Abstract:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\nWe propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
                        "Citation Paper Authors": "Authors:Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "is a popular document expansion technique using a T5 (base) ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a\nUni\ufb01ed Text-to-Text Transformer. Journal of Machine Learning Research , 21(140):1\u201367. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "is a bi-encoder constructing hard negatives from an Approximate Nearest Neighbor\n(ANN) index of the corpus, which in parallel updates to select hard negative training instances during\n\ufb01ne-tuning of the model. We use the publicly available RoBERTa ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT\nPretraining Approach. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". To overcome this, earlier\ntechniques proposed to improve lexical retrieval systems with neural networks. Sparse methods such\nas docT5query ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by\nQuery Prediction. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "have dominated textual information\nretrieval. Recently, there is a strong interest in using neural networks to improve or replace these\nlexical approaches. In this section, we highlight a few neural-based approaches and we refer the\nreader to Lin et al. ",
                    "Citation Text": "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020. Pretrained Transformers for Text\nRanking: BERT and Beyond. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.06467",
                        "Citation Paper Title": "Title:Pretrained Transformers for Text Ranking: BERT and Beyond",
                        "Citation Paper Abstract": "Abstract:The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.",
                        "Citation Paper Authors": "Authors:Jimmy Lin, Rodrigo Nogueira, Andrew Yates"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.10745v2": {
            "Paper Title": "Feature-level Attentive ICF for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 68,
                    "Sentence": ",\nespecially knowledge-graph (KG) based recommendation models [ 21,29,51,56,58]. A representative\nattention-based FM based model is the AFM model ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional Factorization Machines:\nLearning the Weight of Feature Interactions via Attention Networks. In Proceedings of the Twenty-Sixth International\nJoint Conference on Artificial Intelligence . ijcai.org, 3119\u20133125.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": ", which extends the SLIM model to learn the\nitem-itemset similarity for capturing the higher-order relations. More recently, Xue et al. ",
                    "Citation Text": "Feng Xue, Xiangnan He, Xiang Wang, Jiandong Xu, Kai Liu, and Richang Hong. 2019. Deep Item-based Collaborative\nFiltering for Top-N Recommendation. ACM Transactions on Information Systems 37 (2019), 33.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.04392",
                        "Citation Paper Title": "Title:Deep Item-based Collaborative Filtering for Top-N Recommendation",
                        "Citation Paper Abstract": "Abstract:Item-based Collaborative Filtering(short for ICF) has been widely adopted in recommender systems in industry, owing to its strength in user interest modeling and ease in online personalization. By constructing a user's profile with the items that the user has consumed, ICF recommends items that are similar to the user's profile. With the prevalence of machine learning in recent years, significant processes have been made for ICF by learning item similarity (or representation) from data. Nevertheless, we argue that most existing works have only considered linear and shallow relationship between items, which are insufficient to capture the complicated decision-making process of users.\nIn this work, we propose a more expressive ICF solution by accounting for the nonlinear and higher-order relationship among items. Going beyond modeling only the second-order interaction (e.g. similarity) between two items, we additionally consider the interaction among all interacted item pairs by using nonlinear neural networks. Through this way, we can effectively model the higher-order relationship among items, capturing more complicated effects in user decision-making. For example, it can differentiate which historical itemsets in a user's profile are more important in affecting the user to make a purchase decision on an item. We treat this solution as a deep variant of ICF, thus term it as DeepICF. To justify our proposal, we perform empirical studies on two public datasets from MovieLens and Pinterest. Extensive experiments verify the highly positive effect of higher-order item interaction modeling with nonlinear neural networks. Moreover, we demonstrate that by more fine-grained second-order interaction modeling with attention network, the performance of our DeepICF method can be further improved.",
                        "Citation Paper Authors": "Authors:Feng Xue, Xiangnan He, Xiang Wang, Jiandong Xu, Kai Liu, Richang Hong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.10297v1": {
            "Paper Title": "DXML: Distributed Extreme Multilabel Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.12891v1": {
            "Paper Title": "Local Explanations for Clinical Search Engine results",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". Although LIME offers one way\nto solve the black-box problem, it has a few limitations. The first\nlimitation of using LIME is that it is most commonly used for linear\nor classification models ",
                    "Citation Text": "A. B. Arrieta, N. D\u00edaz-Rodr\u00edguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado,\nS. Garc\u00eda, S. Gil-L\u00f3pez, D. Molina, R. Benjamins et al. , \u201cExplainable artificial\nintelligence (xai): Concepts, taxonomies, opportunities and challenges toward\nresponsible ai,\u201d Information Fusion , vol. 58, pp. 82\u2013115, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10045",
                        "Citation Paper Title": "Title:Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
                        "Citation Paper Abstract": "Abstract:In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.",
                        "Citation Paper Authors": "Authors:Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09905v1": {
            "Paper Title": "Show Me the Whole World: Towards Entire Item Space Exploration for\n  Interactive Personalized Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "reviews the existing practi-\ncal applications of contextual bandit algorithms. By assuming the\npayoff model is linear, LinUCB ",
                    "Citation Text": "Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-\nbandit approach to personalized news article recommendation. In Proceedings of\nthe 19th international conference on World wide web . 661\u2013670.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.0146",
                        "Citation Paper Title": "Title:A Contextual-Bandit Approach to Personalized News Article Recommendation",
                        "Citation Paper Abstract": "Abstract:Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\nIn this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.\nThe contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.",
                        "Citation Paper Authors": "Authors:Lihong Li, Wei Chu, John Langford, Robert E. Schapire"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09877v1": {
            "Paper Title": "Two-stage Voice Application Recommender System for Unhandled Utterances\n  in Intelligent Personal Assistant",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ",\netc. In recent years, deep learning techniques are integrated with\nrecommender systems to better utilize the inherent structure of the\nfeatures and to train the system end-to-end. Some important works\nin this realm include NeuralCF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.\n2017. Neural collaborative filtering. In Proceedings ofthe26th international\nconference onworld wide web. 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". [1,32] provide thorough\nreviews of recommender systems. Traditional recommender tech-\nniques include matrix factorization ",
                    "Citation Text": "Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast\nmatrix factorization for online recommendation with implicit feedback. In\nProceedings ofthe39th International ACM SIGIR conference onResearch and\nDevelopment inInformation Retrieval. 549\u2013558.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05024",
                        "Citation Paper Title": "Title:Fast Matrix Factorization for Online Recommendation with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our eALS method consistently outperforms state-of-the-art implicit MF methods. Our implementation is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Hanwang Zhang, Min-Yen Kan, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04049v2": {
            "Paper Title": "Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09248v2": {
            "Paper Title": "Demographic Biases of Crowd Workers in Key Opinion Leaders Finding",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "focusedonthesubjective\ntaskand triedtounderstandthein\ufb02uence ofworker\u2019s prefer ences ontheirperformance.Todothis,theyexamine the\nannotationsofcrowdworkersondi\ufb00erenttopicstoseethee\ufb00 ectofworker\u2019sopinionsontheirannotations.Their\ufb01nd-\nings showthatcrowdworkerswithstrongopinionsproducebi asedannotations.Theproposedapproachispromising\ntomitigatesuchbiasandcanimprovethequalityofthedatac ollected.Chakrabortyetal. ",
                    "Citation Text": "Abhijnan Chakraborty, Johnnatan Messias, Fabricio Ben evenuto, Saptarshi Ghosh, Niloy Ganguly, and Krishna Gumma di. 2017. Who makes\ntrends? understanding demographic biases in crowdsourced recommendations. In Proceedings of the International AAAI Conference on Web and\nSocial Media , Vol. 11.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00139",
                        "Citation Paper Title": "Title:Who Makes Trends? Understanding Demographic Biases in Crowdsourced Recommendations",
                        "Citation Paper Abstract": "Abstract:Users of social media sites like Facebook and Twitter rely on crowdsourced content recommendation systems (e.g., Trending Topics) to retrieve important and useful information. Contents selected for recommendation indirectly give the initial users who promoted (by liking or posting) the content an opportunity to propagate their messages to a wider audience. Hence, it is important to understand the demographics of people who make a content worthy of recommendation, and explore whether they are representative of the media site's overall population. In this work, using extensive data collected from Twitter, we make the first attempt to quantify and explore the demographic biases in the crowdsourced recommendations. Our analysis, focusing on the selection of trending topics, finds that a large fraction of trends are promoted by crowds whose demographics are significantly different from the overall Twitter population. More worryingly, we find that certain demographic groups are systematically under-represented among the promoters of the trending topics. To make the demographic biases in Twitter trends more transparent, we developed and deployed a Web-based service 'Who-Makes-Trends' at this http URL.",
                        "Citation Paper Authors": "Authors:Abhijnan Chakraborty, Johnnatan Messias, Fabricio Benevenuto, Saptarshi Ghosh, Niloy Ganguly, Krishna P. Gummadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09083v1": {
            "Paper Title": "Learning to Learn a Cold-start Sequential Recommender",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "combine the scenario-specific learning with meta-learning for online-recommendation. Zhao\net al. ",
                    "Citation Text": "Liang Zhao, Yang Wang, Daxiang Dong, and Hao Tian. 2019. Learning to Recommend via Meta Parameter Partition.\narXiv preprint arXiv:1912.04108 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04108",
                        "Citation Paper Title": "Title:Learning to Recommend via Meta Parameter Partition",
                        "Citation Paper Abstract": "Abstract:In this paper we propose to solve an important problem in recommendation -- user cold start, based on meta leaning method. Previous meta learning approaches finetune all parameters for each new user, which is both computing and storage expensive. In contrast, we divide model parameters into fixed and adaptive parts and develop a two-stage meta learning algorithm to learn them separately. The fixed part, capturing user invariant features, is shared by all users and is learned during offline meta learning stage. The adaptive part, capturing user specific features, is learned during online meta learning stage. By decoupling user invariant parameters from user dependent parameters, the proposed approach is more efficient and storage cheaper than previous methods. It also has potential to deal with catastrophic forgetting while continually adapting for streaming coming users.\nExperiments on production data demonstrates that the proposed method converges faster and to a better performance than baseline methods. Meta-training without online meta model finetuning increases the AUC from 72.24% to 74.72% (2.48% absolute improvement). Online meta training achieves a further gain of 2.46\\% absolute improvement comparing with offline meta training.",
                        "Citation Paper Authors": "Authors:Liang Zhao, Yang Wang, Daxiang Dong, Hao Tian"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ", to\nlearn a suitable model for initializing the adaption. SML ",
                    "Citation Text": "Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, and Yongdong Zhang. 2020. How to Retrain\nRecommender System? A Sequential Meta-Learning Method. In Proceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920) . Association for\nComputing Machinery, New York, NY, USA, 1479\u20131488. https://doi.org/10.1145/3397271.3401167",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13258",
                        "Citation Paper Title": "Title:How to Retrain Recommender System? A Sequential Meta-Learning Method",
                        "Citation Paper Abstract": "Abstract:Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community.\nOur first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience. Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the \"future performance\" -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: this https URL.",
                        "Citation Paper Authors": "Authors:Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, Yongdong Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09036v1": {
            "Paper Title": "Ranking Facts for Explaining Answers to Elementary Science Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07959v2": {
            "Paper Title": "Low-rank Matrix Recovery With Unknown Correspondence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.08919v1": {
            "Paper Title": "Low-Precision Quantization for Efficient Nearest Neighbor Search",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ", and two public benchmark datasets from Aum\u00fcller\net al. ",
                    "Citation Text": "Martin Aum\u00fcller, Erik Bernhardsson, and Alexander John Faithfull. 2018. ANN-\nBenchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algo-\nrithms. CoRR abs/1807.05614 (2018). arXiv:1807.05614 http://arxiv.org/abs/1807.\n05614",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.05614",
                        "Citation Paper Title": "Title:ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms",
                        "Citation Paper Abstract": "Abstract:This paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on different standard data sets. It supports several different ways of integrating $k$-NN algorithms, and its configuration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting front-ends can visualise these as images, $\\LaTeX$ plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of $k$-NN algorithms. In the short term, this overview allows users to choose the correct $k$-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and refine automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very different approaches to $k$-NN search yield comparable quality-performance trade-offs. The system is available at this http URL .",
                        "Citation Paper Authors": "Authors:Martin Aum\u00fcller, Erik Bernhardsson, Alexander Faithfull"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.08872v1": {
            "Paper Title": "Contrastive Learning of Visual-Semantic Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "ResNet-152 56.4 85.3 91.5 43.9 78.1 88.6 443.8 47.6 77.4 87.1 35.4 68.3 79.9 395.7\nDAN ",
                    "Citation Text": "H. Nam, J.-W. Ha, and J. Kim, \u201cDual attention networks for multimodal\nreasoning and matching,\u201d in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00471",
                        "Citation Paper Title": "Title:Dual Attention Networks for Multimodal Reasoning and Matching",
                        "Citation Paper Abstract": "Abstract:We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.",
                        "Citation Paper Authors": "Authors:Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "VGG 50.1 79.7 89.2 39.6 75.2 86.9 420.7 40.3 68.9 79.9 29.7 60.1 72.1 351.0\nsm-LSTM ",
                    "Citation Text": "Y . Huang, W. Wang, and L. Wang, \u201cInstance-aware image and sentence\nmatching with selective multimodal lstm,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05588",
                        "Citation Paper Title": "Title:Instance-aware Image and Sentence Matching with Selective Multimodal LSTM",
                        "Citation Paper Abstract": "Abstract:Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.",
                        "Citation Paper Authors": "Authors:Yan Huang, Wei Wang, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "VGG 43.4 75.7 85.8 31.0 66.7 79.9 382.5 35.4 63.8 73.7 22.8 50.7 63.1 309.5\nDSPE+FV ",
                    "Citation Text": "L. Wang, Y . Li, and S. Lazebnik, \u201cLearning deep structure-preserving\nimage-text embeddings,\u201d in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06078",
                        "Citation Paper Title": "Title:Learning Deep Structure-Preserving Image-Text Embeddings",
                        "Citation Paper Abstract": "Abstract:This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.",
                        "Citation Paper Authors": "Authors:Liwei Wang, Yin Li, Svetlana Lazebnik"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nThe deep learning based techniques approach this problem by\nlearning non-linear projections for individual modalities that\nare subsequently fused to lead to a common subspace where\nheterogeneous samples can be directly matched ",
                    "Citation Text": "J. Mao, W. Xu, Y . Yang, J. Wang, Z. Huang, and A. Yuille, \u201cDeep\ncaptioning with multimodal recurrent neural networks (m-rnn),\u201d in ICLR ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6632",
                        "Citation Paper Title": "Title:Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: this http URL .",
                        "Citation Paper Authors": "Authors:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". To learn the base embedding\nnetwork, a deep CNN ( e.g., ResNet152 ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in CVPR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03373v4": {
            "Paper Title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "propose an early\nattempt that introduces BERT-based bi-encoders for large-scale\nretrieval. It also studies the effects of several new pretraining tasks.\nHumeau et al. ",
                    "Citation Text": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019.\nPoly-encoders: Transformer architectures and pre-training strategies for fast and\naccurate multi-sentence scoring. arXiv preprint arXiv:1905.01969 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01969",
                        "Citation Paper Title": "Title:Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring",
                        "Citation Paper Abstract": "Abstract:The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.",
                        "Citation Paper Authors": "Authors:Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", have achieved monumental success in natural lan-\nguage understanding. Notably, the recent state-of-the-art PLMs [ 4,\n22,43] are usually based on Transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "advance such bi-encoder with attentive interac-\ntion for the query and document embeddings. Lu et al. ",
                    "Citation Text": "Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, and Yinfei Yang. 2020.\nNeural Passage Retrieval with Improved Negative Contrast. arXiv preprint\narXiv:2010.12523 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12523",
                        "Citation Paper Title": "Title:Neural Passage Retrieval with Improved Negative Contrast",
                        "Citation Paper Abstract": "Abstract:In this paper we explore the effects of negative sampling in dual encoder models used to retrieve passages for automatic question answering. We explore four negative sampling strategies that complement the straightforward random sampling of negatives, typically used to train dual encoder models. Out of the four strategies, three are based on retrieval and one on heuristics. Our retrieval-based strategies are based on the semantic similarity and the lexical overlap between questions and passages. We train the dual encoder models in two stages: pre-training with synthetic data and fine tuning with domain-specific data. We apply negative sampling to both stages. The approach is evaluated in two passage retrieval tasks. Even though it is not evident that there is one single sampling strategy that works best in all the tasks, it is clear that our strategies contribute to improving the contrast between the response and all the other passages. Furthermore, mixing the negatives from different strategies achieve performance on par with the best performing strategy in all tasks. Our results establish a new state-of-the-art level of performance on two of the open-domain question answering datasets that we evaluated.",
                        "Citation Paper Authors": "Authors:Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, Yinfei Yang"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ") exhibit superior capacity in understanding textual\ndata, a handful of studies start to leverage them for semantic re-\ntrieval [ 2,16,24]. For instance, Chang et al. ",
                    "Citation Text": "Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.\n2020. Pre-training tasks for embedding-based large-scale retrieval. arXiv preprint\narXiv:2002.03932 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.03932",
                        "Citation Paper Title": "Title:Pre-training Tasks for Embedding-based Large-scale Retrieval",
                        "Citation Paper Abstract": "Abstract:We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.",
                        "Citation Paper Authors": "Authors:Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", which exploit a deep\nstructure with stacked multi-head attention and fully-connected\nlayers. More importantly, they adopted unsupervised pretraining\nwith large corpus, and thus can incorporate more useful knowl-\nedge in the models. As BERT and its successors (e.g., XLNet ",
                    "Citation Text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. arXiv preprint arXiv:1906.08237 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08237",
                        "Citation Paper Title": "Title:XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                        "Citation Paper Abstract": "Abstract:With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
                        "Citation Paper Authors": "Authors:Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\n2.2 Pretrained Language Models\nPretrained Language Models (PLMs), such as ELMo ",
                    "Citation Text": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. arXiv preprint arXiv:1802.05365 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "employ DNN-\nbased bi-encoder in Facebook Search system, and introduce various\npractical experiences in the end-to-end optimization of the system.\nZhang et al. ",
                    "Citation Text": "Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao,\nWeipeng Yan, and Wen-Yun Yang. 2020. Towards Personalized and Semantic Re-\ntrieval: An End-to-End Solution for E-commerce Search via Embedding Learning.\narXiv preprint arXiv:2006.02282 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.02282",
                        "Citation Paper Title": "Title:Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Nowadays e-commerce search has become an integral part of many people's shopping routines. Two critical challenges stay in today's e-commerce search: how to retrieve items that are semantically relevant but not exact matching to query terms, and how to retrieve items that are more personalized to different users for the same search query. In this paper, we present a novel approach called DPSR, which stands for Deep Personalized and Semantic Retrieval, to tackle this problem. Explicitly, we share our design decisions on how to architect a retrieval system so as to serve industry-scale traffic efficiently and how to train a model so as to learn query and item semantics accurately. Based on offline evaluations and online A/B test with live traffics, we show that DPSR model outperforms existing models, and DPSR system can retrieve more personalized and semantically relevant items to significantly improve users' search experience by +1.29% conversion rate, especially for long tail queries by +10.03%. As a result, our DPSR system has been successfully deployed into this http URL's search production since 2019.",
                        "Citation Paper Authors": "Authors:Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao, Weipeng Yan, Wen-Yun Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.08510v1": {
            "Paper Title": "DFW-PP: Dynamic Feature Weighting based Popularity Prediction for Social\n  Media Content",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "basedB ayesian\nmodelfor theregression task.\nDeep Learning based Models : For the Deep learning-based\nmodels,we use a Resnet 1D variant with 18 layers from ",
                    "Citation Text": "Parth Patwa, Viswanatha Reddy, Rohan Sukumaran, Sethu raman TV, Eptehal\nNashnoush, Sheshank Shankar, Rishemjit Kaur, Abhishek Sin gh, and Ramesh\nRaskar. 2021. Can Self Reported Symptoms Predict Daily COVI D-19 Cases?\narXiv preprint arXiv:2105.08321 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.08321",
                        "Citation Paper Title": "Title:Can Self Reported Symptoms Predict Daily COVID-19 Cases?",
                        "Citation Paper Abstract": "Abstract:The COVID-19 pandemic has impacted lives and economies across the globe, leading to many deaths. While vaccination is an important intervention, its roll-out is slow and unequal across the globe. Therefore, extensive testing still remains one of the key methods to monitor and contain the virus. Testing on a large scale is expensive and arduous. Hence, we need alternate methods to estimate the number of cases. Online surveys have been shown to be an effective method for data collection amidst the pandemic. In this work, we develop machine learning models to estimate the prevalence of COVID-19 using self-reported symptoms. Our best model predicts the daily cases with a mean absolute error (MAE) of 226.30 (normalized MAE of 27.09%) per state, which demonstrates the possibility of predicting the actual number of confirmed cases by utilizing self-reported symptoms. The models are developed at two levels of data granularity - local models, which are trained at the state level, and a single global model which is trained on the combined data aggregated across all states. Our results indicate a lower error on the local models as opposed to the global model. In addition, we also show that the most important symptoms (features) vary considerably from state to state. This work demonstrates that the models developed on crowd-sourced data, curated via online platforms, can complement the existing epidemiological surveillance infrastructure in a cost-effective manner. The code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Parth Patwa, Viswanatha Reddy, Rohan Sukumaran, Sethuraman TV, Eptehal Nashnoush, Sheshank Shankar, Rishemjit Kaur, Abhishek Singh, Ramesh Raskar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.08353v1": {
            "Paper Title": "Revisiting Popularity and Demographic Biases in Recommender Evaluation\n  and Effectiveness",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". In ride-hailing platforms, bias can lead to producer-side starvation and\nloss of income for drivers [41, 42]. Similarly, Ekstrand and Kluver ",
                    "Citation Text": "Ekstrand, M.D., Kluver, D.: Exploring author gender in book rating and\nrecommendation. User Modeling and User-Adapted Interaction pp. 1\u201344\n(2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.07586",
                        "Citation Paper Title": "Title:Exploring Author Gender in Book Rating and Recommendation",
                        "Citation Paper Abstract": "Abstract:Collaborative filtering algorithms find useful patterns in rating and consumption data and exploit these patterns to guide users to good items. Many of the patterns in rating datasets reflect important real-world differences between the various users and items in the data; other patterns may be irrelevant or possibly undesirable for social or ethical reasons, particularly if they reflect undesired discrimination, such as discrimination in publishing or purchasing against authors who are women or ethnic minorities. In this work, we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to a dimension of social concern, namely content creator gender. Using publicly-available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. We find that common collaborative filtering algorithms differ in the gender distribution of their recommendation lists, and in the relationship of that output distribution to user profile distribution.",
                        "Citation Paper Authors": "Authors:Michael D. Ekstrand, Daniel Kluver"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". Past interactions are often subject to biases\u2014such as\nselection bias ",
                    "Citation Text": "Marlin, B.M., Zemel, R.S., Roweis, S., Slaney, M.: Collaborative \ufb01ltering\nand the missing at random assumption. In: Proceedings of the Twenty-Third\nConference on Uncertainty in Arti\ufb01cial Intelligence, pp. 267\u2013275 (2007)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1206.5267",
                        "Citation Paper Title": "Title:Collaborative Filtering and the Missing at Random Assumption",
                        "Citation Paper Abstract": "Abstract:Rating prediction is an important application, and a popular research topic in collaborative filtering. However, both the validity of learning algorithms, and the validity of standard testing procedures rest on the assumption that missing ratings are missing at random (MAR). In this paper we present the results of a user study in which we collect a random sample of ratings from current users of an online radio service. An analysis of the rating data collected in the study shows that the sample of random ratings has markedly different properties than ratings of user-selected songs. When asked to report on their own rating behaviour, a large number of users indicate they believe their opinion of a song does affect whether they choose to rate that song, a violation of the MAR condition. Finally, we present experimental results showing that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance on the random sample of ratings.",
                        "Citation Paper Authors": "Authors:Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07198v2": {
            "Paper Title": "Ultra-High Dimensional Sparse Representations with Binarization for\n  Efficient Text Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02957v2": {
            "Paper Title": "Leveraging Order-Free Tag Relations for Context-Aware Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08140v2": {
            "Paper Title": "On-the-fly Global Embeddings Using Random Projections for Extreme\n  Multi-label Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11984v1": {
            "Paper Title": "Law Smells: Defining and Detecting Problematic Patterns in Legal\n  Drafting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07581v1": {
            "Paper Title": "Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00666v2": {
            "Paper Title": "Unsupervised Document Expansion for Information Retrieval with\n  Stochastic Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09296v1": {
            "Paper Title": "Spark Deficient Gabor Frames for Inverse Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05601v2": {
            "Paper Title": "A Time-Optimized Content Creation Workflow for Remote Teaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06962v1": {
            "Paper Title": "Open-Domain Question-Answering for COVID-19 and Other Emergent Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06654v1": {
            "Paper Title": "State of Security and Privacy Practices of Top Websites in the East\n  African Community (EAC)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06637v1": {
            "Paper Title": "Knowledge Graph-enhanced Sampling for Conversational Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06483v1": {
            "Paper Title": "False Negative Distillation and Contrastive Learning for Personalized\n  Outfit Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06459v1": {
            "Paper Title": "Learning to Select Historical News Articles for Interaction based Neural\n  News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06393v1": {
            "Paper Title": "Attention-guided Generative Models for Extractive Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06125v1": {
            "Paper Title": "Embracing Structure in Data for Billion-Scale Semantic Product Search",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "proposed a data-dependent\nalgorithm for approximate KNN, which they call Neural-LSH (also\nsee ",
                    "Citation Text": "Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herv\u00e9 J\u00e9gou. 2018.\nSpreading vectors for similarity search. arXiv preprint arXiv:1806.03198 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03198",
                        "Citation Paper Title": "Title:Spreading vectors for similarity search",
                        "Citation Paper Abstract": "Abstract:Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko--Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer.",
                        "Citation Paper Authors": "Authors:Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ".\nThe above techniques work in a data-independent manner. In\ncontrast, there is an exciting line of work recently that focuses onlearned indices ",
                    "Citation Text": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (Houston, TX, USA) (SIGMOD \u201918) . ACM,\nNew York, NY, USA, 489\u2013504.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01208",
                        "Citation Paper Title": "Title:The Case for Learned Index Structures",
                        "Citation Paper Abstract": "Abstract:Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.",
                        "Citation Paper Authors": "Authors:Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.05807v1": {
            "Paper Title": "Optimizing Ranking Systems Online as Bandits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05789v1": {
            "Paper Title": "Learning Discrete Representations via Constrained Clustering for\n  Effective and Efficient Dense Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "for compression and is designed for word embedding com-\npression and recommendation systems. Most recently, Zhan et al . ",
                    "Citation Text": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma.\n2021. Jointly Optimizing Query Encoder and Product Quantization to Improve\nRetrieval Performance. In Proceedings of the 30th ACM International Conference\non Information and Knowledge Management .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00644",
                        "Citation Paper Title": "Title:Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance",
                        "Citation Paper Abstract": "Abstract:Recently, Information Retrieval community has witnessed fast-paced advances in Dense Retrieval (DR), which performs first-stage retrieval with embedding-based search. Despite the impressive ranking performance, previous studies usually adopt brute-force search to acquire candidates, which is prohibitive in practical Web search scenarios due to its tremendous memory usage and time cost. To overcome these problems, vector compression methods have been adopted in many practical embedding-based retrieval applications. One of the most popular methods is Product Quantization (PQ). However, although existing vector compression methods including PQ can help improve the efficiency of DR, they incur severely decayed retrieval performance due to the separation between encoding and compression. To tackle this problem, we present JPQ, which stands for Joint optimization of query encoding and Product Quantization. It trains the query encoder and PQ index jointly in an end-to-end manner based on three optimization strategies, namely ranking-oriented loss, PQ centroid optimization, and end-to-end negative sampling. We evaluate JPQ on two publicly available retrieval benchmarks. Experimental results show that JPQ significantly outperforms popular vector compression methods. Compared with previous DR models that use brute-force search, JPQ almost matches the best retrieval performance with 30x compression on index size. The compressed index further brings 10x speedup on CPU and 2x speedup on GPU in query latency.",
                        "Citation Paper Authors": "Authors:Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, Shaoping Ma"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", both of which are also based on PQ. We re-implement DPQ\nsince it is originally designed for word embedding compression ",
                    "Citation Text": "Ting Chen, Lala Li, and Yizhou Sun. 2020. Differentiable product quantization\nfor end-to-end embedding compression. In International Conference on Machine\nLearning . PMLR, 1617\u20131626.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.09756",
                        "Citation Paper Title": "Title:Differentiable Product Quantization for End-to-End Embedding Compression",
                        "Citation Paper Abstract": "Abstract:Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238$\\times$) at negligible or no performance cost on 10 datasets across three different language tasks.",
                        "Citation Paper Authors": "Authors:Ting Chen, Lala Li, Yizhou Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.05409v1": {
            "Paper Title": "Breaking the Softmax Bottleneck for Sequential Recommender Systems with\n  Dropout and Decoupling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05295v1": {
            "Paper Title": "AskMe: Joint Individual-level and Community-level Behavior Interaction\n  for Question Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05056v1": {
            "Paper Title": "Controllable Recommenders using Deep Generative Models and\n  Disentanglement",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ": they\nwere found to be very sensitive to hyperparameters, reliably learn-\ning unsupervised disentangled representations is a very challenging\ntask.There have been several works that use disentanglement in rec-\nommendation: Ma et al . ",
                    "Citation Text": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-\ning disentangled representations for recommendation. In Advances in Neural\nInformation Processing Systems . 5712\u20135723.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14238",
                        "Citation Paper Title": "Title:Learning Disentangled Representations for Recommendation",
                        "Citation Paper Abstract": "Abstract:User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",
                        "Citation Paper Authors": "Authors:Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "disentangle diverse user-intents using graph\nbased collaborative filtering; Cui et al . ",
                    "Citation Text": "Zeyu Cui, Feng Yu, Shu Wu, Qiang Liu, and Liang Wang. 2020. Disentangled\nItem Representation for Recommender Systems. arXiv preprint arXiv:2008.07178\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07178",
                        "Citation Paper Title": "Title:Disentangled Item Representation for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Item representations in recommendation systems are expected to reveal the properties of items. Collaborative recommender methods usually represent an item as one single latent vector. Nowadays the e-commercial platforms provide various kinds of attribute information for items (e.g., category, price and style of clothing). Utilizing these attribute information for better item representations is popular in recent years. Some studies use the given attribute information as side information, which is concatenated with the item latent vector to augment representations. However, the mixed item representations fail to fully exploit the rich attribute information or provide explanation in recommender systems. To this end, we propose a fine-grained Disentangled Item Representation (DIR) for recommender systems in this paper, where the items are represented as several separated attribute vectors instead of a single latent vector. In this way, the items are represented at the attribute level, which can provide fine-grained information of items in recommendation. We introduce a learning strategy, LearnDIR, which can allocate the corresponding attribute vectors to items. We show how DIR can be applied to two typical models, Matrix Factorization (MF) and Recurrent Neural Network (RNN). Experimental results on two real-world datasets show that the models developed under the framework of DIR are effective and efficient. Even using fewer parameters, the proposed model can outperform the state-of-the-art methods, especially in the cold-start situation. In addition, we make visualizations to show that our proposition can provide explanation for users in real-world applications.",
                        "Citation Paper Authors": "Authors:Zeyu Cui, Feng Yu, Shu Wu, Qiang Liu, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "apply disentanglement to the sequential recommendation task,\nwhile Wang et al . ",
                    "Citation Text": "Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.\n2020. Disentangled Graph Collaborative Filtering. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval . 1001\u20131010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.01764",
                        "Citation Paper Title": "Title:Disentangled Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning informative representations of users and items from the interaction data is of crucial importance to collaborative filtering (CF). Present embedding functions exploit user-item relationships to enrich the representations, evolving from a single user-item instance to the holistic interaction graph. Nevertheless, they largely model the relationships in a uniform manner, while neglecting the diversity of user intents on adopting the items, which could be to pass time, for interest, or shopping for others like families. Such uniform approach to model user interests easily results in suboptimal representations, failing to model diverse relationships and disentangle user intents in representations.\nIn this work, we pay special attention to user-item relationships at the finer granularity of user intents. We hence devise a new model, Disentangled Graph Collaborative Filtering (DGCF), to disentangle these factors and yield disentangled representations. Specifically, by modeling a distribution over intents for each user-item interaction, we iteratively refine the intent-aware interaction graphs and representations. Meanwhile, we encourage independence of different intents. This leads to disentangled representations, effectively distilling information pertinent to each intent. We conduct extensive experiments on three benchmark datasets, and DGCF achieves significant improvements over several state-of-the-art models like NGCF, DisenGCN, and MacridVAE. Further analyses offer insights into the advantages of DGCF on the disentanglement of user intents and interpretability of representations. Our codes are available in this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "propose the state-of-the-art RecVAE. Deep\nRecommender systems however are (typically) black-box models\nwhich are difficult to interpret, compared to content-based methods ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1\u201338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". The latter uses\nthe Variational Autoencoder framework [ 5,17] for recommenda-\ntion. Shenbin et al . ",
                    "Citation Text": "Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I\nNikolenko. 2020. RecVAE: A New Variational Autoencoder for Top-N Recommen-\ndations with Implicit Feedback. In Proceedings of the 13th International Conference\non Web Search and Data Mining . 528\u2013536.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11160",
                        "Citation Paper Title": "Title:RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Recent research has shown the advantages of using autoencoders based on deep neural networks for collaborative filtering. In particular, the recently proposed Mult-VAE model, which used the multinomial likelihood variational autoencoders, has shown excellent results for top-N recommendations. In this work, we propose the Recommender VAE (RecVAE) model that originates from our research on regularization techniques for variational autoencoders. RecVAE introduces several novel ideas to improve Mult-VAE, including a novel composite prior distribution for the latent codes, a new approach to setting the $\\beta$ hyperparameter for the $\\beta$-VAE framework, and a new approach to training based on alternating updates. In experimental evaluation, we show that RecVAE significantly outperforms previously proposed autoencoder-based models, including Mult-VAE and RaCT, across classical collaborative filtering datasets, and present a detailed ablation study to assess our new developments. Code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, Sergey I. Nikolenko"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.00826v2": {
            "Paper Title": "WhatTheWikiFact: Fact-Checking Claims Against Wikipedia",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": ", which\nhad an accuracy of 68.21, and a FEVER score of 64.21. They are\nalso on par with the best system from the builder phase of the\nFEVER2.0 shared task ",
                    "Citation Text": "James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos,\nand Arpit Mittal. 2019. The FEVER2.0 Shared Task. In Proceedings of the Sec-\nond Workshop on Fact Extraction and VERification (FEVER \u201919) . Association for\nComputational Linguistics, Hong Kong, China, 1\u20136.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.10971",
                        "Citation Paper Title": "Title:The Fact Extraction and VERification (FEVER) Shared Task",
                        "Citation Paper Abstract": "Abstract:We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be Supported or Refuted using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.",
                        "Citation Paper Authors": "Authors:James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, Arpit Mittal"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", which analyzes news articles\nand media outlets and predicts factuality of reporting, degree of\npropaganda, hyper-partisanship, political bias, framing, and stance\nwith respect to various claims and topics, and FAKTA ",
                    "Citation Text": "Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, and James Glass. 2019.\nFAKTA: An Automatic End-to-End Fact Checking System. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics (Demonstrations) (NAACL-HLT \u201919) . Association for Computational\nLinguistics, Minneapolis, Minnesota, 78\u201383.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04164",
                        "Citation Paper Title": "Title:FAKTA: An Automatic End-to-End Fact Checking System",
                        "Citation Paper Abstract": "Abstract:We present FAKTA which is a unified framework that integrates various components of a fact checking process: document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions",
                        "Citation Paper Authors": "Authors:Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "were used for NLI. Here, we adopt a similar overall architecture.\nSystem Demonstrations. Relevant demos include Hoaxy ",
                    "Citation Text": "Chengcheng Shao, Giovanni Luca Ciampaglia, Alessandro Flammini, and Filippo\nMenczer. 2016. Hoaxy: A Platform for Tracking Online Misinformation. In\nProceedings of the 25th International Conference Companion on World Wide Web\n(WWW \u201916 Companion) . International World Wide Web Conferences Steering\nCommittee, Montr\u00e9al, Qu\u00e9bec, Canada, 745\u2013750.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01511",
                        "Citation Paper Title": "Title:Hoaxy: A Platform for Tracking Online Misinformation",
                        "Citation Paper Abstract": "Abstract:Massive amounts of misinformation have been observed to spread in uncontrolled fashion across social media. Examples include rumors, hoaxes, fake news, and conspiracy theories. At the same time, several journalistic organizations devote significant efforts to high-quality fact checking of online claims. The resulting information cascades contain instances of both accurate and inaccurate information, unfold over multiple time scales, and often reach audiences of considerable size. All these factors pose challenges for the study of the social dynamics of online news sharing. Here we introduce Hoaxy, a platform for the collection, detection, and analysis of online misinformation and its related fact-checking efforts. We discuss the design of the platform and present a preliminary analysis of a sample of public tweets containing both fake news and fact checking. We find that, in the aggregate, the sharing of fact-checking content typically lags that of misinformation by 10--20 hours. Moreover, fake news are dominated by very active users, while fact checking is a more grass-roots activity. With the increasing risks connected to massive online misinformation, social news observatories have the potential to help researchers, journalists, and the general public understand the dynamics of real and fake news sharing.",
                        "Citation Paper Authors": "Authors:Chengcheng Shao, Giovanni Luca Ciampaglia, Alessandro Flammini, Filippo Menczer"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "developed two datasets for detecting previously fact-\nchecked claims, which were extended and used for shared tasks as\npart of the CLEF CheckThat! lab in 2020 and 2021 [ 5,6,24,40,41,\n56,57]. Chen et al . ",
                    "Citation Text": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang\nLi, Xiyou Zhou, and William Yang Wang. 2020. TabFact: A Large-scale Dataset for\nTable-based Fact Verification. In Proceedings of the 8th International Conference\non Learning Representations (ICLR \u201920) . OpenReview.net, Addis Ababa, Ethiopia.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02164",
                        "Citation Paper Title": "Title:TabFact: A Large-scale Dataset for Table-based Fact Verification",
                        "Citation Paper Abstract": "Abstract:The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "presented the LIAR\ndataset, which focuses on fact-checking using only the input claim\n(its text and metadata). Lee et al . ",
                    "Citation Text": "Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021. Towards\nFew-shot Fact-Checking via Perplexity. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT \u201921) . Association for Computational\nLinguistics, Online, 1971\u20131981.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.09535",
                        "Citation Paper Title": "Title:Towards Few-Shot Fact-Checking via Perplexity",
                        "Citation Paper Abstract": "Abstract:Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.",
                        "Citation Paper Authors": "Authors:Nayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, Pascale Fung"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ", on predicting the factuality and the bias of entire\nnews outlets ",
                    "Citation Text": "Preslav Nakov, Husrev Taha Sencar, Jisun An, and Haewoon Kwak. 2021. A Survey\non Predicting the Factuality and the Bias of News Media. arXiv/2103.12506 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12506",
                        "Citation Paper Title": "Title:A Survey on Predicting the Factuality and the Bias of News Media",
                        "Citation Paper Abstract": "Abstract:The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim or article, either manually or automatically. Thus, many researchers are shifting their attention to higher granularity, aiming to profile entire news outlets, which makes it possible to detect likely \"fake news\" the moment it is published, by simply checking the reliability of its source. Source factuality is also an important element of systems for automatic fact-checking and \"fake news\" detection, as they need to assess the reliability of the evidence they retrieve online. Political bias detection, which in the Western political landscape is about predicting left-center-right bias, is an equally important topic, which has experienced a similar shift towards profiling entire news outlets. Moreover, there is a clear connection between the two, as highly biased media are less likely to be factual; yet, the two problems have been addressed separately. In this survey, we review the state of the art on media profiling for factuality and bias, arguing for the need to model them jointly. We further discuss interesting recent advances in using different information sources and modalities, which go beyond the text of the articles the target news outlet has published. Finally, we discuss current challenges and outline future research directions.",
                        "Citation Paper Authors": "Authors:Preslav Nakov, Husrev Taha Sencar, Jisun An, Haewoon Kwak"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". Some very recent\nsurveys focused on stance for misinformation and disinformation\n1The WhatTheWikiFact system is running online at\nhttps://www.tanbih.org/whatthewikifactarXiv:2105.00826v2  [cs.CL]  10 Oct 2021detection ",
                    "Citation Text": "Momchil Hardalov, Arnav Arora, Preslav Nakov, and Isabelle Augenstein. 2021.\nA Survey on Stance Detection for Mis- and Disinformation Identification.\narXiv/2103.00242 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00242",
                        "Citation Paper Title": "Title:A Survey on Stance Detection for Mis- and Disinformation Identification",
                        "Citation Paper Abstract": "Abstract:Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.",
                        "Citation Paper Authors": "Authors:Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "took a fact-checking\nperspective on \u201cfake news\u201d and related problems.\nLi et al . ",
                    "Citation Text": "Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su, Bo Zhao, Wei Fan, and Jiawei\nHan. 2016. A Survey on Truth Discovery. SIGKDD Explor. Newsl. 17, 2 (2016),\n1\u201316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.02463",
                        "Citation Paper Title": "Title:A Survey on Truth Discovery",
                        "Citation Paper Abstract": "Abstract:Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.",
                        "Citation Paper Authors": "Authors:Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su, Bo Zhao, Wei Fan, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": ", who\nadopted a data mining perspective on \u201cfake news\u201d and focused on\nsocial media. Another survey ",
                    "Citation Text": "Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and Rob Procter.\n2018. Detection and Resolution of Rumours in Social Media: A Survey. ACM\nComput. Surv. 51, 2, Article 32 (2018), 36 pages.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00656",
                        "Citation Paper Title": "Title:Detection and Resolution of Rumours in Social Media: A Survey",
                        "Citation Paper Abstract": "Abstract:Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e. pieces of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how natural language processing and data mining techniques may be used to find ways of determining their veracity. In this survey we introduce and discuss two types of rumours that circulate on social media; long-standing rumours that circulate for long periods of time, and newly-emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far towards the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for detection and resolution of rumours.",
                        "Citation Paper Authors": "Authors:Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, Rob Procter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06133v1": {
            "Paper Title": "Hotel Preference Rank based on Online Customer Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04640v1": {
            "Paper Title": "Lookup or Exploratory: What is Your Search Intent?",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "help capture general\nlanguage semantics and may show promising results in modeling\nthe user intent effectively. The GEN Encoder ",
                    "Citation Text": "Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N. Bennett, Nick\nCraswell, and Saurabh Tiwary. 2019. Generic Intent Representation in Web\nSearch. Proceedings of the 42nd International ACM SIGIR Conference on Research\nand Development in Information Retrieval (Jul 2019). https://doi.org/10.1145/\n3331184.3331198",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10710",
                        "Citation Paper Title": "Title:Generic Intent Representation in Web Search",
                        "Citation Paper Abstract": "Abstract:This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a distributed representation space for user intent in search. Leveraging large scale user clicks from Bing search logs as weak supervision of user intent, GEN Encoder learns to map queries with shared clicks into similar embeddings end-to-end and then finetunes on multiple paraphrase tasks. Experimental results on an intrinsic evaluation task - query intent similarity modeling - demonstrate GEN Encoder's robust and significant advantages over previous representation methods. Ablation studies reveal the crucial role of learning from implicit user feedback in representing user intent and the contributions of multi-task learning in representation generality. We also demonstrate that GEN Encoder alleviates the sparsity of tail search traffic and cuts down half of the unseen queries by using an efficient approximate nearest neighbor search to effectively identify previous queries with the same search intent. Finally, we demonstrate distances between GEN encodings reflect certain information seeking behaviors in search sessions.",
                        "Citation Paper Authors": "Authors:Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N. Bennett, Nick Craswell, Saurabh Tiwary"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", which demonstrate the\nability of Transformers to perform a wide range of NLP-related\ntasks ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. HuggingFace\u2019s Transformers: State-of-the-art\nNatural Language Processing. arXiv:1910.03771 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.00949v3": {
            "Paper Title": "Unsupervised paradigm for information extraction from transcripts using\n  BERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04039v1": {
            "Paper Title": "Global Context Enhanced Social Recommendation with Hierarchical Graph\n  Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ", another research line seeks to capture the user-item\nrelationships with graph neural networks ",
                    "Citation Text": "X. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua. Neural graph\ncollaborative \ufb01ltering. In SIGIR , pages 165\u2013174, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06022v1": {
            "Paper Title": "Smart Crawling: A New Approach toward Focus Crawling from Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04000v1": {
            "Paper Title": "Knowledge-Enhanced Hierarchical Graph Transformer Network for\n  Multi-Behavior Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03996v1": {
            "Paper Title": "Graph-Enhanced Multi-Task Learning of Multi-Level Transition Dynamics\n  for Session-based Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03987v1": {
            "Paper Title": "Knowledge-aware Coupled Graph Neural Network for Social Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00818v3": {
            "Paper Title": "Adherence and Constancy in LIME-RS Explanations for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "have shown how the explanations gen-\nerated with LIME are not very robust: their contribution\naims to bring out how small variations or perturbations\nin the input data cause significant variations in the expla-\nnation of that specific input ",
                    "Citation Text": "D. Alvarez-Melis, T. S. Jaakkola, On the robustness of in-\nterpretability methods, CoRR abs/1806.08049 (2018). URL:\nhttp://arxiv.org/abs/1806.08049. arXiv:1806.08049 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08049",
                        "Citation Paper Title": "Title:On the Robustness of Interpretability Methods",
                        "Citation Paper Abstract": "Abstract:We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.",
                        "Citation Paper Authors": "Authors:David Alvarez-Melis, Tommi S. Jaakkola"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "try a deep\nmodel based on attention mechanisms to make relevant\nfeatures emerge. Along the same lines are Pan et al. ",
                    "Citation Text": "D. Pan, X. Li, X. Li, D. Zhu, Explainable recommen-\ndation via interpretable feature mapping and evalua-\ntion of explainability, in: C. Bessiere (Ed.), Proceed-\nings of the Twenty-Ninth International Joint Conference\non Artificial Intelligence, IJCAI 2020, ijcai.org, 2020, pp.\n2690\u20132696. URL: https://doi.org/10.24963/ijcai.2020/373.\ndoi:10.24963/ijcai.2020/373 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06133",
                        "Citation Paper Title": "Title:Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability",
                        "Citation Paper Abstract": "Abstract:Latent factor collaborative filtering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory accuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate the explainability, we propose two new evaluation metrics specifically designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from this https URL.",
                        "Citation Paper Authors": "Authors:Deng Pan, Xiangrui Li, Xin Li, Dongxiao Zhu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "generate an explanation by exploiting the association\nrules between features; Tao et al. ",
                    "Citation Text": "Y. Tao, Y. Jia, N. Wang, H. Wang, The fact: Taming\nlatent factor models for explainability with factoriza-\ntion trees, in: B. Piwowarski, M. Chevalier, \u00c9. Gaussier,\nY. Maarek, J. Nie, F. Scholer (Eds.), Proceedings of\nthe 42nd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval, SI-\nGIR 2019, Paris, France, July 21-25, 2019, ACM, 2019, pp.\n295\u2013304. URL: https://doi.org/10.1145/3331184.3331244.\ndoi:10.1145/3331184.3331244 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02037",
                        "Citation Paper Title": "Title:The FacT: Taming Latent Factor Models for Explainability with Factorization Trees",
                        "Citation Paper Abstract": "Abstract:Latent factor models have achieved great success in personalized recommendations, but they are also notoriously difficult to explain. In this work, we integrate regression trees to guide the learning of latent factor models for recommendation, and use the learnt tree structure to explain the resulting latent factors. Specifically, we build regression trees on users and items respectively with user-generated reviews, and associate a latent profile to each node on the trees to represent users and items. With the growth of regression tree, the latent factors are gradually refined under the regularization imposed by the tree structure. As a result, we are able to track the creation of latent profiles by looking into the path of each factor on regression trees, which thus serves as an explanation for the resulting recommendations. Extensive experiments on two large collections of Amazon and Yelp reviews demonstrate the advantage of our model over several competitive baseline algorithms. Besides, our extensive user study also confirms the practical value of explainable recommendations generated by our model.",
                        "Citation Paper Authors": "Authors:Yiyi Tao, Yiling Jia, Nan Wang, Hongning Wang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The growing interest\nin this area is also dictated by new regulations of both\nEurope ",
                    "Citation Text": "S. Wachter, B. Mittelstadt, C. Russell, Counterfactual\nexplanations without opening the black box: Automated\ndecisions and the gdpr, Harv. JL & Tech. 31 (2017) 841.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00399",
                        "Citation Paper Title": "Title:Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR",
                        "Citation Paper Abstract": "Abstract:There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.",
                        "Citation Paper Authors": "Authors:Sandra Wachter, Brent Mittelstadt, Chris Russell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.03933v1": {
            "Paper Title": "Towards Creating a Standardized Collection of Simple and Targeted\n  Experiments to Analyze Core Aspects of the Recommender Systems Problem",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "3.\n(3)Analysis :\u201ca fixed procedure that maps agent behaviour to results and plots\u201d ",
                    "Citation Text": "Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder\nSingh, Benjamin Van Roy, Richard Sutton, David Silver, and Hado Van Hasselt. 2020. Behaviour Suite for Reinforcement Learning. In International\nConference on Learning Representations . https://openreview.net/forum?id=rygf-kSYwH",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03568",
                        "Citation Paper Title": "Title:Behaviour Suite for Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",
                        "Citation Paper Authors": "Authors:Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, Hado Van Hasselt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.16940v2": {
            "Paper Title": "Learning with Memory-based Virtual Classes for Deep Metric Learning",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ". The performance can be further improvedCUB200 CARS196 SOP\nMethod P@1 RP MAP@R P@1 RP MAP@R P@1 RP MAP@R\nNorm-softmax ",
                    "Citation Text": "Feng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon\nYuille. Normface: L2 hypersphere embedding for face veri-\n\ufb01cation. In Proceedings of the 25th ACM international con-\nference on Multimedia , pages 1041\u20131049, 2017. 1, 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06369",
                        "Citation Paper Title": "Title:NormFace: L2 Hypersphere Embedding for Face Verification",
                        "Citation Paper Abstract": "Abstract:Thanks to the recent developments of Convolutional Neural Networks, the performance of face verification methods has increased rapidly. In a typical face verification method, feature normalization is a critical step for boosting performance. This motivates us to introduce and study the effect of normalization during training. But we find this is non-trivial, despite normalization being differentiable. We identify and study four issues related to normalization through mathematical analysis, which yields understanding and helps with parameter settings. Based on this analysis we propose two strategies for training using normalized features. The first is a modification of softmax loss, which optimizes cosine similarity instead of inner-product. The second is a reformulation of metric learning by introducing an agent vector for each class. We show that both strategies, and small variants, consistently improve performance by between 0.2% to 0.4% on the LFW dataset based on two models. This is significant because the performance of the two models on LFW dataset is close to saturation at over 98%. Codes and models are released on this https URL",
                        "Citation Paper Authors": "Authors:Feng Wang, Xiang Xiang, Jian Cheng, Alan L. Yuille"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "generate synthetic samples by ge-\nometric relations. Meanwhile, utilizing information from\nprevious steps has been explored in many computer vision\ntasks [15, 5, 45, 22]. In supervised DML, XBM ",
                    "Citation Text": "Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R\nScott. Cross-batch memory for embedding learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 6388\u20136397, 2020. 1, 2, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06798",
                        "Citation Paper Title": "Title:Cross-Batch Memory for Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Mining informative negative instances are of central importance to deep metric learning (DML), however this task is intrinsically limited by mini-batch training, where only a mini-batch of instances is accessible at each iteration. In this paper, we identify a \"slow drift\" phenomena by observing that the embedding features drift exceptionally slow even as the model parameters are updating throughout the training process. This suggests that the features of instances computed at preceding iterations can be used to considerably approximate their features extracted by the current model. We propose a cross-batch memory (XBM) mechanism that memorizes the embeddings of past iterations, allowing the model to collect sufficient hard negative pairs across multiple mini-batches - even over the whole dataset. Our XBM can be directly integrated into a general pair-based DML framework, where the XBM augmented DML can boost performance considerably. In particular, without bells and whistles, a simple contrastive loss with our XBM can have large R@1 improvements of 12%-22.5% on three large-scale image retrieval datasets, surpassing the most sophisticated state-of-the-art methods, by a large margin. Our XBM is conceptually simple, easy to implement - using several lines of codes, and is memory efficient - with a negligible 0.2 GB extra GPU memory. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.03902v1": {
            "Paper Title": "Multi-trends Enhanced Dynamic Micro-video Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03567v1": {
            "Paper Title": "GeSERA: General-domain Summary Evaluation by Relevance Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.02069v2": {
            "Paper Title": "OPAD: An Optimized Policy-based Active Learning Framework for Document\n  Content Analysis",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "dataset for the object detec-\ntion task to bootstrap the active learning experiments. For the NER\ntask, we use the BiLSTM-CRF ",
                    "Citation Text": "Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for\nsequence tagging. arXiv preprint arXiv:1508.01991 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01991",
                        "Citation Paper Title": "Title:Bidirectional LSTM-CRF Models for Sequence Tagging",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
                        "Citation Paper Authors": "Authors:Zhiheng Huang, Wei Xu, Kai Yu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "proposed an imitation learning approach\nfor active learning in tasks related to natural language processing,\nrelying on an algorithmic expert to find an optimal acquisition\nfunction. We differ from the work of Casanova et al ",
                    "Citation Text": "Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, and Christopher J. Pal.\n2020. Reinforced active learning for image segmentation. In International Confer-\nence on Learning Representations . https://openreview.net/forum?id=SkgC6TNFvr",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06583",
                        "Citation Paper Title": "Title:Reinforced active learning for image segmentation",
                        "Citation Paper Abstract": "Abstract:Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.",
                        "Citation Paper Authors": "Authors:Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J. Pal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.03068v1": {
            "Paper Title": "Learning the Optimal Recommendation from Explorative Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03039v1": {
            "Paper Title": "Optimized Recommender Systems with Deep Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "reported 35% in 30000 time-steps, using a Trust Region Policy Optimization\n(TRPO) algorithm. They experimented with Q-Learning but found TRPO to wield better\nresults.\nThe 2020 study by Liu et al. ",
                    "Citation Text": "Feng Liu, Ruiming Tang, Huifeng Guo, Xutao Li, Yunming Ye, and Xiuqiang He. Top-\naware reinforcement learning based recommendation. Neurocomputing , 417:255{269, De-\ncember 2020. ISSN 0925-2312. doi: 10.1016/j.neucom.2020.07.057. URL https://www.\nsciencedirect.com/science/article/pii/S0925231220311656 . Accessed on 2021-03-\n09.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.12027",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling",
                        "Citation Paper Abstract": "Abstract:Recommendation is crucial in both academia and industry, and various techniques are proposed such as content-based collaborative filtering, matrix factorization, logistic regression, factorization machines, neural networks and multi-armed bandits. However, most of the previous studies suffer from two limitations: (1) considering the recommendation as a static procedure and ignoring the dynamic interactive nature between users and the recommender systems, (2) focusing on the immediate feedback of recommended items and neglecting the long-term rewards. To address the two limitations, in this paper we propose a novel recommendation framework based on deep reinforcement learning, called DRR. The DRR framework treats recommendation as a sequential decision making procedure and adopts an \"Actor-Critic\" reinforcement learning scheme to model the interactions between the users and recommender systems, which can consider both the dynamic adaptation and long-term rewards. Furthermore, a state representation module is incorporated into DRR, which can explicitly capture the interactions between items and users. Three instantiation structures are developed. Extensive experiments on four real-world datasets are conducted under both the offline and online evaluation settings. The experimental results demonstrate the proposed DRR method indeed outperforms the state-of-the-art competitors.",
                        "Citation Paper Authors": "Authors:Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, Yuzhou Zhang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". We will therefore explore research that was\ndone in designing gym environments that speci\fcally simulate the dynamics of recommendation\nsystems.\nWe start by reviewing Google Research 's 2019 RecSim by Ie et al. ",
                    "Citation Text": "Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui\nWu, and Craig Boutilier. RecSim: A Con\fgurable Simulation Platform for Recommender\nSystems. arXiv:1909.04847 [cs, stat] , September 2019. URL http://arxiv.org/abs/\n1909.04847 . arXiv: 1909.04847. Accessed on 2021-03-17.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04847",
                        "Citation Paper Title": "Title:RecSim: A Configurable Simulation Platform for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:We propose RecSim, a configurable platform for authoring simulation environments for recommender systems (RSs) that naturally supports sequential interaction with users. RecSim allows the creation of new environments that reflect particular aspects of user behavior and item structure at a level of abstraction well-suited to pushing the limits of current reinforcement learning (RL) and RS techniques in sequential interactive recommendation problems. Environments can be easily configured that vary assumptions about: user preferences and item familiarity; user latent state and its dynamics; and choice models and other user response behavior. We outline how RecSim offers value to RL and RS researchers and practitioners, and how it can serve as a vehicle for academic-industrial collaboration.",
                        "Citation Paper Authors": "Authors:Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, Craig Boutilier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.03024v1": {
            "Paper Title": "A Fast Randomized Algorithm for Massive Text Normalization",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "as the word-\nembeddings and apply maximum inner-product search via the FAISS\nlibrary for searching over the high-dimensional space ",
                    "Citation Text": "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity\nsearch with GPUs. IEEE Transactions on Big Data (2019), 1\u201314.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08734",
                        "Citation Paper Title": "Title:Billion-scale similarity search with GPUs",
                        "Citation Paper Abstract": "Abstract:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\nWe propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
                        "Citation Paper Authors": "Authors:Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "with perturbed text, and data sampled from the logs\nof a large e-commerce search engine. The Twitter sentiment140\ndataset contains 1.6 million of tweets with 0.7 million distinct\nwords ",
                    "Citation Text": "Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, and Manish Singh. 2017.\nEfficient Twitter sentiment classification using subjective distant supervision. In\n9th International Conference on Communication Systems and Networks . 548\u2013553.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.03051",
                        "Citation Paper Title": "Title:Efficient Twitter Sentiment Classification using Subjective Distant Supervision",
                        "Citation Paper Abstract": "Abstract:As microblogging services like Twitter are becoming more and more influential in today's globalised world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analysing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model.",
                        "Citation Paper Authors": "Authors:Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, Manish Singh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.07045v1": {
            "Paper Title": "Investigating Health-Aware Smart-Nudging with Machine Learning to Help\n  People Pursue Healthier Eating-Habits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01871v1": {
            "Paper Title": "Emerging trends and collaboration patterns unveil the scientific\n  production in blockchain technology: A bibliometric and network analysis from\n  2014-2020",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01788v1": {
            "Paper Title": "Voice Information Retrieval In Collaborative Information Seeking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01754v1": {
            "Paper Title": "An Integrated System for Mobile Image-Based Dietary Assessment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12622v2": {
            "Paper Title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01599v1": {
            "Paper Title": "Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question\n  Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13626v2": {
            "Paper Title": "Neural Named Entity Recognition for Kazakh",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.02065v1": {
            "Paper Title": "SDR: Efficient Neural Re-ranking using Succinct Document Representation",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "Late-interaction models. The idea of running several transformer\nlayers for the document and the query independently, and then com-\nbining them in the last transformer layers, was developed concur-\nrently by multiple teams: PreTTR ",
                    "Citation Text": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto,\nNazli Goharian, and Ophir Frieder. 2020. Efficient Document Re-Ranking for\nTransformers by Precomputing Term Representations. In Proceedings of the\n43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 49\u201358.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14255",
                        "Citation Paper Title": "Title:Efficient Document Re-Ranking for Transformers by Precomputing Term Representations",
                        "Citation Paper Abstract": "Abstract:Deep pretrained transformer networks are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their computational expenses deem them cost-prohibitive in practice. Our proposed approach, called PreTTR (Precomputing Transformer Term Representations), considerably reduces the query-time latency of deep transformer networks (up to a 42x speedup on web document ranking) making these networks more practical to use in a real-time ranking scenario. Specifically, we precompute part of the document term representations at indexing time (without a query), and merge them with the query representation at query time to compute the final ranking score. Due to the large size of the token representations, we also propose an effective approach to reduce the storage requirement by training a compression layer to match attention scores. Our compression technique reduces the storage required up to 95% and it can be applied without a substantial degradation in ranking performance.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, Ophir Frieder"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "is\nanother variant that runs all transformer layers independently for\nthe query and the document, and the interaction between the final\nvectors is done through a sum-of-max operator. In a similar line of\nwork, the Transformer-Kernel (TK) ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable &\nTime-Budget-Constrained Contextualization for Re-Ranking.. In ECAI . 513\u2013520.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.01854",
                        "Citation Paper Title": "Title:Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking",
                        "Citation Paper Abstract": "Abstract:Search engines operate under a strict time constraint as a fast response is paramount to user satisfaction. Thus, neural re-ranking models have a limited time-budget to re-rank documents. Given the same amount of time, a faster re-ranking model can incorporate more documents than a less efficient one, leading to a higher effectiveness. To utilize this property, we propose TK (Transformer-Kernel): a neural re-ranking model for ad-hoc search using an efficient contextualization mechanism. TK employs a very small number of Transformer layers (up to three) to contextualize query and document word embeddings. To score individual term interactions, we use a document-length enhanced kernel-pooling, which enables users to gain insight into the model. TK offers an optimal ratio between effectiveness and efficiency: under realistic time constraints (max. 200 ms per query) TK achieves the highest effectiveness in comparison to BERT and other re-ranking models. We demonstrate this on three large-scale ranking collections: MSMARCO-Passage, MSMARCO-Document, and TREC CAR. In addition, to gain insight into TK, we perform a clustered query analysis of TK's results, highlighting its strengths and weaknesses on queries with different types of information need and we show how to interpret the cause of ranking differences of two documents by comparing their internal scores.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Markus Zlabinger, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "), the overall ranking quality is generally lower compared to\nmethods that employ a query-document cross-attention interaction.\nKnowledge distillation. Knowledge distillation techniques, such\nas DistilBERT ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.00759v1": {
            "Paper Title": "Person Entity Profiling Framework: Identifying, Integrating and\n  Visualizing Online Freely Available Entity-Related Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00696v1": {
            "Paper Title": "Tao: A Learning Framework for Adaptive Nearest Neighbor Search using\n  Static Features Only",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": ". As with HNSW, all of these proximity graph\nvariants employ \ufb01xed con\ufb01gurations to perform a \ufb01xed amount\nof graph traversal for all queries. Interested readers are referred\nto a recent survey for more detailed discussion ",
                    "Citation Text": "Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. A\ncomprehensive survey and experimental comparison of graph-based\napproximate nearest neighbor search. CoRR , abs/2101.12631, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.12631",
                        "Citation Paper Title": "Title:A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search",
                        "Citation Paper Abstract": "Abstract:Approximate nearest neighbor search (ANNS) constitutes an important operation in a multitude of applications, including recommendation systems, information retrieval, and pattern recognition. In the past decade, graph-based ANNS algorithms have been the leading paradigm in this domain, with dozens of graph-based ANNS algorithms proposed. Such algorithms aim to provide effective, efficient solutions for retrieving the nearest neighbors for a given query. Nevertheless, these efforts focus on developing and optimizing algorithms with different approaches, so there is a real need for a comprehensive survey about the approaches' relative performance, strengths, and pitfalls. Thus here we provide a thorough comparative analysis and experimental evaluation of 13 representative graph-based ANNS algorithms via a new taxonomy and fine-grained pipeline. We compared each algorithm in a uniform test environment on eight real-world datasets and 12 synthetic datasets with varying sizes and characteristics. Our study yields novel discoveries, offerings several useful principles to improve algorithms, thus designing an optimized method that outperforms the state-of-the-art algorithms. This effort also helped us pinpoint algorithms' working portions, along with rule-of-thumb recommendations about promising research directions and suitable algorithms for practitioners in different fields.",
                        "Citation Paper Authors": "Authors:Mengzhao Wang, Xiaoliang Xu, Qiang Yue, Yuxiang Wang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Navigating\nSpreading-out Graph (NSG) aims to reduce the graph edge\ndensity while keeping the search accuracy ",
                    "Citation Text": "Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approxi-\nmate nearest neighbor search with the navigating spreading-out graph.\nPVLDB , 12(5):461\u2013474, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.00143",
                        "Citation Paper Title": "Title:Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph",
                        "Citation Paper Abstract": "Abstract:Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.",
                        "Citation Paper Authors": "Authors:Cong Fu, Chao Xiang, Changxu Wang, Deng Cai"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ". HNSW employs hierarchical network\nstructures to organized vectors, and such networks are essen-\ntially approximate kNN-graphs ",
                    "Citation Text": "Hongya Wang, Zhizheng Wang, Wei Wang, Yingyuan Xiao, Zeng Zhao,\nand Kaixiang Yang. A note on graph-based nearest neighbor search.\nCoRR , abs/2012.11083, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11083",
                        "Citation Paper Title": "Title:A Note on Graph-Based Nearest Neighbor Search",
                        "Citation Paper Abstract": "Abstract:Nearest neighbor search has found numerous applications in machine learning, data mining and massive data processing systems. The past few years have witnessed the popularity of the graph-based nearest neighbor search paradigm because of its superiority over the space-partitioning algorithms. While a lot of empirical studies demonstrate the efficiency of graph-based algorithms, not much attention has been paid to a more fundamental question: why graph-based algorithms work so well in practice? And which data property affects the efficiency and how? In this paper, we try to answer these questions. Our insight is that \"the probability that the neighbors of a point o tends to be neighbors in the KNN graph\" is a crucial data property for query efficiency. For a given dataset, such a property can be qualitatively measured by clustering coefficient of the KNN graph. To show how clustering coefficient affects the performance, we identify that, instead of the global connectivity, the local connectivity around some given query q has more direct impact on recall. Specifically, we observed that high clustering coefficient makes most of the k nearest neighbors of q sit in a maximum strongly connected component (SCC) in the graph. From the algorithmic point of view, we show that the search procedure is actually composed of two phases - the one outside the maximum SCC and the other one in it, which is different from the widely accepted single or multiple paths search models. We proved that the commonly used graph-based search algorithm is guaranteed to traverse the maximum SCC once visiting any point in it. Our analysis reveals that high clustering coefficient leads to large size of the maximum SCC, and thus provides good answer quality with the help of the two-phase search procedure. Extensive empirical results over a comprehensive collection of datasets validate our findings.",
                        "Citation Paper Authors": "Authors:Hongya Wang, Zhizheng Wang, Wei Wang, Yingyuan Xiao, Zeng Zhao, Kaixiang Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.15188v1": {
            "Paper Title": "Assessing Algorithmic Biases for Musical Version Identification",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "Aaron Swartz. 2002. Musicbrainz: a semantic web service. IEEE Intelligent Systems\n17, 1 (2002), 76\u201377. ",
                    "Citation Text": "Furkan Yesiler, Joan Serr\u00e0, and Emilia G\u00f3mez. 2020. Accurate and scalable version\nidentification using musically-motivated embeddings. In Proc. of the IEEE Int.\nConf. on Acoustics, Speech and Signal Processing (ICASSP) . 21\u201325.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.12551",
                        "Citation Paper Title": "Title:Accurate and Scalable Version Identification Using Musically-Motivated Embeddings",
                        "Citation Paper Abstract": "Abstract:The version identification (VI) task deals with the automatic detection of recordings that correspond to the same underlying musical piece. Despite many efforts, VI is still an open problem, with much room for improvement, specially with regard to combining accuracy and scalability. In this paper, we present MOVE, a musically-motivated method for accurate and scalable version identification. MOVE achieves state-of-the-art performance on two publicly-available benchmark sets by learning scalable embeddings in an Euclidean distance space, using a triplet loss and a hard triplet mining strategy. It improves over previous work by employing an alternative input representation, and introducing a novel technique for temporal content summarization, a standardized latent space, and a data augmentation strategy specifically designed for VI. In addition to the main results, we perform an ablation study to highlight the importance of our design choices, and study the relation between embedding dimensionality and model performance.",
                        "Citation Paper Authors": "Authors:Furkan Yesiler, Joan Serr\u00e0, Emilia G\u00f3mez"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Joan Serr\u00e0, Xavier Serra, and Ralph G. Andrzejak. 2009. Cross recurrence quan-\ntification for cover song identification. New Journal of Physics 11 (2009), 093017. ",
                    "Citation Text": "Dougal Shakespeare, Lorenzo Porcaro, Emilia G\u00f3mez, and Carlos Castillo. 2020.\nExploring artist gender bias in music recommendation. In Proc. of the ImpactRS\nWorkshop at ACM RecSys .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.01715",
                        "Citation Paper Title": "Title:Exploring Artist Gender Bias in Music Recommendation",
                        "Citation Paper Abstract": "Abstract:Music Recommender Systems (mRS) are designed to give personalised and meaningful recommendations of items (i.e. songs, playlists or artists) to a user base, thereby reflecting and further complementing individual users' specific music preferences. Whilst accuracy metrics have been widely applied to evaluate recommendations in mRS literature, evaluating a user's item utility from other impact-oriented perspectives, including their potential for discrimination, is still a novel evaluation practice in the music domain. In this work, we center our attention on a specific phenomenon for which we want to estimate if mRS may exacerbate its impact: gender bias. Our work presents an exploratory study, analyzing the extent to which commonly deployed state of the art Collaborative Filtering(CF) algorithms may act to further increase or decrease artist gender bias. To assess group biases introduced by CF, we deploy a recently proposed metric of bias disparity on two listening event datasets: the LFM-1b dataset, and the earlier constructed Celma's dataset. Our work traces the causes of disparity to variations in input gender distributions and user-item preferences, highlighting the effect such configurations can have on user's gender bias after recommendation generation.",
                        "Citation Paper Authors": "Authors:Dougal Shakespeare, Lorenzo Porcaro, Emilia G\u00f3mez, Carlos Castillo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.01345v3": {
            "Paper Title": "Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in\n  Shared Representation Learning",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "2 48.2 75.8 83.6 1.9 48.4 76.1 83.7 7.2 20.3 43.3 54.4 6.6 21.4 44.3 55.2\n\u2020SCAN ",
                    "Citation Text": "Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp, Ee\npeng Lim, and Steven C. H. Hoi. 2020. Cross-Modal Food Retrieval: Learning a\nJoint Embedding of Food Images and Recipes with Semantic Consistency and\nAttention Mechanism. arXiv:2003.03955 [cs.CV]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.03955",
                        "Citation Paper Title": "Title:Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism",
                        "Citation Paper Abstract": "Abstract:Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention mechanism to improve the embedding of recipes. We evaluate the performance of the proposed method on the large-scale Recipe1M dataset, and show that we can outperform several state-of-the-art cross-modal retrieval strategies for food images and cooking recipes by a significant margin.",
                        "Citation Paper Authors": "Authors:Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp, Ee-peng Lim, Steven C. H. Hoi"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "1 60.2 84 89.7 - - - - 4 30 56.5 67 - - - -\n\u2020MCEN ",
                    "Citation Text": "Han Fu, Rui Wu, Chenghao Liu, and Jianling Sun. 2020. MCEN: Bridging Cross-\nModal Gap between Cooking Recipes and Dish Images with Latent Variable\nModel. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.01095",
                        "Citation Paper Title": "Title:MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model",
                        "Citation Paper Abstract": "Abstract:Nowadays, driven by the increasing concern on diet and health, food computing has attracted enormous attention from both industry and research community. One of the most popular research topics in this domain is Food Retrieval, due to its profound influence on health-oriented applications. In this paper, we focus on the task of cross-modal retrieval between food images and cooking recipes. We present Modality-Consistent Embedding Network (MCEN) that learns modality-invariant representations by projecting images and texts to the same embedding space. To capture the latent alignments between modalities, we incorporate stochastic latent variables to explicitly exploit the interactions between textual and visual features. Importantly, our method learns the cross-modal alignments during training but computes embeddings of different modalities independently at inference time for the sake of efficiency. Extensive experimental results clearly demonstrate that the proposed MCEN outperforms all existing approaches on the benchmark Recipe1M dataset and requires less computational cost.",
                        "Citation Paper Authors": "Authors:Han Fu, Rui Wu, Chenghao Liu, Jianling Sun"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "5.2 24 51 65 5.1 25 52 65 41.9 - - - 39.2 - - -\nAdaMine ",
                    "Citation Text": "Micael Carvalho, R\u00e9mi Cad\u00e8ne, David Picard, Laure Soulier, Nicolas Thome, and\nMatthieu Cord. 2018. Cross-Modal Retrieval in the Cooking Context: Learning\nSemantic Text-Image Embeddings. In The 41st International ACM SIGIR Conference\non Research & Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIRFigure 3: Recipe-to-image synthesis examples. These examples are chosen such that the recipes are retrieved successfully\nusing our proposed T-ML retrieval model. In each row, the recipe and its associated real image are shown on the left hand side,\nand the next four images are generated from ACME, CHEF, (our) T and T-ML recipe embeddings, respectively.\nTable 5: Image embedding-to-image synthesis performance. Results are calculated similarly to those in Tab. 4, however, images\nare generated from visual embeddings instead of recipe embeddings. \u2020indicates values taken directly from",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.11146",
                        "Citation Paper Title": "Title:Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings",
                        "Citation Paper Abstract": "Abstract:Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases.",
                        "Citation Paper Authors": "Authors:Micael Carvalho, R\u00e9mi Cad\u00e8ne, David Picard, Laure Soulier, Nicolas Thome, Matthieu Cord"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "led to degraded performance.\nOur GAN model is a simplified version of StackGAN ",
                    "Citation Text": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei\nHuang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic image\nsynthesis with stacked generative adversarial networks. In Proceedings of the\nIEEE international conference on computer vision . 5907\u20135915.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.03242",
                        "Citation Paper Title": "Title:StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
                        "Citation Paper Authors": "Authors:Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "to tokenize the full\nsequence of recipe words. Recipe words, plus an additional [CLS]\ntoken, are encoded into 768 dimensional vectors, then fed to a two-\nlayer two-head transformer encoder model. Similar to ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nCoRR abs/1810.04805 (2018). arXiv:1810.04805 http://arxiv.org/abs/1810.04805",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". Hence, we make use of the complete recipe information:\nTitle, instructions and the complete list of free flowing natural text\ningredients, without any preprocessing.\nOur recipe encoder \ud835\udc392uses WordPiece ",
                    "Citation Text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,\nNishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google\u2019s\nNeural Machine Translation System: Bridging the Gap between Human and\nMachine Translation. CoRR abs/1609.08144 (2016). arXiv:1609.08144 http://arxiv.\norg/abs/1609.08144",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08144",
                        "Citation Paper Title": "Title:Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
                        "Citation Paper Authors": "Authors:Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.14872v1": {
            "Paper Title": "Towards Understanding Trends Manipulation in Pakistan Twitter",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "have been developed for the bot detec-\ntion. BotWalk used an unsupervised adaptive algorithm with\nnetwork, content, and user-based features for classi\ufb01cation\nwith 0:90precision value. Furthermore, Sayyadiharikandeh et\nal.have proposed Ensembles of Specialized Classi\ufb01ers (ESC)\nfor bot detection ",
                    "Citation Text": "M. Sayyadiharikandeh, O. Varol, K.-C. Yang, A. Flammini, and\nF. Menczer, \u201cDetection of novel social bots by ensembles of specialized\nclassi\ufb01ers,\u201d in Proc. 29th ACM Int. Conf. Inf. & Knowl. Manage. , 2020,\npp. 2725\u20132732.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.06867",
                        "Citation Paper Title": "Title:Detection of Novel Social Bots by Ensembles of Specialized Classifiers",
                        "Citation Paper Abstract": "Abstract:Malicious actors create inauthentic social media accounts controlled in part by algorithms, known as social bots, to disseminate misinformation and agitate online discussion. While researchers have developed sophisticated methods to detect abuse, novel bots with diverse behaviors evade detection. We show that different types of bots are characterized by different behavioral features. As a result, supervised learning techniques suffer severe performance deterioration when attempting to detect behaviors not observed in the training data. Moreover, tuning these models to recognize novel bots requires retraining with a significant amount of new annotations, which are expensive to obtain. To address these issues, we propose a new supervised learning method that trains classifiers specialized for each class of bots and combines their decisions through the maximum rule. The ensemble of specialized classifiers (ESC) can better generalize, leading to an average improvement of 56\\% in F1 score for unseen accounts across datasets. Furthermore, novel bot behaviors are learned with fewer labeled examples during retraining. We deployed ESC in the newest version of Botometer, a popular tool to detect social bots in the wild, with a cross-validation AUC of 0.99.",
                        "Citation Paper Authors": "Authors:Mohsen Sayyadiharikandeh, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, Filippo Menczer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.07631v2": {
            "Paper Title": "Accelerating COVID-19 research with graph mining and transformer-based\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.14638v1": {
            "Paper Title": "Privacy Policy Question Answering Assistant: A Query-Guided Extractive\n  Summarization Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.14097v1": {
            "Paper Title": "How Much Data Analytics is Enough? The ROI of Machine Learning\n  Classification and its Application to Requirements Dependency Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00686v3": {
            "Paper Title": "Accelerating Recommendation System Training by Leveraging Popular\n  Choices",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "employ runtime techniques to improve\nmemory, communication, and I/O resources for training and reduce\ndata stall time, respectively. On the hardware side, works in ",
                    "Citation Text": "Assaf Eisenman, Maxim Naumov, Darryl Gardner, Misha Smelyanskiy, Sergey\nPupyrev, Kim Hazelwood, Asaf Cidon, and Sachin Katti. Bandana: Using non-\nvolatile memory for storing deep learning models. Proceedings of Machine Learn-\ning and Systems , 1:40\u201352, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.05922",
                        "Citation Paper Title": "Title:Bandana: Using Non-volatile Memory for Storing Deep Learning Models",
                        "Citation Paper Abstract": "Abstract:Typical large-scale recommender systems use deep learning models that are stored on a large amount of DRAM. These models often rely on embeddings, which consume most of the required memory. We present Bandana, a storage system that reduces the DRAM footprint of embeddings, by using Non-volatile Memory (NVM) as the primary storage medium, with a small amount of DRAM as cache. The main challenge in storing embeddings on NVM is its limited read bandwidth compared to DRAM. Bandana uses two primary techniques to address this limitation: first, it stores embedding vectors that are likely to be read together in the same physical location, using hypergraph partitioning, and second, it decides the number of embedding vectors to cache in DRAM by simulating dozens of small caches. These techniques allow Bandana to increase the effective read bandwidth of NVM by 2-3x and thereby significantly reduce the total cost of ownership.",
                        "Citation Paper Authors": "Authors:Assaf Eisenman, Maxim Naumov, Darryl Gardner, Misha Smelyanskiy, Sergey Pupyrev, Kim Hazelwood, Asaf Cidon, Sachin Katti"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.13984v1": {
            "Paper Title": "Text Simplification for Comprehension-based Question-Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12985v2": {
            "Paper Title": "Synerise at RecSys 2021: Twitter user engagement prediction with a fast\n  neural model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13527v1": {
            "Paper Title": "Concept-Aware Denoising Graph Neural Network for Micro-Video\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "show that homogeneous graphs\nwith directly connected items and users can not make up\nfor the negative impact of noisy information. Attention\n4https://github.com/stanfordnlp/GloVemechanism contributes a lot to GAT ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In Proceedings of the\nInternational Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.04780v2": {
            "Paper Title": "Open Knowledge Graphs Canonicalization using Variational Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13301v1": {
            "Paper Title": "Exploring The Role of Local and Global Explanations in Recommender\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "respectively. However, in a study surrounding\na question-answering task, explanations did help users develop\nmore appropriate trust in comparison to AI confidence ",
                    "Citation Text": "Ana Valeria Gonzalez, Gagan Bansal, Angela Fan, Robin Jia, Yashar Mehdad, and\nSrinivasan Iyer. 2020. Human Evaluation of Spoken vs. Visual Explanations for\nOpen-Domain QA. arXiv preprint arXiv:2012.15075 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15075",
                        "Citation Paper Title": "Title:Human Evaluation of Spoken vs. Visual Explanations for Open-Domain QA",
                        "Citation Paper Abstract": "Abstract:While research on explaining predictions of open-domain QA systems (ODQA) to users is gaining momentum, most works have failed to evaluate the extent to which explanations improve user trust. While few works evaluate explanations using user studies, they employ settings that may deviate from the end-user's usage in-the-wild: ODQA is most ubiquitous in voice-assistants, yet current research only evaluates explanations using a visual display, and may erroneously extrapolate conclusions about the most performant explanations to other modalities. To alleviate these issues, we conduct user studies that measure whether explanations help users correctly decide when to accept or reject an ODQA system's answer. Unlike prior work, we control for explanation modality, e.g., whether they are communicated to users through a spoken or visual interface, and contrast effectiveness across modalities. Our results show that explanations derived from retrieved evidence passages can outperform strong baselines (calibrated confidence) across modalities but the best explanation strategy in fact changes with the modality. We show common failure cases of current explanations, emphasize end-to-end evaluation of explanations, and caution against evaluating them in proxy modalities that are different from deployment.",
                        "Citation Paper Authors": "Authors:Ana Valeria Gonzalez, Gagan Bansal, Angela Fan, Robin Jia, Yashar Mehdad, Srinivasan Iyer"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "observed that local and\nglobal explanations both aid users in estimating model confidence\nand gauging their own confidence in the model output. Huber et\nal. ",
                    "Citation Text": "Tobias Huber, Katharina Weitz, Elisabeth Andr\u00e9, and Ofra Amir. 2020. Local\nand global explanations of agent behavior: integrating strategy summaries with\nsaliency maps. arXiv preprint arXiv:2005.08874 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.08874",
                        "Citation Paper Title": "Title:Local and Global Explanations of Agent Behavior: Integrating Strategy Summaries with Saliency Maps",
                        "Citation Paper Abstract": "Abstract:With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the $\\textit{global}$ behavior of the agent, describing the actions it takes in different states. Other approaches devised $\\textit{local}$ explanations which provide information regarding the agent's decision-making in a particular state. In this paper, we combine global and local explanation methods, and evaluate their joint and separate contributions, providing (to the best of our knowledge) the first user study of combined local and global explanations for RL agents. Specifically, we augment strategy summaries that extract important trajectories of states from simulations of the agent with saliency maps which show what information the agent attends to. Our results show that the choice of what states to include in the summary (global information) strongly affects people's understanding of agents: participants shown summaries that included important states significantly outperformed participants who were presented with agent behavior in a randomly set of chosen world-states. We find mixed results with respect to augmenting demonstrations with saliency maps (local information), as the addition of saliency maps did not significantly improve performance in most cases. However, we do find some evidence that saliency maps can help users better understand what information the agent relies on in its decision making, suggesting avenues for future work that can further improve explanations of RL agents.",
                        "Citation Paper Authors": "Authors:Tobias Huber, Katharina Weitz, Elisabeth Andr\u00e9, Ofra Amir"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "established that local explanations more easily achieve\nmodel faithfulness. Similarly, Guidotti et al. ",
                    "Citation Text": "R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, and F. Giannotti.\n2018. Local Rule-Based Explanations of Black Box Decision Systems. ArXiv\nabs/1805.10820 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10820",
                        "Citation Paper Title": "Title:Local Rule-Based Explanations of Black Box Decision Systems",
                        "Citation Paper Abstract": "Abstract:The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. %Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.",
                        "Citation Paper Authors": "Authors:Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini, Fosca Giannotti"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "2.1 Local and Global Explanations in Machine\nLearning\nIn machine learning broadly, global explanations explain how a\nmodel behaves generally, while local explanations explain a single\nmodel output, as first distinguished by Ribeiro et al. ",
                    "Citation Text": "Marco Tulio Ribeiro, Sameer Singh, and C. Guestrin. 2016. \"Why Should I Trust\nYou?\": Explaining the Predictions of Any Classifier. Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.13921v1": {
            "Paper Title": "Click-through Rate Prediction with Auto-Quantized Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": "is a successful attempt to combine the power of factorization machines in recommendation and deep\nlearning in the feature learning.\n\u2022DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li,\nand Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of the ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining (KDD) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "is the vanilla instance-level contrastive learning. We here use ICL during the training stage rather than\npre-training.\nFor RQ2, we verify AQCL with the following backbones.\n\u2022W&D ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,\nGreg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of\nthe 1st workshop on Deep Learning for Recommender Systems , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", explore to initialize model parameters or embeddings with\nuser and item side information. Some other works train the recommendation model with auxiliary task as explicit\nregularization. DeepMCP ",
                    "Citation Text": "Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Chao Qi, Zhaojie Liu, and Yanlong Du. Representation learning-\nassisted click-through rate prediction. In Proceedings of the International Joint Conference on Arti\ufb01cial Intelli-\ngence (IJCAI) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04365",
                        "Citation Paper Title": "Title:Representation Learning-Assisted Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is a critical task in online advertising systems. Most existing methods mainly model the feature-CTR relationship and suffer from the data sparsity issue. In this paper, we propose DeepMCP, which models other types of relationships in order to learn more informative and statistically reliable feature representations, and in consequence to improve the performance of CTR prediction. In particular, DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Experiments on two large-scale datasets demonstrate that DeepMCP outperforms several state-of-the-art models for CTR prediction.",
                        "Citation Paper Authors": "Authors:Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Chao Qi, Zhaojie Liu, Yanlong Du"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "utilizes extra labels and make each instance\nclose to others with the same class. PCL ",
                    "Citation Text": "Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised\nrepresentations. In International Conference on Learning Representations (ICLR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04966",
                        "Citation Paper Title": "Title:Prototypical Contrastive Learning of Unsupervised Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Junnan Li, Pan Zhou, Caiming Xiong, Steven C.H. Hoi"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ". CL maximizes a lower bound on the mutual information between two or among more \u201cviews\" of an\ninstance ",
                    "Citation Text": "Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah D. Goodman. On mutual information in\ncontrastive learning for visual representations. arXiv preprint arXiv:2005.13149 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13149",
                        "Citation Paper Title": "Title:On Mutual Information in Contrastive Learning for Visual Representations",
                        "Citation Paper Abstract": "Abstract:In recent years, several unsupervised, \"contrastive\" learning algorithms in vision have been shown to learn representations that perform remarkably well on transfer tasks. We show that this family of algorithms maximizes a lower bound on the mutual information between two or more \"views\" of an image where typical views come from a composition of image augmentations. Our bound generalizes the InfoNCE objective to support negative sampling from a restricted region of \"difficult\" contrasts. We find that the choice of negative samples and views are critical to the success of these algorithms. Reformulating previous learning objectives in terms of mutual information also simplifies and stabilizes them. In practice, our new objectives yield representations that outperform those learned with previous approaches for transfer to classification, bounding box detection, instance segmentation, and keypoint detection. % experiments show that choosing more difficult negative samples results in a stronger representation, outperforming those learned with IR, LA, and CMC in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unifying comparison of approaches to contrastive learning and uncovers the choices that impact representation learning.",
                        "Citation Paper Authors": "Authors:Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, Noah Goodman"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "and has shown success\nin computer vision [30,52], audio [3,45], natural language processing [16,28] and many cross-modality tasks [1,42,63].\nContrastive learning (CL) is one representative line of works, including CPC ",
                    "Citation Text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. In\nInternational Conference on Learning Representations (ICLR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "is an early attempt to explore representation learning by designing matching subnet and\ncorrelation subnet. SSL4Rec ",
                    "Citation Text": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H\nChi, Steve Tjoa, Jieqi Kang, et al. Self-supervised learning for large-scale item recommendations. arXiv preprint\narXiv:2007.12865 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12865",
                        "Citation Paper Title": "Title:Self-supervised Learning for Large-scale Item Recommendations",
                        "Citation Paper Abstract": "Abstract:Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.\nInspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.\nWe evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",
                        "Citation Paper Authors": "Authors:Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "applies a data-augmentation on input to encourage\nrobust user or item representations. Many meta-learning based works, e.g., MeLU ",
                    "Citation Text": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learned user preference\nestimator for cold-start recommendation. In Proceedings of the ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining (KDD) , pages 1073\u20131082, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00413",
                        "Citation Paper Title": "Title:MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation",
                        "Citation Paper Abstract": "Abstract:This paper proposes a recommender system to alleviate the cold-start problem that can estimate user preferences based on only a small number of items. To identify a user's preference in the cold state, existing recommender systems, such as Netflix, initially provide items to a user; we call those items evidence candidates. Recommendations are then made based on the items selected by the user. Previous recommendation studies have two limitations: (1) the users who consumed a few items have poor recommendations and (2) inadequate evidence candidates are used to identify user preferences. We propose a meta-learning-based recommender system called MeLU to overcome these two limitations. From meta-learning, which can rapidly adopt new task with a few examples, MeLU can estimate new user's preferences with a few consumed items. In addition, we provide an evidence candidate selection strategy that determines distinguishing items for customized preference estimation. We validate MeLU with two benchmark datasets, and the proposed model reduces at least 5.92% mean absolute error than two comparative models on the datasets. We also conduct a user study experiment to verify the evidence selection strategy.",
                        "Citation Paper Authors": "Authors:Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, Sehee Chung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12758v1": {
            "Paper Title": "Text to Insight: Accelerating Organic Materials Knowledge Extraction via\n  Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12662v1": {
            "Paper Title": "Improving Question Answering Performance Using Knowledge Distillation\n  and Active Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12651v1": {
            "Paper Title": "Why Do We Click: Visual Impression-aware News Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.15120v1": {
            "Paper Title": "SUper Team at SemEval-2016 Task 3: Building a feature-rich system for\n  community question answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13726v1": {
            "Paper Title": "Exposing Paid Opinion Manipulation Trolls",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12541v1": {
            "Paper Title": "Dynamic Sequential Graph Learning for Click-Through Rate Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12512v1": {
            "Paper Title": "DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised\n  Graph Learning for Click-Through Rate Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12314v1": {
            "Paper Title": "MC$^2$-SF: Slow-Fast Learning for Mobile-Cloud Collaborative\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". One representative work of the fast-centralized modeling is DCCL ",
                    "Citation Text": "Jiangchao Yao, Feng Wang, KunYang Jia, Bo Han, Jingren Zhou, and Hongxia Yang. 2021. Device-Cloud Collaborative Learning for Recommendation.\nInSIGKDD. 3865\u20133874.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06624",
                        "Citation Paper Title": "Title:Device-Cloud Collaborative Learning for Recommendation",
                        "Citation Paper Abstract": "Abstract:With the rapid development of storage and computing power on mobile devices, it becomes critical and popular to deploy models on devices to save onerous communication latencies and to capture real-time features. While quite a lot of works have explored to facilitate on-device learning and inference, most of them focus on dealing with response delay or privacy protection. Little has been done to model the collaboration between the device and the cloud modeling and benefit both sides jointly. To bridge this gap, we are among the first attempts to study the Device-Cloud Collaborative Learning (DCCL) framework. Specifically, we propose a novel MetaPatch learning approach on the device side to efficiently achieve \"thousands of people with thousands of models\" given a centralized cloud model. Then, with billions of updated personalized device models, we propose a \"model-over-models\" distillation algorithm, namely MoMoDistill, to update the centralized cloud model. Our extensive experiments over a range of datasets with different settings demonstrate the effectiveness of such collaboration on both cloud and devices, especially its superiority to model long-tailed users.",
                        "Citation Paper Authors": "Authors:Jiangchao Yao, Feng Wang, KunYang Jia, Bo Han, Jingren Zhou, Hongxia Yang"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "employs a generic model shrinking\ntechnique to reduce responding time and memory footprint. Other work ",
                    "Citation Text": "Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, and Wenwu Ou. 2020. EdgeRec: recommender system on edge in Mobile\nTaobao. In Proceedings ofthe29th ACM International Conference onInformation &Knowledge Management. 2477\u20132484.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.08416",
                        "Citation Paper Title": "Title:EdgeRec: Recommender System on Edge in Mobile Taobao",
                        "Citation Paper Abstract": "Abstract:Recommender system (RS) has become a crucial module in most web-scale applications. Recently, most RSs are in the waterfall form based on the cloud-to-edge framework, where recommended results are transmitted to edge (e.g., user mobile) by computing in advance in the cloud server. Despite effectiveness, network bandwidth and latency between cloud server and edge may cause the delay for system feedback and user perception. Hence, real-time computing on edge could help capture user preferences more preciously and thus make more satisfactory recommendations. Our work, to our best knowledge, is the first attempt to design and implement the novel Recommender System on Edge (EdgeRec), which achieves Real-time User Perception and Real-time System Feedback. Moreover, we propose Heterogeneous User Behavior Sequence Modeling and Context-aware Reranking with Behavior Attention Networks to capture user's diverse interests and adjust recommendation results accordingly. Experimental results on both the offline evaluation and online performance in Taobao home-page feeds demonstrate the effectiveness of EdgeRec.",
                        "Citation Paper Authors": "Authors:Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, Wenwu Ou"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". With the development of deep learning, deep neural networks are involved into the recommendation\nto acquire the high-level semantics [ 3,25,27]. For example, DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR\nprediction. arXiv preprint arXiv:1703.04247 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12306v1": {
            "Paper Title": "Topic Model Robustness to Automatic Speech Recognition Errors in Podcast\n  Transcripts",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "was used for both the pre-training procedure as well as fine-tuning. During inference, the\nASR engine performs prefix beam search ",
                    "Citation Text": "Awni Y Hannun, Andrew L Maas, Daniel Jurafsky, and Andrew Y Ng. 2014. First-pass large vocabulary continuous speech recognition using\nbi-directional recurrent dnns. arXiv preprint arXiv:1408.2873 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1408.2873",
                        "Citation Paper Title": "Title:First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs",
                        "Citation Paper Abstract": "Abstract:We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.",
                        "Citation Paper Authors": "Authors:Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "(CTC) loss function on 200 hours\nof labeled data from the Nordisk Spr\u00e5kteknologi (NST) danish training dataset4and 267 hours of aligned audiobook\ndata. The Fairseq library ",
                    "Citation Text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of NAACL-HLT 2019: Demonstrations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01038",
                        "Citation Paper Title": "Title:fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at this https URL",
                        "Citation Paper Authors": "Authors:Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", after subjecting the training data to insertion of frequent words,\ndeletion and rule-based phonetic substitution errors ",
                    "Citation Text": "Jing Su, Derek Greene, and Ois\u00edn Boydell. 2016. Topic Stability over Noisy Sources. In Proceedings of the 2nd Workshop on Noisy User-generated Text\n(WNUT) . The COLING 2016 Organizing Committee, Osaka, Japan, 85\u201393. https://aclanthology.org/W16-3913",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01067",
                        "Citation Paper Title": "Title:Topic Stability over Noisy Sources",
                        "Citation Paper Abstract": "Abstract:Topic modelling techniques such as LDA have recently been applied to speech transcripts and OCR output. These corpora may contain noisy or erroneous texts which may undermine topic stability. Therefore, it is important to know how well a topic modelling algorithm will perform when applied to noisy data. In this paper we show that different types of textual noise will have diverse effects on the stability of different topic models. From these observations, we propose guidelines for text corpus generation, with a focus on automatic speech transcription. We also suggest topic model selection methods for noisy corpora.",
                        "Citation Paper Authors": "Authors:Jing Su, Ois\u00edn Boydell, Derek Greene, Gerard Lynch"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", the recognition performance gap between low and high resource languages remains\nquite high.2Pre-training speech recognition models with cross-lingual data has helped bridge the gap significantly ",
                    "Citation Text": "Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. 2020. Unsupervised cross-lingual representation\nlearning for speech recognition. arXiv preprint arXiv:2006.13979 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13979",
                        "Citation Paper Title": "Title:Unsupervised Cross-lingual Representation Learning for Speech Recognition",
                        "Citation Paper Abstract": "Abstract:This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". Labeled speech recognition training data is highly\naccessible in English [ 19,27] but less so in many other languages. Even with crowd-sourcing data initiatives such as\nCommonVoice from Mozilla ",
                    "Citation Text": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and\nGregor Weber. 2019. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06670",
                        "Citation Paper Title": "Title:Common Voice: A Massively-Multilingual Speech Corpus",
                        "Citation Paper Abstract": "Abstract:The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
                        "Citation Paper Authors": "Authors:Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, Gregor Weber"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "is as low as 1.4% and 3.3% of Word Error Rate (WER)\non the test-clean and the test-other partitions, respectively ",
                    "Citation Text": "Yu Zhang, James Qin, Daniel S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V Le, and Yonghui Wu. 2020. Pushing the limits of\nsemi-supervised learning for automatic speech recognition. arXiv preprint arXiv:2010.10504 (2020).\n10Topic Model Robustness to Automatic Speech Recognition Errors in Podcast Transcripts RecSys \u201921, September 27\u2013October 05, 2021, Amsterdam, NL\nA ADDITIONAL FIGURES\nFig. 2. Distribution of episodes per podcast.\nFig. 3. Word Error Rate as function of \ud835\udefd\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10504",
                        "Citation Paper Title": "Title:Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
                        "Citation Paper Abstract": "Abstract:We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",
                        "Citation Paper Authors": "Authors:Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11903v1": {
            "Paper Title": "Multi-behavior Graph Contextual Aware Network for Session-based\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "firstly proposed\nto learn global item-to-item relations through GNN and integrate\nembedding of target and auxiliary of current session by the gating\nmechanism. MKM-SR ",
                    "Citation Text": "Wenjing Meng, Deqing Yang, and Yanghua Xiao. 2020. Incorporating user micro-\nbehaviors and item knowledge into multi-task learning for session-based rec-\nommendation. In Proceedings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . 1091\u20131100.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.06922",
                        "Citation Paper Title": "Title:Incorporating User Micro-behaviors and Item Knowledge into Multi-task Learning for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation (SR) has become an important and popular component of various e-commerce platforms, which aims to predict the next interacted item based on a given session. Most of existing SR models only focus on exploiting the consecutive items in a session interacted by a certain user, to capture the transition pattern among the items. Although some of them have been proven effective, the following two insights are often neglected. First, a user's micro-behaviors, such as the manner in which the user locates an item, the activities that the user commits on an item (e.g., reading comments, adding to cart), offer fine-grained and deep understanding of the user's preference. Second, the item attributes, also known as item knowledge, provide side information to model the transition pattern among interacted items and alleviate the data sparsity problem. These insights motivate us to propose a novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors and item Knowledge into Multi-task learning for Session-based Recommendation. Specifically, a given session is modeled on micro-behavior level in MKM-SR, i.e., with a sequence of item-operation pairs rather than a sequence of items, to capture the transition pattern in the session sufficiently. Furthermore, we propose a multi-task learning paradigm to involve learning knowledge embeddings which plays a role as an auxiliary task to promote the major task of SR. It enables our model to obtain better session representations, resulting in more precise SR recommendation results. The extensive evaluations on two benchmark datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models, justifying the strategy of incorporating knowledge learning.",
                        "Citation Paper Authors": "Authors:Wenjing Meng, Deqing Yang, Yanghua Xiao"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "proposed a lossless\nencoding scheme which preserved the edge-order and sequential\ninformation. GCE-GNN ",
                    "Citation Text": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui\nQiu. 2020. Global context enhanced graph neural networks for session-based\nrecommendation. In Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 169\u2013178.XXX, June 03\u201305, 2021, XXXX Zhang, et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05081",
                        "Citation Paper Title": "Title:Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
                        "Citation Paper Authors": "Authors:Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Let h(\ud835\udc58)\n\ud835\udc56denote the embedding of item \ud835\udc63\ud835\udc56after\ud835\udc58layers\nGNN propagation. The item IDs are embedded in \ud835\udc51-dimensional\nspace and are used as initial node features in our model, h(0)\n\ud835\udc56\u2208R\ud835\udc51.\nFollowing ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11898v1": {
            "Paper Title": "Graph Learning Augmented Heterogeneous Graph Neural Network for Social\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ": This method proposes a deep learning-based\nframework, which captures the influence of distant social\nrelationships on target users.\n\u2022GC-MC ",
                    "Citation Text": "Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\ntional matrix completion. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ": This is one matrix factorization-based model,\naggregating friends embeddings into target users embed-\ndings to learn explicit and implicit information.\n\u2022DSCF ",
                    "Citation Text": "Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, and Qing Li. 2019.\nDeep social collaborative filtering. In RecSys . 305\u2013313.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.06853",
                        "Citation Paper Title": "Title:Deep Social Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Recommender systems are crucial to alleviate the information overload problem in online worlds. Most of the modern recommender systems capture users' preference towards items via their interactions based on collaborative filtering techniques. In addition to the user-item interactions, social networks can also provide useful information to understand users' preference as suggested by the social theories such as homophily and influence. Recently, deep neural networks have been utilized for social recommendations, which facilitate both the user-item interactions and the social network information. However, most of these models cannot take full advantage of the social network information. They only use information from direct neighbors, but distant neighbors can also provide helpful information. Meanwhile, most of these models treat neighbors' information equally without considering the specific recommendations. However, for a specific recommendation case, the information relevant to the specific item would be helpful. Besides, most of these models do not explicitly capture the neighbor's opinions to items for social recommendations, while different opinions could affect the user differently. In this paper, to address the aforementioned challenges, we propose DSCF, a Deep Social Collaborative Filtering framework, which can exploit the social relations with various aspects for recommender systems. Comprehensive experiments on two-real world datasets show the effectiveness of the proposed framework.",
                        "Citation Paper Authors": "Authors:Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, Qing Li"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed to model each edge inside the adjacency matrix.\nIDGL ",
                    "Citation Text": "Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020. Iterative Deep Graph Learning\nfor Graph Neural Networks: Better and Robust Node Embeddings. NIPS 33 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13009",
                        "Citation Paper Title": "Title:Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph structure approaches close enough to the graph optimized for the downstream prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-Anch, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match the state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
                        "Citation Paper Authors": "Authors:Yu Chen, Lingfei Wu, Mohammed J. Zaki"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11790v1": {
            "Paper Title": "Learning Dual Dynamic Representations on Time-Sliced User-Item\n  Interaction Graphs for Sequential Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nIn light of session-based recommendation where only user dy-\nnamics are considered, SR-GNN ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In AAAI . 346\u2013353.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ", sequential rec-\nommendation studied in this paper considers user IDs and their\nbehavior sequences in a longer time period. The early work ",
                    "Citation Text": "Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin\nWang, and Tie-Yan Liu. 2014. Sequential Click Prediction for Sponsored Search\nwith Recurrent Neural Networks. In AAAI . 1369\u20131375.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1404.5772",
                        "Citation Paper Title": "Title:Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user's behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user's sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.",
                        "Citation Paper Authors": "Authors:Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, Tie-Yan Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11654v1": {
            "Paper Title": "Modeling Dynamic Attributes for Next Basket Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "extends SASRec using self-attention to model the time\ninterval between two adjacent interactions. BERT4Rec ",
                    "Citation Text": "Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional\nencoder representations from transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management .\n1441\u20131450.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06690",
                        "Citation Paper Title": "Title:BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
                        "Citation Paper Abstract": "Abstract:Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly \"see the target item\". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.",
                        "Citation Paper Authors": "Authors:Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "proposes a CIN module to take the outer product at a vector-wise level. These works, which use DNN\nto capture high-order feature interactions implicitly, lack good explanation ability in general. To this end, AutoInt ",
                    "Citation Text": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction\nlearning via self-attentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management .\n1161\u20131170.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11921",
                        "Citation Paper Title": "Title:AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features (\\textit{a.k.a.} cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the \\emph{AutoInt} to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "model uses a wide part to model second-order interactions and a deep part to model the higher-order\ninteractions. Different from Wide&Deep, Deep&Cross ",
                    "Citation Text": "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD\u201917 . 1\u20137.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "combines FM with a DNN to model high-order feature interactions. The\nWide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa\nIspir, et al .2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "learns the correlation between the most recent basket items and the target item to summarize users\u2019 short-term\ninterests. In this work, we use multi-head self-attention within each basket so that complex item interrelationships (e.g.,\nco-occurrences) can be captured. MITGNN ",
                    "Citation Text": "Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, and S Yu Philip. 2020. Basket recommendation with multi-intent translation graph\nneural network. In 2020 IEEE International Conference on Big Data (Big Data) . IEEE, 728\u2013737.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11419",
                        "Citation Paper Title": "Title:Basket Recommendation with Multi-Intent Translation Graph Neural Network",
                        "Citation Paper Abstract": "Abstract:The problem of basket recommendation~(BR) is to recommend a ranking list of items to the current basket. Existing methods solve this problem by assuming the items within the same basket are correlated by one semantic relation, thus optimizing the item embeddings. However, this assumption breaks when there exist multiple intents within a basket. For example, assuming a basket contains \\{\\textit{bread, cereal, yogurt, soap, detergent}\\} where \\{\\textit{bread, cereal, yogurt}\\} are correlated through the \"breakfast\" intent, while \\{\\textit{soap, detergent}\\} are of \"cleaning\" intent, ignoring multiple relations among the items spoils the ability of the model to learn the embeddings. To resolve this issue, it is required to discover the intents within the basket. However, retrieving a multi-intent pattern is rather challenging, as intents are latent within the basket. Additionally, intents within the basket may also be correlated. Moreover, discovering a multi-intent pattern requires modeling high-order interactions, as the intents across different baskets are also correlated. To this end, we propose a new framework named as \\textbf{M}ulti-\\textbf{I}ntent \\textbf{T}ranslation \\textbf{G}raph \\textbf{N}eural \\textbf{N}etwork~({\\textbf{MITGNN}}). MITGNN models $T$ intents as tail entities translated from one corresponding basket embedding via $T$ relation vectors. The relation vectors are learned through multi-head aggregators to handle user and item information. Additionally, MITGNN propagates multiple intents across our defined basket graph to learn the embeddings of users and items by aggregating neighbors. Extensive experiments on two real-world datasets prove the effectiveness of our proposed model on both transductive and inductive BR. The code is available online at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, Philip S. Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.06416v2": {
            "Paper Title": "MMCoVaR: Multimodal COVID-19 Vaccine Focused Data Repository for Fake\n  News Detection and a Baseline Architecture for Classification",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". The MHA is followed\nby an attention layer, a dense layer and a softmax layer to get the \ufb01nal classi\ufb01cation. The architecture is depicted in\nFig 8 and is inspired by our earlier work on interpretable fake tweet detection ",
                    "Citation Text": "Mingxuan Chen, Ning Wang, and K.P. Subbalakshmi. Explainable rumor detection using inter and intra-feature\nattention networks. KDD TrueFact Conference , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11057",
                        "Citation Paper Title": "Title:Explainable Rumor Detection using Inter and Intra-feature Attention Networks",
                        "Citation Paper Abstract": "Abstract:With social media becoming ubiquitous, information consumption from this media has also increased. However, one of the serious problems that have emerged with this increase, is the propagation of rumors. Therefore, rumor identification is a very critical task with significant implications to economy, democracy as well as public health and safety. We tackle the problem of automated detection of rumors in social media in this paper by designing a modular explainable architecture that uses both latent and handcrafted features and can be expanded to as many new classes of features as desired. This approach will allow the end user to not only determine whether the piece of information on the social media is real of a rumor, but also give explanations on why the algorithm arrived at its conclusion. Using attention mechanisms, we are able to interpret the relative importance of each of these features as well as the relative importance of the feature classes themselves. The advantage of this approach is that the architecture is expandable to more handcrafted features as they become available and also to conduct extensive testing to determine the relative influences of these features in the final decision. Extensive experimentation on popular datasets and benchmarking against eleven contemporary algorithms, show that our approach performs significantly better in terms of F-score and accuracy while also being interpretable.",
                        "Citation Paper Authors": "Authors:Mingxuan Chen, Ning Wang, K.P. Subbalakshmi"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ": Logistic regression models the probabilities for classi\ufb01cation problems with two\npossible outcomes.\n\u2022Gradient Boost ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\nMachine learning in Python. Journal of Machine Learning Research , 12:2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "to compute a similarity metric on tweet-misconception pairs and use that for stance detection.\nReCoVery ",
                    "Citation Text": "Xinyi Zhou, Apurva Mulay, Emilio Ferrara, and Reza Zafarani. ReCOVery: A multimodal repository for covid-19\nnews credibility research. In Proceedings of the 29th ACM International Conference on Information & Knowledge\nManagement , pages 3205\u20133212, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05557",
                        "Citation Paper Title": "Title:ReCOVery: A Multimodal Repository for COVID-19 News Credibility Research",
                        "Citation Paper Abstract": "Abstract:First identified in Wuhan, China, in December 2019, the outbreak of COVID-19 has been declared as a global emergency in January, and a pandemic in March 2020 by the World Health Organization (WHO). Along with this pandemic, we are also experiencing an \"infodemic\" of information with low credibility such as fake news and conspiracies. In this work, we present ReCOVery, a repository designed and constructed to facilitate research on combating such information regarding COVID-19. We first broadly search and investigate ~2,000 news publishers, from which 60 are identified with extreme [high or low] levels of credibility. By inheriting the credibility of the media on which they were published, a total of 2,029 news articles on coronavirus, published from January to May 2020, are collected in the repository, along with 140,820 tweets that reveal how these news articles have spread on the Twitter social network. The repository provides multimodal information of news articles on coronavirus, including textual, visual, temporal, and network information. The way that news credibility is obtained allows a trade-off between dataset scalability and label accuracy. Extensive experiments are conducted to present data statistics and distributions, as well as to provide baseline performances for predicting news credibility so that future methods can be compared. Our repository is available at this http URL.",
                        "Citation Paper Authors": "Authors:Xinyi Zhou, Apurva Mulay, Emilio Ferrara, Reza Zafarani"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is a large-scale Twitter dataset containing more than 524 million multilingual tweets posted for 90 days\nfrom Feb 1st2020. GeoCoV19 focuses on geographical and topic distribution analysis and provides the distribution of\ntopics by countries and cities. The CoVaxxy ",
                    "Citation Text": "Matthew DeVerna, Francesco Pierri, Bao Truong, John Bollenbacher, David Axelrod, Niklas Loynes, Cristopher\nTorres-Lugo, Kai-Cheng Yang, Fil Menczer, and John Bryden. Covaxxy: A global collection of english twitter\nposts about covid-19 vaccines. arXiv preprint arXiv:2101.07694 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.07694",
                        "Citation Paper Title": "Title:CoVaxxy: A Collection of English-language Twitter Posts About COVID-19 Vaccines",
                        "Citation Paper Abstract": "Abstract:With a substantial proportion of the population currently hesitant to take the COVID-19 vaccine, it is important that people have access to accurate information. However, there is a large amount of low-credibility information about vaccines spreading on social media. In this paper, we present the CoVaxxy dataset, a growing collection of English-language Twitter posts about COVID-19 vaccines. Using one week of data, we provide statistics regarding the numbers of tweets over time, the hashtags used, and the websites shared. We also illustrate how these data might be utilized by performing an analysis of the prevalence over time of high- and low-credibility sources, topic groups of hashtags, and geographical distributions. Additionally, we develop and present the CoVaxxy dashboard, allowing people to visualize the relationship between COVID-19 vaccine adoption and U.S. geo-located posts in our dataset. This dataset can be used to study the impact of online information on COVID-19 health outcomes (e.g., vaccine uptake) and our dashboard can help with exploration of the data.",
                        "Citation Paper Authors": "Authors:Matthew R. DeVerna, Francesco Pierri, Bao Tran Truong, John Bollenbacher, David Axelrod, Niklas Loynes, Christopher Torres-Lugo, Kai-Cheng Yang, Filippo Menczer, John Bryden"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "is also a Twitter dataset containing tweets gathered from US-based users from Mar 5th2020 to Jul\n2nd2020. It focuses on sentiment and topic analysis. They also build a web application for topic trend analysis. The\nGeoCoV19 ",
                    "Citation Text": "Umair Qazi, Muhammad Imran, and Ferda O\ufb02i. Geocov19: a dataset of hundreds of millions of multilingual\ncovid-19 tweets with location information. SIGSPATIAL Special , 12(1):6\u201315, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11177",
                        "Citation Paper Title": "Title:GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 Tweets with Location Information",
                        "Citation Paper Abstract": "Abstract:The past several years have witnessed a huge surge in the use of social media platforms during mass convergence events such as health emergencies, natural or human-induced disasters. These non-traditional data sources are becoming vital for disease forecasts and surveillance when preparing for epidemic and pandemic outbreaks. In this paper, we present GeoCoV19, a large-scale Twitter dataset containing more than 524 million multilingual tweets posted over a period of 90 days since February 1, 2020. Moreover, we employ a gazetteer-based approach to infer the geolocation of tweets. We postulate that this large-scale, multilingual, geolocated social media data can empower the research communities to evaluate how societies are collectively coping with this unprecedented global crisis as well as to develop computational methods to address challenges such as identifying fake news, understanding communities' knowledge gaps, building disease forecast and surveillance models, among others.",
                        "Citation Paper Authors": "Authors:Umair Qazi, Muhammad Imran, Ferda Ofli"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "released two types\nof Twitter-based datasets; one gathered from Dec 2019 to May 2020 and the other from Jan 2020 to Mar 2020. They\nprovide the sentiment analysis for this tweet dataset. The dataset in ",
                    "Citation Text": "Catherine Ordun, Sanjay Purushotham, and Edward Raff. Exploratory analysis of covid-19 tweets using topic\nmodeling, umap, and digraphs. arXiv preprint arXiv:2005.03082 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03082",
                        "Citation Paper Title": "Title:Exploratory Analysis of Covid-19 Tweets using Topic Modeling, UMAP, and DiGraphs",
                        "Citation Paper Abstract": "Abstract:This paper illustrates five different techniques to assess the distinctiveness of topics, key terms and features, speed of information dissemination, and network behaviors for Covid19 tweets. First, we use pattern matching and second, topic modeling through Latent Dirichlet Allocation (LDA) to generate twenty different topics that discuss case spread, healthcare workers, and personal protective equipment (PPE). One topic specific to U.S. cases would start to uptick immediately after live White House Coronavirus Task Force briefings, implying that many Twitter users are paying attention to government announcements. We contribute machine learning methods not previously reported in the Covid19 Twitter literature. This includes our third method, Uniform Manifold Approximation and Projection (UMAP), that identifies unique clustering-behavior of distinct topics to improve our understanding of important themes in the corpus and help assess the quality of generated topics. Fourth, we calculated retweeting times to understand how fast information about Covid19 propagates on Twitter. Our analysis indicates that the median retweeting time of Covid19 for a sample corpus in March 2020 was 2.87 hours, approximately 50 minutes faster than repostings from Chinese social media about H7N9 in March 2013. Lastly, we sought to understand retweet cascades, by visualizing the connections of users over time from fast to slow retweeting. As the time to retweet increases, the density of connections also increase where in our sample, we found distinct users dominating the attention of Covid19 retweeters. One of the simplest highlights of this analysis is that early-stage descriptive methods like regular expressions can successfully identify high-level themes which were consistently verified as important through every subsequent analysis.",
                        "Citation Paper Authors": "Authors:Catherine Ordun, Sanjay Purushotham, Edward Raff"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "With the explosion of COVID-19, a lot of datasets have been released to support big data and deep learning based\nanalyses of the disease and its effects. One of the earliest COVID related datasets was the CORD-19 dataset ",
                    "Citation Text": "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk,\nRodney Kinney, Ziyang Liu, William Merrill, et al. Cord-19: The covid-19 open research dataset. ArXiv , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.10706",
                        "Citation Paper Title": "Title:CORD-19: The COVID-19 Open Research Dataset",
                        "Citation Paper Abstract": "Abstract:The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.",
                        "Citation Paper Authors": "Authors:Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm, Boya Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, Sebastian Kohlmeier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11231v1": {
            "Paper Title": "Dynamic inference of user context through social tag embedding for music\n  recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12986v1": {
            "Paper Title": "Findings of the NLP4IF-2021 Shared Tasks on Fighting the COVID-19\n  Infodemic and Censorship Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12987v1": {
            "Paper Title": "Overview of the CLEF--2021 CheckThat! Lab on Detecting Check-Worthy\n  Claims, Previously Fact-Checked Claims, and Fake News",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00813v2": {
            "Paper Title": "Transformers: \"The End of History\" for NLP?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.11085v1": {
            "Paper Title": "Towards Universal Dense Retrieval for Open-domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10759v1": {
            "Paper Title": "Astronomical Pipeline Provenance: A Use Case Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00033v5": {
            "Paper Title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of\n  Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the\n  Society",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10602v1": {
            "Paper Title": "Context-aware Tree-based Deep Model for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "proposethePinSagealgorithmusedinPinterestbya dding\na random walk based neighbour sampling strategy to GraphSag e ",
                    "Citation Text": "Will Hamilton, Zhitao Ying, and Jure Leskovec.2017. Ind uctive representation\nlearning on large graphs. In Advances in neural information processing systems .\n1024\u20131034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "is proposed. More recently, An o p-\ntimal beam search aware training framework of tree-based de ep\nmodels ",
                    "Citation Text": "Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, Han Li, Jian Xu, a nd Kun Gai.\n2020. Learning Optimal Tree Models under Beam Search. arXiv preprint\narXiv:2006.15408 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15408",
                        "Citation Paper Title": "Title:Learning Optimal Tree Models Under Beam Search",
                        "Citation Paper Abstract": "Abstract:Retrieving relevant targets from an extremely large target set under computational limits is a common challenge for information retrieval and recommendation systems. Tree models, which formulate targets as leaves of a tree with trainable node-wise scorers, have attracted a lot of interests in tackling this challenge due to their logarithmic computational complexity in both training and testing. Tree-based deep models (TDMs) and probabilistic label trees (PLTs) are two representative kinds of them. Though achieving many practical successes, existing tree models suffer from the training-testing discrepancy, where the retrieval performance deterioration caused by beam search in testing is not considered in training. This leads to an intrinsic gap between the most relevant targets and those retrieved by beam search with even the optimally trained node-wise scorers. We take a first step towards understanding and analyzing this problem theoretically, and develop the concept of Bayes optimality under beam search and calibration under beam search as general analyzing tools for this purpose. Moreover, to eliminate the discrepancy, we propose a novel algorithm for learning optimal tree models under beam search. Experiments on both synthetic and real data verify the rationality of our theoretical analysis and demonstrate the superiority of our algorithm compared to state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, Han Li, Jian Xu, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "use both recurrent neur al\nnetwork andattentionmechanism inlearning user vectors.W ang\net al. ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-S eng Chua. 2019.\nNeuralgraphcollaborative\ufb01ltering.In Proceedingsofthe42ndinternationalACM\nSIGIRconferenceonResearchanddevelopmentinInformatio nRetrieval .165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "etc..\nVariants of improvements have been made in learning the vect or\nmappings. For example, Lv et al. ",
                    "Citation Text": "Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Kep ing Yang, and Wil-\nfred Ng. 2019. SDM: Sequential deep matching model for onlin e large-scale\nrecommender system. In Proceedings of the 28th ACM International Conference\non Informationand Knowledge Management .2635\u20132643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.00385",
                        "Citation Paper Title": "Title:SDM: Sequential Deep Matching Model for Online Large-scale Recommender System",
                        "Citation Paper Abstract": "Abstract:Capturing users' precise preferences is a fundamental problem in large-scale recommender system. Currently, item-based Collaborative Filtering (CF) methods are common matching approaches in industry. However, they are not effective to model dynamic and evolving preferences of users. In this paper, we propose a new sequential deep matching (SDM) model to capture users' dynamic preferences by combining short-term sessions and long-term behaviors. Compared with existing sequence-aware recommendation methods, we tackle the following two inherent problems in real-world applications: (1) there could exist multiple interest tendencies in one session. (2) long-term preferences may not be effectively fused with current session interests. Long-term behaviors are various and complex, hence those highly related to the short-term session should be kept for fusion. We propose to encode behavior sequences with two corresponding components: multi-head self-attention module to capture multiple types of interests and long-short term gated fusion module to incorporate long-term preferences. Successive items are recommended after matching between sequential user behavior vector and item embedding vectors. Offline experiments on real-world datasets show the superior performance of the proposed SDM. Moreover, SDM has been successfully deployed on online large-scale recommender system at Taobao and achieves improvements in terms of a range of commercial metrics.",
                        "Citation Paper Authors": "Authors:Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, Wilfred Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.10560v1": {
            "Paper Title": "Why Don't You Click: Neural Correlates of Non-Click Behaviors in Web\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10509v1": {
            "Paper Title": "Unsupervised Contextualized Document Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10477v1": {
            "Paper Title": "Generating Compositional Color Representations from Text",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "labels textual phrases with colors, and was\nsetup via a crowd-sourced survey. ",
                    "Citation Text": "Lyndon White, Roberto Togneri, Wei Liu, and Mohammed Bennamoun. 2017.\nLearning distributions of meant color. arXiv preprint arXiv:1709.09360 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.09360",
                        "Citation Paper Title": "Title:Learning of Colors from Color Names: Distribution and Point Estimation",
                        "Citation Paper Abstract": "Abstract:Color names are often made up of multiple words. As a task in natural language understanding we investigate in depth the capacity of neural networks based on sums of word embeddings (SOWE), recurrence (LSTM and GRU based RNNs) and convolution (CNN), to estimate colors from sequences of terms. We consider both point and distribution estimates of color. We argue that the latter has a particular value as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange'', for example. Surprisingly, despite it's simplicity, the sum of word embeddings generally performs the best on almost all evaluations.",
                        "Citation Paper Authors": "Authors:Lyndon White, Roberto Togneri, Wei Liu, Mohammed Bennamoun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.10410v1": {
            "Paper Title": "RETRONLU: Retrieval Augmented Task-Oriented Semantic Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10329v1": {
            "Paper Title": "Homography augumented momentum constrastive learning for SAR image\n  retrieval",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ", have been partially replaced by DNN based approaches.\nNoh et al. ",
                    "Citation Text": "Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han.\nLarge-scale image retrieval with attentive deep local features. In Proceedings\nof the IEEE international conference on computer vision , pages 3456\u20133465,\n2017.\nDISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited,\nAFRL-2021-3176.15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.06321",
                        "Citation Paper Title": "Title:Large-Scale Image Retrieval with Attentive Deep Local Features",
                        "Citation Paper Abstract": "Abstract:We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins. Code and dataset can be found at the project webpage: this https URL .",
                        "Citation Paper Authors": "Authors:Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, Bohyung Han"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ",suggestedan\nonline method generating keys in the same batch to reduce the memory uses while\nmaintaining fast learning. It also uses a form of the infoNCE ",
                    "Citation Text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is not scalable. Thus, replacing these matching tech-\nniques with scalable DNN-based methods is in an active research area where many\napproachessuchasSuperGlue ",
                    "Citation Text": "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Ra-\nbinovich. Superglue: Learning feature matching with graph neural networks.\nInProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4938\u20134947, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11763",
                        "Citation Paper Title": "Title:SuperGlue: Learning Feature Matching with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "as the backbone architecture instead of\nCNN.Therearevariousapproachesincontrastivelearning thatdi\ufb00erdependingon\nthe usages of the queue matrix and the forms of the loss function. They include\nSimCLR ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geo\ufb00rey Hinton. A\nsimple framework for contrastive learning of visual representations. In Inter-\nnational conference on machine learning , pages 1597\u20131607. PMLR, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "as a loss function\nallowingforrepresentativedi\ufb00erentiationsbetweenimagesofagivensu\ufb03cientbatch\nsize (e.g., 4096). Additionally, to further improve the performance of MoCo, they\nadopted a vision transformation (ViT) ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929 ,2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.10256v1": {
            "Paper Title": "Towards a computational definition of the tresillo rhythm and its\n  tracing in popular music",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.11345v1": {
            "Paper Title": "Graph Neural Netwrok with Interaction Pattern for Group Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13336v1": {
            "Paper Title": "Fake or Credible? Towards Designing Services to Support Users'\n  Credibility Assessment of News Content",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ".  Moreover, the emphasis on the \ncredibility of the presented information should be \nenhanced ",
                    "Citation Text": "Lazer, D. M. J., Baum, M. A., Benkler, Y., Berinsky, A. J., \nGreenhill, K. M., Menczer, F., Metzger, M. J., Nyhan, B., \nPennycook, G., Rothschild, D., Schudson, M., Sloman, S. \nA., Sunstein, C. R., Thorson, E. A., W atts, D. J., and \nZittrain, J. L. (2018 ). The science of fake news . Science, \n359(6380), 1094 -1096 . \nhttps://doi.org/ 10.1126/science.aao2998",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.07903",
                        "Citation Paper Title": "Title:The science of fake news",
                        "Citation Paper Abstract": "Abstract:Fake news emerged as an apparent global problem during the 2016 U.S. Presidential election. Addressing it requires a multidisciplinary effort to define the nature and extent of the problem, detect fake news in real time, and mitigate its potentially harmful effects. This will require a better understanding of how the Internet spreads content, how people process news, and how the two interact. We review the state of knowledge in these areas and discuss two broad potential mitigation strategies: better enabling individuals to identify fake news, and intervention within the platforms to reduce the attention given to fake news. The cooperation of Internet platforms (especially Facebook, Google, and Twitter) with researchers will be critical to understanding the scale of the issue and the effectiveness of possible interventions.",
                        "Citation Paper Authors": "Authors:David M. J. Lazer, Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo Menczer, Miriam J. Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan J. Watts, Jonathan L. Zittrain"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". \nThe classificat ion of content such as fake news is \nchallenging. Nevertheless, deep learning can process a \nmultitude of different features to reach high \nperformance in this task ",
                    "Citation Text": "Borges, L., Martins, B., and Calado, P. (2019 ). \nCombining Similarity Features and Deep representation \nLearning for Stance Detection in the Context of Checking \nFake News . Journal of Data and Information Quality, \n11(3), 1-26. https://doi.org/ 10.1145/3287763",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00706",
                        "Citation Paper Title": "Title:Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News",
                        "Citation Paper Abstract": "Abstract:Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage the development of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a new approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural architecture that leverages ideas previously advanced in the context of learning efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks, together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:Lu\u00eds Borges, Bruno Martins, P\u00e1vel Calado"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.10086v1": {
            "Paper Title": "SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "to predict expansion words for documents. The document\nexpansion adds new terms to documents \u2013 hence fighting the vo-\ncabulary mismatch \u2013 as well as repeats existing terms, implicitly\nperforming re-weighting by boosting important terms.\nRecently, DeepImpact ",
                    "Citation Text": "Antonio Mallia, Omar Khattab, Torsten Suel, and Nicola Tonellotto. 2021. Learn-\ning Passage Impacts for Inverted Indexes. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(Virtual Event, Canada) (SIGIR \u201921) . Association for Computing Machinery, New\nYork, NY, USA, 1723\u20131727. https://doi.org/10.1145/3404835.3463030",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.12016",
                        "Citation Paper Title": "Title:Learning Passage Impacts for Inverted Indexes",
                        "Citation Paper Abstract": "Abstract:Neural information retrieval systems typically use a cascading pipeline, in which a first-stage model retrieves a candidate set of documents and one or more subsequent stages re-rank this set using contextualized language models such as BERT. In this paper, we propose DeepImpact, a new document term-weighting scheme suitable for efficient retrieval using a standard inverted index. Compared to existing methods, DeepImpact improves impact-score modeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact leverages DocT5Query to enrich the document collection and, using a contextualized language model, directly estimates the semantic importance of tokens in a document, producing a single-value representation for each token in each document. Our experiments show that DeepImpact significantly outperforms prior first-stage retrieval approaches by up to 17% on effectiveness metrics w.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the same effectiveness of state-of-the-art approaches with up to 5.1x speedup in efficiency.",
                        "Citation Paper Authors": "Authors:Antonio Mallia, Omar Khattab, Nicola Tonellotto, Torsten Suel"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ": the\nmodel embeds documents and queries in a sparse high-dimensional\nlatent space by means of \u21131regularization on representations. How-\never, SNRM effectiveness remains limited and its efficiency has been\nquestioned ",
                    "Citation Text": "Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and\nBarnab\u00e1s P\u00f3czos. 2020. Minimizing FLOPs to Learn Efficient Sparse Representa-\ntions. arXiv:2004.05665 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05665",
                        "Citation Paper Title": "Title:Minimizing FLOPs to Learn Efficient Sparse Representations",
                        "Citation Paper Abstract": "Abstract:Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.",
                        "Citation Paper Authors": "Authors:Biswajit Paria, Chih-Kuan Yeh, Ian E.H. Yen, Ning Xu, Pradeep Ravikumar, Barnab\u00e1s P\u00f3czos"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nA first solution to this problem consists in expanding documents\nusing generative approaches such as doc2query ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by Query Prediction. arXiv:1904.08375 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11343v1": {
            "Paper Title": "Towards Explainable Scientific Venue Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "are employed. Enhancements are made by incorporating\npaper titles and keywords and applying convolutions separately to them. Expla-\nnations through the integrated gradients method are added ",
                    "Citation Text": "M. Sundararajan, A. Taly, and Q. Yan. \u201cAxiomatic Attribution for Deep\nNetworks.\u201d In: Proc. of the 34th Int. Conf. on Machine Learning, ICML\n2017. Ed. by D. Precup and Y. W. Teh. Vol. 70. Proceedings of Machine\nLearning Research. PMLR, 2017, pp. 3319\u20133328.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01365",
                        "Citation Paper Title": "Title:Axiomatic Attribution for Deep Networks",
                        "Citation Paper Abstract": "Abstract:We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
                        "Citation Paper Authors": "Authors:Mukund Sundararajan, Ankur Taly, Qiqi Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.09979v1": {
            "Paper Title": "When expertise gone missing: Uncovering the loss of prolific\n  contributors in Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.09795v1": {
            "Paper Title": "The Vision and the Perspective of Digital Tourism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08253v2": {
            "Paper Title": "Condenser: a Pre-training Architecture for Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.14289v2": {
            "Paper Title": "Multi-class Text Classification using BERT-based Active Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13063v1": {
            "Paper Title": "An Automated Multi-Web Platform Voting Framework to Predict Misleading\n  Information Proliferated during COVID-19 Outbreak using Ensemble Method",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "the author reported a set of feature s to distinguish among \nfake news, real news , and satire. The SVMs are also employed  for clickbait detection in ",
                    "Citation Text": "A. Chakraborty, B. Paranjape, S. Kakarla, and N. Ganguly, \u201cStop clickbait: Detecting \nand preventing clickbaits i n online news media, \u201d in 2016 IEEE/ACM International \nConference on Advances in Social Networks Analysis and Mining (ASONAM) , 2016, pp. \n9\u201316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.09786",
                        "Citation Paper Title": "Title:Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media",
                        "Citation Paper Abstract": "Abstract:Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention. To attract the readers to click on an article and subsequently visit the media site, the outlets often come up with catchy headlines accompanying the article links, which lure the readers to click on the link. Such headlines are known as Clickbaits. While these baits may trick the readers into clicking, in the long run, clickbaits usually don't live up to the expectation of the readers, and leave them disappointed.\nIn this work, we attempt to automatically detect clickbaits and then build a browser extension which warns the readers of different media sites about the possibility of being baited by such headlines. The extension also offers each reader an option to block clickbaits she doesn't want to see. Then, using such reader choices, the extension automatically blocks similar clickbaits during her future visits. We run extensive offline and online experiments across multiple media sites and find that the proposed clickbait detection and the personalized blocking approaches perform very well achieving 93% accuracy in detecting and 89% accuracy in blocking clickbaits.",
                        "Citation Paper Authors": "Authors:Abhijnan Chakraborty, Bhargavi Paranjape, Sourya Kakarla, Niloy Ganguly"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.09044v1": {
            "Paper Title": "\"Don't Downvote A\\$\\$\\$\\$\\$\\$s!!\": An Exploration of Reddit's Advice\n  Communities",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "investigated the\nrelationship between user reputation and engagement\non the site. The possibility of transferring results\nacross subreddits is a persistent theme in the literature.\nMost notably, Botzer et al. (2021) ",
                    "Citation Text": "N. Botzer, S. Gu, and T. Weninger, \u201cAnalysis of moral\njudgement on reddit,\u201d /01/19 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.07664",
                        "Citation Paper Title": "Title:Analysis of Moral Judgement on Reddit",
                        "Citation Paper Abstract": "Abstract:Moral outrage has become synonymous with social media in recent years. However, the preponderance of academic analysis on social media websites has focused on hate speech and misinformation. This paper focuses on analyzing moral judgements rendered on social media by capturing the moral judgements that are passed in the subreddit /r/AmITheAsshole on Reddit. Using the labels associated with each judgement we train a classifier that can take a comment and determine whether it judges the user who made the original post to have positive or negative moral valence. Then, we use this classifier to investigate an assortment of website traits surrounding moral judgements in ten other subreddits, including where negative moral users like to post and their posting patterns. Our findings also indicate that posts that are judged in a positive manner will score higher.",
                        "Citation Paper Authors": "Authors:Nicholas Botzer, Shawn Gu, Tim Weninger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.08855v1": {
            "Paper Title": "Feature Engineering for US State Legislative Hearings: Stance,\n  Affiliation, Engagement and Absentees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08805v1": {
            "Paper Title": "BERT-Beta: A Proactive Probabilistic Approach to Text Moderation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08794v1": {
            "Paper Title": "A Comprehensive Overview of Recommender System and Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08091v3": {
            "Paper Title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08589v1": {
            "Paper Title": "Event Flow -- How Events Shaped the Flow of the News, 1950-1995",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00199v2": {
            "Paper Title": "Pattern-based Acquisition of Scientific Entities from Scholarly Article\n  Titles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08235v1": {
            "Paper Title": "Integrating Flowsheet Data in OMOP Common Data Model for Clinical\n  Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08133v1": {
            "Paper Title": "Phrase Retrieval Learns Passage Retrieval, Too",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08059v1": {
            "Paper Title": "FOMO: Topics versus documents in legal eDiscovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07946v1": {
            "Paper Title": "Popularity Bias Is Not Always Evil: Disentangling Benign and Harmful\n  Bias for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ": the basic matrix factorization model with\nBPR loss.\n\u000fMF-IPS ",
                    "Citation Text": "T. Joachims, A. Swaminathan, and T. Schnabel, \u201cUnbiased\nlearning-to-rank with biased feedback,\u201d in Proceedings of the Tenth\nACM International Conference on Web Search and Data Mining , 2017,\npp. 781\u2013789.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.04468",
                        "Citation Paper Title": "Title:Unbiased Learning-to-Rank with Biased Feedback",
                        "Citation Paper Abstract": "Abstract:Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-bias the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.",
                        "Citation Paper Authors": "Authors:Thorsten Joachims, Adith Swaminathan, Tobias Schnabel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11899v3": {
            "Paper Title": "Patent-KG: Patent Knowledge Graph Use for Engineering Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07791v1": {
            "Paper Title": "A Qualitative Evaluation of User Preference for Link-based vs.\n  Text-based Recommendations of Wikipedia Articles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07692v1": {
            "Paper Title": "Evaluating Music Recommendations with Binary Feedback for Multiple\n  Stakeholders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07689v1": {
            "Paper Title": "Quoka Atlas of Scholarly Knowledge Production: An Interactive\n  Sensemaking Tool for Exploring the Outputs of Research Institutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07611v1": {
            "Paper Title": "On-the-Fly Ensemble Pruning in Evolving Data Streams",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07401v1": {
            "Paper Title": "Matching with Transformers in MELT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06583v1": {
            "Paper Title": "A Semantic Indexing Structure for Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06573v1": {
            "Paper Title": "The Impact of User Demographics and Task Types on Cross-App Mobile\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06501v1": {
            "Paper Title": "conSultantBERT: Fine-tuned Siamese Sentence-BERT for Matching Jobs and\n  Job Seekers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06436v1": {
            "Paper Title": "YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement\n  Ranker",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13934v2": {
            "Paper Title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06306v1": {
            "Paper Title": "BERT for Target Apps Selection: Analyzing the Diversity and Performance\n  of BERT in Unified Mobile Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03154v2": {
            "Paper Title": "PEEK: A Large Dataset of Learner Engagement with Educational Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02444v2": {
            "Paper Title": "Top-N Recommendation with Counterfactual User Preference Simulation",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "applies adversarial training\nmethod to enhance the performance of BPR. With the ever pros-\npering of deep neural network, recent years have witnessed the\nsurge of neural recommender models [ 7,8,15,18,32]. For exam-\nple, NeuMF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In Proceedings of the 26th interna-\ntional conference on world wide web . International World Wide Web Conferences\nSteering Committee, 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "improves BPR with a better negative sampler which leverages addi-\ntional data in E-commerce. AMF ",
                    "Citation Text": "Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial\npersonalized ranking for recommendation. In The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval . 355\u2013364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.03908",
                        "Citation Paper Title": "Title:Adversarial Personalized Ranking for Recommendation",
                        "Citation Paper Abstract": "Abstract:Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) --- the most widely used model in recommendation --- as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization.\nTo enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposes a general knowl-\nedge distillation framework to debias the training data. ",
                    "Citation Text": "Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He.\n2020. Bias and Debias in Recommender System: A Survey and Future Directions.\narXiv preprint arXiv:2010.03240 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.03240",
                        "Citation Paper Title": "Title:Bias and Debias in Recommender System: A Survey and Future Directions",
                        "Citation Paper Abstract": "Abstract:While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, etc. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology ``bias'' is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic. The summary of debiasing methods reviewed in this survey can be found at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, Xiangnan He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.06037v1": {
            "Paper Title": "Correcting the User Feedback-Loop Bias for Recommendation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06022v1": {
            "Paper Title": "Graph-based Retrieval for Claim Verification over Cross-Document\n  Evidence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14541v2": {
            "Paper Title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain\n  Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05794v1": {
            "Paper Title": "Building and Evaluating Open-Domain Dialogue Corpora with Clarifying\n  Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08513v2": {
            "Paper Title": "Fast Passage Re-ranking with Contextualized Exact Term Matching and\n  Efficient Passage Expansion",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ", a more robust version of BERT, as the\nencoder. For both methods, we use the model checkpoints provided\nby the authors and the FAISS ",
                    "Citation Text": "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08734",
                        "Citation Paper Title": "Title:Billion-scale similarity search with GPUs",
                        "Citation Paper Abstract": "Abstract:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\nWe propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
                        "Citation Paper Authors": "Authors:Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", as means of very efficient\nneural methods for retrieval. RepBERT uses BERT to encode the\nquery and the passages and is trained with BM25 hard negatives.\nANCE uses RoBERTa ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". Beyond BERT, other gen-\nerative pre-trained LMs such as GPT ",
                    "Citation Text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al .2020. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Gao et al .further showed\nthat BERT trained with localized NCE loss achieves better effec-\ntiveness for document re-ranking ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink Training of BERT\nRerankers in Multi-Stage Retrieval Pipeline. In The 43rd European Conference On\nInformation Retrieval (ECIR) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08751",
                        "Citation Paper Title": "Title:Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline",
                        "Citation Paper Abstract": "Abstract:Pre-trained deep language models~(LM) have advanced the state-of-the-art of text retrieval. Rerankers fine-tuned from deep LM estimates candidate relevance based on rich contextualized matching signals. Meanwhile, deep LMs can also be leveraged to improve search index, building retrievers with better recall. One would expect a straightforward combination of both in a pipeline to have additive performance gain. In this paper, we discover otherwise and that popular reranker cannot fully exploit the improved retrieval result. We, therefore, propose a Localized Contrastive Estimation (LCE) for training rerankers and demonstrate it significantly improves deep two-stage models.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ",\nhave been shown to provide rich contextualized information, deliv-\nering high effectiveness in many NLP [ 1,35,43] and downstream\nretrieval and ranking tasks ",
                    "Citation Text": "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020. Pretrained transformers\nfor text ranking: Bert and beyond. arXiv preprint arXiv:2010.06467 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.06467",
                        "Citation Paper Title": "Title:Pretrained Transformers for Text Ranking: BERT and Beyond",
                        "Citation Paper Abstract": "Abstract:The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.",
                        "Citation Paper Authors": "Authors:Jimmy Lin, Rodrigo Nogueira, Andrew Yates"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.05677v1": {
            "Paper Title": "An Adaptive Boosting Technique to Mitigate Popularity Bias in\n  Recommender System",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ".\nThe IPS estimator\u2019s unbiasedness is desired; nevertheless, this\nfeature is dependent on the true propensity scores which need to\nbe approximated using various approaches. These methods majorly\nsuffers from two problems ",
                    "Citation Text": "Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. 2017. Asymmetric tri-\ntraining for unsupervised domain adaptation. In International Conference on\nMachine Learning . PMLR, 2988\u20132997.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08400",
                        "Citation Paper Title": "Title:Asymmetric Tri-training for Unsupervised Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different this http URL this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.",
                        "Citation Paper Authors": "Authors:Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". As a result, a more effective strategy\nis required.\nFurthermore, all of the above works focus on reducing recom-\nmender system errors without explicitly being fair to non-popular\nitems.Two exceptions to this are [ 6,32]. ",
                    "Citation Text": "Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2021. Connecting user and\nitem perspectives in popularity debiasing for collaborative recommendation.\nInformation Processing & Management 58, 1 (2021), 102387.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04275",
                        "Citation Paper Title": "Title:Connecting User and Item Perspectives in Popularity Debiasing for Collaborative Recommendation",
                        "Citation Paper Abstract": "Abstract:Recommender systems learn from historical users' feedback that is often non-uniformly distributed across items. As a consequence, these systems may end up suggesting popular items more than niche items progressively, even when the latter would be of interest for users. This can hamper several core qualities of the recommended lists (e.g., novelty, coverage, diversity), impacting on the future success of the underlying platform itself. In this paper, we formalize two novel metrics that quantify how much a recommender system equally treats items along the popularity tail. The first one encourages equal probability of being recommended across items, while the second one encourages true positive rates for items to be equal. We characterize the recommendations of representative algorithms by means of the proposed metrics, and we show that the item probability of being recommended and the item true positive rate are biased against the item popularity. To promote a more equal treatment of items along the popularity tail, we propose an in-processing approach aimed at minimizing the biased correlation between user-item relevance and item popularity. Extensive experiments show that, with small losses in accuracy, our popularity-mitigation approach leads to important gains in beyond-accuracy recommendation quality.",
                        "Citation Paper Authors": "Authors:Ludovico Boratto, Gianni Fenu, Mirko Marras"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.02735v2": {
            "Paper Title": "Fairness of Exposure in Stochastic Bandits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05516v1": {
            "Paper Title": "An Improved Hybrid Recommender System: Integrating Document\n  Context-Based and Behavior-Based Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.00919v3": {
            "Paper Title": "Fast-adapting and Privacy-preserving Federated Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12289v2": {
            "Paper Title": "MedLatinEpi and MedLatinLit: Two Datasets for the Computational\n  Authorship Analysis of Medieval Latin Texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05236v1": {
            "Paper Title": "Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework\n  for Model Training and Online Serving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06736v1": {
            "Paper Title": "Sequential Modelling with Applications to Music Recommendation,\n  Fact-Checking, and Speed Reading",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05125v1": {
            "Paper Title": "MURAL: Multimodal, Multitask Retrieval Across Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07814v2": {
            "Paper Title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12800v2": {
            "Paper Title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04716v1": {
            "Paper Title": "You Get What You Chat: Using Conversations to Personalize Search-based\n  Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04713v1": {
            "Paper Title": "Personalized Entity Search by Sparse and Scrutable User Profiles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04467v1": {
            "Paper Title": "Mining Points of Interest via Address Embeddings: An Unsupervised\n  Approach",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "forclusters obtained through the leader clustering algorithm with cosine similarity metric over Word2Vec based ",
                    "Citation Text": "Tom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Je\uffffrey Dean. 2013. E\uffffcient Estimation of Word Representations in Vector Space. In1stInternational Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.04432v1": {
            "Paper Title": "Detecting and Mitigating Test-time Failure Risks via Model-agnostic\n  Uncertainty Learning",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". For\nLR, the con\ufb01dence score is given by the distance from the\ndecision boundary (LR-Con\ufb01dence).\nOur main comparison is with the state-of-the-art method\nTrust Score ",
                    "Citation Text": "H. Jiang, B. Kim, M. Y Guan, and M. R Gupta. 2018. To Trust Or Not\nTo Trust A Classi\ufb01er.. In NeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11783",
                        "Citation Paper Title": "Title:To Trust Or Not To Trust A Classifier",
                        "Citation Paper Abstract": "Abstract:Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",
                        "Citation Paper Authors": "Authors:Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "consists of 60K grayscale\nimages of handwritten digits in 10 classes. Due to the\nvariability in writing style, certain images are prone to\nmisclassi\ufb01cation.\n\u000fFashion MNIST: The fashion MNIST dataset ",
                    "Citation Text": "H. Xiao, K. Rasul, and R. V ollgraf. 2017. Fashion-mnist: a novel image\ndataset for benchmarking machine learning algorithms. arXiv preprint\narXiv:1708.07747 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07747",
                        "Citation Paper Title": "Title:Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Han Xiao, Kashif Rasul, Roland Vollgraf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.04312v1": {
            "Paper Title": "MATE: Multi-view Attention for Table Transformer Efficiency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03540v2": {
            "Paper Title": "A Survey of Deep Reinforcement Learning in Recommender Systems: A\n  Systematic Review and Future Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00968v2": {
            "Paper Title": "Self-supervised Representation Learning for Trip Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03798v1": {
            "Paper Title": "AppQ: Warm-starting App Recommendation Based on View Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03538v1": {
            "Paper Title": "Tracing Affordance and Item Adoption on Music Streaming Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03502v1": {
            "Paper Title": "R2-D2: A Modular Baseline for Open-Domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03821v1": {
            "Paper Title": "Recommend for a Reason: Unlocking the Power of Unsupervised\n  Aspect-Sentiment Co-Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12822v3": {
            "Paper Title": "Recommending Burgers based on Pizza Preferences: Addressing Data\n  Sparsity with a Product of Experts",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "There has been a large amount of research dedicated to the usage of Variational Autoencoders in the setting of single-\ndomain recommender systems [ 9,11,15,16]. Our work is a direct extension of ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of\nthe 2018 world wide web conference . 689\u2013698.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". MVAE approximates the joint posterior distribution as Product of Experts over the marginal\nposteriors. This enables cross-modal generation at inference time without the need of additional inference networks. ",
                    "Citation Text": "Yuge Shi, Narayanaswamy Siddharth, Brooks Paige, and Philip HS Torr. 2019. Variational mixture-of-experts autoencoders for multi-modal deep\ngenerative models. arXiv preprint arXiv:1911.03393 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.03393",
                        "Citation Paper Title": "Title:Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models",
                        "Citation Paper Abstract": "Abstract:Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfillment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multimodal variational autoencoder (MMVAE) to learn generative models on different sets of modalities, including a challenging image-language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively.",
                        "Citation Paper Authors": "Authors:Yuge Shi, N. Siddharth, Brooks Paige, Philip H.S. Torr"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.02874v1": {
            "Paper Title": "DeepFakes: Detecting Forged and Synthetic Media Content Using Machine\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02867v1": {
            "Paper Title": "Refining BERT Embeddings for Document Hashing via Mutual Information\n  Maximization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.09475v1": {
            "Paper Title": "Knowledge Graph Question Answering via SPARQL Silhouette Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02202v1": {
            "Paper Title": "Fairness via AI: Bias Reduction in Medical Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08978v3": {
            "Paper Title": "Modeling Online Behavior in Recommender Systems: The Importance of\n  Temporal Context",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "is used, taking\nthe top 100 dimensions.\n1https://grouplens.org/datasets/movielens/20m/.\n2https://cseweb.ucsd.edu/~jmcauley/datasets.html.\n3https://www.kaggle.com/netflix-inc/netflix-prize-data.\n4https://github.com/swisscom/ai-research-mamo-framework.\n5https://pytorch.org/docs/stable/generated/torch.svd.html.NCF. The Neural Collaborative Filtering ",
                    "Citation Text": "X. He, L. Liao, H. Zhang, L. Nie, X. Hu, T.-S. Chua, Neural collaborative filtering, in:\nProceedings of the 26th international conference on world wide web, 2017, pp. 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ".\nWe underline that this is a naive function which we found to best approximate user in-\nteractions observed in-house. Works such as ",
                    "Citation Text": "D. Kowald, S. C. Pujari, E. Lex, Temporal effects on hashtag reuse in twitter: A cognitive-\ninspired hashtag recommendation approach, in: Proceedings of the 26th International\nConference on World Wide Web, 2017, pp. 1401\u20131410.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.01276",
                        "Citation Paper Title": "Title:Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired Hashtag Recommendation Approach",
                        "Citation Paper Abstract": "Abstract:Hashtags have become a powerful tool in social platforms such as Twitter to categorize and search for content, and to spread short messages across members of the social network. In this paper, we study temporal hashtag usage practices in Twitter with the aim of designing a cognitive-inspired hashtag recommendation algorithm we call BLLi,s. Our main idea is to incorporate the effect of time on (i) individual hashtag reuse (i.e., reusing own hashtags), and (ii) social hashtag reuse (i.e., reusing hashtags, which has been previously used by a followee) into a predictive model. For this, we turn to the Base-Level Learning (BLL) equation from the cognitive architecture ACT-R, which accounts for the time-dependent decay of item exposure in human memory. We validate BLLi,s using two crawled Twitter datasets in two evaluation scenarios: firstly, only temporal usage patterns of past hashtag assignments are utilized and secondly, these patterns are combined with a content-based analysis of the current tweet. In both scenarios, we find not only that temporal effects play an important role for both individual and social hashtag reuse but also that BLLi,s provides significantly better prediction accuracy and ranking results than current state-of-the-art hashtag recommendation methods.",
                        "Citation Paper Authors": "Authors:Dominik Kowald, Subhash Pujari, Elisabeth Lex"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13875v2": {
            "Paper Title": "When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01815v1": {
            "Paper Title": "Representation Learning for Efficient and Effective Similarity Search\n  and Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01732v1": {
            "Paper Title": "Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to\n  the Visual Layouts of Multi-Ethnic Periodicals",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ", annotated with\nbounding boxes of visual content derived from the Newspaper Navigator dataset ",
                    "Citation Text": "B. C. G. Lee, J. Mears, E. Jakeway, M. Ferriter, C. Adams, N. Yarasavage, D. Thomas,\nK. Zwaard, D. S. Weld, The Newspaper Navigator Dataset: Extracting Headlines and Visual\nContent from 16 Million Historic Newspaper Pages in Chronicling America, Association\nfor Computing Machinery, New York, NY, USA, 2020, p. 3055\u20133062. URL: https://doi-org.\noffcampus.lib.washington.edu/10.1145/3340531.3412767.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.01583",
                        "Citation Paper Title": "Title:The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America",
                        "Citation Paper Abstract": "Abstract:Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.",
                        "Citation Paper Authors": "Authors:Benjamin Charles Germain Lee, Jaime Mears, Eileen Jakeway, Meghan Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, Daniel S. Weld"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.01628v1": {
            "Paper Title": "Cross-Lingual Training with Dense Retrieval for Document Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10333v2": {
            "Paper Title": "CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for\n  Conversational Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01274v1": {
            "Paper Title": "UserBERT: Contrastive User Model Pre-training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01084v1": {
            "Paper Title": "Text Classification for Predicting Multi-level Product Categories",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", i.e., by averaging the WAF1 for\nthe category and the subcategory.\nParameter Settings. For the implementation of the BiLSTM and pretrained\nlanguage models, we use the Tensor\row and Transformers ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u0013 emi Louf,\nMorgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\nTransformers: State-of-the-art natural language processing. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , pages 38{45, Online, October 2020.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "became increasingly popular. More recently, advanced NLP models such as\nBERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", we use word embeddings as the numeric\nrepresentation, and apply a weighted average of the vocabularies given a\nproduct title, where weights are the TF-IDF of each word ",
                    "Citation Text": "Hadi Jahanshahi, Syed Kazmi, and Mucahit Cevik. Auto response gen-\neration in online medical chat services. arXiv preprint arXiv:2104.12755 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.12755",
                        "Citation Paper Title": "Title:Auto Response Generation in Online Medical Chat Services",
                        "Citation Paper Abstract": "Abstract:Telehealth helps to facilitate access to medical professionals by enabling remote medical services for the patients. These services have become gradually popular over the years with the advent of necessary technological infrastructure. The benefits of telehealth have been even more apparent since the beginning of the COVID-19 crisis, as people have become less inclined to visit doctors in person during the pandemic. In this paper, we focus on facilitating the chat sessions between a doctor and a patient. We note that the quality and efficiency of the chat experience can be critical as the demand for telehealth services increases. Accordingly, we develop a smart auto-response generation mechanism for medical conversations that helps doctors respond to consultation requests efficiently, particularly during busy sessions. We explore over 900,000 anonymous, historical online messages between doctors and patients collected over nine months. We implement clustering algorithms to identify the most frequent responses by doctors and manually label the data accordingly. We then train machine learning algorithms using this preprocessed data to generate the responses. The considered algorithm has two steps: a filtering (i.e., triggering) model to filter out infeasible patient messages and a response generator to suggest the top-3 doctor responses for the ones that successfully pass the triggering phase. The method provides an accuracy of 83.28\\% for precision@3 and shows robustness to its parameters.",
                        "Citation Paper Authors": "Authors:Hadi Jahanshahi, Syed Kazmi, Mucahit Cevik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00982v1": {
            "Paper Title": "How does the User's Knowledge of the Recommender Influence their\n  Behavior?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00978v1": {
            "Paper Title": "Accurate shape and phase averaging of time series through Dynamic Time\n  Warping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.12599v2": {
            "Paper Title": "Smoothing Dialogue States for Open Conversational Machine Reading",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.11483v2": {
            "Paper Title": "Investigating Dissemination of Scientific Information on Twitter: A\n  Study of Topic Networks in Opioid Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00656v1": {
            "Paper Title": "Developing Products Update-Alert System for e-Commerce Websites Users\n  Using HTML Data and Web Scraping Technique",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00635v1": {
            "Paper Title": "Selecting Optimal Trace Clustering Pipelines with AutoML",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04695v3": {
            "Paper Title": "Momentum-based Gradient Methods in Multi-Objective Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "showed promising results. Later, the Varia-\ntional Autoencoders for Collaborative Filtering ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of\nthe 2018 World Wide Web Conference . 689\u2013698.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". To alleviate the inaccuracies, an additional condition is presented for the\ndescent direction. ",
                    "Citation Text": "Nikola Milojkovic, Diego Antognini, Giancarlo Bergamin, Boi Faltings, and Claudiu Musat. 2020. Multi-Gradient Descent for Multi-Objective\nRecommender Systems. Proceedings of the AAAI (2020) - Workshop on Interactive and Conversational Recommendation Systems (WICRS) (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.00846",
                        "Citation Paper Title": "Title:Multi-Gradient Descent for Multi-Objective Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems need to mirror the complexity of the environment they are applied in. The more we know about what might benefit the user, the more objectives the recommender system has. In addition there may be multiple stakeholders - sellers, buyers, shareholders - in addition to legal and ethical constraints. Simultaneously optimizing for a multitude of objectives, correlated and not correlated, having the same scale or not, has proven difficult so far.\nWe introduce a stochastic multi-gradient descent approach to recommender systems (MGDRec) to solve this problem. We show that this exceeds state-of-the-art methods in traditional objective mixtures, like revenue and recall. Not only that, but through gradient normalization we can combine fundamentally different objectives, having diverse scales, into a single coherent framework. We show that uncorrelated objectives, like the proportion of quality products, can be improved alongside accuracy. Through the use of stochasticity, we avoid the pitfalls of calculating full gradients and provide a clear setting for its applicability.",
                        "Citation Paper Authors": "Authors:Nikola Milojkovic, Diego Antognini, Giancarlo Bergamin, Boi Faltings, Claudiu Musat"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "With the advances of neural approaches in other fields, they also found their way into recommendation systems.\nFirst, the introduction of Neural network-based Collaborative Filtering ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th\ninternational conference on world wide web . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00450v1": {
            "Paper Title": "Hypergraph-of-Entity: A General Model for Entity-Oriented Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08809v3": {
            "Paper Title": "SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00253v1": {
            "Paper Title": "Aligning Cross-lingual Sentence Representations with Dual Momentum\n  Contrast",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00217v1": {
            "Paper Title": "Multi-Sample based Contrastive Loss for Top-k Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ": LightGCN is the state-of-\nthe-art GCN based collaborative \ufb01ltering model, and sLight-\nGCN is a variant. They are described in detail in Section III.\nMult-V AE ",
                    "Citation Text": "Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony\nJebara. Variational autoencoders for collaborative \ufb01ltering. In WWW ,\npages 689\u2013698. ACM, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "are the competing baselines with GCN\nof top-k recommendation recently which having shown to\noutperform several methods including GC-MC ",
                    "Citation Text": "Rianne van den Berg, Thomas N. Kipf, and Max Welling. Graph\nconvolutional matrix completion. CoRR , abs/1706.02263, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "is proposed by maximizing a lower\nbound on mutual information based on Noise-Contrastive\nEstimation. Non-Parametric Softmax Classi\ufb01er ",
                    "Citation Text": "Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised\nfeature learning via non-parametric instance discrimination. In CVPR ,\npages 3733\u20133742. IEEE Computer Society, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01978",
                        "Citation Paper Title": "Title:Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination",
                        "Citation Paper Abstract": "Abstract:Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.",
                        "Citation Paper Authors": "Authors:Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "is proposed from a deep metric learning\nperspective, which greatly improves the triplet loss by jointly\npushing out multiple negative samples at each update. In-\nfoNCE loss ",
                    "Citation Text": "A \u00a8aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation\nlearning with contrastive predictive coding. CoRR , abs/1807.03748,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "loss aims\nto maximize the distance between positive pair and negative\npair, which is proposed for the ranking task and widely used in\nthe top-k recommendation. Triplet loss ",
                    "Citation Text": "Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A\nuni\ufb01ed embedding for face recognition and clustering. In CVPR , pages\n815\u2013823, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03832",
                        "Citation Paper Title": "Title:FaceNet: A Unified Embedding for Face Recognition and Clustering",
                        "Citation Paper Abstract": "Abstract:Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.\nOur method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.\nOn the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets.\nWe also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.",
                        "Citation Paper Authors": "Authors:Florian Schroff, Dmitry Kalenichenko, James Philbin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00177v1": {
            "Paper Title": "Problem Learning: Towards the Free Will of Machines",
            "Sentences": [
                {
                    "Sentence ID": 103,
                    "Sentence": ", as well as model agnostic explanation methods such as counterfactual explanation ",
                    "Citation Text": "Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang. Counterfactual explainable\nrecommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge\nManagement , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10539",
                        "Citation Paper Title": "Title:Counterfactual Explainable Recommendation",
                        "Citation Paper Abstract": "Abstract:By providing explanations for users and system designers to facilitate better understanding and decision making, explainable recommendation has been an important research problem. In this paper, we propose Counterfactual Explainable Recommendation (CountER), which takes the insights of counterfactual reasoning from causal inference for explainable recommendation. CountER is able to formulate the complexity and the strength of explanations, and it adopts a counterfactual learning framework to seek simple (low complexity) and effective (high strength) explanations for the model decision. Technically, for each item recommended to each user, CountER formulates a joint optimization problem to generate minimal changes on the item aspects so as to create a counterfactual item, such that the recommendation decision on the counterfactual item is reversed. These altered aspects constitute the explanation of why the original item is recommended. The counterfactual explanation helps both the users for better understanding and the system designers for better model debugging. Another contribution of the work is the evaluation of explainable recommendation, which has been a challenging task. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. To measure the explanation quality, we design two types of evaluation metrics, one from user's perspective (i.e. why the user likes the item), and the other from model's perspective (i.e. why the item is recommended by the model). We apply our counterfactual learning algorithm on a black-box recommender system and evaluate the generated explanations on five real-world datasets. Results show that our model generates more accurate and effective explanations than state-of-the-art explainable recommendation models.",
                        "Citation Paper Authors": "Authors:Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, Yongfeng Zhang"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "proposed a learning-based\n4discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions\nof images. Besides, recent advances on causal machine learning has made it possible for learning to evaluate AI systems\nbased on counterfactual reasoning ",
                    "Citation Text": "Yingqiang Ge, Zelong Li, Shuyuan Xu, Shuchang Liu, Yunqi Li, Juntao Tan, Shijie Geng, Fei Sun, and Yongfeng\nZhang. Counterfactual evaluation for explainable ai. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01962",
                        "Citation Paper Title": "Title:Counterfactual Evaluation for Explainable AI",
                        "Citation Paper Abstract": "Abstract:While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \\textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \\textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",
                        "Citation Paper Authors": "Authors:Yingqiang Ge, Shuchang Liu, Zelong Li, Shuyuan Xu, Shijie Geng, Yunqi Li, Juntao Tan, Fei Sun, Yongfeng Zhang"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ", researchers\nsometimes also design tailored metrics for speci\ufb01c tasks. For example, to evaluate image captioning, the semantic\npropositional image caption evaluation (SPICE) metric ",
                    "Citation Text": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image\ncaption evaluation. In European conference on computer vision , pages 382\u2013398. Springer, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08822",
                        "Citation Paper Title": "Title:SPICE: Semantic Propositional Image Caption Evaluation",
                        "Citation Paper Abstract": "Abstract:There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as `which caption-generator best understands colors?' and `can caption-generators count?'",
                        "Citation Paper Authors": "Authors:Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". As a result, researchers have been trying to make machines automatically learn good optimization\nalgorithms, for example, using learned rather than designed gradients for parameter updating based on LSTM ",
                    "Citation Text": "Marcin Andrychowicz, Misha Denil, Sergio G\u00f3mez Colmenarejo, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In\nProceedings of the 30th International Conference on Neural Information Processing Systems , pages 3988\u20133996,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.04474",
                        "Citation Paper Title": "Title:Learning to learn by gradient descent by gradient descent",
                        "Citation Paper Abstract": "Abstract:The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.",
                        "Citation Paper Authors": "Authors:Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ", and ranking problems can be formulated into a pair-wise ranking loss ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized\nranking from implicit feedback. Uncertainty in Arti\ufb01cial Intelligence , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00165v1": {
            "Paper Title": "An Unsupervised Method for Building Sentence Simplification Corpora in\n  Multiple Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01165v1": {
            "Paper Title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model\n  Extraction",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "investigate data poisoning in the neural collaborative filtering\nframework (NCF) ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th\ninternational conference on world wide web . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "poisons co-visitation-based recommenders. Recently, as deep learning\nhas been widely applied to recommendation, ",
                    "Citation Text": "Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, and Mingwei Xu. 2021. Data Poisoning Attacks to Deep Learning Based Recommender\nSystems. arXiv preprint arXiv:2101.02644 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.02644",
                        "Citation Paper Title": "Title:Data Poisoning Attacks to Deep Learning Based Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.\nIn this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",
                        "Citation Paper Authors": "Authors:Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, Mingwei Xu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Even though attacks against BERT-based models\nexist for NLP, where authors find random word sequences and a surrogate dataset (e.g. WikiText-103 ",
                    "Citation Text": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.07843",
                        "Citation Paper Title": "Title:Pointer Sentinel Mixture Models",
                        "Citation Paper Abstract": "Abstract:Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
                        "Citation Paper Authors": "Authors:Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "to generate synthetic samples, which are subsequently labeled by the target model. MAZE ",
                    "Citation Text": "Sanjay Kariyappa, Atul Prakash, and Moinuddin Qureshi. 2020. MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation.\narXiv preprint arXiv:2005.03161 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03161",
                        "Citation Paper Title": "Title:MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
                        "Citation Paper Abstract": "Abstract:Model Stealing (MS) attacks allow an adversary with black-box access to a Machine Learning model to replicate its functionality, compromising the confidentiality of the model. Such attacks train a clone model by using the predictions of the target model for different inputs. The effectiveness of such attacks relies heavily on the availability of data necessary to query the target model. Existing attacks either assume partial access to the dataset of the target model or availability of an alternate dataset with semantic similarities. This paper proposes MAZE -- a data-free model stealing attack using zeroth-order gradient estimation. In contrast to prior works, MAZE does not require any data and instead creates synthetic data using a generative model. Inspired by recent works in data-free Knowledge Distillation (KD), we train the generative model using a disagreement objective to produce inputs that maximize disagreement between the clone and the target model. However, unlike the white-box setting of KD, where the gradient information is available, training a generator for model stealing requires performing black-box optimization, as it involves accessing the target model under attack. MAZE relies on zeroth-order gradient estimation to perform this optimization and enables a highly accurate MS attack. Our evaluation with four datasets shows that MAZE provides a normalized clone accuracy in the range of 0.91x to 0.99x, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13x to 0.69x) and surrogate data (KnockoffNets, clone accuracy 0.52x to 0.97x). We also study an extension of MAZE in the partial-data setting and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97x to 1.0x) and reduces the query required for the attack by 2x-24x.",
                        "Citation Paper Authors": "Authors:Sanjay Kariyappa, Atul Prakash, Moinuddin Qureshi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05716v2": {
            "Paper Title": "Auctus: A Dataset Search Engine for Data Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13892v1": {
            "Paper Title": "Like Article, Like Audience: Enforcing Multimodal Correlations for\n  Disinformation Detection",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "), which is the concatenated last hidden state\nof the bidirectional GRU.\n\u2022DistilBERT ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "and the GloVe 6B\nvocabulary of 400k unique tokens with 300-dimensional word embedding pretrained\non Wikipedia 2014 and Gigaword 5 (uncased, 6 billion tokens). DistilBERT: We adopt\nthe pretrained DistilBERT model and tokenizer (\u2018distilbert-base-uncased\u2019) from the\nHuggingface Transformers library ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational\nLinguistics, Online, 38\u201345. https://www.aclweb.org/anthology/2020.emnlp-\ndemos.6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13824v1": {
            "Paper Title": "Aligning Hotel Embeddings using Domain Adaptation for Next-Item\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ".\nFinally, we would like to explore adversarial cross-domain adap-\ntation for aligning the embedding spaces ",
                    "Citation Text": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-Adversarial Training of Neural Networks. J. Mach. Learn. Res. 17, 1 (Jan.\n2016), 2096\u20132030.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.07818",
                        "Citation Paper Title": "Title:Domain-Adversarial Training of Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
                        "Citation Paper Authors": "Authors:Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "or try\nto capture different aspects of the product from different sources\n(clickstream data, text and images) ",
                    "Citation Text": "Loveperteek Singh, Shreya Singh, Sagar Arora, and Sumit Borar. 2019. One\nEmbedding To Do Them All. arXiv preprint arXiv:1906.12120 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.12120",
                        "Citation Paper Title": "Title:One Embedding To Do Them All",
                        "Citation Paper Abstract": "Abstract:Online shopping caters to the needs of millions of users daily. Search, recommendations, personalization have become essential building blocks for serving customer needs. Efficacy of such systems is dependent on a thorough understanding of products and their representation. Multiple information sources and data types provide a complete picture of the product on the platform. While each of these tasks shares some common characteristics, typically product embeddings are trained and used in isolation.\nIn this paper, we propose a framework to combine multiple data sources and learn unified embeddings for products on our e-commerce platform. Our product embeddings are built from three types of data sources - catalog text data, a user's clickstream session data and product images. We use various techniques like denoising auto-encoders for text, Bayesian personalized ranking (BPR) for clickstream data, Siamese neural network architecture for image data and combined ensemble over the above methods for unified embeddings. Further, we compare and analyze the performance of these embeddings across three unrelated real-world e-commerce tasks specifically checking product attribute coverage, finding similar products and predicting returns. We show that unified product embeddings perform uniformly well across all these tasks.",
                        "Citation Paper Authors": "Authors:Loveperteek Singh, Shreya Singh, Sagar Arora, Sumit Borar"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "the authors propose to learn\nembeddings for YouTube videos by combining multiple features,\nwhich are used for candidate generation and ranking. Other ap-\nproaches have also been proposed that include metadata ",
                    "Citation Text": "Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-prod2vec:\nProduct embeddings using side-information for recommendation. In Proceedings\nof the 10th ACM Conference on Recommender Systems . ACM, 225\u2013232.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.07326",
                        "Citation Paper Title": "Title:Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation",
                        "Citation Paper Abstract": "Abstract:We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is in- jected into the model as side information to regularize the item embeddings. We show that the new item representa- tions lead to better performance on recommendation tasks on an open music dataset.",
                        "Citation Paper Authors": "Authors:Flavian Vasile, Elena Smirnova, Alexis Conneau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.12189v2": {
            "Paper Title": "Query-Focused Extractive Summarisation for Finding Ideal Answers to\n  Biomedical and COVID-19 Questions",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". We used huggingface\u2019s\nmodel \u201cmonologg/biobert_v1.1_pubmed\u201d.\nDistilBERT DistilBERT\u2019s architecture is a reduced version of BERT, which has been trained\nto replicate the soft predictions made by BERT ",
                    "Citation Text": "V. Sanh, L. Debut, J. Chaumond, T. Wolf, DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter, in: 33rd Conference on Neural Information Processing Systems\n(NeurIPS 2019), 2019. arXiv:1910.01108 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "reports the success of BERT architectures for various tasks, by simply adding\na task-specific layer and fine-tuning the system ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional\ntransformers for language understanding, in: Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), Association for Computational\nLinguistics, Minneapolis, Minnesota, 2019, pp. 4171\u20134186. URL: https://www.aclweb.org/\nanthology/N19-1423. doi: 10.18653/v1/N19-1423 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13454v1": {
            "Paper Title": "Improving Query Representations for Dense Retrieval with Pseudo\n  Relevance Feedback",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ",RM3 [14,19], a classical PRF frame-\nwork in sparse retrieval. We also compare with several recent\ndense retrievers. ME-BERT ",
                    "Citation Text": "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse,\nDense, and Attentional Representations for Text Retrieval. Trans. Assoc. Comput.\nLinguistics 9 (2021), 329\u2013345.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00181",
                        "Citation Paper Title": "Title:Sparse, Dense, and Attentional Representations for Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
                        "Citation Paper Authors": "Authors:Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "updated hard negatives index, constructing hard\nnegatives using document index from an existing dense retrieval\nmodel ",
                    "Citation Text": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. Learning\nTo Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently.\narXiv preprint arXiv:2010.10469 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10469",
                        "Citation Paper Title": "Title:Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently",
                        "Citation Paper Abstract": "Abstract:Ranking has always been one of the top concerns in information retrieval research. For decades, lexical matching signal has dominated the ad-hoc retrieval process, but it also has inherent defects, such as the vocabulary mismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to alleviate these limitations by capturing the deep semantic relationship between queries and documents. The training of most existing Dense Retrieval models relies on sampling negative instances from the corpus to optimize a pairwise loss function. Through investigation, we find that this kind of training strategy is biased and fails to optimize full retrieval performance effectively and efficiently. To solve this problem, we propose a Learning To Retrieve (LTRe) training technique. LTRe constructs the document index beforehand. At each training iteration, it performs full retrieval without negative sampling and then updates the query representation model parameters. Through this process, it teaches the DR model how to retrieve relevant documents from the entire corpus instead of how to rerank a potentially biased sample of documents. Experiments in both passage retrieval and document retrieval tasks show that: 1) in terms of effectiveness, LTRe significantly outperforms all competitive sparse and dense baselines. It even gains better performance than the BM25-BERT cascade system under reasonable latency constraints. 2) in terms of training efficiency, compared with the previous state-of-the-art DR method, LTRe provides more than 170x speed-up in the training process. Training with a compressed index further saves computing resources with minor performance loss.",
                        "Citation Paper Authors": "Authors:Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", modify query-\ndocument interaction using encoded feedback documents [ 1,3],\nor learn contextualized query-document interactions ",
                    "Citation Text": "HongChien Yu, Zhuyun Dai, and Jamie Callan. 2021. PGT: Pseudo Relevance\nFeedback Using a Graph-Based Transformer. In Advances in Information Retrieval\n- 43rd European Conference on IR Research (Lecture Notes in Computer Science,\nVol. 12657) . Springer, 440\u2013447.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.07918",
                        "Citation Paper Title": "Title:PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer",
                        "Citation Paper Abstract": "Abstract:Most research on pseudo relevance feedback (PRF) has been done in vector space and probabilistic retrieval models. This paper shows that Transformer-based rerankers can also benefit from the extra context that PRF provides. It presents PGT, a graph-based Transformer that sparsifies attention between graph nodes to enable PRF while avoiding the high computational complexity of most Transformer architectures. Experiments show that PGT improves upon non-PRF Transformer reranker, and it is at least as accurate as Transformer PRF models that use full attention, but with lower computational costs.",
                        "Citation Paper Authors": "Authors:HongChien Yu, Zhuyun Dai, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nIn recent research, PRFinformation has been leveraged by neural\nnetworks to combine feedback relevance scores ",
                    "Citation Text": "Canjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, Andrew Yates, Le Sun, and\nJungang Xu. 2018. NPRF: A Neural Pseudo Relevance Feedback Framework\nfor Ad-hoc Information Retrieval. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing . Association for Computational\nLinguistics, 4482\u20134491.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.12936",
                        "Citation Paper Title": "Title:NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval",
                        "Citation Paper Abstract": "Abstract:Pseudo-relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mismatches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural architectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.",
                        "Citation Paper Authors": "Authors:Canjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, Andrew Yates, Le Sun, Jungang Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07316v2": {
            "Paper Title": "Exploiting Sentence-Level Representations for Passage Ranking",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "embeddings to the shared LSTM and use a dropout\nrate of 0.5.\n2.K-NRM ",
                    "Citation Text": "C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc ranking with kernel\npooling, in: Proceedings of the 40th International ACM SIGIR conference on research and\ndevelopment in information retrieval, ACM, 2017, pp. 55\u201364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06613",
                        "Citation Paper Title": "Title:End-to-End Neural Ad-hoc Ranking with Kernel Pooling",
                        "Citation Paper Abstract": "Abstract:This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.",
                        "Citation Paper Authors": "Authors:Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "aim to improve the answer selection process by\nfiltering out noisy, irrelevant paragraphs. Afterwards, the answer is selected from the remaining,\nrelevant paragraphs. Some works have used evidence aggregation to re-rank passages based\non information from multiple other passages ",
                    "Citation Text": "S. Wang, M. Yu, J. Jiang, W. Zhang, X. Guo, S. Chang, Z. Wang, T. Klinger, G. Tesauro,\nM. Campbell, Evidence aggregation for answer re-ranking in open-domain question\nanswering, in: International Conference on Learning Representations, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05116",
                        "Citation Paper Title": "Title:Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:A popular recent approach to answering open-domain questions is to first search for question-related passages and then apply reading comprehension models to extract answers. Existing methods usually extract answers from single passages independently. But some questions require a combination of evidence from across different sources to answer correctly. In this paper, we propose two models which make use of multiple passages to generate their answers. Both use an answer-reranking approach which reorders the answer candidates generated by an existing state-of-the-art QA model. We propose two methods, namely, strength-based re-ranking and coverage-based re-ranking, to make use of the aggregated evidence from different passages to better determine the answer. Our models have achieved state-of-the-art results on three public open-domain QA datasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8 percentage points of improvement over the former two datasets.",
                        "Citation Paper Authors": "Authors:Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, Murray Campbell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.14463v3": {
            "Paper Title": "RadGraph: Extracting Clinical Entities and Relations from Radiology\n  Reports",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nBiomedical pretraining For each of our approaches, in addition to using BERT weight initial-\nizations, we use weight initializations from four different biomedical pretrained models, which are\nBioBERT ",
                    "Citation Text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo\nKang. BioBERT: a pre-trained biomedical language representation model for biomedical text mining.\nBioinformatics , 36(4):1234\u20131240, 09 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08746",
                        "Citation Paper Title": "Title:BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
                        "Citation Paper Abstract": "Abstract:Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at this https URL, and the source code for fine-tuning BioBERT available at this https URL.",
                        "Citation Paper Authors": "Authors:Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Various natural language processing (NLP) approaches have been developed and used to extract\ninformation from radiology reports. One approach uses automated radiology report labelers [ 2\u20136]\nto label reports within large chest radiograph datasets, such as MIMIC-CXR ",
                    "Citation Text": "Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,\nChih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identi\ufb01ed publicly available database\nof chest radiographs with free-text reports. Scienti\ufb01c data , 6(1):1\u20138, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.07042",
                        "Citation Paper Title": "Title:MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs",
                        "Citation Paper Abstract": "Abstract:Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.",
                        "Citation Paper Authors": "Authors:Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, Steven Horng"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "is a multi-institutional database of\nradiology reports that contains entity-level annotations. PadChest ",
                    "Citation Text": "Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vay\u00e1. Padchest: A large\nchest x-ray image dataset with multi-label annotated reports. Medical image analysis , 66:101797, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.07441",
                        "Citation Paper Title": "Title:PadChest: A large chest x-ray image dataset with multi-label annotated reports",
                        "Citation Paper Abstract": "Abstract:We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray database suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, Maria de la Iglesia-Vay\u00e1"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.12898v1": {
            "Paper Title": "Generating Answer Candidates for Quizzes and Answer-Aware Question\n  Generators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.12752v1": {
            "Paper Title": "TAR on Social Media: A Framework for Online Content Moderation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.12519v1": {
            "Paper Title": "Predicting the Factuality of Reporting of News Media Using Observations\n  About User Attention in Their YouTube Channels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10715v2": {
            "Paper Title": "Investigating Software Usage in the Social Sciences: A Knowledge Graph\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.11948v1": {
            "Paper Title": "SAUCE: Truncated Sparse Document Signature Bit-Vectors for Fast\n  Web-Scale Corpus Expansion",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ".\n\u2022RoBERTa pre-trained language model, in particular, a dis-\ntilled version of the RoBERTa model ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In\nWorkshop on Energy Efficient Machine Learning and Cognitive Computing at\nNeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". Recent works\nrepresent text with dense vector representations extracted from lan-\nguage models that capture term co-occurrence statistics, and com-\npute document similarity with pairwise distance metrics such as\ncosine ",
                    "Citation Text": "Christophe Van Gysel, Maarten De Rijke, and Evangelos Kanoulas. 2018. Neu-\nral vector spaces for unsupervised information retrieval. ACM Transactions on\nInformation Systems (TOIS) 36, 4 (2018), 1\u201325.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.02702",
                        "Citation Paper Title": "Title:Neural Vector Spaces for Unsupervised Information Retrieval",
                        "Citation Paper Abstract": "Abstract:We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed.\nNVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.",
                        "Citation Paper Authors": "Authors:Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". Dominant approaches involve cascade archi-\ntectures, where standard bag-of-words term-matching techniques\nare first employed towards creating the initial list of candidate docu-\nments, followed by a neural re-ranker, typically a language model\nthat performs additional rescoring ",
                    "Citation Text": "Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of BERT\nfor ad hoc document retrieval. arXiv preprint arXiv:1903.10972 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10972",
                        "Citation Paper Title": "Title:Simple Applications of BERT for Ad Hoc Document Retrieval",
                        "Citation Paper Abstract": "Abstract:Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval. This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle. We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of.",
                        "Citation Paper Authors": "Authors:Wei Yang, Haotian Zhang, Jimmy Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11714v1": {
            "Paper Title": "Photos Are All You Need for Reciprocal Recommendation in Online Dating",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03569v4": {
            "Paper Title": "Socially-Aware Self-Supervised Tri-Training for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "mine extra signals for supervision by looking at the longer-term\nfuture and reconstruct the future sequence for self-supervision,\nwhich adopts feature masking in essence. Wu et al. ",
                    "Citation Text": "Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,\nand Xing Xie. 2020. Self-supervised Graph Learning for Recommendation. arXiv\npreprint arXiv:2010.10783 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10783",
                        "Citation Paper Title": "Title:Self-supervised Graph Learning for Recommendation",
                        "Citation Paper Abstract": "Abstract:Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.\nIn this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -- node dropout, edge dropout, and random walk -- that change the graph structure in different manners. We term this new learning paradigm as \\textit{Self-supervised Graph Learning} (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "devise aux-\niliary self-supervised objectives by randomly masking attributes of\nitems and skipping items and subsequences of a given sequence for\npretraining sequential recommendation model. Yao et al. ",
                    "Citation Text": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Aditya Menon,\nLichan Hong, Ed H Chi, Steve Tjoa, Evan Ettinger, et al .2020. Self-supervised\nLearning for Deep Models in Recommendations. arXiv preprint arXiv:2007.12865\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12865",
                        "Citation Paper Title": "Title:Self-supervised Learning for Large-scale Item Recommendations",
                        "Citation Paper Abstract": "Abstract:Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.\nInspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.\nWe evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",
                        "Citation Paper Authors": "Authors:Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". That is the reason why we resort to social networks for\nself-supervisory signals generated by graph neural encoders.\n2.2 Self-Supervised Learning in RS\nSelf-supervised learning ",
                    "Citation Text": "Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie\nTang. 2020. Self-supervised learning: Generative or contrastive. arXiv preprint\narXiv:2006.08218 1, 2 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.08218",
                        "Citation Paper Title": "Title:Self-supervised Learning: Generative or Contrastive",
                        "Citation Paper Abstract": "Abstract:Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",
                        "Citation Paper Authors": "Authors:Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". The basic idea of these\nGCN-based models is to exploit the high-order neighbors in the\nuser-item graph by aggregating the embeddings of neighbors to\nrefine the target node\u2019s embeddings ",
                    "Citation Text": "Shiwen Wu, Wentao Zhang, Fei Sun, and Bin Cui. 2020. Graph Neural Networks\nin Recommender Systems: A Survey. arXiv preprint arXiv:2011.02260 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.02260",
                        "Citation Paper Title": "Title:Graph Neural Networks in Recommender Systems: A Survey",
                        "Citation Paper Abstract": "Abstract:With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in this https URL.",
                        "Citation Paper Authors": "Authors:Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, Bin Cui"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", as the prevalent formulation of GNNs which\nis a first-order approximation of spectral graph convolutions, has\ndriven a multitude of graph neural recommendation models like\nGCMC ",
                    "Citation Text": "Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\ntional matrix completion. arXiv preprint arXiv:1706.02263 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11213v1": {
            "Paper Title": "AccoMontage: Accompaniment Arrangement via Phrase Selection and Style\n  Transfer",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". In\nparticular, accompaniment arrangement refers to generat-\ning accompaniment conditioned on the lead sheet, and this\ntopic has recently drawn much research attention. We even\nsee tailored datasets for piano arrangement tasks ",
                    "Citation Text": "Z. Wang, K. Chen, J. Jiang, Y . Zhang, M. Xu, S. Dai,\nX. Gu, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d arXiv preprint\narXiv:2008.07142 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07142",
                        "Citation Paper Title": "Title:POP909: A Pop-song Dataset for Music Arrangement Generation",
                        "Citation Paper Abstract": "Abstract:Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.",
                        "Citation Paper Authors": "Authors:Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, Xianbin Gu, Gus Xia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.10560v1": {
            "Paper Title": "Self-Supervised Graph Co-Training for Session-based Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "re-\nthinks the sequence order of items to exploit users\u2019 intrinsic intents\nusing GNNs. GCE-GNN ",
                    "Citation Text": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui\nQiu. 2020. Global context enhanced graph neural networks for session-based\nrecommendation. In Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 169\u2013178.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05081",
                        "Citation Paper Title": "Title:Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
                        "Citation Paper Authors": "Authors:Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ": adopts attention layers to replace all RNN encoders\nin the previous work and employs the self-attention mechanism ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Advances in neural information processing systems . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "have been applied to session-based recommendation models\nto capture sequential order between items and achieved great suc-\ncess [ 19,55]. Hidasi et al. ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", we impose the divergence constraint on the\nself-supervised graph co-training regime by integrating adversarial\nexamples into the training.\nTheoretically, the adversarial examples targeting one encoder ",
                    "Citation Text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\nharnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "proposed a sequence-to-sequence training strategy based on la-\ntent self-supervision and disentanglement of user intention behind\nbehavior sequences. Besides, SSL is also applied to other recom-\nmendation paradigms such as general recommendation ",
                    "Citation Text": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya\nMenon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al .2020. Self-supervised\nlearning for deep models in recommendations. arXiv e-prints (2020), arXiv\u20132007.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12865",
                        "Citation Paper Title": "Title:Self-supervised Learning for Large-scale Item Recommendations",
                        "Citation Paper Abstract": "Abstract:Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.\nInspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.\nWe evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",
                        "Citation Paper Authors": "Authors:Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "utilizes the intrinsic data corre-\nlations among attribute, item, subsequence and sequence to gener-\nate self-supervision signals and enhance the data representations\nvia pre-training. Xie et al. ",
                    "Citation Text": "Xu Xie, Fei Sun, Zhaoyang Liu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020.\nContrastive Pre-training for Sequential Recommendation. arXiv preprint\narXiv:2010.14395 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.14395",
                        "Citation Paper Title": "Title:Contrastive Learning for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:Sequential recommendation methods play a crucial role in modern recommender systems because of their ability to capture a user's dynamic interest from her/his historical interactions. Despite their success, we argue that these approaches usually rely on the sequential prediction task to optimize the huge amounts of parameters. They usually suffer from the data sparsity problem, which makes it difficult for them to learn high-quality user representations. To tackle that, inspired by recent advances of contrastive learning techniques in the computer version, we propose a novel multi-task model called \\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential \\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of the traditional next item prediction task but also utilizes the contrastive learning framework to derive self-supervision signals from the original user behavior sequences. Therefore, it can extract more meaningful user patterns and further encode the user representation effectively. In addition, we propose three data augmentation approaches to construct self-supervision signals. Extensive experiments on four public datasets demonstrate that CL4SRec achieves state-of-the-art performance over existing baselines by inferring better user representations.",
                        "Citation Paper Authors": "Authors:Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, Bin Cui"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "transfers the cloze\nobjective from language modeling to sequential recommendation\nby predicting the random masked items in the sequence with the\nsurrounding contexts. \ud835\udc463-Rec ",
                    "Citation Text": "Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,\nZhongyuan Wang, and Ji-Rong Wen. 2020. S\u02c6 3-Rec: Self-Supervised Learning\nfor Sequential Recommendation with Mutual Information Maximization. arXiv\npreprint arXiv:2008.07873 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07873",
                        "Citation Paper Title": "Title:S^3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization",
                        "Citation Paper Abstract": "Abstract:Recently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn model parameters or data representations. However, the model trained with this loss is prone to suffer from data sparsity problem. Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation. To tackle this problem, we propose the model S^3-Rec, which stands for Self-Supervised learning for Sequential Recommendation, based on the self-attentive neural architecture. The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation. For our task, we devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable in our scenario. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art methods, especially when only limited training data is available. Besides, we extend our self-supervised learning method to other recommendation models, which also improve their performance.",
                        "Citation Paper Authors": "Authors:Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "contrasted multiple\nviews of graphs and nodes to learn their representations. Qiu ",
                    "Citation Text": "Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,\nKuansan Wang, and Jie Tang. 2020. GCC: Graph Contrastive Coding for Graph\nNeural Network Pre-Training. In Proceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining . 1150\u20131160.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09963",
                        "Citation Paper Title": "Title:GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training",
                        "Citation Paper Abstract": "Abstract:Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",
                        "Citation Paper Authors": "Authors:Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "maximizes mutual information between the local patch and the\nglobal graph to refine node representations, making them as the\nground-truth of each other. In InfoGraph ",
                    "Citation Text": "Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograph: Un-\nsupervised and semi-supervised graph-level representation learning via mutual\ninformation maximization. arXiv preprint arXiv:1908.01000 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01000",
                        "Citation Paper Title": "Title:InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization",
                        "Citation Paper Abstract": "Abstract:This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.",
                        "Citation Paper Authors": "Authors:Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "is the\nfirst to model session sequences in session graphs and applies a\ngated GNN model to aggregate information between items into\nsession representations. MGNN-SPred ",
                    "Citation Text": "Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, and Hongyuan\nZha. 2020. Beyond clicks: Modeling multi-relational item graph for session-based\ntarget behavior prediction. In Proceedings of The Web Conference 2020 . 3056\u20133062.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.07993",
                        "Citation Paper Title": "Title:Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based Target Behavior Prediction",
                        "Citation Paper Abstract": "Abstract:Session-based target behavior prediction aims to predict the next item to be interacted with specific behavior types (e.g., clicking). Although existing methods for session-based behavior prediction leverage powerful representation learning approaches to encode items' sequential relevance in a low-dimensional space, they suffer from several limitations. Firstly, they focus on only utilizing the same type of user behavior for prediction, but ignore the potential of taking other behavior data as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally in one behavior sequence, and they lack a principled way to globally encode these relations more effectively. To overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. Specifically, we build a Multi-Relational Item Graph (MRIG) based on all behavior sequences from all sessions, involving target and auxiliary behavior types. Based on MRIG, MGNN-SPred learns global item-to-item relations and further obtains user preferences w.r.t. current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse user representations for predicting next item interacted with target behavior. The extensive experiments on two real-world datasets demonstrate the superiority of MGNN-SPred by comparing with state-of-the-art session-based prediction methods, validating the benefits of leveraging auxiliary behavior and learning item-to-item relations over MRIG.",
                        "Citation Paper Authors": "Authors:Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, Hongyuan Zha"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "was the first that applied RNNs to\nmodel the whole session and introduced several modifications to\nvanilla RNNs such as a ranking loss function and session-parallel\nmini-batch training to generate more accurate recommendations.\nAs a follow-up study ",
                    "Citation Text": "Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\nnetworks for session-based recommendations. In Proceedings of the 1st Workshop\non Deep Learning for Recommender Systems . 17\u201322.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.08117",
                        "Citation Paper Title": "Title:Improved Recurrent Neural Networks for Session-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.",
                        "Citation Paper Authors": "Authors:Yong Kiam Tan, Xinxing Xu, Yong Liu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "investigated the order of tempo-\nral data based on Markov chain and used a probability decision-tree\nto model sequential patterns between items. Shan et al. ",
                    "Citation Text": "Guy Shani, David Heckerman, and Ronen I Brafman. 2005. An MDP-based\nrecommender system. Journal of Machine Learning Research 6, Sep (2005), 1265\u2013\n1295.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.0600",
                        "Citation Paper Title": "Title:An MDP-based Recommender System",
                        "Citation Paper Abstract": "Abstract:Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.",
                        "Citation Paper Authors": "Authors:Guy Shani, Ronen I. Brafman, David Heckerman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04690v4": {
            "Paper Title": "POSO: Personalized Cold Start Modules for Large-scale Recommender\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "proposes multiple memory: profile memory, user memory and\ntask-specific memory. Such memory can be viewed as user fea-\nture bases, which are used to decompose cold users into warm\nfeatures. MWUF ",
                    "Citation Text": "Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang,\nLeyu Lin, and Juan Cao. 2021. Learning to Warm Up Cold Item Embed-\ndings for Cold-Start Recommendation with Meta Scaling and Shifting Net-\nworks. In Proceedings of the 44th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval (Virtual Event, Canada) (SIGIR\n\u201921). Association for Computing Machinery, New York, NY, USA, 1167\u20131176.\nhttps://doi.org/10.1145/3404835.3462843",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04790",
                        "Citation Paper Title": "Title:Learning to Warm Up Cold Item Embeddings for Cold-start Recommendation with Meta Scaling and Shifting Networks",
                        "Citation Paper Abstract": "Abstract:Recently, embedding techniques have achieved impressive success in recommender systems. However, the embedding techniques are data demanding and suffer from the cold-start problem. Especially, for the cold-start item which only has limited interactions, it is hard to train a reasonable item ID embedding, called cold ID embedding, which is a major challenge for the embedding techniques. The cold item ID embedding has two main problems: (1) A gap is existing between the cold ID embedding and the deep model. (2) Cold ID embedding would be seriously affected by noisy interaction. However, most existing methods do not consider both two issues in the cold-start problem, simultaneously. To address these problems, we adopt two key ideas: (1) Speed up the model fitting for the cold item ID embedding (fast adaptation). (2) Alleviate the influence of noise. Along this line, we propose Meta Scaling and Shifting Networks to generate scaling and shifting functions for each item, respectively. The scaling function can directly transform cold item ID embeddings into warm feature space which can fit the model better, and the shifting function is able to produce stable embeddings from the noisy embeddings. With the two meta networks, we propose Meta Warm Up Framework (MWUF) which learns to warm up cold ID embeddings. Moreover, MWUF is a general framework that can be applied upon various existing deep recommendation models. The proposed model is evaluated on three popular benchmarks, including both recommendation and advertising datasets. The evaluation results demonstrate its superior performance and compatibility.",
                        "Citation Paper Authors": "Authors:Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang, Leyu Lin, Juan Cao"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "can be viewed as a similar try on improving\ngeneration. It randomly masks user inputs to imitate new users.\nAnother genre tries to generate meaningful IDs embedding by\nother features. Meta-E ",
                    "Citation Text": "Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm\nUp Cold-Start Advertisements: Improving CTR Predictions via Learning to Learn\nID Embeddings. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval (Paris, France) (SIGIR\u201919) .\nAssociation for Computing Machinery, New York, NY, USA, 695\u2013704. https:\n//doi.org/10.1145/3331184.3331268",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11547",
                        "Citation Paper Title": "Title:Warm Up Cold-start Advertisements: Improving CTR Predictions via Learning to Learn ID Embeddings",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction has been one of the most central problems in computational advertising. Lately, embedding techniques that produce low-dimensional representations of ad IDs drastically improve CTR prediction accuracies. However, such learning techniques are data demanding and work poorly on new ads with little logging data, which is known as the cold-start problem.\nIn this paper, we aim to improve CTR predictions during both the cold-start phase and the warm-up phase when a new ad is added to the candidate pool. We propose Meta-Embedding, a meta-learning-based approach that learns to generate desirable initial embeddings for new ad IDs. The proposed method trains an embedding generator for new ad IDs by making use of previously learned ads through gradient-based meta-learning. In other words, our method learns how to learn better embeddings. When a new ad comes, the trained generator initializes the embedding of its ID by feeding its contents and attributes. Next, the generated embedding can speed up the model fitting during the warm-up phase when a few labeled examples are available, compared to the existing initialization methods.\nExperimental results on three real-world datasets showed that Meta-Embedding can significantly improve both the cold-start and warm-up performances for six existing CTR prediction models, ranging from lightweight models such as Factorization Machines to complicated deep models such as PNN and DeepFM. All of the above apply to conversion rate (CVR) predictions as well.",
                        "Citation Paper Authors": "Authors:Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, Qing He"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "shows promising results on few-shot learning, but\nmainly focuses on classification tasks. Following its idea, meta-\nlearning based methods are introduced to the recommender system:\nMeLU ",
                    "Citation Text": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.\nMeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation.\nInProceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining . ACM, 1073\u20131082.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00413",
                        "Citation Paper Title": "Title:MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation",
                        "Citation Paper Abstract": "Abstract:This paper proposes a recommender system to alleviate the cold-start problem that can estimate user preferences based on only a small number of items. To identify a user's preference in the cold state, existing recommender systems, such as Netflix, initially provide items to a user; we call those items evidence candidates. Recommendations are then made based on the items selected by the user. Previous recommendation studies have two limitations: (1) the users who consumed a few items have poor recommendations and (2) inadequate evidence candidates are used to identify user preferences. We propose a meta-learning-based recommender system called MeLU to overcome these two limitations. From meta-learning, which can rapidly adopt new task with a few examples, MeLU can estimate new user's preferences with a few consumed items. In addition, we provide an evidence candidate selection strategy that determines distinguishing items for customized preference estimation. We validate MeLU with two benchmark datasets, and the proposed model reduces at least 5.92% mean absolute error than two comparative models on the datasets. We also conduct a user study experiment to verify the evidence selection strategy.",
                        "Citation Paper Authors": "Authors:Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, Sehee Chung"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "is\napplied to fuse the sequence of embedding into a single one, which\nhas been introduced to recommender system by ",
                    "Citation Text": "Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Be-\nhavior Sequence Transformer for E-Commerce Recommendation in Alibaba.\nInProceedings of the 1st International Workshop on Deep Learning Practice\nfor High-Dimensional Sparse Data (Anchorage, Alaska) (DLP-KDD \u201919) . Asso-\nciation for Computing Machinery, New York, NY, USA, Article 12, 4 pages.\nhttps://doi.org/10.1145/3326937.3341261",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06874",
                        "Citation Paper Title": "Title:Behavior Sequence Transformer for E-commerce Recommendation in Alibaba",
                        "Citation Paper Abstract": "Abstract:Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding&MLP paradigm: raw features are embedded into low-dimensional vectors, which are then fed on to MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users' behaviors. In this paper, we propose to use the powerful Transformer model to capture the sequential signals underlying users' behavior sequences for recommendation in Alibaba. Experimental results demonstrate the superiority of the proposed model, which is then deployed online at Taobao and obtain significant improvements in online Click-Through-Rate (CTR) comparing to two baselines.",
                        "Citation Paper Authors": "Authors:Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "learns to generate user IDs embed-\nding from other embedding. The learning procedure is supervised\nby cold-start phase and warm-up phase respectively. MAMO ",
                    "Citation Text": "Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO:\nMemory-Augmented Meta-Optimization for Cold-Start Recommendation. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining (Virtual Event, CA, USA) (KDD \u201920) . Association for\nComputing Machinery, New York, NY, USA, 688\u2013697. https://doi.org/10.1145/\n3394486.3403113",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.03183",
                        "Citation Paper Title": "Title:MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation",
                        "Citation Paper Abstract": "Abstract:A common challenge for most current recommender systems is the cold-start problem. Due to the lack of user-item interactions, the fine-tuned recommender systems are unable to handle situations with new users or new items. Recently, some works introduce the meta-optimization idea into the recommendation scenarios, i.e. predicting the user preference by only a few of past interacted items. The core idea is learning a global sharing initialization parameter for all users and then learning the local parameters for each user separately. However, most meta-learning based recommendation approaches adopt model-agnostic meta-learning for parameter initialization, where the global sharing parameter may lead the model into local optima for some users. In this paper, we design two memory matrices that can store task-specific memories and feature-specific memories. Specifically, the feature-specific memories are used to guide the model with personalized parameter initialization, while the task-specific memories are used to guide the model fast predicting the user preference. And we adopt a meta-optimization approach for optimizing the proposed method. We test the model on two widely used recommendation datasets and consider four cold-start situations. The experimental results show the effectiveness of the proposed methods.",
                        "Citation Paper Authors": "Authors:Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, Liming Zhu"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "treats recommendation for each user as an individual\ntask. In local update steps, embedding receives no gradients to en-\nsure the stability of the network. Similarly, Du and Wang et al. ",
                    "Citation Text": "Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. 2019.\nSequential Scenario-Specific Meta Learner for Online Recommendation. In Pro-\nceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining (Anchorage, AK, USA) (KDD \u201919) . Association for Computing\nMachinery, New York, NY, USA, 2895\u20132904. https://doi.org/10.1145/3292500.\n3330726",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00391",
                        "Citation Paper Title": "Title:Sequential Scenario-Specific Meta Learner for Online Recommendation",
                        "Citation Paper Abstract": "Abstract:Cold-start problems are long-standing challenges for practical recommendations. Most existing recommendation algorithms rely on extensive observed data and are brittle to recommendation scenarios with few interactions. This paper addresses such problems using few-shot learning and meta learning. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. To accomplish this, we combine the scenario-specific learning with a model-agnostic sequential meta-learning and unify them into an integrated end-to-end framework, namely Scenario-specific Sequential Meta learner (or s^2 meta). By doing so, our meta-learner produces a generic initial model through aggregating contextual information from a variety of prediction tasks while effectively adapting to specific tasks by leveraging learning-to-learn knowledge. Extensive experiments on various real-world datasets demonstrate that our proposed model can achieve significant gains over the state-of-the-arts for cold-start problems in online recommendation. Deployment is at the Guess You Like session, the front page of the Mobile Taobao.",
                        "Citation Paper Authors": "Authors:Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, Jie Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.03491v2": {
            "Paper Title": "AI-enabled Prediction of eSports Player Performance Using the Data from\n  Heterogeneous Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.10019v1": {
            "Paper Title": "Sequence-to-Sequence Learning on Keywords for Efficient FAQ Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.09843v1": {
            "Paper Title": "Multi-Server Private Linear Transformation with Joint Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04907v2": {
            "Paper Title": "Disentangled Contrastive Learning for Learning Robust Textual\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03618v2": {
            "Paper Title": "Document-level Relation Extraction as Semantic Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06625v2": {
            "Paper Title": "Continuous-Time Sequential Recommendation with Temporal Graph\n  Collaborative Transformer",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "also introduces temporal attention for learning dynamic graph em-\nbeddings. JODIE ",
                    "Citation Text": "Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-\nbedding Trajectory in Temporal Interaction Networks. In KDD . ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01207",
                        "Citation Paper Title": "Title:Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks",
                        "Citation Paper Abstract": "Abstract:Modeling sequential interactions between users and items/products is crucial in domains such as e-commerce, social networking, and education. Representation learning presents an attractive opportunity to model the dynamic evolution of users and items, where each user/item can be embedded in a Euclidean space and its evolution can be modeled by an embedding trajectory in this space. However, existing dynamic embedding methods generate embeddings only when users take actions and do not explicitly model the future trajectory of the user/item in the embedding space. Here we propose JODIE, a coupled recurrent neural network model that learns the embedding trajectories of users and items. JODIE employs two recurrent neural networks to update the embedding of a user and an item at every interaction. Crucially, JODIE also models the future embedding trajectory of a user/item. To this end, it introduces a novel projection operator that learns to estimate the embedding of the user at any time in the future. These estimated embeddings are then used to predict future user-item interactions. To make the method scalable, we develop a t-Batch algorithm that creates time-consistent batches and leads to 9x faster training. We conduct six experiments to validate JODIE on two prediction tasks---future interaction prediction and state change prediction---using four real-world datasets. We show that JODIE outperforms six state-of-the-art algorithms in these tasks by at least 20% in predicting future interactions and 12% in state change prediction.",
                        "Citation Paper Authors": "Authors:Srijan Kumar, Xikun Zhang, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "learns the dynamic graph embeddings\nbased on the graph attention model. Basconv ",
                    "Citation Text": "Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, and Philip S Yu. 2020.\nBasConv: Aggregating Heterogeneous Interactions for Basket Recommendation\nwith Graph Convolutional Neural Network. In Proceedings of the 2020 SIAM\nInternational Conference on Data Mining . SIAM, 64\u201372.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.09900",
                        "Citation Paper Title": "Title:BasConv: Aggregating Heterogeneous Interactions for Basket Recommendation with Graph Convolutional Neural Network",
                        "Citation Paper Abstract": "Abstract:Within-basket recommendation reduces the exploration time of users, where the user's intention of the basket matters. The intent of a shopping basket can be retrieved from both user-item collaborative filtering signals and multi-item correlations. By defining a basket entity to represent the basket intent, we can model this problem as a basket-item link prediction task in the User-Basket-Item~(UBI) graph. Previous work solves the problem by leveraging user-item interactions and item-item interactions simultaneously. However, collectivity and heterogeneity characteristics are hardly investigated before. Collectivity defines the semantics of each node which should be aggregated from both directly and indirectly connected neighbors. Heterogeneity comes from multi-type interactions as well as multi-type nodes in the UBI graph. To this end, we propose a new framework named \\textbf{BasConv}, which is based on the graph convolutional neural network. Our BasConv model has three types of aggregators specifically designed for three types of nodes. They collectively learn node embeddings from both neighborhood and high-order context. Additionally, the interactive layers in the aggregators can distinguish different types of interactions. Extensive experiments on two real-world datasets prove the effectiveness of BasConv. Our code is available online at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "is proposed by applying temporal random\nwalk over its defined continuous-time dynamic network. TGAT ",
                    "Citation Text": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.\n2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint\narXiv:2002.07962 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.07962",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Temporal Graphs",
                        "Citation Paper Abstract": "Abstract:Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",
                        "Citation Paper Authors": "Authors:Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "develops user and item RNNs to update user\nand item embeddings. Regarding the SR problem, a few recent\nworks [ 19,50] also notice the importance of temporal information.\nCTA ",
                    "Citation Text": "Jibang Wu, Renqin Cai, and Hongning Wang. 2020. D\u00e9j\u00e0 vu: A Contextualized\nTemporal Attention Mechanism for Sequential Recommendation. In The Web\nConference . 2199\u20132209.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.00741",
                        "Citation Paper Title": "Title:D\u00e9j\u00e0 vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:Predicting users' preferences based on their sequential behaviors in history is challenging and crucial for modern recommender systems. Most existing sequential recommendation algorithms focus on transitional structure among the sequential actions, but largely ignore the temporal and context information, when modeling the influence of a historical event to current prediction.\nIn this paper, we argue that the influence from the past events on a user's current action should vary over the course of time and under different context. Thus, we propose a Contextualized Temporal Attention Mechanism that learns to weigh historical actions' influence on not only what action it is, but also when and how the action took place. More specifically, to dynamically calibrate the relative input dependence from the self-attention mechanism, we deploy multiple parameterized kernel functions to learn various temporal dynamics, and then use the context information to determine which of these reweighing kernels to follow for each input. In empirical evaluations on two large public recommendation datasets, our model consistently outperformed an extensive set of state-of-the-art sequential recommendation methods.",
                        "Citation Paper Authors": "Authors:Jibang Wu, Renqin Cai, Hongning Wang"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "designs a personalized transformer\nto improve the SR performance. ASReP ",
                    "Citation Text": "Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. 2021. Augmenting Sequential\nRecommendation with Pseudo-Prior Items via Reversely Pre-training Trans-\nformer. In Proceedings of the 44th International ACM SIGIR Conference on Research\nand Development in Information Retrieval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.00522",
                        "Citation Paper Title": "Title:Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer",
                        "Citation Paper Abstract": "Abstract:Sequential Recommendation characterizes the evolving patterns by modeling item sequences chronologically. The essential target of it is to capture the item transition correlations. The recent developments of transformer inspire the community to design effective sequence encoders, \\textit{e.g.,} SASRec and BERT4Rec. However, we observe that these transformer-based models suffer from the cold-start issue, \\textit{i.e.,} performing poorly for short sequences. Therefore, we propose to augment short sequences while still preserving original sequential correlations. We introduce a new framework for \\textbf{A}ugmenting \\textbf{S}equential \\textbf{Re}commendation with \\textbf{P}seudo-prior items~(ASReP). We firstly pre-train a transformer with sequences in a reverse direction to predict prior items. Then, we use this transformer to generate fabricated historical items at the beginning of short sequences. Finally, we fine-tune the transformer using these augmented sequences from the time order to predict the next item. Experiments on two real-world datasets verify the effectiveness of ASReP. The code is available on \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "applies the transformer\nlayer to assign weights to items in the sequence. Later, inspired by\nthe BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.00644v2": {
            "Paper Title": "Jointly Optimizing Query Encoder and Product Quantization to Improve\n  Retrieval Performance",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "30x \u221212% \u22122% \u221210% \u22121% 30x \u221218% \u22126% \u22128%\nBM25 Neg ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2020. Complementing lexical\nretrieval with semantic residual embedding. arXiv preprint arXiv:2004.13969\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13969",
                        "Citation Paper Title": "Title:Complementing Lexical Retrieval with Semantic Residual Embedding",
                        "Citation Paper Abstract": "Abstract:This paper presents CLEAR, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model. CLEAR explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of CLEAR over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ".\n4.2.2 Augmented BoW Models.\nSeveral methods use deep language models to improve BoW\nmodels. We use them as our baselines, including doc2query ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nexpansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "30.96 0.150** 0.274** 0.431** 0.157** 11.36 0.304** 0.638** 0.553**\nFALCONN ",
                    "Citation Text": "Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig\nSchmidt. 2015. Practical and Optimal LSH for Angular Distance. In Proceedings of\nthe 28th International Conference on Neural Information Processing Systems - Vol-\nume 1 (Montreal, Canada) (NIPS\u201915) . MIT Press, Cambridge, MA, USA, 1225\u20131233.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02897",
                        "Citation Paper Title": "Title:Practical and Optimal LSH for Angular Distance",
                        "Citation Paper Abstract": "Abstract:We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn 2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.",
                        "Citation Paper Authors": "Authors:Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, Ludwig Schmidt"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "and \u2018JPQ negs\u2019 neg-\native sampling methods. The QID is 443396 from TREC DL\nTrack ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M.\nVoorhees. 2020. Overview of the TREC 2019 deep learning track. In Text REtrieval\nConference (TREC) . TREC.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07820",
                        "Citation Paper Title": "Title:Overview of the TREC 2019 deep learning track",
                        "Citation Paper Abstract": "Abstract:The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the first track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs significantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M. Voorhees"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.07094v2": {
            "Paper Title": "Deep Self-Adaptive Hashing for Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "IJCAI-20 0.562 0.588 0.595 0.582 0.797 0.809 0.809 0.804 0.730 0.754 0.764 0.769\nTBH ",
                    "Citation Text": "Yuming Shen, Jie Qin, Jiaxin Chen, Mengyang Yu, Li Liu, Fan Zhu, Fumin Shen,\nand Ling Shao. 2020. Auto-Encoding Twin-Bottleneck Hashing. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2818\u20132827.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.11930",
                        "Citation Paper Title": "Title:Auto-Encoding Twin-Bottleneck Hashing",
                        "Citation Paper Abstract": "Abstract:Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods. Our source code can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Yuming Shen, Jie Qin, Jiaxin Chen, Mengyang Yu, Li Liu, Fan Zhu, Fumin Shen, Ling Shao"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "CVPR-15 0.280 0.343 0.365 0.406 0.726 0.705 0.713 0.703 0.581 0.603 0.673 0.681\nSGH+VGG ",
                    "Citation Text": "Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and Le Song. 2017. Stochastic gen-\nerative hashing. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70 . 913\u2013922.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.02815",
                        "Citation Paper Title": "Title:Stochastic Generative Hashing",
                        "Citation Paper Abstract": "Abstract:Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09076v1": {
            "Paper Title": "PASTO: Strategic Parameter Optimization in Recommendation Systems --\n  Probabilistic is Better than Deterministic",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "maximizes a so-called economic value based on\nreinforcement learning. Recently, much attention has been paid to \ufb01nding the Pareto frontier of multiple goals ",
                    "Citation Text": "Nikola Milojkovic, Diego Antognini, Giancarlo Bergamin, Boi Faltings, and Claudiu Musat. Multi-gradient\ndescent for multi-objective recommender systems. arXiv preprint arXiv:2001.00846 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.00846",
                        "Citation Paper Title": "Title:Multi-Gradient Descent for Multi-Objective Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems need to mirror the complexity of the environment they are applied in. The more we know about what might benefit the user, the more objectives the recommender system has. In addition there may be multiple stakeholders - sellers, buyers, shareholders - in addition to legal and ethical constraints. Simultaneously optimizing for a multitude of objectives, correlated and not correlated, having the same scale or not, has proven difficult so far.\nWe introduce a stochastic multi-gradient descent approach to recommender systems (MGDRec) to solve this problem. We show that this exceeds state-of-the-art methods in traditional objective mixtures, like revenue and recall. Not only that, but through gradient normalization we can combine fundamentally different objectives, having diverse scales, into a single coherent framework. We show that uncorrelated objectives, like the proportion of quality products, can be improved alongside accuracy. Through the use of stochasticity, we avoid the pitfalls of calculating full gradients and provide a clear setting for its applicability.",
                        "Citation Paper Authors": "Authors:Nikola Milojkovic, Diego Antognini, Giancarlo Bergamin, Boi Faltings, Claudiu Musat"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "uses an evolutionary algorithm to\ninclude a diversity indicator on top of item rating evaluation; ",
                    "Citation Text": "Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng Jiang, Wenwu Ou, and Yongfeng Zhang.\nValue-aware recommendation based on reinforced pro\ufb01t maximization in e-commerce systems. arXiv preprint\narXiv:1902.00851 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.00851",
                        "Citation Paper Title": "Title:Value-aware Recommendation based on Reinforced Profit Maximization in E-commerce Systems",
                        "Citation Paper Abstract": "Abstract:Existing recommendation algorithms mostly focus on optimizing traditional recommendation measures, such as the accuracy of rating prediction in terms of RMSE or the quality of top-$k$ recommendation lists in terms of precision, recall, MAP, etc. However, an important expectation for commercial recommendation systems is to improve the final revenue/profit of the system. Traditional recommendation targets such as rating prediction and top-$k$ recommendation are not directly related to this goal. In this work, we blend the fundamental concepts in online advertising and micro-economics into personalized recommendation for profit maximization. Specifically, we propose value-aware recommendation based on reinforcement learning, which directly optimizes the economic value of candidate items to generate the recommendation list. In particular, we generalize the basic concept of click conversion rate (CVR) in computational advertising into the conversation rate of an arbitrary user action (XVR) in E-commerce, where the user actions can be clicking, adding to cart, adding to wishlist, etc. In this way, each type of user action is mapped to its monetized economic value. Economic values of different user actions are further integrated as the reward of a ranking list, and reinforcement learning is used to optimize the recommendation list for the maximum total value. Experimental results in both offline benchmarks and online commercial systems verified the improved performance of our framework, in terms of both traditional top-$k$ ranking tasks and the economic profits of the system.",
                        "Citation Paper Authors": "Authors:Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng Jiang, Wenwu Ou, Yongfeng Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09070v1": {
            "Paper Title": "SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software\n  Mentions in Scientific Articles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06467v3": {
            "Paper Title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03599v2": {
            "Paper Title": "Improving Document Representations by Generating Pseudo Query Embeddings\n  for Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08199v1": {
            "Paper Title": "Modeling Performance and Energy trade-offs in Online Data-Intensive\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06208v3": {
            "Paper Title": "LT-OCF: Learnable-Time ODE-based Collaborative Filtering",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". We conjecture that this characteristic is also suitable\nfor learning reliable user/product representations, i.e., embeddings,\nwhen there is no abundant information. As mentioned earlier, CF\ntypically includes only user-product interactions without addi-\ntional user/product features. LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network\nfor Recommendation. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". We change the original loss of\nGRMF to the BPR loss for fair comparison. GRMG-norm is\na slight variation from GRMF by adding a normalization to\ngraph Laplacian.\n(8)NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is a graph-based model, which uses the high-\norder user-item interactions by random walks to enrich the\noriginal training data.\n(5)GC-MC ",
                    "Citation Text": "Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2017. Graph Convolu-\ntional Matrix Completion. CoRR abs/1706.02263 (2017). arXiv:1706.02263",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". This\nmethod uses multiple hidden layers above the element-wise\nconcatenation of user and item embeddings to capture their\nnon-linear feature interactions.\n(3)CMN ",
                    "Citation Text": "Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative Memory Network for\nRecommendation Systems. In SIGIR . ACM, 515\u2013524.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10862",
                        "Citation Paper Title": "Title:Collaborative Memory Network for Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.",
                        "Citation Paper Authors": "Authors:Travis Ebesu, Bin Shen, Yi Fang"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "is a matrix decomposition optimized by Bayesian\nPersonalization Rank (BPR) loss, which utilizes the user-item\ndirect interaction only as the target value of the interaction\nfunction.\n(2)Neu-MF is a neural collaborative filtering method ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-seng\nChua. 2017. Neural Collaborative Filtering. In WWW . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "We introduce our literature survey and preliminary knowledge to\nunderstand our work.\n2.1 Neural Ordinary Differential Equations\n(NODEs)\nNODEs calculate \ud835\udc89(\ud835\udc61\ud835\udc56+1)from \ud835\udc89(\ud835\udc61\ud835\udc56)by solving the following Rie-\nmann integral problem ",
                    "Citation Text": "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud.\n2018. Neural Ordinary Differential Equations. In NeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.07366",
                        "Citation Paper Title": "Title:Neural Ordinary Differential Equations",
                        "Citation Paper Abstract": "Abstract:We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",
                        "Citation Paper Authors": "Authors:Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ",\nand so on. All these methods utilize user-product interaction his-\ntory ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based\nRecommender System: A Survey and New Perspectives. ACM Comput. Surv. 52,\n1 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ") are similar to RK4 (as\nso are residual networks to the explicit Euler method) ",
                    "Citation Text": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. 2018. Beyond Finite\nLayer Neural Networks: Bridging Deep Architectures and Numerical Differential\nEquations. In ICML .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10121",
                        "Citation Paper Title": "Title:Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations",
                        "Citation Paper Abstract": "Abstract:In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ($>50$\\%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.",
                        "Citation Paper Authors": "Authors:Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". We fully enjoy the\nsecond advantage while designing our method.\nFig. 3 shows the typical architecture of NODEs which we took\nfrom ",
                    "Citation Text": "Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. 2019. Augmented Neural\nODEs. In NeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01681",
                        "Citation Paper Title": "Title:Augmented Neural ODEs",
                        "Citation Paper Abstract": "Abstract:We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.",
                        "Citation Paper Authors": "Authors:Emilien Dupont, Arnaud Doucet, Yee Whye Teh"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "showed that, in such a\ncase, linear GCNs with zero non-linearity, which are known to be\nsmooth ",
                    "Citation Text": "Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring\nand Relieving the Over-Smoothing Problem for Graph Neural Networks from\nthe Topological View. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.03211",
                        "Citation Paper Title": "Title:Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective;(2) AdaGraph which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.",
                        "Citation Paper Authors": "Authors:Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.07976v1": {
            "Paper Title": "A Unified Framework for Cross-Domain and Cross-System Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.07571v1": {
            "Paper Title": "ACM-CR: A Manually Annotated Test Collection for Citation Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.07567v1": {
            "Paper Title": "How Powerful is Graph Convolution for Recommendation?",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "inCF and GCNs. Finally , we\nconcludethis work in Section 7.The codetoreproducethe exp er-\niments is availableat https://github.com/yshenaw/GF_CF .\n2 PRELIMINARIES\n2.1 Notationsand Terminology\nThissubsectionpresentssomeusefulnotationsandde\ufb01niti ons.We\n\ufb01rst de\ufb01ne user set Uand item set I. As in ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zha ng, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convo lution network\nfor recommendation. In Proceedings of the International ACM SIGIR Conference\non Researchand Development in Information Retrieval . 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ":LightGCNisthestate-of-the-artmethod for\nCF. Please refer toSection2.3 fora detaileddescription.\n(2) NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-S eng Chua. 2019.\nNeuralgraph collaborative\ufb01ltering. In Proceedings of the International ACM SI-\nGIR Conferenceon Researchand Development in Information R etrieval. 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ": NGCF is a nonlinear deep GCN-based method.\nBesidesthecomponentsinLightGCN,itcontainsoffeature\ntransformation,and nonlinear activation.\n(3) GRMF and GRMF-norm[13, 30]: GRMF adds a graph Lapla-\ncian regularizer to thetraining objectiveof BPR loss in ma-\ntrixfactorization.InGRMF-norm,thenormalizedLaplacia n\nis adoptedinstead of thegraph Laplacian.\n(4) Mult-VAE ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Ho\ufb00man, and Ton y Jebara. 2018.\nVariational autoencoders for collaborative \ufb01ltering. In Proceedings of the Inter-\nnational World Wide WebConference . 689\u2013698.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nFor the proposed graph \ufb01lter based methods, we focus on the\nfollowingtwovariants:\n(1) GF-CF:TheproposedsimplebaselinemethodforCFin(22) .\n(2) LGCN-IDE:TheuntrainedLightGCNwithin\ufb01nitelydimen-\nsional embedding. Theclosed-formis given in (12).\nFor the implementation of graph \ufb01lters, we adopt Scipy ",
                    "Citation Text": "Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt H aberland, Tyler\nReddy,DavidCournapeau,EvgeniBurovski,PearuPeterson, WarrenWeckesser,\nJonathanBright,etal.2020.SciPy1.0:fundamentalalgori thmsforscienti\ufb01ccom-\nputing inPython. Nature methods 17,3 (2020), 261\u2013272.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10121",
                        "Citation Paper Title": "Title:SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python",
                        "Citation Paper Abstract": "Abstract:SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.",
                        "Citation Paper Authors": "Authors:Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, \u0130lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, SciPy 1.0 Contributors"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.07505v1": {
            "Paper Title": "MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in\n  Sequential Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.07450v1": {
            "Paper Title": "Identifying Biased Subgroups in Ranking and Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01377v3": {
            "Paper Title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued\n  Prediction",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "investigate recent neural network\napproaches for /p.sc/o.sc/s.sc-tagging and other /n.sc/l.sc/p.sc-tasks. Huang, Xu, and Yu ",
                    "Citation Text": "Zhiheng Huang, Wei Xu, and Kai Yu. \u201cBidirectional LSTM-CRF models for sequence\ntagging\u201d. In: (2015). arXiv: 1508.01991 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01991",
                        "Citation Paper Title": "Title:Bidirectional LSTM-CRF Models for Sequence Tagging",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
                        "Citation Paper Authors": "Authors:Zhiheng Huang, Wei Xu, Kai Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13300v1": {
            "Paper Title": "Deep Natural Language Processing for LinkedIn Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08957v2": {
            "Paper Title": "Clinical Relation Extraction Using Transformer-based Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.07081v1": {
            "Paper Title": "Toward the Understanding of Deep Text Matching Models for Information\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "to generate high level matching patterns. After a \ud835\udc58-max pooling ",
                    "Citation Text": "Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng.\n2016. A Deep Architecture for Semantic Matching with Multiple Positional\nSentence Representations.. In AAAI , Vol. 16. 2835\u20132841.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.08277",
                        "Citation Paper Title": "Title:A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations",
                        "Citation Paper Abstract": "Abstract:Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.",
                        "Citation Paper Authors": "Authors:Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, Xueqi Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06783v1": {
            "Paper Title": "Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series\n  Anomaly Detection",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ",\nwhich provides us with a robust distance matrix that is insensitive\nto small misalignments between two time series segments. After\nthat, we employed the H-DBSCAN ",
                    "Citation Text": "McInnes, L., Healy, J., and Astels, S. hdbscan: Hierarchical density based\nclustering. J. Open Source Softw. 2 , 11 (2017), 205.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07321",
                        "Citation Paper Title": "Title:Accelerated Hierarchical Density Clustering",
                        "Citation Paper Abstract": "Abstract:We present an accelerated algorithm for hierarchical density based clustering. Our new algorithm improves upon HDBSCAN*, which itself provided a significant qualitative improvement over the popular DBSCAN algorithm. The accelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while supporting variable density clusters, and eliminating the need for the difficult to tune distance scale parameter. This makes accelerated HDBSCAN* the default choice for density based clustering.\nLibrary available at: this https URL",
                        "Citation Paper Authors": "Authors:Leland McInnes, John Healy"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". The model is built upon the temporal graph\nattention network (TGAT) ",
                    "Citation Text": "Xu, D., Ruan, C., K\u00f6rpeoglu, E., Kumar, S., and Achan, K. Inductive rep-\nresentation learning on temporal graphs. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020\n(2020), OpenReview.net.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.07962",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Temporal Graphs",
                        "Citation Paper Abstract": "Abstract:Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",
                        "Citation Paper Authors": "Authors:Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06479v1": {
            "Paper Title": "Contrastive Self-supervised Sequential Recommendation with Robust\n  Augmentation",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "fuses contrastive SSL with Transformer-based SR\nmodel. It only has random augmentation methods for SSL.\n\u2022Other SR models: GRU4Rec ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXivpreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "argues\nTransformer is vulnerable to severe user cold-start issues in SR,\nwhere existing Transformer-based models yield unsatisfying results\nfor short sequences. Therefore, augmentation for short sequences is\ndesirable. S3-Rec ",
                    "Citation Text": "Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,\nZhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning\nfor sequential recommendation with mutual information maximization. In Pro-\nceedings of the 29th ACM International Conference on Information & Knowledge\nManagement . 1893\u20131902.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07873",
                        "Citation Paper Title": "Title:S^3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization",
                        "Citation Paper Abstract": "Abstract:Recently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn model parameters or data representations. However, the model trained with this loss is prone to suffer from data sparsity problem. Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation. To tackle this problem, we propose the model S^3-Rec, which stands for Self-Supervised learning for Sequential Recommendation, based on the self-attentive neural architecture. The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation. For our task, we devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable in our scenario. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art methods, especially when only limited training data is available. Besides, we extend our self-supervised learning method to other recommendation models, which also improve their performance.",
                        "Citation Paper Authors": "Authors:Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "of data\ninstances, which are diverse with respect to the data domains and\nobjective tasks.\nSimCLR ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning . PMLR, 1597\u20131607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "is proposed with a bidirectional Transformer layer.\nOther works [ 6,7,20,24,43] also extend Transformer to incorpo-\nrate complex signals in sequences, which verifies the efficacy of\nTransformer in solving SR. However, a recent work ",
                    "Citation Text": "Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. 2021. Augmenting Sequential\nRecommendation with Pseudo-Prior Items via Reversely Pre-training Trans-\nformer. Proceedings of the 44th international ACM SIGIR conference on Research\nand development in information retrieval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.00522",
                        "Citation Paper Title": "Title:Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer",
                        "Citation Paper Abstract": "Abstract:Sequential Recommendation characterizes the evolving patterns by modeling item sequences chronologically. The essential target of it is to capture the item transition correlations. The recent developments of transformer inspire the community to design effective sequence encoders, \\textit{e.g.,} SASRec and BERT4Rec. However, we observe that these transformer-based models suffer from the cold-start issue, \\textit{i.e.,} performing poorly for short sequences. Therefore, we propose to augment short sequences while still preserving original sequential correlations. We introduce a new framework for \\textbf{A}ugmenting \\textbf{S}equential \\textbf{Re}commendation with \\textbf{P}seudo-prior items~(ASReP). We firstly pre-train a transformer with sequences in a reverse direction to predict prior items. Then, we use this transformer to generate fabricated historical items at the beginning of short sequences. Finally, we fine-tune the transformer using these augmented sequences from the time order to predict the next item. Experiments on two real-world datasets verify the effectiveness of ASReP. The code is available on \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ",\nboth adopt pair-wise contrastive losses on positive and negative\nsamples. Their recent union with SSL shows promise in improving\nrecommender systems ",
                    "Citation Text": "Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,\nand Xing Xie. 2020. Self-supervised Graph Learning for Recommendation. arXiv\npreprint arXiv:2010.10783 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10783",
                        "Citation Paper Title": "Title:Self-supervised Graph Learning for Recommendation",
                        "Citation Paper Abstract": "Abstract:Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.\nIn this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -- node dropout, edge dropout, and random walk -- that change the graph structure in different manners. We term this new learning paradigm as \\textit{Self-supervised Graph Learning} (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "devises contrastive SSL\nto maximize the mutual information over attributes and sequence\naugmentations, which adopts random masks of attributes and items. ",
                    "Citation Text": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Aditya Menon,\nLichan Hong, Ed H Chi, Steve Tjoa, Evan Ettinger, et al .2020. Self-supervised\nLearning for Deep Models in Recommendations. arXiv preprint arXiv:2007.12865\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12865",
                        "Citation Paper Title": "Title:Self-supervised Learning for Large-scale Item Recommendations",
                        "Citation Paper Abstract": "Abstract:Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.\nInspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.\nWe evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",
                        "Citation Paper Authors": "Authors:Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "for\ntext is of a discrete nature. Deleting [ 10,44], reordering ",
                    "Citation Text": "Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma.\n2020. CLEAR: Contrastive Learning for Sentence Representation. arXiv preprint\narXiv:2012.15466 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15466",
                        "Citation Paper Title": "Title:CLEAR: Contrastive Learning for Sentence Representation",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks. Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks.",
                        "Citation Paper Authors": "Authors:Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, Hao Ma"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "applies Transformer layer to\nlearn item importance in sequences, which characterize complex\nitem transition correlations. Later, inspired by BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.02462v1": {
            "Paper Title": "A Dynamic Topic Identification and Labeling Approach of COVID-19 Tweets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05618v2": {
            "Paper Title": "Conditional Sequential Slate Optimization",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "proposes a deep neural network based algorithm\nsuch that the relevance score of a document is determined jointly by\nmultiple documents in the list. Different from just using relevance\nscore to recommend, List-CVAE ",
                    "Citation Text": "Ray Jiang, Sven Gowal, Yuqiu Qian, Timothy Mann, and Danilo J. Rezende.\n2019. Beyond Greedy Ranking: Slate Optimization via List-CVAE. In Interna-\ntional Conference on Learning Representations . https://openreview.net/forum?id=\nr1xX42R5Fm",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01682",
                        "Citation Paper Title": "Title:Beyond Greedy Ranking: Slate Optimization via List-CVAE",
                        "Citation Paper Abstract": "Abstract:The conventional solution to the recommendation problem greedily ranks individual document candidates by prediction scores. However, this method fails to optimize the slate as a whole, and hence, often struggles to capture biases caused by the page layout and document interdepedencies. The slate recommendation problem aims to directly find the optimally ordered subset of documents (i.e. slates) that best serve users' interests. Solving this problem is hard due to the combinatorial explosion in all combinations of document candidates and their display positions on the page. Therefore we propose a paradigm shift from the traditional viewpoint of solving a ranking problem to a direct slate generation framework. In this paper, we introduce List Conditional Variational Auto-Encoders (List-CVAE), which learns the joint distribution of documents on the slate conditioned on user responses, and directly generates full slates. Experiments on simulated and real-world data show that List-CVAE outperforms popular comparable ranking methods consistently on various scales of documents corpora.",
                        "Citation Paper Authors": "Authors:Ray Jiang, Sven Gowal, Timothy A. Mann, Danilo J. Rezende"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06069v1": {
            "Paper Title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA\n  Ensemble",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13299v1": {
            "Paper Title": "Incremental Learning for Personalized Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06014v1": {
            "Paper Title": "TPRM: A Topic-based Personalized Ranking Model for Web Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05891v1": {
            "Paper Title": "Page-level Optimization of e-Commerce Item Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13297v1": {
            "Paper Title": "VTLayout: Fusion of Visual and Text Features for Document Layout\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05774v1": {
            "Paper Title": "HopfE: Knowledge Graph Representation Learning using Inverse Hopf\n  Fibrations",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "is a modificationof DistMult where the parameters are in the complex space. RotatE ",
                    "Citation Text": "Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2018. Ro-\ntatE: Knowledge Graph Embedding by Relational Rotation in Complex\nSpace. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10197",
                        "Citation Paper Title": "Title:RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
                        "Citation Paper Abstract": "Abstract:We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
                        "Citation Paper Authors": "Authors:Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "is a semantic\nmatching model where the entities are represented as vectors. The\nrelations are diagonal matrices, and the composition is a bilinear\nproduct of the entities and relations. ComplEx ",
                    "Citation Text": "Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and\nGuillaume Bouchard. 2016. Complex embeddings for simple link\nprediction. In International Conference on Machine Learning . PMLR,\n2071\u20132080.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06357",
                        "Citation Paper Title": "Title:Complex Embeddings for Simple Link Prediction",
                        "Citation Paper Abstract": "Abstract:In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.00151v2": {
            "Paper Title": "An Empirical Analysis on Transparent Algorithmic Exploration in\n  Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05652v1": {
            "Paper Title": "Modeling Relevance Ranking under the Pre-training and Fine-tuning\n  Paradigm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05540v1": {
            "Paper Title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage\n  Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05278v2": {
            "Paper Title": "Are Negative Samples Necessary in Entity Alignment? An Approach with\n  High Performance, Scalability and Robustness",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "presents the \"momentum encoder\" to alleviate\nthis problem, but negative sampling remains to be the bottleneck.\nMore recently, BYOL ",
                    "Citation Text": "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H.\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan\nGuo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos,\nand Michal Valko. 2020. Bootstrap Your Own Latent - A New Approach to Self-\nSupervised Learning. In Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual , Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.\ncc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.07733",
                        "Citation Paper Title": "Title:Bootstrap your own latent: A new approach to self-supervised Learning",
                        "Citation Paper Abstract": "Abstract:We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
                        "Citation Paper Authors": "Authors:Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "regards all\nthe other samples in the current batch as the negative samples, and\nthe batch size is set to 8,192for best performance. Obviously, most\nresearchers can not afford the cost of such large-scale hardware\nresources. MoCo ",
                    "Citation Text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.\nMomentum Contrast for Unsupervised Visual Representation Learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020,\nSeattle, WA, USA, June 13-19, 2020 . IEEE, 9726\u20139735. https://doi.org/10.1109/\nCVPR42600.2020.00975",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.05341v1": {
            "Paper Title": "The Forgotten Role of Search Queries in IR-based Bug Localization: An\n  Empirical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14018v3": {
            "Paper Title": "GLIMG: Global and Local Item Graphs for Top-N Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "propose to preserve a\ufb03nity and structure information about rat ing matrix\nby constructing both user and item graphs. He et al. [5, 23] treat r ecommen-\ndation task as a vertex ranking problem. They propose BiRank ",
                    "Citation Text": "X. He, M. Gao, M.-Y. Kan, D. Wang, Birank: Towards ranking on bi-\npartite graphs, IEEE Transactions on Knowledge and Data Enginee ring\n29 (2017) 57\u201371.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04396",
                        "Citation Paper Title": "Title:BiRank: Towards Ranking on Bipartite Graphs",
                        "Citation Paper Abstract": "Abstract:The bipartite graph is a ubiquitous data structure that can model the relationship between two entity types: for instance, users and items, queries and webpages. In this paper, we study the problem of ranking vertices of a bipartite graph, based on the graph's link structure as well as prior information about vertices (which we term a query vector). We present a new solution, BiRank, which iteratively assigns scores to vertices and finally converges to a unique stationary ranking. In contrast to the traditional random walk-based methods, BiRank iterates towards optimizing a regularization function, which smooths the graph under the guidance of the query vector. Importantly, we establish how BiRank relates to the Bayesian methodology, enabling the future extension in a probabilistic way. To show the rationale and extendability of the ranking methodology, we further extend it to rank for the more generic n-partite graphs. BiRank's generic modeling of both the graph structure and vertex features enables it to model various ranking hypotheses flexibly. To illustrate its functionality, we apply the BiRank and TriRank (ranking for tripartite graphs) algorithms to two real-world applications: a general ranking scenario that predicts the future popularity of items, and a personalized ranking scenario that recommends items of interest to users. Extensive experiments on both synthetic and real-world datasets demonstrate BiRank's soundness (fast convergence), efficiency (linear in the number of graph edges) and effectiveness (achieving state-of-the-art in the two real-world tasks).",
                        "Citation Paper Authors": "Authors:Xiangnan He, Ming Gao, Min-Yen Kan, Dingxian Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.05252v1": {
            "Paper Title": "Retrieval & Interaction Machine for Tabular Data Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05152v1": {
            "Paper Title": "Estimation of Fair Ranking Metrics with Incomplete Judgments",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "propose a family of\nfraction based measures by comparing the distributions of different\ngroups to adapt statistical parity into ranked outputs. Zehlike et\nal. ",
                    "Citation Text": "Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Mega-\nhed, and Ricardo Baeza-Yates. 2017. Fa* ir: A fair top-k ranking algorithm. InProceedings of the 2017 ACM on Conference on Information and Knowledge Man-\nagement . 1569\u20131578.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06368",
                        "Citation Paper Title": "Title:FA*IR: A Fair Top-k Ranking Algorithm",
                        "Citation Paper Abstract": "Abstract:In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n >> k candidates, maximizing utility (i.e., select the \"best\" candidates) subject to group fairness criteria. Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum.\nUtility is operationalized in two ways: (i) every candidate included in the top-$k$ should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above. An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria.\nTo the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.",
                        "Citation Paper Authors": "Authors:Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, Ricardo Baeza-Yates"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". Individual fairness\nis concerned with ensuring that similar individuals receive similar\ndecisions; in an IR system, this could mean that documents with\nsimilar relevance to a query should have equivalent probabilities\nof being retrieved in a valuable ranking position ",
                    "Citation Text": "Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. 2018. Equity of attention:\nAmortizing individual fairness in rankings. In The 41st international acm sigir\nconference on research & development in information retrieval . 405\u2013414.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01788",
                        "Citation Paper Title": "Title:Equity of Attention: Amortizing Individual Fairness in Rankings",
                        "Citation Paper Abstract": "Abstract:Rankings of people and items are at the heart of selection-making, match-making, and recommender systems, ranging from employment sites to sharing economy platforms. As ranking positions influence the amount of attention the ranked subjects receive, biases in rankings can lead to unfair distribution of opportunities and resources, such as jobs or income.\nThis paper proposes new measures and mechanisms to quantify and mitigate unfairness from a bias inherent to all rankings, namely, the position bias, which leads to disproportionately less attention being paid to low-ranked subjects. Our approach differs from recent fair ranking approaches in two important ways. First, existing works measure unfairness at the level of subject groups while our measures capture unfairness at the level of individual subjects, and as such subsume group unfairness. Second, as no single ranking can achieve individual attention fairness, we propose a novel mechanism that achieves amortized fairness, where attention accumulated across a series of rankings is proportional to accumulated relevance.\nWe formulate the challenge of achieving amortized individual fairness subject to constraints on ranking quality as an online optimization problem and show that it can be solved as an integer linear program. Our experimental evaluation reveals that unfair attention distribution in rankings can be substantial, and demonstrates that our method can improve individual fairness while retaining high ranking quality.",
                        "Citation Paper Authors": "Authors:Asia J. Biega, Krishna P. Gummadi, Gerhard Weikum"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "studies\nequity of attention among positions in rankings that are relevant to\na query. Morik et al. ",
                    "Citation Text": "Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020.\nControlling Fairness and Bias in Dynamic Learning-to-Rank. arXiv preprint\narXiv:2005.14713 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14713",
                        "Citation Paper Title": "Title:Controlling Fairness and Bias in Dynamic Learning-to-Rank",
                        "Citation Paper Abstract": "Abstract:Rankings are the primary interface through which many online platforms match users to items (e.g. news, products, music, video). In these two-sided markets, not only the users draw utility from the rankings, but the rankings also determine the utility (e.g. exposure, revenue) for the item providers (e.g. publishers, sellers, artists, studios). It has already been noted that myopically optimizing utility to the users, as done by virtually all learning-to-rank algorithms, can be unfair to the item providers. We, therefore, present a learning-to-rank approach for explicitly enforcing merit-based fairness guarantees to groups of items (e.g. articles by the same publisher, tracks by the same artist). In particular, we propose a learning algorithm that ensures notions of amortized group fairness, while simultaneously learning the ranking function from implicit feedback data. The algorithm takes the form of a controller that integrates unbiased estimators for both fairness and utility, dynamically adapting both as more data becomes available. In addition to its rigorous theoretical foundation and convergence guarantees, we find empirically that the algorithm is highly practical and robust.",
                        "Citation Paper Authors": "Authors:Marco Morik, Ashudeep Singh, Jessica Hong, Thorsten Joachims"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "provide a useful\noverview. Most of this work, however, has focused on classification\nand regression models.\nOne key distinction in the algorithmic fairness literature is the\nline between individual andgroup fairness ",
                    "Citation Text": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard\nZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in\ntheoretical computer science conference . 214\u2013226.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1104.3913",
                        "Citation Paper Title": "Title:Fairness Through Awareness",
                        "Citation Paper Abstract": "Abstract:We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.",
                        "Citation Paper Authors": "Authors:Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Rich Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.05073v1": {
            "Paper Title": "ULTRA: An Unbiased Learning To Rank Algorithm Toolbox",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "extends DBGD by ran-\ndomizing multiple perturbed parameters simultaneously in order to\nspeed up convergence. Wang et al. ",
                    "Citation Text": "Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, and Hongn-\ning Wang. 2018. Efficient Exploration of Gradient Space for Online Learning\nto Rank. In The 41st International ACM SIGIR Conference on Research &amp;\nDevelopment in Information Retrieval (Ann Arbor, MI, USA) (SIGIR \u201918) . As-\nsociation for Computing Machinery, New York, NY, USA, 145\u2013154. https:\n//doi.org/10.1145/3209978.3210045",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07317",
                        "Citation Paper Title": "Title:Efficient Exploration of Gradient Space for Online Learning to Rank",
                        "Citation Paper Abstract": "Abstract:Online learning to rank (OL2R) optimizes the utility of returned search results based on implicit feedback gathered directly from users. To improve the estimates, OL2R algorithms examine one or more exploratory gradient directions and update the current ranker if a proposed one is preferred by users via an interleaved test. In this paper, we accelerate the online learning process by efficient exploration in the gradient space. Our algorithm, named as Null Space Gradient Descent, reduces the exploration space to only the \\emph{null space} of recent poorly performing gradients. This prevents the algorithm from repeatedly exploring directions that have been discouraged by the most recent interactions with users. To improve sensitivity of the resulting interleaved test, we selectively construct candidate rankers to maximize the chance that they can be differentiated by candidate ranking documents in the current query; and we use historically difficult queries to identify the best ranker when tie occurs in comparing the rankers. Extensive experimental comparisons with the state-of-the-art OL2R algorithms on several public benchmarks confirmed the effectiveness of our proposal algorithm, especially in its fast learning convergence and promising ranking quality at an early stage.",
                        "Citation Paper Authors": "Authors:Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, Hongning Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.00502v2": {
            "Paper Title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.04468v1": {
            "Paper Title": "End-to-End User Behavior Retrieval in Click-Through RatePrediction Model",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". This baseline is mainly used to show the neces-\nsity of target attention when compared with DIN.\n\u2022DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining . 1059\u20131068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". An industrial dataset is prepared as\nthe supplement for public dataset. Table 3 gives a brief introduction\nof the datasets.\nTaobao Dataset1: This dataset is first released by ",
                    "Citation Text": "Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian\nXu, and Kun Gai. 2019. Joint optimization of tree-based index and deep model\nfor recommender systems. arXiv preprint arXiv:1902.07565 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07565",
                        "Citation Paper Title": "Title:Joint Optimization of Tree-based Index and Deep Model for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Large-scale industrial recommender systems are usually confronted with computational problems due to the enormous corpus size. To retrieve and recommend the most relevant items to users under response time limits, resorting to an efficient index structure is an effective and practical solution. The previous work Tree-based Deep Model (TDM) \\cite{zhu2018learning} greatly improves recommendation accuracy using tree index. By indexing items in a tree hierarchy and training a user-node preference prediction model satisfying a max-heap like property in the tree, TDM provides logarithmic computational complexity w.r.t. the corpus size, enabling the use of arbitrary advanced models in candidate retrieval and recommendation.\nIn tree-based recommendation methods, the quality of both the tree index and the user-node preference prediction model determines the recommendation accuracy for the most part. We argue that the learning of tree index and preference model has interdependence. Our purpose, in this paper, is to develop a method to jointly learn the index structure and user preference prediction model. In our proposed joint optimization framework, the learning of index and user preference prediction model are carried out under a unified performance measure. Besides, we come up with a novel hierarchical user preference representation utilizing the tree index hierarchy. Experimental evaluations with two large-scale real-world datasets show that the proposed method improves recommendation accuracy significantly. Online A/B test results at a display advertising platform also demonstrate the effectiveness of the proposed method in production environments.",
                        "Citation Paper Authors": "Authors:Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian Xu, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "on CTR prediction task, plenty\nof works aim to improve the efficiency and effectiveness of trans-\nformer. Reformer ",
                    "Citation Text": "Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\ntransformer. arXiv preprint arXiv:2001.04451 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.04451",
                        "Citation Paper Title": "Title:Reformer: The Efficient Transformer",
                        "Citation Paper Abstract": "Abstract:Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
                        "Citation Paper Authors": "Authors:Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04452v1": {
            "Paper Title": "High Quality Related Search Query Suggestions using Deep Reinforcement\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "used estimated\nannotated user feedback on generated summaries as the reward\nsignal. Immediate implicit user feedback is sparse, asking users to\nprovide explicit feedback is intrusive ",
                    "Citation Text": "Praveen Kumar Bodigutla, Longshaokan Wang, Kate Ridgeway, Joshua Levy,\nSwanand Joshi, Alborz Geramifard, and Spyros Matsoukas. 2019. Domain-\nIndependent turn-level Dialogue Quality Evaluation via User Satisfaction Esti-\nmation. arXiv:cs.LG/1908.07064",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07064",
                        "Citation Paper Title": "Title:Domain-Independent turn-level Dialogue Quality Evaluation via User Satisfaction Estimation",
                        "Citation Paper Abstract": "Abstract:An automated metric to evaluate dialogue quality is vital for optimizing data driven dialogue management. The common approach of relying on explicit user feedback during a conversation is intrusive and sparse. Current models to estimate user satisfaction use limited feature sets and rely on annotation schemes with low inter-rater reliability, limiting generalizability to conversations spanning multiple domains. To address these gaps, we created a new Response Quality annotation scheme, based on which we developed turn-level User Satisfaction metric. We introduced five new domain-independent feature sets and experimented with six machine learning models to estimate the new satisfaction metric.\nUsing Response Quality annotation scheme, across randomly sampled single and multi-turn conversations from 26 domains, we achieved high inter-annotator agreement (Spearman's rho 0.94). The Response Quality labels were highly correlated (0.76) with explicit turn-level user ratings. Gradient boosting regression achieved best correlation of ~0.79 between predicted and annotated user satisfaction labels. Multi Layer Perceptron and Gradient Boosting regression models generalized to an unseen domain better (linear correlation 0.67) than other models. Finally, our ablation study verified that our novel features significantly improved model performance.",
                        "Citation Paper Authors": "Authors:Praveen Kumar Bodigutla, Longshaokan Wang, Kate Ridgeway, Joshua Levy, Swanand Joshi, Alborz Geramifard, Spyros Matsoukas"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ".\nSupervised reward estimation models are commonly used to\noptimize text-generation policy [ 23,27,30,33]. SeqGAN model used\nGAN ",
                    "Citation Text": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\nAdversarial Nets. In Proceedings of the 27th International Conference on Neural\nInformation Processing Systems - Volume 2 (NIPS\u201914) . MIT Press, Cambridge, MA,\nUSA, 2672\u20132680.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "achieved\nexcellent performance on generating creative text sequences. Most\nrecently, on summary generation task, fine-tuning pre-trained su-\npervised model using Proximal Policy Optimization (PPO) ",
                    "Citation Text": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017).\narXiv:1707.06347 http://arxiv.org/abs/1707.06347",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Reinforcement Learning techniques were\nproposed to address these limitations ",
                    "Citation Text": "Xiao Wang, Craig Macdonald, and Iadh Ounis. 2020. Deep Reinforced Query\nReformulation for Information Retrieval. arXiv:cs.IR/2007.07987",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.07987",
                        "Citation Paper Title": "Title:Deep Reinforced Query Reformulation for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:Query reformulations have long been a key mechanism to alleviate the vocabulary-mismatch problem in information retrieval, for example by expanding the queries with related query terms or by generating paraphrases of the queries. In this work, we propose a deep reinforced query reformulation (DRQR) model to automatically generate new reformulations of the query. To encourage the model to generate queries which can achieve high performance when performing the retrieval task, we incorporate query performance prediction into our reward function. In addition, to evaluate the quality of the reformulated query in the context of information retrieval, we first train our DRQR model, then apply the retrieval ranking model on the obtained reformulated query. Experiments are conducted on the TREC 2020 Deep Learning track MSMARCO document ranking dataset. Our results show that our proposed model outperforms several query reformulation model baselines when performing retrieval task. In addition, improvements are also observed when combining with various retrieval models, such as query expansion and BERT.",
                        "Citation Paper Authors": "Authors:Xiao Wang, Craig Macdonald, Iadh Ounis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.05248v4": {
            "Paper Title": "Practical Relative Order Attack in Deep Ranking",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "datasets which comprise im-\nages of retail commodity. Firstly, we train a CNN with 2-\nconvolution- 1-fully-connected network on Fashion-MNIST,\nand a ResNet-18 ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR , June\n2016. 1, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04031v1": {
            "Paper Title": "DGEM: A New Dual-modal Graph Embedding Method in Recommendation System",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". Among the graph embedding methods based on\nrandom walk ",
                    "Citation Text": "B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning\nof social representations,\u201d in Proceedings of the 20th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining ,\n2014, p. 701\u2013710.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04026v1": {
            "Paper Title": "IntenT5: Search Result Diversification using Causal Language Models",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ".\nCLMs have been used in neural ranking tasks. Nogueira et al . ",
                    "Citation Text": "Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document Ranking with\na Pretrained Sequence-to-Sequence Model. In Findings of EMNLP .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06713",
                        "Citation Paper Title": "Title:Document Ranking with a Pretrained Sequence-to-Sequence Model",
                        "Citation Paper Abstract": "Abstract:This work proposes a novel adaptation of a pretrained sequence-to-sequence model to the task of document ranking. Our approach is fundamentally different from a commonly-adopted classification-based formulation of ranking, based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \"target words\", and how the underlying logits of these target words can be interpreted as relevance probabilities for ranking. On the popular MS MARCO passage ranking task, experimental results show that our approach is at least on par with previous classification-based models and can surpass them with larger, more-recent models. On the test collection from the TREC 2004 Robust Track, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-dataset cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only model in a data-poor regime (i.e., with few training examples). We investigate this observation further by varying target words to probe the model's use of latent knowledge.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". Networks pre-trained\nwith a causal language modeling objective, for instance T5 ",
                    "Citation Text": "Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the Limits of\nTransfer Learning with a Unified Text-to-Text Transformer. ArXiv abs/1910.10683\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "predicted the relevance of a document to a query using the T5\nmodel (monoT5). Pradeep et al . ",
                    "Citation Text": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The Expando-Mono-\nDuo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence\nModels. ArXiv abs/2101.05667 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05667",
                        "Citation Paper Title": "Title:The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models",
                        "Citation Paper Abstract": "Abstract:We propose a design pattern for tackling text ranking problems, dubbed \"Expando-Mono-Duo\", that has been empirically validated for a number of ad hoc retrieval tasks in different domains. At the core, our design relies on pretrained sequence-to-sequence models within a standard multi-stage ranking architecture. \"Expando\" refers to the use of document expansion techniques to enrich keyword representations of texts prior to inverted indexing. \"Mono\" and \"Duo\" refer to components in a reranking pipeline based on a pointwise model and a pairwise model that rerank initial candidates retrieved using keyword search. We present experimental results from the MS MARCO passage and document ranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge that validate our design. In all these tasks, we achieve effectiveness that is at or near the state of the art, in some cases using a zero-shot approach that does not exploit any training data from the target task. To support replicability, implementations of our design pattern are open-sourced in the Pyserini IR toolkit and PyGaggle neural reranking library.",
                        "Citation Paper Authors": "Authors:Ronak Pradeep, Rodrigo Nogueira, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "for more\nfurther details about beam search and text generation strategies.\nWhile CLMs previously accomplished modeling with recurrent\nneural networks [ 24,33], this modeling has recently been accom-\nplished through transformer networks ",
                    "Citation Text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a\nFixed-Length Context. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02860",
                        "Citation Paper Title": "Title:Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                        "Citation Paper Abstract": "Abstract:Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
                        "Citation Paper Authors": "Authors:Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "have\nbeen proposed, but we found these techniques unnecessary for short\ntexts like queries. We refer the reader to Meister et al . ",
                    "Citation Text": "Clara Meister, Tim Vieira, and Ryan Cotterell. 2020. If Beam Search Is the Answer,\nWhat Was the Question?. In EMNLP .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02650",
                        "Citation Paper Title": "Title:If beam search is the answer, what was the question?",
                        "Citation Paper Abstract": "Abstract:Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.",
                        "Citation Paper Authors": "Authors:Clara Meister, Tim Vieira, Ryan Cotterell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04024v1": {
            "Paper Title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ".\nThe model accepts variable-length sequential inputs iVLP,\nwhich consist of a concatenation among words in the text\nsequence(s) w=fw1;:::;w Tg, regional features from the\nimage v=fv1;:::;v Kg, and other optional tokens. For in-\nstance, in OSCAR ",
                    "Citation Text": "X. Li, X. Yin, C. Li, X. Hu, P. Zhang, L. Zhang, L. Wang,\nH. Hu, L. Dong, F. Wei, Y . Choi, and J. Gao. Oscar: Object-\nsemantics aligned pre-training for vision-language tasks. In\nEuropean Conference on Computer Vision , 2020. 2, 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06165",
                        "Citation Paper Title": "Title:Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
                        "Citation Paper Abstract": "Abstract:Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.",
                        "Citation Paper Authors": "Authors:Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "modulates the image feature map condi-\ntioned on text features after the layers of CNN.\n\u2022 Relationship ",
                    "Citation Text": "A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple neu-\nral network module for relational reasoning. In Advances\nin neural information processing systems , pages 4967\u20134976,\n2017. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01427",
                        "Citation Paper Title": "Title:A simple neural network module for relational reasoning",
                        "Citation Paper Abstract": "Abstract:Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.",
                        "Citation Paper Authors": "Authors:Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "uses stacked blocks of element-wise prod-\nucts with residual learning to embed V&L jointly.\n\u2022 FiLM ",
                    "Citation Text": "E. Perez, F. Strub, H. de Vries, V . Dumoulin, and\nA. Courville. Film: Visual reasoning with a general con-\nditioning layer, 2017. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07871",
                        "Citation Paper Title": "Title:FiLM: Visual Reasoning with a General Conditioning Layer",
                        "Citation Paper Abstract": "Abstract:We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.",
                        "Citation Paper Authors": "Authors:Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "for processing text sequences, but introduce the\nfollowing adaptations on image representations.\nRather than including a set of regional features,\nwe pre-process images through an ImageNet pre-trained\nResNet ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In IEEE Conference on Computer\nVision and Pattern Recognition , 2016. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "uses the syn-\nthetic images of geometric 3D shapes from CLEVR ",
                    "Citation Text": "J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.\nZitnick, and R. Girshick. CLEVR: A diagnostic dataset for\ncompositional language and elementary visual reasoning. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.06890",
                        "Citation Paper Title": "Title:CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                        "Citation Paper Abstract": "Abstract:When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "use region-based visual features\nfrom R-CNN models [10, 32] originally proposed for im-\nage captioning ",
                    "Citation Text": "P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,\nS. Gould, and L. Zhang. Bottom-up and top-down atten-\ntion for image captioning and visual question answering. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07998",
                        "Citation Paper Title": "Title:Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
                        "Citation Paper Authors": "Authors:Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06206v1": {
            "Paper Title": "An Intelligent Recommendation-cum-Reminder System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.03586v1": {
            "Paper Title": "PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning &\n  Ranking Balance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.03440v1": {
            "Paper Title": "Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to\n  Rank Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "proposed the idea of aggregate diversity maximization using\na maximum flow approach. They showed that the minimum-cost\nnetwork flow method can be efficiently used for finding recommen-\ndation subgraphs that optimizes the aggregate diversity. Mansoury\net al. in ",
                    "Citation Text": "Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad\nMobasher, and Robin Burke. 2020. FairMatch: A Graph-based Approach for\nImproving Aggregate Diversity in Recommender Systems. In Proceedings of the\n28th ACM Conference on User Modeling, Adaptation and Personalization . 154\u2013162.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.01148",
                        "Citation Paper Title": "Title:FairMatch: A Graph-based Approach for Improving Aggregate Diversity in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems are often biased toward popular items. In other words, few items are frequently recommended while the majority of items do not get proportionate attention. That leads to low coverage of items in recommendation lists across users (i.e. low aggregate diversity) and unfair distribution of recommended items. In this paper, we introduce FairMatch, a general graph-based algorithm that works as a post-processing approach after recommendation generation for improving aggregate diversity. The algorithm iteratively finds items that are rarely recommended yet are high-quality and add them to the users' final recommendation lists. This is done by solving the maximum flow problem on the recommendation bipartite graph. While we focus on aggregate diversity and fair distribution of recommended items, the algorithm can be adapted to other recommendation scenarios using different underlying definitions of fairness. A comprehensive set of experiments on two datasets and comparison with state-of-the-art baselines show that FairMatch, while significantly improving aggregate diversity, provides comparable recommendation accuracy.",
                        "Citation Paper Authors": "Authors:Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, Robin Burke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.03311v1": {
            "Paper Title": "Profiling Web Archival Voids for Memento Routing",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". Kelly et al. developed a framework to archive the\nprivate web and integrate it with the public web to \ufb01ll some of\nthese cavities ",
                    "Citation Text": "M. Kelly, M. L. Nelson, and M. C. Weigle, \u201cA Framework for Aggre-\ngating Private and Public Web Archives,\u201d in Proceedings of the 18th\nACM/IEEE on Joint Conference on Digital Libraries , ser. JCDL \u201918,\n2018, pp. 273\u2013282.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.00871",
                        "Citation Paper Title": "Title:A Framework for Aggregating Private and Public Web Archives",
                        "Citation Paper Abstract": "Abstract:Personal and private Web archives are proliferating due to the increase in the tools to create them and the realization that Internet Archive and other public Web archives are unable to capture personalized (e.g., Facebook) and private (e.g., banking) Web pages. We introduce a framework to mitigate issues of aggregation in private, personal, and public Web archives without compromising potential sensitive information contained in private captures. We amend Memento syntax and semantics to allow TimeMap enrichment to account for additional attributes to be expressed inclusive of the requirements for dereferencing private Web archive captures. We provide a method to involve the user further in the negotiation of archival captures in dimensions beyond time. We introduce a model for archival querying precedence and short-circuiting, as needed when aggregating private and personal Web archive captures with those from public Web archives through Memento. Negotiation of this sort is novel to Web archiving and allows for the more seamless aggregation of various types of Web archives to convey a more accurate picture of the past Web.",
                        "Citation Paper Authors": "Authors:Mat Kelly, Michael L. Nelson, Michele C. Weigle"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.08553v4": {
            "Paper Title": "Generation-Augmented Retrieval for Open-domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.00294v3": {
            "Paper Title": "Rider: Reader-Guided Passage Reranking for Open-Domain Question\n  Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.03052v1": {
            "Paper Title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.02625v1": {
            "Paper Title": "MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics\n  Transcription",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06210v1": {
            "Paper Title": "Recommending Insurance products by using Users' Sentiments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.02344v1": {
            "Paper Title": "LHRM: A LBS based Heterogeneous Relations Model for User Cold Start\n  Recommendation in Online Travel Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.02343v1": {
            "Paper Title": "Itinerary-aware Personalized Deep Matching at Fliggy",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "has become the most popular techniqu e\ninbothacademiaandindustrycirclesforsolvingthematchi ngand\nrankingproblemsinRSs[6,7,11,17,19,32,34,35],duetoit sbetter\nrepresentation andgeneralizationabilitycomparedwithC Fbased\nsolutions ",
                    "Citation Text": "Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Kep ing Yang, and Wil-\nfred Ng. 2019. SDM: Sequential Deep Matching Model for Onlin e Large-scale\nRecommender System. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management, CIKM 2019, Beijin g, China, Novem-\nber 3-7, 2019 , Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Run-\ndensteiner, David Carmel, Qi He, and Je\ufb00rey Xu Yu (Eds.). ACM , 2635\u20132643.\nhttps://doi.org/10.1145/3357384.3357818",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.00385",
                        "Citation Paper Title": "Title:SDM: Sequential Deep Matching Model for Online Large-scale Recommender System",
                        "Citation Paper Abstract": "Abstract:Capturing users' precise preferences is a fundamental problem in large-scale recommender system. Currently, item-based Collaborative Filtering (CF) methods are common matching approaches in industry. However, they are not effective to model dynamic and evolving preferences of users. In this paper, we propose a new sequential deep matching (SDM) model to capture users' dynamic preferences by combining short-term sessions and long-term behaviors. Compared with existing sequence-aware recommendation methods, we tackle the following two inherent problems in real-world applications: (1) there could exist multiple interest tendencies in one session. (2) long-term preferences may not be effectively fused with current session interests. Long-term behaviors are various and complex, hence those highly related to the short-term session should be kept for fusion. We propose to encode behavior sequences with two corresponding components: multi-head self-attention module to capture multiple types of interests and long-short term gated fusion module to incorporate long-term preferences. Successive items are recommended after matching between sequential user behavior vector and item embedding vectors. Offline experiments on real-world datasets show the superior performance of the proposed SDM. Moreover, SDM has been successfully deployed on online large-scale recommender system at Taobao and achieves improvements in terms of a range of commercial metrics.",
                        "Citation Paper Authors": "Authors:Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, Wilfred Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07511v3": {
            "Paper Title": "Ensemble of MRR and NDCG models for Visual Dialog",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01928v1": {
            "Paper Title": "How to Query Language Models?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01915v1": {
            "Paper Title": "An analytical study of content and contexts of keywords on physics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01632v2": {
            "Paper Title": "An Interpretable Music Similarity Measure Based on Path Interestingness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01711v1": {
            "Paper Title": "Improving Music Performance Assessment with Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". Khosla et al. investigate a supervised\ncontrastive loss to train deep neural networks for classi-\n\ufb01cation tasks on the ImageNet dataset, and found that it\noutperforms general cross entropy based methods ",
                    "Citation Text": "P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y . Tian,\nP. Isola, A. Maschinot, C. Liu, and D. Krishnan, \u201cSu-\npervised Contrastive Learning,\u201d Advances in Neural\nInformation Processing Systems , vol. 33, pp. 18 661\u2013\n18 673, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11362",
                        "Citation Paper Title": "Title:Supervised Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.01532v1": {
            "Paper Title": "Effective Model Integration Algorithm for Improving Link and Sign\n  Prediction in Complex Networks",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "is an online socialnetworkand consumerreview\nsite whose members can decide whether to \u201ctrust\u201d each\nother. (vii) Wikirfa ",
                    "Citation Text": "Robert West, Hristo S Paskov, Jure Leskovec, and\nChristopher Potts. Exploiting social network structure\nfor person-to-person sentiment analysis. Transactions\nof the Association for Computational Linguistics , 2:297\u2013\n310, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.2450",
                        "Citation Paper Title": "Title:Exploiting Social Network Structure for Person-to-Person Sentiment Analysis",
                        "Citation Paper Abstract": "Abstract:Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A's opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus.",
                        "Citation Paper Authors": "Authors:Robert West, Hristo S. Paskov, Jure Leskovec, Christopher Potts"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "as the basic classi\ufb01ers (see Fig. 1(E)).\nTaking the Slashdot ",
                    "Citation Text": "Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg .\nSigned networks in social media. In Proceedings of the\nSIGCHI Conference on Human Factors in Computing\nSystems, pages 1361\u20131370, 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.2424",
                        "Citation Paper Title": "Title:Signed Networks in Social Media",
                        "Citation Paper Abstract": "Abstract:Relations between users on social media sites often reflect a mixture of positive (friendly) and negative (antagonistic) interactions. In contrast to the bulk of research on social networks that has focused almost exclusively on positive interpretations of links between people, we study how the interplay between positive and negative relationships affects the structure of on-line social networks. We connect our analyses to theories of signed networks from social psychology. We find that the classical theory of structural balance tends to capture certain common patterns of interaction, but that it is also at odds with some of the fundamental phenomena we observe --- particularly related to the evolving, directed nature of these on-line networks.  We then develop an alternate theory of status that better explains the observed edge signs and provides insights into the underlying social mechanisms. Our work provides one of the first large-scale evaluations of theories of signed networks using on-line datasets, as well as providing a perspective for reasoning about social media sites.",
                        "Citation Paper Authors": "Authors:Jure Leskovec, Daniel Huttenlocher, Jon Kleinberg"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ".\nSome models explore user personality as complementary\ninformation from social media ",
                    "Citation Text": "Ghazaleh Beigi, Suhas Ranganath, andHuan Liu. Signed\nlink prediction with sparse data: The role of personal-\nity information. In Companion Proceedings of The 2019\nWorld Wide Web Conference , pages 1270\u20131278, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.02125",
                        "Citation Paper Title": "Title:Signed Link Prediction with Sparse Data: The Role of Personality Information",
                        "Citation Paper Abstract": "Abstract:Predicting signed links in social networks often faces the problem of signed link data sparsity, i.e., only a small percentage of signed links are given. The problem is exacerbated when the number of negative links is much smaller than that of positive links. Boosting signed link prediction necessitates additional information to compensate for data sparsity. According to psychology theories, one rich source of such information is user's personality such as optimism and pessimism that can help determine her propensity in establishing positive and negative links. In this study, we investigate how personality information can be obtained, and if personality information can help alleviate the data sparsity problem for signed link prediction. We propose a novel signed link prediction model that enables empirical exploration of user personality via social media data. We evaluate our proposed model on two datasets of real-world signed link networks. The results demonstrate the complementary role of personality information in the signed link prediction problem. Experimental results also indicate the effectiveness of different levels of personality information for signed link data sparsity problem.",
                        "Citation Paper Authors": "Authors:Ghazaleh Beigi, Suhas Ranganath, Huan Liu"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", and it uses links as super-\nvising information for link prediction instead of learningnode representation from node label information. Be-\nsides the link prediction algorithms, the perturbation of\nthe adjacency matrix ",
                    "Citation Text": "Linyuan Lu, Liming Pan, Tao Zhou, Yi-Cheng Zhang,\nand Stanley H. Eugene. Toward link predictability of\ncomplex networks. Proceedings of the National Academy\nof Sciences of the United States of America , 112:2325\u2013\n2330, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1010.0725",
                        "Citation Paper Title": "Title:Link Prediction in Complex Networks: A Survey",
                        "Citation Paper Abstract": "Abstract:Link prediction in complex networks has attracted increasing attention from both physical and computer science communities. The algorithms can be used to extract missing information, identify spurious interactions, evaluate network evolving mechanisms, and so on. This article summaries recent progress about link prediction algorithms, emphasizing on the contributions from physical perspectives and approaches, such as the random-walk-based methods and the maximum likelihood methods. We also introduce three typical applications: reconstruction of networks, evaluation of network evolving mechanism and classification of partially labelled networks. Finally, we introduce some applications and outline future challenges of link prediction algorithms.",
                        "Citation Paper Authors": "Authors:Linyuan Lu, Tao Zhou"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "simultaneously learns\nrelation and entity embeddings. DeepLinker deploys a\ngraph attention network ",
                    "Citation Text": "Weiwei Gu, Fei Gao, Xiaodan Lou, and Jiang Zhang.\nLink prediction via graph attention network. arXiv\npreprint arXiv:1910.04807 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.04807",
                        "Citation Paper Title": "Title:Link Prediction via Graph Attention Network",
                        "Citation Paper Abstract": "Abstract:Link prediction aims to infer missing links or predicting the future ones based on currently observed partial networks, it is a fundamental problem in network science with tremendous real-world applications. However, conventional link prediction approaches neither have high prediction accuracy nor being capable of revealing the hidden information behind links. To address this problem, we generalize the latest techniques in deep learning on graphs and present a new link prediction model - DeepLinker. Instead of learning node representation with the node label information, DeepLinker uses the links as supervised information. Experiments on five graphs show that DeepLinker can not only achieve the state-of-the-art link prediction accuracy, but also acquire the efficient node representations and node centrality ranking as the byproducts. Although the representations are obtained without any supervised node label information, they still perform well on node ranking and node classification tasks.",
                        "Citation Paper Authors": "Authors:Weiwei Gu, Fei Gao, Xiaodan Lou, Jiang Zhang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "is network embedding\nmethod, which is based on the Skip-gram language\nmodel. It cannot distinguish positive and negative\nedges, so the sign information wasdiscarded during\nthe random walk in sign prediction.\n\u2022LINE ",
                    "Citation Text": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun\nYan, and Qiaozhu Mei. Line: Large-scale information\nnetwork embedding. In Proceedings of the 24th Interna-\ntional Conference on World Wide Web , pages 1067\u20131077,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Amir Ghasemian, Homa Hosseinmardi, and Aaron\nClauset. Evaluating over\ufb01t and under\ufb01t in models of\nnetwork community structure. IEEE Transactions on\nKnowledge and Data Engineering , 2019. ",
                    "Citation Text": "Aaron Clauset, Cristopher Moore, and Mark EJ New-\nman. Hierarchical structure and the prediction of missing\nlinks in networks. Nature, 453(7191):98\u2013101, 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0811.0484",
                        "Citation Paper Title": "Title:Hierarchical structure and the prediction of missing links in networks",
                        "Citation Paper Abstract": "Abstract:  Networks have in recent years emerged as an invaluable tool for describing and quantifying complex systems in many branches of science. Recent studies suggest that networks often exhibit hierarchical organization, where vertices divide into groups that further subdivide into groups of groups, and so forth over multiple scales. In many cases these groups are found to correspond to known functional units, such as ecological niches in food webs, modules in biochemical networks (protein interaction networks, metabolic networks, or genetic regulatory networks), or communities in social networks. Here we present a general technique for inferring hierarchical structure from network data and demonstrate that the existence of hierarchy can simultaneously explain and quantitatively reproduce many commonly observed topological properties of networks, such as right-skewed degree distributions, high clustering coefficients, and short path lengths. We further show that knowledge of hierarchical structure can be used to predict missing connections in partially known networks with high accuracy, and for more general network structures than competing techniques. Taken together, our results suggest that hierarchy is a central organizing principle of complex networks, capable of offering insight into many network phenomena.",
                        "Citation Paper Authors": "Authors:Aaron Clauset, Cristopher Moore, M.E.J. Newman"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "Tao Zhou, Linyuan L\u00a8 u, and Yi-Cheng Zhang. Predict-\ning missing links via local information. The European\nPhysical Journal B , 71(4):623\u2013630, 2009. ",
                    "Citation Text": "Amir Ghasemian, Homa Hosseinmardi, and Aaron\nClauset. Evaluating over\ufb01t and under\ufb01t in models of\nnetwork community structure. IEEE Transactions on\nKnowledge and Data Engineering , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.10582",
                        "Citation Paper Title": "Title:Evaluating Overfit and Underfit in Models of Network Community Structure",
                        "Citation Paper Abstract": "Abstract:A common data mining task on networks is community detection, which seeks an unsupervised decomposition of a network into structural groups based on statistical regularities in the network's connectivity. Although many methods exist, the No Free Lunch theorem for community detection implies that each makes some kind of tradeoff, and no algorithm can be optimal on all inputs. Thus, different algorithms will over or underfit on different inputs, finding more, fewer, or just different communities than is optimal, and evaluation methods that use a metadata partition as a ground truth will produce misleading conclusions about general accuracy. Here, we present a broad evaluation of over and underfitting in community detection, comparing the behavior of 16 state-of-the-art community detection algorithms on a novel and structurally diverse corpus of 406 real-world networks. We find that (i) algorithms vary widely both in the number of communities they find and in their corresponding composition, given the same input, (ii) algorithms can be clustered into distinct high-level groups based on similarities of their outputs on real-world networks, and (iii) these differences induce wide variation in accuracy on link prediction and link description tasks. We introduce a new diagnostic for evaluating overfitting and underfitting in practice, and use it to roughly divide community detection methods into general and specialized learning algorithms. Across methods and inputs, Bayesian techniques based on the stochastic block model and a minimum description length approach to regularization represent the best general learning approach, but can be outperformed under specific circumstances. These results introduce both a theoretically principled approach to evaluate over and underfitting in models of network community structure and a realistic benchmark by which new methods may be evaluated and compared.",
                        "Citation Paper Authors": "Authors:Amir Ghasemian, Homa Hosseinmardi, Aaron Clauset"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "David Liben-Nowell and Jon Kleinberg. The link-\nprediction problem for social networks. Journal of the\nAmerican society for Information Science and Technol-\nogy, 58(7):1019\u20131031, 2007. ",
                    "Citation Text": "Tao Zhou, Linyuan L\u00a8 u, and Yi-Cheng Zhang. Predict-\ning missing links via local information. The European\nPhysical Journal B , 71(4):623\u2013630, 2009.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0901.0553",
                        "Citation Paper Title": "Title:Predicting Missing Links via Local Information",
                        "Citation Paper Abstract": "Abstract:  Missing link prediction of networks is of both theoretical interest and practical significance in modern science. In this paper, we empirically investigate a simple framework of link prediction on the basis of node similarity. We compare nine well-known local similarity measures on six real networks. The results indicate that the simplest measure, namely common neighbors, has the best overall performance, and the Adamic-Adar index performs the second best. A new similarity measure, motivated by the resource allocation process taking place on networks, is proposed and shown to have higher prediction accuracy than common neighbors. It is found that many links are assigned same scores if only the information of the nearest neighbors is used. We therefore design another new measure exploited information of the next nearest neighbors, which can remarkably enhance the prediction accuracy.",
                        "Citation Paper Authors": "Authors:Tao Zhou, Linyuan Lu, Yi-Cheng Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.01053v1": {
            "Paper Title": "Cold Start Similar Artists Ranking with Gravity-Inspired Graph\n  Autoencoders",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": ". In such a setting, the simplest strategy for similar items ranking would consist in relying on pop-\nularity metrics ",
                    "Citation Text": "Markus Schedl, Hamed Zamani, Ching-Wei Chen, Yashar Deldjoo, and Mehdi Elahi. 2018. Current Challenges and Visions in Music Recommender\nSystems Research. Int. Journal of Multimedia Information Retrieval 7, 2 (2018), 95\u2013116.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.03208",
                        "Citation Paper Title": "Title:Current Challenges and Visions in Music Recommender Systems Research",
                        "Citation Paper Abstract": "Abstract:Music recommender systems (MRS) have experienced a boom in recent years, thanks to the emergence and success of online streaming services, which nowadays make available almost all music in the world at the user's fingertip. While today's MRS considerably help users to find interesting music in these huge catalogs, MRS research is still facing substantial challenges. In particular when it comes to build, incorporate, and evaluate recommendation strategies that integrate information beyond simple user--item interactions or content-based descriptors, but dig deep into the very essence of listener needs, preferences, and intentions, MRS research becomes a big endeavor and related publications quite sparse.\nThe purpose of this trends and survey article is twofold. We first identify and shed light on what we believe are the most pressing challenges MRS research is facing, from both academic and industry perspectives. We review the state of the art towards solving these challenges and discuss its limitations. Second, we detail possible future directions and visions we contemplate for the further evolution of the field. The article should therefore serve two purposes: giving the interested reader an overview of current challenges in MRS research and providing guidance for young researchers by identifying interesting, yet under-researched, directions in the field.",
                        "Citation Paper Authors": "Authors:Markus Schedl, Hamed Zamani, Ching-Wei Chen, Yashar Deldjoo, Mehdi Elahi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.01473v1": {
            "Paper Title": "A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation\n  with Nonoverlapping Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00814v2": {
            "Paper Title": "Relevance-guided Supervision for OpenQA with ColBERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.00775v1": {
            "Paper Title": "Self-supervised Answer Retrieval on Clinical Notes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06387v2": {
            "Paper Title": "A Zero Attentive Relevance Matching Networkfor Review Modeling in\n  Recommendation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.00529v1": {
            "Paper Title": "BigGraphVis: Leveraging Streaming Algorithms and GPU Acceleration for\n  Visualizing Big Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09060v4": {
            "Paper Title": "Self-Supervised Learning of Context-Aware Pitch Prosody Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07734v2": {
            "Paper Title": "SamWalker++: recommendation with informative sampling strategy",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "also leverage social information to\nhandle non-random missing data.\nAlso, there are two recent works claim that comparing\nwith users\u2019 preference, users\u2019 exposure is more in\ufb02uenced\nby their social friends (neighbors). Thus, ",
                    "Citation Text": "M. Wang, X. Zheng, Y. Yang, and K. Zhang, \u201cCollaborative \ufb01ltering\nwith social exposure: A modular approach to social recommenda-\ntion,\u201d in AAAI, New Orleans, Louisiana, USA, February 2-7, 2018 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11458",
                        "Citation Paper Title": "Title:Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation",
                        "Citation Paper Abstract": "Abstract:This paper is concerned with how to make efficient use of social information to improve recommendations. Most existing social recommender systems assume people share similar preferences with their social friends. Which, however, may not hold true due to various motivations of making online friends and dynamics of online social networks. Inspired by recent causal process based recommendations that first model user exposures towards items and then use these exposures to guide rating prediction, we utilize social information to capture user exposures rather than user preferences. We assume that people get information of products from their online friends and they do not have to share similar preferences, which is less restrictive and seems closer to reality. Under this new assumption, in this paper, we present a novel recommendation approach (named SERec) to integrate social exposure into collaborative filtering. We propose two methods to implement SERec, namely social regularization and social boosting, each with different ways to construct social exposures. Experiments on four real-world datasets demonstrate that our methods outperform the state-of-the-art methods on top-N recommendations. Further study compares the robustness and scalability of the two proposed methods.",
                        "Citation Paper Authors": "Authors:Menghan Wang, Xiaolin Zheng, Yang Yang, Kun Zhang"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ") used a simple heuristic where all nega-\ntive feedback data are equally downweighted vis-a-vis the\npositive feedback data; ",
                    "Citation Text": "X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua, \u201cFast matrix fac-\ntorization for online recommendation with implicit feedback,\u201d\ninProceedings of the 39th International ACM SIGIR conference on\nResearch and Development in Information Retrieval . ACM, 2016, pp.\n549\u2013558.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05024",
                        "Citation Paper Title": "Title:Fast Matrix Factorization for Online Recommendation with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our eALS method consistently outperforms state-of-the-art implicit MF methods. Our implementation is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Hanwang Zhang, Min-Yen Kan, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "pro-\npose item popularity-based and item-user co-bias sampling\nstrategy to reduce sampling variance; ",
                    "Citation Text": "T. Chen, Y. Sun, Y. Shi, and L. Hong, \u201cOn sampling strategies for\nneural network-based collaborative \ufb01ltering,\u201d in Proceedings of the\n23rd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining . ACM, 2017, pp. 767\u2013776.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.07881",
                        "Citation Paper Title": "Title:On Sampling Strategies for Neural Network-based Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing state-of-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the user-item interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions \"graph-based\" loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to $\\times 30$ times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graph-based loss functions, and would also enable more research under the neural network-based recommendation framework.",
                        "Citation Paper Authors": "Authors:Ting Chen, Yizhou Sun, Yue Shi, Liangjie Hong"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "assign the con\ufb01dence\nweights based on item popularity. The readers can refer to\nthe survey ",
                    "Citation Text": "J. Chen, H. Dong, X. Wang, F. Feng, M. Wang, and X. He, \u201cBias and\ndebias in recommender system: A survey and future directions,\u201d\narXiv preprint arXiv:2010.03240 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.03240",
                        "Citation Paper Title": "Title:Bias and Debias in Recommender System: A Survey and Future Directions",
                        "Citation Paper Abstract": "Abstract:While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, etc. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology ``bias'' is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic. The summary of debiasing methods reviewed in this survey can be found at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, Xiangnan He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.06423v3": {
            "Paper Title": "Learning to Recommend Items to Wikidata Editors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06216v1": {
            "Paper Title": "MAIR: Framework for mining relationships between research articles,\n  strategies, and regulations in the field of explainable artificial\n  intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13752v1": {
            "Paper Title": "ExpertRank: A Multi-level Coarse-grained Expert-based Listwise Ranking\n  Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13751v1": {
            "Paper Title": "The Cross-Lingual Arabic Information REtrieval (CLAIRE) System",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "have been recently proposed. They extract semantic\nmatching signals based on word-level embeddings of queries and\ndocuments. Usually, word embeddings are initially trained via skip-\ngram ",
                    "Citation Text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching Word Vectors with Subword Information. Transactions of the Association\nfor Computational Linguistics 5, 0 (2017), 135\u2013146. https://transacl.org/ojs/index.\nphp/tacl/article/view/999",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". (2) Pseudo-cross-lingual, trains word\nembedding models on a pseudo-cross-lingual corpus by mixing\ncontexts of di\ufffferent languages [ 8?]. (3) Joint optimization, jointly\noptimises a combination of monolingual and cross-lingual losses ",
                    "Citation Text": "Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, and Amine Benhal-\nloum. 2015. Trans-gram, Fast Cross-lingual Word-embeddings. In Proceed-\nings of the 2015 Conference on Empirical Methods in Natural Language Process-\ning. Association for Computational Linguistics, Lisbon, Portugal, 1109\u20131113.\nhttps://doi.org/10.18653/v1/D15-1131",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.02502",
                        "Citation Paper Title": "Title:Trans-gram, Fast Cross-lingual Word-embeddings",
                        "Citation Paper Abstract": "Abstract:We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.",
                        "Citation Paper Authors": "Authors:Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, Amine Benhalloum"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". Also, it opens the retrieval system to the\npossibilities of exploring semantic matching signals across di\ufffferent\nlanguages directly and explicitly.\nThere are four major ways to construct cross-lingual word em-\nbeddings ",
                    "Citation Text": "Sebastian Ruder, Ivan Vuli\u0107, and Anders S\u00f8gaard. 2019. A Survey of Cross-\nLingual Word Embedding Models. J. Artif. Int. Res. 65, 1 (May 2019), 569\u2013630.\nhttps://doi.org/10.1613/jair.1.11640",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04902",
                        "Citation Paper Title": "Title:A Survey Of Cross-lingual Word Embedding Models",
                        "Citation Paper Abstract": "Abstract:Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.",
                        "Citation Paper Authors": "Authors:Sebastian Ruder, Ivan Vuli\u0107, Anders S\u00f8gaard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.15843v2": {
            "Paper Title": "A Tale of Two Efficient and Informative Negative Sampling Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13637v1": {
            "Paper Title": "Sign and Search: Sign Search Functionality for Sign Language Lexica",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13617v1": {
            "Paper Title": "Pitch-Informed Instrument Assignment Using a Deep Convolutional Network\n  with Multiple Kernel Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13602v1": {
            "Paper Title": "Domain-matched Pre-training Tasks for Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13327v1": {
            "Paper Title": "Ranker-agnostic Contextual Position Bias Estimation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ".\nOne simple method to compute the position bias without model-\ning the relevance, is to apply a controlled randomization by swap-\nping the order of the presented items ",
                    "Citation Text": "Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased\nLearning-to-Rank with Biased Feedback. In Proceedings of the Tenth ACM Interna-\ntional Conference on Web Search and Data Mining (Cambridge, United Kingdom)\n(WSDM \u201917) . Association for Computing Machinery, New York, NY, USA, 781\u2013789.\nhttps://doi.org/10.1145/3018661.3018699",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.04468",
                        "Citation Paper Title": "Title:Unbiased Learning-to-Rank with Biased Feedback",
                        "Citation Paper Abstract": "Abstract:Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-bias the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.",
                        "Citation Paper Authors": "Authors:Thorsten Joachims, Adith Swaminathan, Tobias Schnabel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.13052v1": {
            "Paper Title": "Understanding and Generalizing Monotonic Proximity Graphs for\n  Approximate Nearest Neighbor Search",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". For each query, it then performs a walk, which eventu-\nally converges to the nearest neighbor in logarithmic complexity. Subsequently, Fu et al. proposed\nNSG, which approximates Monotonic Relative Neighbor Graph (MRNG) ",
                    "Citation Text": "Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast Approximate Nearest Neighbor Search with\nthe Navigating Spreading-out Graph. In VLDB'19 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.00143",
                        "Citation Paper Title": "Title:Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph",
                        "Citation Paper Abstract": "Abstract:Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.",
                        "Citation Paper Authors": "Authors:Cong Fu, Chao Xiang, Changxu Wang, Deng Cai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.12677v1": {
            "Paper Title": "Deep Variational Models for Collaborative Filtering-based Recommender\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.12360v1": {
            "Paper Title": "IRLCov19: A Large COVID-19 Multilingual Twitter Dataset of Indian\n  Regional Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.04655v1": {
            "Paper Title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07619v2": {
            "Paper Title": "MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models\n  by Instance-Guided Mask",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "propose a novel multi-task learning approach, Multi-gate\nMixture-of-Experts (MMoE), which explicitly learns to model task\nrelationships from data. Ma et al. ",
                    "Citation Text": "Chen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical gating networks for\nsequential recommendation. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 825\u2013833.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.09217",
                        "Citation Paper Title": "Title:Hierarchical Gating Networks for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:The chronological order of user-item interactions is a key feature in many recommender systems, where the items that users will interact may largely depend on those items that users just accessed recently. However, with the tremendous increase of users and items, sequential recommender systems still face several challenging problems: (1) the hardness of modeling the long-term user interests from sparse implicit feedback; (2) the difficulty of capturing the short-term user interests given several items the user just accessed. To cope with these challenges, we propose a hierarchical gating network (HGN), integrated with the Bayesian Personalized Ranking (BPR) to capture both the long-term and short-term user interests. Our HGN consists of a feature gating module, an instance gating module, and an item-item product module. In particular, our feature gating and instance gating modules select what item features can be passed to the downstream layers from the feature and instance levels, respectively. Our item-item product module explicitly captures the item relations between the items that users accessed in the past and those items users will access in the future. We extensively evaluate our model with several state-of-the-art methods and different validation metrics on five real-world datasets. The experimental results demonstrate the effectiveness of our model on Top-N sequential recommendation.",
                        "Citation Paper Authors": "Authors:Chen Ma, Peng Kang, Xue Liu"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "can dynamically learn feature importance via\nthe Squeeze-Excitation network (SENET) mechanism and feature\ninteractions via bilinear function.\n2.2 Feature-Wise Mask Or Gating\nFeature-wise mask or gating has been explored widely in vision [ 8,\n20], natural language processing ",
                    "Citation Text": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Lan-\nguage modeling with gated convolutional networks. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70 . JMLR. org, 933\u2013941.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.08083",
                        "Citation Paper Title": "Title:Language Modeling with Gated Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
                        "Citation Paper Authors": "Authors:Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "utilize feature gating\nto ease gradient-based training of very deep networks. Squeeze-\nand-Excitation Networks ",
                    "Citation Text": "Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition . 7132\u20137141.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01507",
                        "Citation Paper Title": "Title:Squeeze-and-Excitation Networks",
                        "Citation Paper Abstract": "Abstract:The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "uses a multi-head self-attentive neural network to\nexplicitly model the feature interactions in the low-dimensional\nspace. FiBiNET ",
                    "Citation Text": "Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-\nture importance and bilinear feature interaction for click-through rate prediction.\nInProceedings of the 13th ACM Conference on Recommender Systems . 169\u2013177.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09433",
                        "Citation Paper Title": "Title:FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).",
                        "Citation Paper Authors": "Authors:Tongwen Huang, Zhiqi Zhang, Junlin Zhang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "also models the low-\norder and high-order feature interactions in an explicit way by\nproposing a novel Compressed Interaction Network (CIN) part.\nAutoInt ",
                    "Citation Text": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\nattentive neural networks. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management . 1161\u20131170.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11921",
                        "Citation Paper Title": "Title:AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features (\\textit{a.k.a.} cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the \\emph{AutoInt} to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "is\na feed-forward neural network using FM to pre-train the embed-\nding layer. Wide & Deep Learning ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.12025v1": {
            "Paper Title": "ContextNet: A Click-Through Rate Prediction Framework Using Contextual\n  information to Refine Feature Embedding",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "proposes a multi-head self-attentive neural network with residual\nconnections to explicitly model the feature interactions in the low-\ndimensional space. Fi-GNN ",
                    "Citation Text": "Z. Li, Z. Cui, S. Wu, X. Zhang, and L. Wang. 2019. Fi-GNN: Modeling Feature\nInteractions via Graph Neural Networks for CTR Prediction. (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.05552",
                        "Citation Paper Title": "Title:Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is an essential task in web applications such as online advertising and recommender systems, whose features are usually in multi-field form. The key of this task is to model feature interactions among different feature fields. Recently proposed deep learning based models follow a general paradigm: raw sparse input multi-filed features are first mapped into dense field embedding vectors, and then simply concatenated together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. However, the simple \\emph{unstructured combination} of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.\nIn this work, we propose to represent the multi-field features in a graph structure intuitively, where each node corresponds to a feature field and different fields can interact through edges. The task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph. To this end, we design a novel model Feature Interaction Graph Neural Networks (Fi-GNN). Taking advantage of the strong representative power of graphs, our proposed model can not only model sophisticated feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction. Experimental results on two real-world datasets show its superiority over the state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "represents the multi-field features\nin a graph structure, where each node corresponds to a feature field\nand different fields can interact through edges. Though AutoInt ",
                    "Citation Text": "Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.\n2016. Product-based neural networks for user response prediction. In 2016 IEEE\n16th International Conference on Data Mining (ICDM) . IEEE, 1149\u20131154.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00144",
                        "Citation Paper Title": "Title:Product-based Neural Networks for User Response Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.",
                        "Citation Paper Authors": "Authors:Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "can dynamically learn feature\nimportance via the Squeeze-Excitation network (SENET) mecha-\nnism and feature interactions via bilinear function. AutoInt ",
                    "Citation Text": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\nattentive neural networks. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management . 1161\u20131170.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11921",
                        "Citation Paper Title": "Title:AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features (\\textit{a.k.a.} cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the \\emph{AutoInt} to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "also models the low-order and high-order feature interactions\nin an explicit way by proposing a novel Compressed Interaction\nNetwork (CIN) part. FiBiNET ",
                    "Citation Text": "Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-\nture importance and bilinear feature interaction for click-through rate prediction.\nInProceedings of the 13th ACM Conference on Recommender Systems . 169\u2013177.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09433",
                        "Citation Paper Title": "Title:FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).",
                        "Citation Paper Authors": "Authors:Tongwen Huang, Zhiqi Zhang, Junlin Zhang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "has proved MLP is an ineffective way to capture\nthe high-order interactions.\nSome works explicitly introduce high-order feature interactions\nby sub-network. Deep & Cross Network (DCN) ",
                    "Citation Text": "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network\nfor ad click predictions. In Proceedings of the ADKDD\u201917 . ACM, 12.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is\na feed-forward neural network using FM to pre-train the embedding\nlayer. Wide & Deep Learning ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.12024v1": {
            "Paper Title": "Leaf-FM: A Learnable Feature Generation Factorization Machine for\n  Click-Through Rate Prediction",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "normalizes the features by the mean and\nvariance computed within a mini-batch. This has been shown by\nmany practices to ease optimization and enable very deep networks\nto converge. Another example is layer normalization (Layer Norm\nor LN) ",
                    "Citation Text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06450",
                        "Citation Paper Title": "Title:Layer Normalization",
                        "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                        "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "also mod-\nels the low-order and high-order feature interactions in an explicit\nway by proposing a novel Compressed Interaction Network (CIN)\npart. AutoInt ",
                    "Citation Text": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\nattentive neural networks. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management . 1161\u20131170.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11921",
                        "Citation Paper Title": "Title:AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features (\\textit{a.k.a.} cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the \\emph{AutoInt} to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "replaces the wide part of\nWide & Deep model with FM and shares the feature embedding\nbetween the FM and deep component. FiBiNET ",
                    "Citation Text": "Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-\nture importance and bilinear feature interaction for click-through rate prediction.\nInProceedings of the 13th ACM Conference on Recommender Systems . 169\u2013177.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09433",
                        "Citation Paper Title": "Title:FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).",
                        "Citation Paper Authors": "Authors:Tongwen Huang, Zhiqi Zhang, Junlin Zhang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "by combining shallow components with deep ones to\ncapture both low- and high-order feature interactions. Wide & Deep\nLearning ",
                    "Citation Text": "Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04617",
                        "Citation Paper Title": "Title:Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks",
                        "Citation Paper Abstract": "Abstract:Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", are feed-forward neural networks us-\ning FM to pre-train the embedding layer. More recently, hybrid\narchitectures are introduced in Wide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .\n2016. Wide & deep learning for recommender systems. In Proceedings of the 1st\nworkshop on deep learning for recommender systems . ACM, 7\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". However, these models\nshow limited effectiveness in mining high-order latent patterns or\nlearning quality feature representations.\nAnother line of research on FM-based models is to incorporate\ndeep neural networks (DNNs). For example, Factorization-Machine\nSupported Neural Networks (FNN) ",
                    "Citation Text": "Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field\ncategorical data. In European conference on information retrieval . Springer, 45\u201357.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.02376",
                        "Citation Paper Title": "Title:Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Weinan Zhang, Tianming Du, Jun Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.13983v1": {
            "Paper Title": "PAD: a graphical and numerical enhancement of structural coding to\n  facilitate thematic analysis of a literature corpus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03844v3": {
            "Paper Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ",\nthe authors developed a dataset and experiment with character level embedding for authorship\nattribution. Using the same dataset, Alam et al. ",
                    "Citation Text": "Tanvirul Alam, Akib Khan, and Firoj Alam. 2020. Bangla Text Classification using Transformers. arXiv preprint\narXiv:2011.04446 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.04446",
                        "Citation Paper Title": "Title:Bangla Text Classification using Transformers",
                        "Citation Paper Abstract": "Abstract:Text classification has been one of the earliest problems in NLP. Over time the scope of application areas has broadened and the difficulty of dealing with new areas (e.g., noisy social media content) has increased. The problem-solving strategy switched from classical machine learning to deep learning algorithms. One of the recent deep neural network architecture is the Transformer. Models designed with this type of network and its variants recently showed their success in many downstream natural language processing tasks, especially for resource-rich languages, e.g., English. However, these models have not been explored fully for Bangla text classification tasks. In this work, we fine-tune multilingual transformer models for Bangla text classification tasks in different domains, including sentiment analysis, emotion detection, news categorization, and authorship attribution. We obtain the state of the art results on six benchmark datasets, improving upon the previous results by 5-29% accuracy across different tasks.",
                        "Citation Paper Authors": "Authors:Tanvirul Alam, Akib Khan, Firoj Alam"
                    }
                },
                {
                    "Sentence ID": 155,
                    "Sentence": "to\nsegment the text into subword units and applied byte pair encoding to increase the consistency\nof data segmentation for handling rare words. Finally, we applied the fairseq ",
                    "Citation Text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.\n2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics (Demonstrations) . Association for Computational\nLinguistics, 48\u201353.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01038",
                        "Citation Paper Title": "Title:fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at this https URL",
                        "Citation Paper Authors": "Authors:Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 117,
                    "Sentence": "26for the punctuation restoration\ntask. The dataset consists of train, development, and test splits prepared from a publicly available\ncorpus of Bangla newspaper articles ",
                    "Citation Text": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, and Ayesha Tasnim. 2019. A Subword\nLevel Language Model for Bangla Language. CoRR abs/1911.07613 (2019). arXiv:1911.07613 http://arxiv.org/abs/1911.\n07613",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.07613",
                        "Citation Paper Title": "Title:A Subword Level Language Model for Bangla Language",
                        "Citation Paper Abstract": "Abstract:Language models are at the core of natural language processing. The ability to represent natural language gives rise to its applications in numerous NLP tasks including text classification, summarization, and translation. Research in this area is very limited in Bangla due to the scarcity of resources, except for some count-based models and very recent neural language models being proposed, which are all based on words and limited in practical tasks due to their high perplexity. This paper attempts to approach this issue of perplexity and proposes a subword level neural language model with the AWD-LSTM architecture and various other techniques suitable for training in Bangla language. The model is trained on a corpus of Bangla newspaper articles of an appreciable size consisting of more than 28.5 million word tokens. The performance comparison with various other models depicts the significant reduction in perplexity the proposed model provides, reaching as low as 39.84, in just 20 epochs.",
                        "Citation Paper Authors": "Authors:Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim"
                    }
                },
                {
                    "Sentence ID": 192,
                    "Sentence": "is designed to learn contextualized word representation from unlabeled\ntexts by jointly conditioning on the left and right contexts of a token. It uses the encoder part\nof the transformer architecture introduced in ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30 . Curran\nAssociates, Inc., 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ". The authors explored\ndifferent Transformer architectures for punctuation restoration in English and Bangla. For Bangla,\nthey used different multi-lingual transformer models, including multi-lingual BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) .\nAssociation for Computational Linguistics, 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 115,
                    "Sentence": "provides a comparative\nanalysis using both classical \u2013 SVM, and deep learning algorithms \u2013 LSTM and CNN, for sentiment\nclassification of Bangla news comments. The study in ",
                    "Citation Text": "Md. Rezaul Karim, Bharathi Raja Chakravarthi, John P. McCrae, and Michael Cochez. 2020. Classification Benchmarks\nfor Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network. CoRR abs / 2004.07807\n(2020). arXiv:2004.07807",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07807",
                        "Citation Paper Title": "Title:Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network",
                        "Citation Paper Abstract": "Abstract:Exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices but also enables people to express anti-social behaviour like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize these data for social and anti-social behaviours analysis, document characterization, and sentiment analysis by predicting the contexts mostly for highly resourced languages such as English. However, there are languages that are under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese, Telugu that lack of computational resources for the NLP tasks. In this paper, we provide several classification benchmarks for Bengali, an under-resourced language. We prepared three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. We built the largest Bengali word embedding models to date based on 250 million articles, which we call BengFastText. We perform three different experiments, covering document classification, sentiment analysis, and hate speech detection. We incorporate word embeddings into a Multichannel Convolutional-LSTM (MConv-LSTM) network for predicting different types of hate speech, document classification, and sentiment analysis. Experiments demonstrate that BengFastText can capture the semantics of words from respective contexts correctly. Evaluations against several baseline embedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and 90.45% F1-scores in case of document classification, sentiment analysis, and hate speech detection, respectively during 5-fold cross-validation tests.",
                        "Citation Paper Authors": "Authors:Md. Rezaul Karim, Bharathi Raja Chakravarthi, John P. McCrae, Michael Cochez"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "Vanilla phrase-based\nSMT and NMTMicrosoft dataset 16.6 & 20.2\nKhan et al. ",
                    "Citation Text": "Nadeem Jadoon Khan, Waqas Anwar, and Nadir Durrani. 2017. Machine translation approaches and survey for indian\nlanguages. arXiv preprint arXiv:1701.04290 (2017).\n44",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.04290",
                        "Citation Paper Title": "Title:Machine Translation Approaches and Survey for Indian Languages",
                        "Citation Paper Abstract": "Abstract:In this study, we present an analysis regarding the performance of the state-of-art Phrase-based Statistical Machine Translation (SMT) on multiple Indian languages. We report baseline systems on several language pairs. The motivation of this study is to promote the development of SMT and linguistic resources for these language pairs, as the current state-of-the-art is quite bleak due to sparse data resources. The success of an SMT system is contingent on the availability of a large parallel corpus. Such data is necessary to reliably estimate translation probabilities. We report the performance of baseline systems translating from Indian languages (Bengali, Guajarati, Hindi, Malayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10% accurate results for all the language pairs.",
                        "Citation Paper Authors": "Authors:Nadeem Jadoon Khan, Waqas Anwar, Nadir Durrani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.11250v1": {
            "Paper Title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02192v2": {
            "Paper Title": "Audio Retrieval with Natural Language Queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.09980v2": {
            "Paper Title": "Fine-Grained Causality Extraction From Natural Language Requirements\n  Using Recursive Neural Tensor Networks",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "to assign the\nPOS tags to the respective tokens and fastText ",
                    "Citation Text": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \u201cEnriching word\nvectors with subword information,\u201d CoRR , vol. abs/1607.04606, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.10457v1": {
            "Paper Title": "Ready for Emerging Threats to Recommender Systems? A Graph\n  Convolution-based Generative Shilling Attack",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "develop a data-e\ufb03cient GCN algorithm for web-\nscale recommender systems. Wang et al. ",
                    "Citation Text": "Wang, X., He, X., Wang, M., Feng, F., Chua, T.S., 2019a. Neu-\nral graph collaborative \ufb01ltering, in: Proceedings of the 42nd Inter-\nnational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pp. 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Bayesianpersonalizedrankingisagenericop-\ntimizationcriterionthatmaximizestheposteriorprobabil-\nityofaBayesiananalysisproblem. Itlearnsamatrixfac-\ntorization model by the pairwise ranking objective func-\ntion.\u2022APR ",
                    "Citation Text": "He, X., He, Z., Du, X., Chua, T.S., 2018. Adversarial personalized\nrankingforrecommendation,in: The41stInternationalACMSIGIR\nConference on Research and Development in Information Retrieval,\npp. 355\u2013364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.03908",
                        "Citation Paper Title": "Title:Adversarial Personalized Ranking for Recommendation",
                        "Citation Paper Abstract": "Abstract:Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) --- the most widely used model in recommendation --- as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization.\nTo enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "applyagreedyalgorithmtoiteratively\nadd an item from the candidate set for item selection. Fang\netal. ",
                    "Citation Text": "Fang, M., Gong, N.Z., Liu, J., 2020. In\ufb02uence function based data\npoisoningattackstotop-nrecommendersystems,in: TheWorldWide\nWeb Conference, pp. 3019\u20133025.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08025",
                        "Citation Paper Title": "Title:Influence Function based Data Poisoning Attacks to Top-N Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-$N$ items that match the best with a user's preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods.",
                        "Citation Paper Authors": "Authors:Minghong Fang, Neil Zhenqiang Gong, Jia Liu"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "use\nGNNtohandlesocialrecommendationsonratingprediction\ntasks that consider item aggregation and social aggregation\nfor users and user aggregation for items. Wu et al. ",
                    "Citation Text": "Wu, Q., He, P., Zhang, H., Weng, P., Chen, G., Gao, X., Gao, H.,\n2019. Dualgraphattentionnetworksfordeeplatentrepresentationof\nmultifaceted social e\ufb00ects in recommender systems, in: The World\nWide Web Conference, pp. 2091\u20132102.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10433",
                        "Citation Paper Title": "Title:Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Social recommendation leverages social information to solve data sparsity and cold-start problems in traditional collaborative filtering methods. However, most existing models assume that social effects from friend users are static and under the forms of constant weights or fixed constraints. To relax this strong assumption, in this paper, we propose dual graph attention networks to collaboratively learn representations for two-fold social effects, where one is modeled by a user-specific attention weight and the other is modeled by a dynamic and context-aware attention weight. We also extend the social effects in user domain to item domain, so that information from related items can be leveraged to further alleviate the data sparsity problem. Furthermore, considering that different social effects in two domains could interact with each other and jointly influence user preferences for items, we propose a new policy-based fusion strategy based on contextual multi-armed bandit to weigh interactions of various social effects. Experiments on one benchmark dataset and a commercial dataset verify the efficacy of the key components in our model. The results show that our model achieves great improvement for recommendation accuracy compared with other state-of-the-art social recommendation methods.",
                        "Citation Paper Authors": "Authors:Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, Guihai Chen"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "uses GAN to generate entire user pro\ufb01les and further\noptimizesthembyapproximategradientdescent. Tangetal. ",
                    "Citation Text": "Tang, J., Wen, H., Wang, K., 2020. Revisiting adversarially learned\ninjection attacks against recommender systems, in: The 14th ACM\nConference on Recommender Systems, pp. 318\u2013327.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.04876",
                        "Citation Paper Title": "Title:Revisiting Adversarially Learned Injection Attacks Against Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems play an important role in modern information and e-commerce applications. While increasing research is dedicated to improving the relevance and diversity of the recommendations, the potential risks of state-of-the-art recommendation models are under-explored, that is, these models could be subject to attacks from malicious third parties, through injecting fake user interactions to achieve their purposes. This paper revisits the adversarially-learned injection attack problem, where the injected fake user `behaviors' are learned locally by the attackers with their own model -- one that is potentially different from the model under attack, but shares similar properties to allow attack transfer. We found that most existing works in literature suffer from two major limitations: (1) they do not solve the optimization problem precisely, making the attack less harmful than it could be, (2) they assume perfect knowledge for the attack, causing the lack of understanding for realistic attack capabilities. We demonstrate that the exact solution for generating fake users as an optimization problem could lead to a much larger impact. Our experiments on a real-world dataset reveal important properties of the attack, including attack transferability and its limitations. These findings can inspire useful defensive methods against this possible existing attack.",
                        "Citation Paper Authors": "Authors:Jiaxi Tang, Hongyi Wen, Ke Wang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "donotdi\ufb00erentiatetheselecteditemsand\ufb01lleritems,\nwhich means that the same rule applies to all rated items in\nthe fake user pro\ufb01les. Likewise, some optimization-based\nshilling attack models use the uniform optimization algo-\nrithm to generate fake ratings. Fang et al. ",
                    "Citation Text": "Fang, M., Yang, G., Gong, N.Z., Liu, J., 2018. Poisoning attacks\nto graph-based recommender systems, in: Proceedings of the 34th\nAnnual Computer Security Applications Conference, pp. 381\u2013392.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.04127",
                        "Citation Paper Title": "Title:Poisoning Attacks to Graph-Based Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning attacks, in which an attacker injects fake data to a given system such that the system makes recommendations as the attacker desires. However, these poisoning attacks are either agnostic to recommendation algorithms or optimized to recommender systems that are not graph-based. Like association-rule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store. However, how to design optimized poisoning attacks for graph-based recommender systems is still an open problem. In this work, we perform a systematic study on poisoning attacks to graph-based recommender systems. Due to limited resources and to avoid detection, we assume the number of fake users that can be injected into the system is bounded. The key challenge is how to assign rating scores to the fake users such that the target item is recommended to as many normal users as possible. To address the challenge, we formulate the poisoning attacks as an optimization problem, solving which determines the rating scores for the fake users. We also propose techniques to solve the optimization problem. We evaluate our attacks and compare them with existing attacks under white-box (recommendation algorithm and its parameters are known), gray-box (recommendation algorithm is known but its parameters are unknown), and black-box (recommendation algorithm is unknown) settings using two real-world datasets. Our results show that our attack is effective and outperforms existing attacks for graph-based recommender systems. For instance, when 1% fake users are injected, our attack can make a target item recommended to 580 times more normal users in certain scenarios.",
                        "Citation Paper Authors": "Authors:Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, Jia Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.09558v2": {
            "Paper Title": "Into Summarization Techniques for IoT Data Discovery Routing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.09966v1": {
            "Paper Title": "Provenance, Anonymisation and Data Environments: a Unifying Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.07453v2": {
            "Paper Title": "Next-item Recommendations in Short Sessions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08973v1": {
            "Paper Title": "Unsupervised Identification of Relevant Prior Cases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08291v1": {
            "Paper Title": "Neural Search: Learning Query and Product Representations in Fashion\n  E-commerce",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "and in other search engines\ninclude Search engine EBR papers ",
                    "Citation Text": "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,\nJanani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-\nBased Retrieval in Facebook Search. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining (Virtual Event,\nCA, USA) (KDD \u201920) . Association for Computing Machinery, New York, NY, USA,\n2553\u20132561. https://doi.org/10.1145/3394486.3403305",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11632",
                        "Citation Paper Title": "Title:Embedding-based Retrieval in Facebook Search",
                        "Citation Paper Abstract": "Abstract:Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in eb search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.",
                        "Citation Paper Authors": "Authors:Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "have been further developed to include traditional\nIR lexical matching signals. Few examples of applying EBR in e-\ncommerce settings include JD.com ",
                    "Citation Text": "Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun\nXiao, Weipeng Yan, and Wen-Yun Yang. 2020. Towards Personalized and Seman-\ntic Retrieval: An End-to-End Solution for E-Commerce Search via EmbeddingSIGIR eCom\u201921, July 15, 2021, Virtual Event, Montreal, Canada Sagnik and Lakshya, et al.\nLearning. In Proceedings of the 43rd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval (Virtual Event, China) (SIGIR\n\u201920). Association for Computing Machinery, New York, NY, USA, 2407\u20132416.\nhttps://doi.org/10.1145/3397271.3401446",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.02282",
                        "Citation Paper Title": "Title:Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Nowadays e-commerce search has become an integral part of many people's shopping routines. Two critical challenges stay in today's e-commerce search: how to retrieve items that are semantically relevant but not exact matching to query terms, and how to retrieve items that are more personalized to different users for the same search query. In this paper, we present a novel approach called DPSR, which stands for Deep Personalized and Semantic Retrieval, to tackle this problem. Explicitly, we share our design decisions on how to architect a retrieval system so as to serve industry-scale traffic efficiently and how to train a model so as to learn query and item semantics accurately. Based on offline evaluations and online A/B test with live traffics, we show that DPSR model outperforms existing models, and DPSR system can retrieve more personalized and semantically relevant items to significantly improve users' search experience by +1.29% conversion rate, especially for long tail queries by +10.03%. As a result, our DPSR system has been successfully deployed into this http URL's search production since 2019.",
                        "Citation Paper Authors": "Authors:Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao, Weipeng Yan, Wen-Yun Yang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "provides a good survey of neural\nmethods in IR. Guo et. al. ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance\nMatching Model for Ad-Hoc Retrieval. In Proceedings of the 25th ACM Interna-\ntional on Conference on Information and Knowledge Management (Indianapolis,\nIndiana, USA) (CIKM \u201916) . Association for Computing Machinery, New York, NY,\nUSA, 55\u201364. https://doi.org/10.1145/2983323.2983769",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.09423v4": {
            "Paper Title": "SIGIR 2021 E-Commerce Workshop Data Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.07831v1": {
            "Paper Title": "Modeling User Behaviour in Research Paper Recommendation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.07773v1": {
            "Paper Title": "More Robust Dense Retrieval with Contrastive Dual Learning",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "and carefully optimize the embedding\nspace locally with lots of tricks ",
                    "Citation Text": "Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang,\nand Luke Zettlemoyer. 2020. Pre-training via Paraphrasing. In Proceedings of\nNeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15020",
                        "Citation Paper Title": "Title:Pre-training via Paraphrasing",
                        "Citation Paper Abstract": "Abstract:We introduce MARGE, a pre-trained sequence-to-sequence model learned with an unsupervised multi-lingual multi-document paraphrasing objective. MARGE provides an alternative to the dominant masked language modeling paradigm, where we self-supervise the reconstruction of target text by retrieving a set of related texts (in many languages) and conditioning on them to maximize the likelihood of generating the original. We show it is possible to jointly learn to do retrieval and reconstruction, given only a random initialization. The objective noisily captures aspects of paraphrase, translation, multi-document summarization, and information retrieval, allowing for strong zero-shot performance on several tasks. For example, with no additional task-specific training we achieve BLEU scores of up to 35.8 for document translation. We further show that fine-tuning gives strong performance on a range of discriminative and generative tasks in many languages, making MARGE the most generally applicable pre-training method to date.",
                        "Citation Paper Authors": "Authors:Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "to evaluate retrieval effectiveness. In FirstP\nsetting, queries and documents are truncated and padded to the\nsequences with the maximum lengths of 64 and 512, respectively.\nDANCE is optimized with LAMB ",
                    "Citation Text": "Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bho-\njanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2020.\nLarge Batch Optimization for Deep Learning: Training BERT in 76 minutes. In\nProceedings of ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00962",
                        "Citation Paper Title": "Title:Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
                        "Citation Paper Abstract": "Abstract:Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "also improves\nsuch training paradigm by alleviating the affect from incomplete\nrelevant labels.\nRecent work in computer vision further discusses how the em-\nbedding space is optimized with contrastive training ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.\nA Simple Framework for Contrastive Learning of Visual Representations. In\nProceedings of ICML . 1597\u20131607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ":\n\ud835\udc59(\ud835\udc5e,\ud835\udc51+,\ud835\udc37\u2212)=\u2212log\ud835\udc52\ud835\udc53(\ud835\udc5e+,\ud835\udc51+)\n\ud835\udc52\ud835\udc53(\ud835\udc5e+,\ud835\udc51+)+\u00cd\n\ud835\udc51\u2212\u2208\ud835\udc37\u2212\ud835\udc52\ud835\udc53(\ud835\udc5e+,\ud835\udc51\u2212), (5)\nwhere\ud835\udc37\u2212is the collection of negative documents for query \ud835\udc5esam-\npled with different retrieval methods, such as BM25 ",
                    "Citation Text": "Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Ques-\ntion Answering. In Proceedings of EMNLP . 6769\u20136781.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04906",
                        "Citation Paper Title": "Title:Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.07284v1": {
            "Paper Title": "Auto-detecting groups based on textual similarity for group\n  recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05315v3": {
            "Paper Title": "Contrastive Learning for Cold-Start Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "compared the\nglobal and local features of each image and maximized their\nmutual information to discover the invariant semantic signal. With\ntheir success on representation learning, several approaches are\nproposed for various applications. For the recommender system,\nseveral models ",
                    "Citation Text": "Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang.\n2020. Contrastive learning for debiased candidate generation in large-scale\nrecommender systems. arXiv preprint cs.IR/2005.12964 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12964",
                        "Citation Paper Title": "Title:Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Deep candidate generation (DCG) that narrows down the collection of relevant items from billions to hundreds via representation learning has become prevalent in industrial recommender systems. Standard approaches approximate maximum likelihood estimation (MLE) through sampling for better scalability and address the problem of DCG in a way similar to language modeling. However, live recommender systems face severe exposure bias and have a vocabulary several orders of magnitude larger than that of natural language, implying that MLE will preserve and even exacerbate the exposure bias in the long run in order to faithfully fit the observed samples. In this paper, we theoretically prove that a popular choice of contrastive loss is equivalent to reducing the exposure bias via inverse propensity weighting, which provides a new perspective for understanding the effectiveness of contrastive learning. Based on the theoretical discovery, we design CLRec, a contrastive learning method to improve DCG in terms of fairness, effectiveness and efficiency in recommender systems with extremely large candidate size. We further improve upon CLRec and propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our methods have been successfully deployed in Taobao, where at least four-month online A/B tests and offline analyses demonstrate its substantial improvements, including a dramatic reduction in the Matthew effect.",
                        "Citation Paper Authors": "Authors:Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, Hongxia Yang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "proposed a probabilistic contrastive\nloss, termed InfoNCE, in a way that maximally preserves the\nmutual information between the observation and context signals.\nProposed independently of CPC, Hjelm et al. ",
                    "Citation Text": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal,\nPhil Bachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep\nrepresentations by mutual information estimation and maximization. Proceedings\nof International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.06670",
                        "Citation Paper Title": "Title:Learning deep representations by mutual information estimation and maximization",
                        "Citation Paper Abstract": "Abstract:In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",
                        "Citation Paper Authors": "Authors:R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.07002v1": {
            "Paper Title": "The Benchmark Lottery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04019v2": {
            "Paper Title": "Differentiable Sorting Networks for Scalable Sorting and Ranking\n  Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06472v1": {
            "Paper Title": "Linking Health News to Research Literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06427v1": {
            "Paper Title": "Sequential Recommendation for Cold-start Users with Meta Transitional\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06400v1": {
            "Paper Title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06751v1": {
            "Paper Title": "Tortured phrases: A dubious writing style emerging in science. Evidence\n  of critical issues affecting established journals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05720v1": {
            "Paper Title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ": the\nmodel embeds documents and queries in a sparse high-dimensional\nlatent space by means of \u21131regularization on representations. How-\never, SNRM effectiveness remains limited and its efficiency has\nbeen questioned ",
                    "Citation Text": "Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and\nBarnab\u00e1s P\u00f3czos. 2020. Minimizing FLOPs to Learn Efficient Sparse Representa-\ntions. arXiv:2004.05665 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05665",
                        "Citation Paper Title": "Title:Minimizing FLOPs to Learn Efficient Sparse Representations",
                        "Citation Paper Abstract": "Abstract:Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.",
                        "Citation Paper Authors": "Authors:Biswajit Paria, Chih-Kuan Yeh, Ian E.H. Yen, Ning Xu, Pradeep Ravikumar, Barnab\u00e1s P\u00f3czos"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". A first solution to this problem consists in expanding\ndocuments using generative approaches such as doc2query ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by Query Prediction. arXiv:1904.08375 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05677v1": {
            "Paper Title": "Codified audio language modeling learns useful representations for music\n  information retrieval",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nWhile CALM has not previously been explored for\nMIR transfer learning, it has been explored for other pur-\nposes. van den Oord et al. 2017 ",
                    "Citation Text": "A. van den Oord, O. Vinyals, and K. Kavukcuoglu,\n\u201cNeural discrete representation learning,\u201d\narXiv:1711.00937 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00937",
                        "Citation Paper Title": "Title:Neural Discrete Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". This work pre-trained deep\nneural networks on MSD and demonstrated promising per-\nformance on other tagging and genre classi\ufb01cation tasks.\nChoi et al. 2017 ",
                    "Citation Text": "K. Choi, G. Fazekas, M. Sandler, and K. Cho, \u201cTrans-\nfer learning for music classi\ufb01cation and regression\ntasks,\u201d in ISMIR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.09179",
                        "Citation Paper Title": "Title:Transfer learning for music classification and regression tasks",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.",
                        "Citation Paper Authors": "Authors:Keunwoo Choi, Gy\u00f6rgy Fazekas, Mark Sandler, Kyunghyun Cho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05474v1": {
            "Paper Title": "Denoising User-aware Memory Network for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "proposes an attention mechanism to capture the relative interest of candidates and obtain adaptive interest\nrepresentation. Inspired by the success of the Transformer network in neural machine translation ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In NIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "considers the wide part for memory and the deep part for\ngeneralization together, making the model have the advantages of both logistic regression and deep neural network.\nDIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network\nfor click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1059\u20131068.\n17",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "used weak supervision method to\nmodel feedback information. Furthermore, more algorithms use multi-task learning frameworks to jointly solve ranking\nand rating tasks by combining various explicit feedback and implicit feedback ",
                    "Citation Text": "Guy Hadash, Oren Sar Shalom, and Rita Osadchy. 2018. Rank and rate: multi-task learning for recommender systems. In Proceedings of the 12th\nACM Conference on Recommender Systems . 451\u2013454.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11698",
                        "Citation Paper Title": "Title:Rank and Rate: Multi-task Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:The two main tasks in the Recommender Systems domain are the ranking and rating prediction tasks. The rating prediction task aims at predicting to what extent a user would like any given item, which would enable to recommend the items with the highest predicted scores. The ranking task on the other hand directly aims at recommending the most valuable items for the user. Several previous approaches proposed learning user and item representations to optimize both tasks simultaneously in a multi-task framework. In this work we propose a novel multi-task framework that exploits the fact that a user does a two-phase decision process - first decides to interact with an item (ranking task) and only afterward to rate it (rating prediction task). We evaluated our framework on two benchmark datasets, on two different configurations and showed its superiority over state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Guy Hadash, Oren Sar Shalom, Rita Osadchy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05124v1": {
            "Paper Title": "Transformers with multi-modal features and post-fusion context for\n  e-commerce session-based recommendation",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "is a causal language model that solves con-\ntext fragmentation by introducing a segment recurrence mechanism\nwhere the hidden states of previous blocks are cached and used to\nextend the context of the new upcoming segment.\nXLNet ",
                    "Citation Text": "Zhilin Yang, Zihang Dai, Yiming Yang Jaime G. Carbonell, Ruslan Salakhutdi-\nnov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in Neural Information Processing Systems .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08237",
                        "Citation Paper Title": "Title:XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                        "Citation Paper Abstract": "Abstract:With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
                        "Citation Paper Authors": "Authors:Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". We employed causal LM and\nmasked LM training approaches with Transformer-XL and XLNet,\nrespectively, which are described next.\nTransformer-XL ",
                    "Citation Text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a\nFixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02860",
                        "Citation Paper Title": "Title:Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                        "Citation Paper Abstract": "Abstract:Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
                        "Citation Paper Authors": "Authors:Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05025v1": {
            "Paper Title": "Similarity Guided Deep Face Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei\nLiu, and Jiashi Feng. Central similarity quantization for ef\ufb01cient image\nand video retrieval. In CVPR , pages 3083\u20133092, 2020. ",
                    "Citation Text": "Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer.\nS4l: Self-supervised semi-supervised learning. In ICCV , pages 1476\u2013\n1485, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03670",
                        "Citation Paper Title": "Title:S4L: Self-Supervised Semi-Supervised Learning",
                        "Citation Paper Abstract": "Abstract:This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that our approach and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.",
                        "Citation Paper Authors": "Authors:Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "Zhi Xiong, Dayan Wu, Wen Gu, Haisu Zhang, Bo Li, and Weiping\nWang. Deep discrete attention guided hashing for face image retrieval.\nInICMR , pages 136\u2013144, 2020. ",
                    "Citation Text": "Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei\nLiu, and Jiashi Feng. Central similarity quantization for ef\ufb01cient image\nand video retrieval. In CVPR , pages 3083\u20133092, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00347",
                        "Citation Paper Title": "Title:Central Similarity Quantization for Efficient Image and Video Retrieval",
                        "Citation Paper Abstract": "Abstract:Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new \\emph{global} similarity metric, termed as \\emph{central similarity}, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., \\emph{hash center} that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3\\%-20\\% in mAP over the previous state-of-the-arts. The code is at: \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, Jiashi Feng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.04984v1": {
            "Paper Title": "SVP-CF: Selection via Proxy for Collaborative Filtering Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.04953v1": {
            "Paper Title": "Designing Recommender Systems to Depolarize",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.04846v1": {
            "Paper Title": "Propagation-aware Social Recommendation by Transfer Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12637v3": {
            "Paper Title": "Comparative analysis of word embeddings in assessing semantic similarity\n  of complex sentences",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "have\nproven to be major breakthroughs in the \ufb01eld of semantic\nsimilarity and they are two of the most widely used word\nembeddings to date. In 2019, Delvin et al. ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of\ndeep bidirectional transformers for language understanding,\u201d in Proceed-\nings of the 2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , 2019, pp. 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "In 2019, the BERT transformer model surpassed the state of\nthe art results in 11 different NLP tasks, including semantic\nsimilarity. BERT uses the transformer model proposed by\nVaswani et al. ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NIPS , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.04964v4": {
            "Paper Title": "News Recommender System: A review of recent progress, challenges, and\n  opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03936v2": {
            "Paper Title": "Graph Neural Pre-training for Enhancing Recommendations using Side\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00774v2": {
            "Paper Title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03272v1": {
            "Paper Title": "How Big Are Peoples' Computer Files? File Size Distributions Among\n  User-managed Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00855v2": {
            "Paper Title": "Computationally Efficient Optimization of Plackett-Luce Ranking Models\n  for Relevance and Fairness",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ", in the context of fairness this is often considered the\nmerit of an item ",
                    "Citation Text": "Fernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben\nCarterette. 2020. Evaluating Stochastic Rankings with Expected Exposure .\nAssociation for Computing Machinery, New York, NY, USA, 275\u2013284.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13157",
                        "Citation Paper Title": "Title:Evaluating Stochastic Rankings with Expected Exposure",
                        "Citation Paper Abstract": "Abstract:We introduce the concept of \\emph{expected exposure} as the average attention ranked items receive from users over repeated samples of the same query. Furthermore, we advocate for the adoption of the principle of equal expected exposure: given a fixed information need, no item should receive more or less expected exposure than any other item of the same relevance grade. We argue that this principle is desirable for many retrieval objectives and scenarios, including topical diversity and fair ranking. Leveraging user models from existing retrieval metrics, we propose a general evaluation methodology based on expected exposure and draw connections to related metrics in information retrieval evaluation. Importantly, this methodology relaxes classic information retrieval assumptions, allowing a system, in response to a query, to produce a \\emph{distribution over rankings} instead of a single fixed ranking. We study the behavior of the expected exposure metric and stochastic rankers across a variety of information access conditions, including \\emph{ad hoc} retrieval and recommendation. We believe that measuring and optimizing expected exposure metrics using randomization opens a new area for retrieval algorithm development and progress.",
                        "Citation Paper Authors": "Authors:Fernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, Ben Carterette"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "show that frequently optimizing the logging-policy model during\nthe gathering of data greatly reduces the data-requirements for on-\nline/counterfactual LTR. Similarly, Morik et al . ",
                    "Citation Text": "Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020.\nControlling Fairness and Bias in Dynamic Learning-to-Rank . ACM, 429\u2013438.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14713",
                        "Citation Paper Title": "Title:Controlling Fairness and Bias in Dynamic Learning-to-Rank",
                        "Citation Paper Abstract": "Abstract:Rankings are the primary interface through which many online platforms match users to items (e.g. news, products, music, video). In these two-sided markets, not only the users draw utility from the rankings, but the rankings also determine the utility (e.g. exposure, revenue) for the item providers (e.g. publishers, sellers, artists, studios). It has already been noted that myopically optimizing utility to the users, as done by virtually all learning-to-rank algorithms, can be unfair to the item providers. We, therefore, present a learning-to-rank approach for explicitly enforcing merit-based fairness guarantees to groups of items (e.g. articles by the same publisher, tracks by the same artist). In particular, we propose a learning algorithm that ensures notions of amortized group fairness, while simultaneously learning the ranking function from implicit feedback data. The algorithm takes the form of a controller that integrates unbiased estimators for both fairness and utility, dynamically adapting both as more data becomes available. In addition to its rigorous theoretical foundation and convergence guarantees, we find empirically that the algorithm is highly practical and robust.",
                        "Citation Paper Authors": "Authors:Marco Morik, Ashudeep Singh, Jessica Hong, Thorsten Joachims"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.01453v1": {
            "Paper Title": "PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior\n  for Joint Image-Text Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05368v1": {
            "Paper Title": "A Three Phase Semantic Web Matchmaker",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03323v3": {
            "Paper Title": "KATRec: Knowledge Aware aTtentive Sequential Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05366v1": {
            "Paper Title": "HCGR: Hyperbolic Contrastive Graph Representation Learning for\n  Session-based Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01655v1": {
            "Paper Title": "Attribute-aware Explainable Complementary Clothing Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01529v1": {
            "Paper Title": "Learning Complex Users' Preferences for Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02765v1": {
            "Paper Title": "Exploring the Scope of Using News Articles to Understand Development\n  Patterns of Districts in India",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06835v1": {
            "Paper Title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,\n  Future Directions, and Applications",
            "Sentences": [
                {
                    "Sentence ID": 92,
                    "Sentence": "S. C. Wong, A. Gatt, V. Stamatescu, M. D. McDonnell, Under-\nstanding data augmentation for classi\ufb01cation: When to warp?,\nin: 2016 International Conference on Digital Image Comput-\ning: Techniques and Applications (DICTA), 2016, pp. 1\u20136.\ndoi:10.1109/DICTA.2016.7797091 . ",
                    "Citation Text": "S. Nayak, R. Patgiri, 6g communication: Envisioning the key\nissues and challenges (2020). arXiv:2004.04024 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04024",
                        "Citation Paper Title": "Title:6G Communication: Envisioning the Key Issues and Challenges",
                        "Citation Paper Abstract": "Abstract:In 2030, we are going to evidence the 6G mobile communication technology, which will enable the Internet of Everything. Yet 5G has to be experienced by people worldwide and B5G has to be developed; the researchers have already started planning, visioning, and gathering requirements of the 6G. Moreover, many countries have already initiated the research on 6G. 6G promises connecting every smart device to the Internet from smartphone to intelligent vehicles. 6G will provide sophisticated and high QoS such as holographic communication, augmented reality/virtual reality and many more. Also, it will focus on Quality of Experience (QoE) to provide rich experiences from 6G technology. Notably, it is very important to vision the issues and challenges of 6G technology, otherwise, promises may not be delivered on time. The requirements of 6G poses new challenges to the research community. To achieve desired parameters of 6G, researchers are exploring various alternatives. Hence, there are diverse research challenges to envision, from devices to softwarization. Therefore, in this article, we discuss the future issues and challenges to be faced by the 6G technology. We have discussed issues and challenges from every aspect from hardware to the enabling technologies which will be utilized by 6G.",
                        "Citation Paper Authors": "Authors:Sabuzima Nayak, Ripon Patgiri"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": "Y. LeCun, Y. Bengio, G. Hinton, Deep learning, nature\n521 (7553) (2015) 436\u2013444. ",
                    "Citation Text": "W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y. C.\nLiang, Q. Yang, D. Niyato, C. Miao, Federated learning in mo-\nbile edge networks: A comprehensive survey, IEEE Commu-\nnications Surveys & Tutorials 22 (3) (2020) 2031\u20132063. doi:\n10.1109/COMST.2020.2986024 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11875",
                        "Citation Paper Title": "Title:Federated Learning in Mobile Edge Networks: A Comprehensive Survey",
                        "Citation Paper Abstract": "Abstract:In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications. Traditional cloudbased Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL",
                        "Citation Paper Authors": "Authors:Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang, Qiang Yang, Dusit Niyato, Chunyan Miao"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "R.Patgiri,S.Nayak,S.Borgohain,Preventingddosusingbloom\n\ufb01lter: A survey, ICST Transactions on Scalable InformationSystems 5 (19) (2018) 155865. doi:10.4108/eai.19-6-2018.\n155865. ",
                    "Citation Text": "R. Patgiri, S. Nayak, S. Kumar, Role of bloom \ufb01lter in big data\nresearch: A survey, International Journal of Advanced Com-\nputer Science and Applications 9 (11). doi:10.14569/ijacsa.\n2018.091193 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06565",
                        "Citation Paper Title": "Title:Role of Bloom Filter in Big Data Research: A Survey",
                        "Citation Paper Abstract": "Abstract:Big Data is the most popular emerging trends that becomes a blessing for human kinds and it is the necessity of day-to-day life. For example, Facebook. Every person involves with producing data either directly or indirectly. Thus, Big Data is a high volume of data with exponential growth rate that consists of a variety of data. Big Data touches all fields, including Government sector, IT industry, Business, Economy, Engineering, Bioinformatics, and other basic sciences. Thus, Big Data forms a data silo. Most of the data are duplicates and unstructured. To deal with such kind of data silo, Bloom Filter is a precious resource to filter out the duplicate data. Also, Bloom Filter is inevitable in a Big Data storage system to optimize the memory consumption. Undoubtedly, Bloom Filter uses a tiny amount of memory space to filter a very large data size and it stores information of a large set of data. However, functionality of the Bloom Filter is limited to membership filter, but it can be adapted in various applications. Besides, the Bloom Filter is deployed in diverse field, and also used in the interdisciplinary research area. Bioinformatics, for instance. In this article, we expose the usefulness of Bloom Filter in Big Data research.",
                        "Citation Paper Authors": "Authors:Ripon Patgiri, Sabuzima Nayak, Samir Kumar Borgohain"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "Z. M. Fadlullah, A.-S. K. Pathan, H. Gacanin, On delay-\nsensitive healthcare data analytics at the network edge based\non deep learning, in: 2018 14th International Wireless Commu-\nnications & Mobile Computing Conference (IWCMC), IEEE,\n2018, pp. 388\u2013393. ",
                    "Citation Text": "P. Porambage, J. Okwuibe, M. Liyanage, M. Ylianttila,\nT. Taleb, Survey on multi-access edge computing for internet of\nthings realization, IEEE Communications Surveys & Tutorials\n20 (4) (2018) 2961\u20132991. doi:10.1109/COMST.2018.2849509 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06695",
                        "Citation Paper Title": "Title:Survey on Multi-Access Edge Computing for Internet of Things Realization",
                        "Citation Paper Abstract": "Abstract:The Internet of Things (IoT) has recently advanced from an experimental technology to what will become the backbone of future customer value for both product and service sector businesses. This underscores the cardinal role of IoT on the journey towards the fifth generation (5G) of wireless communication systems. IoT technologies augmented with intelligent and big data analytics are expected to rapidly change the landscape of myriads of application domains ranging from health care to smart cities and industrial automations. The emergence of Multi-Access Edge Computing (MEC) technology aims at extending cloud computing capabilities to the edge of the radio access network, hence providing real-time, high-bandwidth, low-latency access to radio network resources. IoT is identified as a key use case of MEC, given MEC's ability to provide cloud platform and gateway services at the network edge. MEC will inspire the development of myriads of applications and services with demand for ultra low latency and high Quality of Service (QoS) due to its dense geographical distribution and wide support for mobility. MEC is therefore an important enabler of IoT applications and services which require real-time operations. In this survey, we provide a holistic overview on the exploitation of MEC technology for the realization of IoT applications and their synergies. We further discuss the technical aspects of enabling MEC in IoT and provide some insight into various other integration technologies therein.",
                        "Citation Paper Authors": "Authors:Pawani Porambage, Jude Okwuibe, Madhusanka Liyanage, Mika Ylianttila, Tarik Taleb"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "Z. Cao, T. Simon, S.-E. Wei, Y. Sheikh, Realtime multi-person\n2d pose estimation using part a\ufb03nity \ufb01elds, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017, pp. 1302\u20131310. ",
                    "Citation Text": "B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,\nH. Adam, D. Kalenichenko, Quantization and training of neu-\nral networks for e\ufb03cient integer-arithmetic-only inference, in:\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018, pp. 2704\u20132713.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05877",
                        "Citation Paper Title": "Title:Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference",
                        "Citation Paper Abstract": "Abstract:The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.",
                        "Citation Paper Authors": "Authors:Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "C. Ne\ufb00, M. Mendieta, S. Mohan, M. Baharani, S. Rogers,\nH. Tabkhi, Revamp2t: Real-time edge video analytics for mul-\nticamera privacy-aware pedestrian tracking, IEEE Internet of\nThings Journal 7 (4) (2020) 2591\u20132602. doi:10.1109/JIOT.\n2019.2954804 . ",
                    "Citation Text": "Z. Cao, T. Simon, S.-E. Wei, Y. Sheikh, Realtime multi-person\n2d pose estimation using part a\ufb03nity \ufb01elds, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017, pp. 1302\u20131310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08008",
                        "Citation Paper Title": "Title:OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
                        "Citation Paper Abstract": "Abstract:Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.",
                        "Citation Paper Authors": "Authors:Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.00221v1": {
            "Paper Title": "Embedding-based Recommender System for Job to Candidate Matching on\n  Scale",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Although Rendle et al. argue\nthat simple dot-product substantially outperforms NCF learned sim-\nilarities ",
                    "Citation Text": "Rendle, S., Krichene, W., Zhang, L. & Anderson, J. 2020. Neural Collaborative\nFiltering vs. Matrix Factorization Revisited.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.09683",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering vs. Matrix Factorization Revisited",
                        "Citation Paper Abstract": "Abstract:Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Walid Krichene, Li Zhang, John Anderson"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". He et al have proposed a\nneural network based collaborative filtering architecture (NCF) for\nmodeling user-item interactions ",
                    "Citation Text": "X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.S. Chua. 2017. Neural Collaborative\nFiltering. In Proceedings of the 26th International Conference on World Wide Web\n(WWW \u201917). International World Wide Web Conferences Steering Committee,\nRepublic and Canton of Geneva, Switzerland, 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". The rapid develop-\nment of deep neural networks(DNN) in recent years has opened a\nnew racetrack for developing recommender system. Researchers\nat YouTube have proposed a recommender system with a Wide &\nDeep neural networks architecture ",
                    "Citation Text": "H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson,\nG. Corrado, W. Chai, M. Ispir, et al. Wide & deep learning for recommender\nsystems. 2016. Technical report.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.00161v1": {
            "Paper Title": "The Use of Bandit Algorithms in Intelligent Interactive Recommender\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", a deep reinforcement\nlearning framework is proposed for news recommendation. There-\nfore, it is interesting to design the deep bandit framework ",
                    "Citation Text": "C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian\nbandits showdown: An empirical comparison of bayesian\ndeep networks for thompson sampling. arXiv preprint\narXiv:1802.09127 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09127",
                        "Citation Paper Title": "Title:Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
                        "Citation Paper Abstract": "Abstract:Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",
                        "Citation Paper Authors": "Authors:Carlos Riquelme, George Tucker, Jasper Snoek"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "is developed\nto study the bandit problem with dependent arms. In light of the\ntopic modeling techniques, Wang et al. ",
                    "Citation Text": "Q. Wang, C. Zeng, W. Zhou, T. Li, L. Shwartz, and G. Y .\nGrabarnik. Online interactive collaborative \ufb01ltering using\nmulti-armed bandit with dependent arms. preprint\narXiv:1708.03058 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.03058",
                        "Citation Paper Title": "Title:Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms",
                        "Citation Paper Abstract": "Abstract:Online interactive recommender systems strive to promptly suggest to consumers appropriate items (e.g., movies, news articles) according to the current context including both the consumer and item content information. However, such context information is often unavailable in practice for the recommendation, where only the users' interaction data on items can be utilized. Moreover, the lack of interaction records, especially for new users and items, worsens the performance of recommendation further. To address these issues, collaborative filtering (CF), one of the recommendation techniques relying on the interaction data only, as well as the online multi-armed bandit mechanisms, capable of achieving the balance between exploitation and exploration, are adopted in the online interactive recommendation settings, by assuming independent items (i.e., arms). Nonetheless, the assumption rarely holds in reality, since the real-world items tend to be correlated with each other (e.g., two articles with similar topics). In this paper, we study online interactive collaborative filtering problems by considering the dependencies among items. We explicitly formulate the item dependencies as the clusters on arms, where the arms within a single cluster share the similar latent topics. In light of the topic modeling techniques, we come up with a generative model to generate the items from their underlying topics. Furthermore, an efficient online algorithm based on particle learning is developed for inferring both latent parameters and states of our model. Additionally, our inferred model can be naturally integrated with existing multi-armed selection strategies in the online interactive collaborating setting. Empirical studies on two real-world applications, online recommendations of movies and news, demonstrate both the effectiveness and efficiency of the proposed approach.",
                        "Citation Paper Authors": "Authors:Qing Wang, Chunqiu Zeng, Wubai Zhou, Tao Li, Larisa Shwartz, Genady Ya. Grabarnik"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "come up with a novel parameter-\nfree algorithm based on a principled sampling approach. In ",
                    "Citation Text": "D. N. Hill, H. Nassif, Y . Liu, A. Iyer, and S. Vishwanathan.\nAn ef\ufb01cient bandit algorithm for realtime multivariate\noptimization. In SIGKDD , pages 1813\u20131821. ACM, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.09558",
                        "Citation Paper Title": "Title:An Efficient Bandit Algorithm for Realtime Multivariate Optimization",
                        "Citation Paper Abstract": "Abstract:Optimization is commonly employed to determine the content of web pages, such as to maximize conversions on landing pages or click-through rates on search engine result pages. Often the layout of these pages can be decoupled into several separate decisions. For example, the composition of a landing page may involve deciding which image to show, which wording to use, what color background to display, etc. Such optimization is a combinatorial problem over an exponentially large decision space. Randomized experiments do not scale well to this setting, and therefore, in practice, one is typically limited to optimizing a single aspect of a web page at a time. This represents a missed opportunity in both the speed of experimentation and the exploitation of possible interactions between layout decisions.\nHere we focus on multivariate optimization of interactive web pages. We formulate an approach where the possible interactions between different components of the page are modeled explicitly. We apply bandit methodology to explore the layout space efficiently and use hill-climbing to select optimal content in realtime. Our algorithm also extends to contextualization and personalization of layout selection. Simulation results show the suitability of our approach to large decision spaces with strong interactions between content. We further apply our algorithm to optimize a message that promotes adoption of an Amazon service. After only a single week of online optimization, we saw a 21% conversion increase compared to the median layout. Our technique is currently being deployed to optimize content across several locations at this http URL.",
                        "Citation Paper Authors": "Authors:Daniel N Hill, Houssam Nassif, Yi Liu, Anand Iyer, S V N Vishwanathan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.12683v2": {
            "Paper Title": "GraphHINGE: Learning Interaction Models of Structured Neighborhood on\n  Heterogeneous Information Network",
            "Sentences": [
                {
                    "Sentence ID": 60,
                    "Sentence": "proposed to integrate both local\nand global information from HIN to enhance the recommendation performance. Wang et al . ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network\nfor recommendation. In KDD .\nACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: April 2021.GraphHINGE 1:35",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.00082v1": {
            "Paper Title": "A Search Engine for Scientific Publications: a Cybersecurity Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.00833v1": {
            "Paper Title": "Quantifying Availability and Discovery in Recommender Systems via\n  Stochastic Reachability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02757v1": {
            "Paper Title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.15903v1": {
            "Paper Title": "Learning to Ask Conversational Questions by Optimizing Levenshtein\n  Distance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.15876v1": {
            "Paper Title": "Incorporating Domain Knowledge for Extractive Summarization of Legal\n  Case Documents",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "that a mapping between the different sets of rhetorical roles is\npossible.In this work, we consider a set of eight rhetorical roles suggested\nin our prior work ",
                    "Citation Text": "Paheli Bhattacharya, Shounak Paul, Kripabandhu Ghosh, Saptarshi Ghosh, and\nAdam Wyner. 2019. Identification of Rhetorical Roles of Sentences in IndianLegal Judgments. Proc. Legal knowledge and information systems (JURIX) (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05405",
                        "Citation Paper Title": "Title:Identification of Rhetorical Roles of Sentences in Indian Legal Judgments",
                        "Citation Paper Abstract": "Abstract:Automatically understanding the rhetorical roles of sentences in a legal case judgement is an important problem to solve, since it can help in several downstream tasks like summarization of legal judgments, legal search, and so on. The task is challenging since legal case documents are usually not well-structured, and these rhetorical roles may be subjective (as evident from variation of opinions between legal experts). In this paper, we address this task for judgments from the Supreme Court of India. We label sentences in 50 documents using multiple human annotators, and perform an extensive analysis of the human-assigned labels. We also attempt automatic identification of the rhetorical roles of sentences. While prior approaches towards this task used Conditional Random Fields over manually handcrafted features, we explore the use of deep neural models which do not require hand-crafting of features. Experiments show that neural models perform much better in this task than baseline methods which use handcrafted features.",
                        "Citation Paper Authors": "Authors:Paheli Bhattacharya, Shounak Paul, Kripabandhu Ghosh, Saptarshi Ghosh, Adam Wyner"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nSummarization has also been treated as an Integer Linear Pro-\ngramming (ILP) optimization problem. Such methods have been\n1Implementation publicly available at https://github.com/Law-AI/DELSummapplied for summarizing news documents ",
                    "Citation Text": "Siddhartha Banerjee, Prasenjit Mitra, and Kazunari Sugiyama. 2015. Multi-\ndocument abstractive summarization using ILP based multi-sentence compres-\nsion. In Proc. International Conference on Artificial Intelligence .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.07034",
                        "Citation Paper Title": "Title:Multi-document abstractive summarization using ILP based multi-sentence compression",
                        "Citation Paper Abstract": "Abstract:Abstractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. In this work, we aim at developing an abstractive summarizer. First, our proposed approach identifies the most important document in the multi-document set. The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences. Second, we generate K-shortest paths from the sentences in each cluster using a word-graph structure. Finally, we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming (ILP) model with the objective of maximizing information content and readability of the final summary. Our ILP model represents the shortest paths as binary variables and considers the length of the path, information score and linguistic quality score in the objective function. Experimental results on the DUC 2004 and 2005 multi-document summarization datasets show that our proposed approach outperforms all the baselines and state-of-the-art extractive summarizers as measured by the ROUGE scores. Our method also outperforms a recent abstractive summarization technique. In manual evaluation, our approach also achieves promising results on informativeness and readability.",
                        "Citation Paper Authors": "Authors:Siddhartha Banerjee, Prasenjit Mitra, Kazunari Sugiyama"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.15779v1": {
            "Paper Title": "Dual Adversarial Variational Embedding for Robust Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ": CVAE-GAN is variational genera-\ntive adversarial networks, which is a general learn-\ning framework that combines a variational auto-\nencoder with a generative adversarial network.\n\u000fRecVAE ",
                    "Citation Text": "I. Shenbin, A. Alekseev, E. Tutubalina, V . Malykh, and S. I.\nNikolenko. Recvae: A new variational autoencoder for top-n\nrecommendations with implicit feedback. In Proceedings of the\n13th International Conference on Web Search and Data Mining , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11160",
                        "Citation Paper Title": "Title:RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Recent research has shown the advantages of using autoencoders based on deep neural networks for collaborative filtering. In particular, the recently proposed Mult-VAE model, which used the multinomial likelihood variational autoencoders, has shown excellent results for top-N recommendations. In this work, we propose the Recommender VAE (RecVAE) model that originates from our research on regularization techniques for variational autoencoders. RecVAE introduces several novel ideas to improve Mult-VAE, including a novel composite prior distribution for the latent codes, a new approach to setting the $\\beta$ hyperparameter for the $\\beta$-VAE framework, and a new approach to training based on alternating updates. In experimental evaluation, we show that RecVAE significantly outperforms previously proposed autoencoder-based models, including Mult-VAE and RaCT, across classical collaborative filtering datasets, and present a detailed ablation study to assess our new developments. Code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, Sergey I. Nikolenko"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ",\nfor a speci\ufb01c user uwe have\nlogp(u)\u0015Exu\u0018qu(xuju)[loggu(ujxu)]\n\u0000KL(qu(xuju)kpu(xu));(3)\nwhere the right side is the evidence lower bound (ELBO) ",
                    "Citation Text": "D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational\ninference: A review for statisticians. Journal of the American\nStatistical Association , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.00670",
                        "Citation Paper Title": "Title:Variational Inference: A Review for Statisticians",
                        "Citation Paper Abstract": "Abstract:One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.",
                        "Citation Paper Authors": "Authors:David M. Blei, Alp Kucukelbir, Jon D. McAuliffe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.15541v1": {
            "Paper Title": "When standard network measures fail to rank journals: A theoretical and\n  empirical analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14652v2": {
            "Paper Title": "Context-aware Heterogeneous Graph Attention Network for User Behavior\n  Prediction in Local Consumer Service Platform",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "is a neural network with multiple\nlayers, which is able to transform categorical features into\nembedding vectors.\n\u2022Wide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan\nAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n2016. Wide & Deep Learning for Recommender Systems. CoRR (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.14031v2": {
            "Paper Title": "Improving Sequential Recommendation Consistency with Self-Supervised\n  Imitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14731v1": {
            "Paper Title": "The DELICES project: Indexing scientific literature through semantic\n  expansion",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "Yuqing Mao and Zhiyong Lu. 2017. MeSH Now: automatic MeSH indexing at\nPubMed scale via learning to rank. Journal of Biomedical Semantics 8, 1 (2017). ",
                    "Citation Text": "Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, and Yu\nChi. 2017. Deep Keyphrase Generation. In Proceedings of ACL . 582\u2013592.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06879",
                        "Citation Paper Title": "Title:Deep Keyphrase Generation",
                        "Citation Paper Abstract": "Abstract:Keyphrase provides highly-condensed information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.16153v1": {
            "Paper Title": "Multi-Modal Chorus Recognition for Improving Song Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14174v1": {
            "Paper Title": "Transfer-based adaptive tree for multimodal sentiment analysis based on\n  user latent aspects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11347v2": {
            "Paper Title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal\n  Assistance Shared Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.13528v1": {
            "Paper Title": "Interactive query expansion for professional search applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11108v3": {
            "Paper Title": "Pre-trained Language Model based Ranking in Baidu Search",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "designs a gap-sentences genera-\ntion task as a pre-training objective tailored for abstractive text sum-\nmarization; Zhou et al . ",
                    "Citation Text": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill Yuchen\nLin, and Xiang Ren. 2020. Pre-training text-to-text transformers for concept-\ncentric common sense. arXiv:2011.07956 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.07956",
                        "Citation Paper Title": "Title:Pre-training Text-to-Text Transformers for Concept-centric Common Sense",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.",
                        "Citation Paper Authors": "Authors:Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill Yuchen Lin, Xiang Ren"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "proposes to\npre-train the Transformer model to predict the pairwise prefer-\nence between the two sets of words given a document; Chang et al . ",
                    "Citation Text": "Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.\n2019. Pre-training Tasks for Embedding-based Large-scale Retrieval. In ICLR\u201919 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.03932",
                        "Citation Paper Title": "Title:Pre-training Tasks for Embedding-based Large-scale Retrieval",
                        "Citation Paper Abstract": "Abstract:We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.",
                        "Citation Paper Authors": "Authors:Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "shows that the second phase of pre-training in-domain\ndata leads to performance gains under both high- and low-resource\nsettings [ 2,3,31,52]. To name a few, Ma et al . ",
                    "Citation Text": "Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng.\n2020. PROP: Pre-training with Representative Words Prediction for Ad-hoc\nRetrieval. arXiv:2010.10137 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10137",
                        "Citation Paper Title": "Title:PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:Recently pre-trained language representation models such as BERT have shown great success when fine-tuned on downstream tasks including information retrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval have not been well explored. In this paper, we propose Pre-training with Representative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired by the classical statistical language model for IR, specifically the query likelihood model, which assumes that the query is generated as the piece of text representative of the \"ideal\" document. Based on this idea, we construct the representative words prediction (ROP) task for pre-training. Given an input document, we sample a pair of word sets according to the document language model, where the set with higher likelihood is deemed as more representative of the document. We then pre-train the Transformer model to predict the pairwise preference between the two word sets, jointly with the Masked Language Model (MLM) objective. By further fine-tuning on a variety of representative downstream ad-hoc retrieval tasks, PROP achieves significant improvements over baselines without pre-training or with other pre-training methods. We also show that PROP can achieve exciting performance under both the zero- and low-resource IR settings. The code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", and quantiza-\ntion [ 22,29]. Besides, many works attempt to model long-text with\nmore efficient PLMs, such as Longformer ",
                    "Citation Text": "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\ndocument transformer. arXiv:2004.05150 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05150",
                        "Citation Paper Title": "Title:Longformer: The Long-Document Transformer",
                        "Citation Paper Abstract": "Abstract:Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
                        "Citation Paper Authors": "Authors:Iz Beltagy, Matthew E. Peters, Arman Cohan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.13386v1": {
            "Paper Title": "Balancing Accuracy and Fairness for Interactive Recommendation with\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12970v1": {
            "Paper Title": "RikoNet: A Novel Anime Recommendation Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12875v1": {
            "Paper Title": "Detection, Analysis, and Prediction of Research Topics with Scientific\n  Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12857v1": {
            "Paper Title": "Pattern-based Visualization of Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12744v1": {
            "Paper Title": "An Automated Knowledge Mining and Document Classification System with\n  Multi-model Transfer Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12741v1": {
            "Paper Title": "Discovering novel drug-supplement interactions using a dietary\n  supplements knowledge graph generated from the biomedical literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12657v1": {
            "Paper Title": "Extreme Multi-label Learning for Semantic Matching in Product Search",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "allows different index-\ning and matching methods for XMC problems, resulting in two\nrealizations, XR-Linear and X-Transformer. Also note that tree-\nbased methods with neural encoders such as AttentionXML ",
                    "Citation Text": "Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and\nShanfeng Zhu. 2019. AttentionXML: Label Tree-based Attention-Aware Deep\nModel for High-Performance Extreme Multi-Label Text Classification. In Ad-\nvances in Neural Information Processing Systems . 5812\u20135822.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.01727",
                        "Citation Paper Title": "Title:AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification",
                        "Citation Paper Abstract": "Abstract:Extreme multi-label text classification (XMTC) is an important problem in the era of big data, for tagging a given text with the most relevant multiple labels from an extremely large-scale label set. XMTC can be found in many applications, such as item categorization, web page tagging, and news annotation. Traditionally most methods used bag-of-words (BOW) as inputs, ignoring word context as well as deep semantic information. Recent attempts to overcome the problems of BOW by deep learning still suffer from 1) failing to capture the important subtext for each label and 2) lack of scalability against the huge number of labels. We propose a new label tree-based deep learning model for XMTC, called AttentionXML, with two unique features: 1) a multi-label attention mechanism with raw text as input, which allows to capture the most relevant part of text to each label; and 2) a shallow and wide probabilistic label tree (PLT), which allows to handle millions of labels, especially for \"tail labels\". We empirically compared the performance of AttentionXML with those of eight state-of-the-art methods over six benchmark datasets, including Amazon-3M with around 3 million labels. AttentionXML outperformed all competing methods under all experimental settings. Experimental results also show that AttentionXML achieved the best performance against tail labels among label tree-based methods. The code and datasets are available at this http URL .",
                        "Citation Paper Authors": "Authors:Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, Shanfeng Zhu"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "partitions the labels through a balanced 2-means la-\nbel tree using label features constructed from the instances. Other\napproaches attempt to improve on Parabel, for instance, eXtreme-\nText ",
                    "Citation Text": "Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, R\u00f3bert Busa-Fekete, and\nKrzysztof Dembczynski. 2018. A no-regret generalization of hierarchical softmax\nto extreme multi-label classification. In NIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11671",
                        "Citation Paper Title": "Title:A no-regret generalization of hierarchical softmax to extreme multi-label classification",
                        "Citation Paper Abstract": "Abstract:Extreme multi-label classification (XMLC) is a problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree, like in the hierarchical softmax (HSM) approach commonly used for multi-class problems. In this paper, we investigate probabilistic label trees (PLTs) that have been recently devised for tackling XMLC problems. We show that PLTs are a no-regret multi-label generalization of HSM when precision@k is used as a model evaluation metric. Critically, we prove that pick-one-label heuristic - a reduction technique from multi-label to multi-class that is routinely used along with HSM - is not consistent in general. We also show that our implementation of PLTs, referred to as extremeText (XT), obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN, a deep network specifically designed for XMLC problems. Moreover, XT is competitive to many state-of-the-art approaches in terms of statistical performance, model size and prediction time which makes it amenable to deploy in an online system.",
                        "Citation Paper Authors": "Authors:Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, R\u00f3bert Busa-Fekete, Krzysztof Dembczy\u0144ski"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", where they also em-\nbrace shallow MLP encoders for fast vectorization of novel queries\nto meet online latency constraints. Finally, ",
                    "Citation Text": "Liuyihan Song, Pan Pan, Kang Zhao, Hao Yang, Yiming Chen, Yingya Zhang,\nYinghui Xu, and Rong Jin. 2020. Large-Scale Training System for 100-Million\nClassification at Alibaba. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 2909\u20132930.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06025",
                        "Citation Paper Title": "Title:Large-Scale Training System for 100-Million Classification at Alibaba",
                        "Citation Paper Abstract": "Abstract:In the last decades, extreme classification has become an essential topic for deep learning. It has achieved great success in many areas, especially in computer vision and natural language processing (NLP). However, it is very challenging to train a deep model with millions of classes due to the memory and computation explosion in the last output layer. In this paper, we propose a large-scale training system to address these challenges. First, we build a hybrid parallel training framework to make the training process feasible. Second, we propose a novel softmax variation named KNN softmax, which reduces both the GPU memory consumption and computation costs and improves the throughput of training. Then, to eliminate the communication overhead, we propose a new overlapping pipeline and a gradient sparsification method. Furthermore, we design a fast continuous convergence strategy to reduce total training iterations by adaptively adjusting learning rate and updating model parameters. With the help of all the proposed methods, we gain 3.9$\\times$ throughput of our training system and reduce almost 60\\% of training iterations. The experimental results show that using an in-house 256 GPUs cluster, we could train a classifier of 100 million classes on Alibaba Retail Product Dataset in about five days while achieving a comparable accuracy with the naive softmax training process.",
                        "Citation Paper Authors": "Authors:Liuyihan Song, Pan Pan, Kang Zhao, Hao Yang, Yiming Chen, Yingya Zhang, Yinghui Xu, Rong Jin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.09807v2": {
            "Paper Title": "BERT Goes Shopping: Comparing Distributional Models for Product\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.13732v1": {
            "Paper Title": "Recurrent Coupled Topic Modeling over Sequential Documents",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "proposes a Poisson-Gamma dynamic system (PGDS) for sequentially count data, where the\nlatent states of topic proportions are chained via the Gamma shape parameter. Its later deep variant, ",
                    "Citation Text": "Dandan Guo, Bo Chen, Hao Zhang, and Mingyuan Zhou. 2018. Deep Poisson gamma dynamical systems. In NeurIPS .\n8442\u20138452.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11209",
                        "Citation Paper Title": "Title:Deep Poisson gamma dynamical systems",
                        "Citation Paper Abstract": "Abstract:We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.",
                        "Citation Paper Authors": "Authors:Dandan Guo, Bo Chen, Hao Zhang, Mingyuan Zhou"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "employs the P\u00f3lya-Gamma augmentation trick to provide\na conditionally conjugate scheme for Gaussian priors. To mitigate the scalability limit from the\nstate space modeling, the work ",
                    "Citation Text": "Patrick J\u00e4hnichen, Florian Wenzel, Marius Kloft, and Stephan Mandt. 2018. Scalable Generalized Dynamic Topic\nModels. In AISTATS , Vol. 84. 1427\u20131435.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07868",
                        "Citation Paper Title": "Title:Scalable Generalized Dynamic Topic Models",
                        "Citation Paper Abstract": "Abstract:Dynamic topic models (DTMs) model the evolution of prevalent themes in literature, online media, and other forms of text over time. DTMs assume that word co-occurrence statistics change continuously and therefore impose continuous stochastic process priors on their model parameters. These dynamical priors make inference much harder than in regular topic models, and also limit scalability. In this paper, we present several new results around DTMs. First, we extend the class of tractable priors from Wiener processes to the generic class of Gaussian processes (GPs). This allows us to explore topics that develop smoothly over time, that have a long-term memory or are temporally concentrated (for event detection). Second, we show how to perform scalable approximate inference in these models based on ideas around stochastic variational inference and sparse Gaussian processes. This way we can train a rich family of DTMs to massive data. Our experiments on several large-scale datasets show that our generalized model allows us to find interesting patterns that were not accessible by previous approaches.",
                        "Citation Paper Authors": "Authors:Patrick J\u00e4hnichen, Florian Wenzel, Marius Kloft, Stephan Mandt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.12120v1": {
            "Paper Title": "Improving Transformer-based Sequential Recommenders through Preference\n  Editing",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "propose SASRec which introduces a self-attention mechanism (the most\nimportant component in the transformer) to SRs to identify important items from interactions.\nSeveral variants have been proposed to improve upon SASRec. Mi et al . ",
                    "Citation Text": "Fei Mi, Xiaoyu Lin, and Boi Faltings. 2020. Ader: Adaptively distilled exemplar replay towards continual learning for\nsession-based recommendation. In The 14th ACM Conference on Recommender Systems . 408\u2013413.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12000",
                        "Citation Paper Title": "Title:ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation has received growing attention recently due to the increasing privacy concern. Despite the recent success of neural session-based recommenders, they are typically developed in an offline manner using a static dataset. However, recommendation requires continual adaptation to take into account new and obsolete items and users, and requires \"continual learning\" in real-life applications. In this case, the recommender is updated continually and periodically with new data that arrives in each update cycle, and the updated model needs to provide recommendations for user activities before the next model update. A major challenge for continual learning with neural models is catastrophic forgetting, in which a continually trained model forgets user preference patterns it has learned before. To deal with this challenge, we propose a method called Adaptively Distilled Exemplar Replay (ADER) by periodically replaying previous training samples (i.e., exemplars) to the current model with an adaptive distillation loss. Experiments are conducted based on the state-of-the-art SASRec model using two widely used datasets to benchmark ADER with several well-known continual learning techniques. We empirically demonstrate that ADER consistently outperforms other baselines, and it even outperforms the method using all historical data at every update cycle. This result reveals that ADER is a promising solution to mitigate the catastrophic forgetting issue towards building more realistic and scalable session-based recommenders.",
                        "Citation Paper Authors": "Authors:Fei Mi, Xiaoyu Lin, Boi Faltings"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.01033v2": {
            "Paper Title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment\n  Extraction in News Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.10880v2": {
            "Paper Title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the\n  US",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.11403v1": {
            "Paper Title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event\n  Signals from Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.11218v1": {
            "Paper Title": "Data Optimisation for a Deep Learning Recommender System",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "is a recent method\nthat circumvents liabilities of centralising the data,\nby a distribution of the algorithm to use data only\nlocally on-device. Similarly, the idea of di\ufb00eren-\ntial privacy is incorporated in deep learning in ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMa-\nhan, I. Mironov, K. Talwar, and L. Zhang.\nDeep learning with di\ufb00erential privacy. In Pro-\nceedings of the 2016 ACM SIGSAC Conference\non Computer and Communications Security ,\npages 308\u2013318, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", the notion of privacy is commonly as-\nsociated with data as a tangible asset, with focus\non secure storage, distribution and also location of\nuser data. Decentralisation ",
                    "Citation Text": "B. McMahan, E. Moore, D. Ramage, S. Hamp-\nson, and B. A. y Arcas. Communication-\ne\ufb03cient learning of deep networks from de-\ncentralized data. In Arti\ufb01cial Intelligence and\nStatistics , pages 1273\u20131282. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.10977v1": {
            "Paper Title": "Computational Pronunciation Analysis in Sung Utterances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10681v1": {
            "Paper Title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for\n  Visual Information Extraction using Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10547v1": {
            "Paper Title": "Leveraging Multiple Online Sources for Accurate Income Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04494v2": {
            "Paper Title": "Optimization of Service Addition in Multilevel Index Model for Edge\n  Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10159v1": {
            "Paper Title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K\n  Profitable Stocks",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "Liheng Zhang, Charu Aggarwal, and Guo-Jun Qi. Stock price\nprediction via discovering multi-frequency trading patterns. In\nProceedings of the 23rd ACM SIGKDD international conference on\nknowledge discovery and data mining , pages 2141\u20132149, 2017. ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based\nrecommender system: A survey and new perspectives. ACM\nComput. Surv. , 52(1), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07864v2": {
            "Paper Title": "User-specific Adaptive Fine-tuning for Cross-domain Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis, K. Grauman,\nand R. Feris, \u201cBlockdrop: Dynamic inference paths in residual\nnetworks,\u201d in CVPR , 2018, pp. 8817\u20138826. ",
                    "Citation Text": "Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris,\n\u201cSpottune: transfer learning through adaptive \ufb01ne-tuning,\u201d in\nCVPR , 2019, pp. 4805\u20134814.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08737",
                        "Citation Paper Title": "Title:SpotTune: Transfer Learning through Adaptive Fine-tuning",
                        "Citation Paper Abstract": "Abstract:Transfer learning, which allows a source task to affect the inductive bias of the target task, is widely used in computer vision. The typical way of conducting transfer learning with deep neural networks is to fine-tune a model pre-trained on the source task using data from the target task. In this paper, we propose an adaptive fine-tuning approach, called SpotTune, which finds the optimal fine-tuning strategy per instance for the target data. In SpotTune, given an image from the target task, a policy network is used to make routing decisions on whether to pass the image through the fine-tuned layers or the pre-trained layers. We conduct extensive experiments to demonstrate the effectiveness of the proposed approach. Our method outperforms the traditional fine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune with other state-of-the-art fine-tuning strategies, showing superior performance. On the Visual Decathlon datasets, our method achieves the highest score across the board without bells and whistles.",
                        "Citation Paper Authors": "Authors:Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, Rogerio Feris"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "E. Jang, S. Gu, and B. Poole, \u201cCategorical reparameterization with\ngumbel-softmax,\u201d arXiv preprint arXiv:1611.01144 , 2016. ",
                    "Citation Text": "Z. Wu, T. Nagarajan, A. Kumar, S. Rennie, L. S. Davis, K. Grauman,\nand R. Feris, \u201cBlockdrop: Dynamic inference paths in residual\nnetworks,\u201d in CVPR , 2018, pp. 8817\u20138826.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08393",
                        "Citation Paper Title": "Title:BlockDrop: Dynamic Inference Paths in Residual Networks",
                        "Citation Paper Abstract": "Abstract:Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\\% on average, going as high as 36\\% for some images, while maintaining the same 76.4\\% top-1 accuracy on ImageNet.",
                        "Citation Paper Authors": "Authors:Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, Rogerio Feris"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "C. J. Maddison, A. Mnih, and Y. W. Teh, \u201cThe concrete distribu-\ntion: A continuous relaxation of discrete random variables,\u201d arXiv\npreprint arXiv:1611.00712 , 2016. ",
                    "Citation Text": "E. Jang, S. Gu, and B. Poole, \u201cCategorical reparameterization with\ngumbel-softmax,\u201d arXiv preprint arXiv:1611.01144 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01144",
                        "Citation Paper Title": "Title:Categorical Reparameterization with Gumbel-Softmax",
                        "Citation Paper Abstract": "Abstract:Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",
                        "Citation Paper Authors": "Authors:Eric Jang, Shixiang Gu, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and\nD. Erhan, \u201cDomain separation networks,\u201d in Advances in neural\ninformation processing systems , 2016, pp. 343\u2013351. ",
                    "Citation Text": "F. Yuan, L. Yao, and B. Benatallah, \u201cDarec: Deep domain adap-\ntation for cross-domain recommendation via transferring rating\npatterns,\u201d arXiv preprint arXiv:1905.10760 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10760",
                        "Citation Paper Title": "Title:DARec: Deep Domain Adaptation for Cross-Domain Recommendation via Transferring Rating Patterns",
                        "Citation Paper Abstract": "Abstract:Cross-domain recommendation has long been one of the major topics in recommender systems. Recently, various deep models have been proposed to transfer the learned knowledge across domains, but most of them focus on extracting abstract transferable features from auxilliary contents, e.g., images and review texts, and the patterns in the rating matrix itself is rarely touched. In this work, inspired by the concept of domain adaptation, we proposed a deep domain adaptation model (DARec) that is capable of extracting and transferring patterns from rating matrices {\\em only} without relying on any auxillary information. We empirically demonstrate on public datasets that our method achieves the best performance among several state-of-the-art alternative cross-domain recommendation models.",
                        "Citation Paper Authors": "Authors:Feng Yuan, Lina Yao, Boualem Benatallah"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "H. Kanagawa, H. Kobayashi, N. Shimizu, Y. Tagami, and\nT. Suzuki, \u201cCross-domain recommendation via deep domain\nadaptation,\u201d in European Conference on Information Retrieval .\nSpringer, 2019, pp. 20\u201329. ",
                    "Citation Text": "K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and\nD. Erhan, \u201cDomain separation networks,\u201d in Advances in neural\ninformation processing systems , 2016, pp. 343\u2013351.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.06019",
                        "Citation Paper Title": "Title:Domain Separation Networks",
                        "Citation Paper Abstract": "Abstract:The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.",
                        "Citation Paper Authors": "Authors:Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ",\u201d ACM Computing\nSurveys (CSUR) , vol. 50, no. 3, pp. 1\u201334, 2017. ",
                    "Citation Text": "H. Kanagawa, H. Kobayashi, N. Shimizu, Y. Tagami, and\nT. Suzuki, \u201cCross-domain recommendation via deep domain\nadaptation,\u201d in European Conference on Information Retrieval .\nSpringer, 2019, pp. 20\u201329.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.03018",
                        "Citation Paper Title": "Title:Cross-domain Recommendation via Deep Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:The behavior of users in certain services could be a clue that can be used to infer their preferences and may be used to make recommendations for other services they have never used. However, the cross-domain relationships between items and user consumption patterns are not simple, especially when there are few or no common users and items across domains. To address this problem, we propose a content-based cross-domain recommendation method for cold-start users that does not require user- and item- overlap. We formulate recommendation as extreme multi-class classification where labels (items) corresponding to the users are predicted. With this formulation, the problem is reduced to a domain adaptation setting, in which a classifier trained in the source domain is adapted to the target domain. For this, we construct a neural network that combines an architecture for domain adaptation, Domain Separation Network, with a denoising autoencoder for item representation. We assess the performance of our approach in experiments on a pair of data sets collected from movie and news services of Yahoo! JAPAN and show that our approach outperforms several baseline methods including a cross-domain collaborative filtering method.",
                        "Citation Paper Authors": "Authors:Heishiro Kanagawa, Hayato Kobayashi, Nobuyuki Shimizu, Yukihiro Tagami, Taiji Suzuki"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.09590v1": {
            "Paper Title": "Open Data and the Status Quo -- A Fine-Grained Evaluation Framework for\n  Open Data Quality and an Analysis of Open Data portals in Germany",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09306v1": {
            "Paper Title": "PEN4Rec: Preference Evolution Networks for Session-based Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09297v1": {
            "Paper Title": "Embedding-based Product Retrieval in Taobao Search",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ",where one side uses n-gram query features and the other side\nexploits item features, without considering user personalization.\nRecently, JD ",
                    "Citation Text": "Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao,\nWeipeng Yan, and Wen-Yun Yang. 2020. Towards Personalized and Semantic Re-\ntrieval: An End-to-End Solution for E-commerce Search via Embedding Learning.\narXiv preprint arXiv:2006.02282 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.02282",
                        "Citation Paper Title": "Title:Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Nowadays e-commerce search has become an integral part of many people's shopping routines. Two critical challenges stay in today's e-commerce search: how to retrieve items that are semantically relevant but not exact matching to query terms, and how to retrieve items that are more personalized to different users for the same search query. In this paper, we present a novel approach called DPSR, which stands for Deep Personalized and Semantic Retrieval, to tackle this problem. Explicitly, we share our design decisions on how to architect a retrieval system so as to serve industry-scale traffic efficiently and how to train a model so as to learn query and item semantics accurately. Based on offline evaluations and online A/B test with live traffics, we show that DPSR model outperforms existing models, and DPSR system can retrieve more personalized and semantically relevant items to significantly improve users' search experience by +1.29% conversion rate, especially for long tail queries by +10.03%. As a result, our DPSR system has been successfully deployed into this http URL's search production since 2019.",
                        "Citation Paper Authors": "Authors:Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun Xiao, Weipeng Yan, Wen-Yun Yang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "for\nCPM (cost per mile) maximization in the web ads retrieval phase, re-\nducing the objective distinction between ranking and matching. For\nweb search, Google ",
                    "Citation Text": "Tao Wu, Ellie Ka-In Chio, Heng-Tze Cheng, Yu Du, Steffen Rendle, Dima Kuzmin,\nRitesh Agarwal, Li Zhang, John Anderson, Sarvjeet Singh, et al .2020. Zero-Shot\nHeterogeneous Transfer Learning from Recommender Systems to Cold-Start\nSearch Retrieval. In Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management . 2821\u20132828.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02930",
                        "Citation Paper Title": "Title:Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval",
                        "Citation Paper Abstract": "Abstract:Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
                        "Citation Paper Authors": "Authors:Tao Wu, Ellie Ka-In Chio, Heng-Tze Cheng, Yu Du, Steffen Rendle, Dima Kuzmin, Ritesh Agarwal, Li Zhang, John Anderson, Sarvjeet Singh, Tushar Chandra, Ed H. Chi, Wen Li, Ankit Kumar, Xiang Ma, Alex Soares, Nitin Jindal, Pei Cao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.09227v1": {
            "Paper Title": "Current Challenges and Future Directions in Podcast Information Access",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". A very recent approach is trajectory-based podcast\nrecommendation, which models short-term listening behavior of\nusers as trajectory in a podcast graph, and predicts the next shows a\nuser is likely to access ",
                    "Citation Text": "Greg Benton, Ghazal Fazelnia, Alice Wang, and Ben Carterette. 2020. Trajectory\nBased Podcast Recommendation. CoRR abs/2009.03859 (2020). arXiv:2009.03859\nhttps://arxiv.org/abs/2009.03859",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.03859",
                        "Citation Paper Title": "Title:Trajectory Based Podcast Recommendation",
                        "Citation Paper Abstract": "Abstract:Podcast recommendation is a growing area of research that presents new challenges and opportunities. Individuals interact with podcasts in a way that is distinct from most other media; and primary to our concerns is distinct from music consumption. We show that successful and consistent recommendations can be made by viewing users as moving through the podcast library sequentially. Recommendations for future podcasts are then made using the trajectory taken from their sequential behavior. Our experiments provide evidence that user behavior is confined to local trends, and that listening patterns tend to be found over short sequences of similar types of shows. Ultimately, our approach gives a450%increase in effectiveness over a collaborative filtering baseline.",
                        "Citation Paper Authors": "Authors:Greg Benton, Ghazal Fazelnia, Alice Wang, Ben Carterette"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07380v2": {
            "Paper Title": "Predicting the Popularity of Reddit Posts with AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.13767v1": {
            "Paper Title": "Sentiment Progression based Searching and Indexing of Literary Textual\n  Artefacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08908v1": {
            "Paper Title": "A Neural Model for Joint Document and Snippet Ranking in Question\n  Answering for Large Document Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08770v1": {
            "Paper Title": "TSSuBERT: Tweet Stream Summarization Using BERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08700v1": {
            "Paper Title": "Topology Distillation for Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.11846v1": {
            "Paper Title": "Quantifying the Impact of Human Capital, Job History, and Language\n  Factors on Job Seniority with a Large-scale Analysis of Resumes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01894v3": {
            "Paper Title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "trained on 960 hours of English read speech.\nImage encoders: Prior work has used various image en-\ncoders, including VGG16 [23, 12, 4], ResNet-152 [26, 9],\nResNet50 ",
                    "Citation Text": "D. Harwath, A. Recasens, D. Sur\u00b4 \u0131s, G. Chuang, A. Torral ba, and\nJ. Glass, \u201cJointly discovering visual objects and spoken wo rds\nfrom raw sensory input,\u201d in International Journal of Computer\nVision , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.01452",
                        "Citation Paper Title": "Title:Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input",
                        "Citation Paper Abstract": "Abstract:In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.",
                        "Citation Paper Authors": "Authors:David Harwath, Adri\u00e0 Recasens, D\u00eddac Sur\u00eds, Galen Chuang, Antonio Torralba, James Glass"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "has spontaneous\nspoken captions for Places 205, a dataset with images from 20 5\nscene classes. Localized Narratives ( LOCNARR) ",
                    "Citation Text": "J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut, and V . Fer-\nrari, \u201cConnecting vision and language with localized narra tives,\u201d\ninEuropean Conference on Computer Vision (ECCV) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.03098",
                        "Citation Paper Title": "Title:Connecting Vision and Language with Localized Narratives",
                        "Citation Paper Abstract": "Abstract:We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.",
                        "Citation Paper Authors": "Authors:Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio Ferrari"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.08252v1": {
            "Paper Title": "Interpretable Self-supervised Multi-task Learning for COVID-19\n  Information Retrieval and Extraction",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", query and the candidate articles\nare tokenized into frequent sub-words using WordPiece unsupervised algorithm ",
                    "Citation Text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap\nbetween human and machine translation. arXiv preprint arXiv:1609.08144 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08144",
                        "Citation Paper Title": "Title:Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
                        "Citation Paper Authors": "Authors:Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ".\n2.3 Transformer-based Ranking\nSimilar to other studies that apply BERT to biomedical ",
                    "Citation Text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics ,\n36(4):1234\u20131240, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08746",
                        "Citation Paper Title": "Title:BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
                        "Citation Paper Abstract": "Abstract:Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at this https URL, and the source code for fine-tuning BioBERT available at this https URL.",
                        "Citation Paper Authors": "Authors:Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.08072v1": {
            "Paper Title": "Full Bitcoin Blockchain Data Made Easy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08042v1": {
            "Paper Title": "Hotel Recognition via Latent Image Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07813v1": {
            "Paper Title": "To Infinity and Beyond! Accessibility is the Future for Kids' Search\n  Engines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07742v1": {
            "Paper Title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval\n  in the Archaeology Domain",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ".\nSciBERT, trained on a large amount of scientific texts from different domains, shows an increase in F1 for NER of\n2 to 5% points, indicating that domain pre-training is useful for NER ",
                    "Citation Text": "Iz Beltagy, Kyle Lo, and Arman Cohan. 2020. SCIBERT: A pretrained language model for scientific text. In EMNLP-IJCNLP 2019 - 2019\nConference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing,\nProceedings of the Conference . Association for Computational Linguistics, Hong Kong, China. https://doi.org/10.18653/v1/d19-1371",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10676",
                        "Citation Paper Title": "Title:SciBERT: A Pretrained Language Model for Scientific Text",
                        "Citation Paper Abstract": "Abstract:Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Iz Beltagy, Kyle Lo, Arman Cohan"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "has been used extensively for NER [ 42,43]. These embeddings-based methods\ntypically feed the embeddings to CRF and/or Bi-LSTM algorithms to make NER predictions.\nA big shift in NLP was introduced by Devlin et al . ",
                    "Citation Text": "Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , Vol. 1. Association for Computational Linguistics, Minneapolis,\nMinnesota, 4171\u20134186. https://doi.org/10.18653/v1/N19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "for definitions and examples of these entity types.\n5Zenodo repository: http://doi.org/10.5281/zenodo.3544544\n6We used HuggingFace\u2019s ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf,\nMorgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing.\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for\nComputational Linguistics, Stroudsburg, PA, USA, 38\u201345. https://doi.org/10.18653/v1/2020.emnlp-demos.6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nHowever, recent work has shown that for some languages, multiBERT is outperformed by language-specific\nBERT models ",
                    "Citation Text": "Debora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? Making Sense of Language-Specific BERT Models. arXiv (3\n2020). http://arxiv.org/abs/2003.02912",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.02912",
                        "Citation Paper Title": "Title:What the [MASK]? Making Sense of Language-Specific BERT Models",
                        "Citation Paper Abstract": "Abstract:Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at this https URL.",
                        "Citation Paper Authors": "Authors:Debora Nozza, Federico Bianchi, Dirk Hovy"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". Moon et al .also\nshowed that fine-tuning multiBERT on a mixed language NER dataset provided better results than fine-tuning on\nindividual languages ",
                    "Citation Text": "Taesun Moon, Parul Awasthy, Jian Ni, and Radu Florian. 2019. Towards Lingua Franca Named Entity Recognition with BERT. arXiv (11\n2019). http://arxiv.org/abs/1912.01389",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01389",
                        "Citation Paper Title": "Title:Towards Lingua Franca Named Entity Recognition with BERT",
                        "Citation Paper Abstract": "Abstract:Information extraction is an important task in NLP, enabling the automatic extraction of data for relational database filling. Historically, research and data was produced for English text, followed in subsequent years by datasets in Arabic, Chinese (ACE/OntoNotes), Dutch, Spanish, German (CoNLL evaluations), and many others. The natural tendency has been to treat each language as a different dataset and build optimized models for each. In this paper we investigate a single Named Entity Recognition model, based on a multilingual BERT, that is trained jointly on many languages simultaneously, and is able to decode these languages with better accuracy than models trained only on one language. To improve the initial model, we study the use of regularization strategies such as multitask learning and partial gradient updates. In addition to being a single model that can tackle multiple languages (including code switch), the model could be used to make zero-shot predictions on a new language, even ones for which training data is not available, out of the box. The results show that this model not only performs competitively with monolingual models, but it also achieves state-of-the-art results on the CoNLL02 Dutch and Spanish datasets, OntoNotes Arabic and Chinese datasets. Moreover, it performs reasonably well on unseen languages, achieving state-of-the-art for zero-shot on three CoNLL languages.",
                        "Citation Paper Authors": "Authors:Taesun Moon, Parul Awasthy, Jian Ni, Radu Florian"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "tokeniser.\nRecent results indicate that ensemble methods that combining generic and domain-specific BERT models ",
                    "Citation Text": "Jenny Copara, Nona Naderi, Julien Knafou, Patrick Ruch, and Douglas Teodoro. 2020. Named entity recognition in chemical patents\nusing ensemble of contextual language models. http://arxiv.org/abs/2007.12569",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12569",
                        "Citation Paper Title": "Title:Named entity recognition in chemical patents using ensemble of contextual language models",
                        "Citation Paper Abstract": "Abstract:Chemical patent documents describe a broad range of applications holding key reaction and compound information, such as chemical structure, reaction formulas, and molecular properties. These informational entities should be first identified in text passages to be utilized in downstream tasks. Text mining provides means to extract relevant information from chemical patents through information extraction techniques. As part of the Information Extraction task of the Cheminformatics Elsevier Melbourne University challenge, in this work we study the effectiveness of contextualized language models to extract reaction information in chemical patents. We assess transformer architectures trained on a generic and specialised corpora to propose a new ensemble model. Our best model, based on a majority ensemble approach, achieves an exact F1-score of 92.30% and a relaxed F1-score of 96.24%. The results show that ensemble of contextualized language models can provide an effective method to extract information from chemical patents.",
                        "Citation Paper Authors": "Authors:Jenny Copara, Nona Naderi, Julien Knafou, Patrick Ruch, Douglas Teodoro"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", which has become a\ncommon baseline for NER. Since 2011, word embeddings have become increasingly important as representations\nin NER. Especially Word2vec ",
                    "Citation Text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In 1st\nInternational Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.09931v2": {
            "Paper Title": "Field-Embedded Factorization Machines for Click-through rate prediction",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "use custom-designed core layers along with DNNs. They also combine their core layers\nwith DNNs but their way of generating the intermediate layers is quite different from our method. Wide and Deep\nmodels ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,\nGreg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of\nthe 1st workshop on deep learning for recommender systems , pages 7\u201310, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.06983v2": {
            "Paper Title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07453v1": {
            "Paper Title": "Efficient Data-specific Model Search for Collaborative Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09029v2": {
            "Paper Title": "CHECKED: Chinese COVID-19 Fake News Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06739v1": {
            "Paper Title": "Engineering Knowledge Graph from Patent Database",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06722v1": {
            "Paper Title": "Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": "is a recently proposed pre-training method for homogeneous graph via contrastive\nlearning. We fine-tune the pre-trained model released by the authors on our datasets.\n\u2022Graph-BERT ",
                    "Citation Text": "Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. 2020. Graph-Bert: Only Attention is Needed for Learning\nGraph Representations. CoRR abs/2001.05140 (2020). arXiv:2001.05140",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.05140",
                        "Citation Paper Title": "Title:Graph-Bert: Only Attention is Needed for Learning Graph Representations",
                        "Citation Paper Abstract": "Abstract:The dominant graph neural networks (GNNs) over-rely on the graph links, several serious performance problems with which have been witnessed already, e.g., suspended animation problem and over-smoothing problem. What's more, the inherently inter-connected nature precludes parallelization within the graph, which becomes critical for large-sized graph, as memory constraints limit batching across the nodes. In this paper, we will introduce a new graph neural network, namely GRAPH-BERT (Graph based BERT), solely based on the attention mechanism without any graph convolution or aggregation operators. Instead of feeding GRAPH-BERT with the complete large input graph, we propose to train GRAPH-BERT with sampled linkless subgraphs within their local contexts. GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a pre-trained GRAPH-BERT can also be transferred to other application tasks directly or with necessary fine-tuning if any supervised label information or certain application oriented objective is available. We have tested the effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the pre-trained GRAPH-BERT with the node attribute reconstruction and structure recovery tasks, we further fine-tune GRAPH-BERT on node classification and graph clustering tasks specifically. The experimental results have demonstrated that GRAPH-BERT can out-perform the existing GNNs in both the learning effectiveness and efficiency.",
                        "Citation Paper Authors": "Authors:Jiawei Zhang, Haopeng Zhang, Congying Xia, Li Sun"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "utilizes convolutional neural network to construct meta-path embeddings and\nfurther leverages co-attention mechanism to model interactions among users, items and\nmeta-paths.\n\u2022PF-HIN ",
                    "Citation Text": "Yang Fang, Xiang Zhao, and Weidong Xiao. 2020. Exploring Heterogeneous Information Networks via Pre-Training.\nCoRR abs/2007.03184 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.03184",
                        "Citation Paper Title": "Title:Pre-Trained Models for Heterogeneous Information Networks",
                        "Citation Paper Abstract": "Abstract:In network representation learning we learn how to represent heterogeneous information networks in a low-dimensional space so as to facilitate effective search, classification, and prediction solutions. Previous network representation learning methods typically require sufficient task-specific labeled data to address domain-specific problems. The trained model usually cannot be transferred to out-of-domain datasets. We propose a self-supervised pre-training and fine-tuning framework, PF-HIN, to capture the features of a heterogeneous information network. Unlike traditional network representation learning models that have to train the entire model all over again for every downstream task and dataset, PF-HIN only needs to fine-tune the model and a small number of extra task-specific parameters, thus improving model efficiency and effectiveness. During pre-training, we first transform the neighborhood of a given node into a sequence. PF-HIN is pre-trained based on two self-supervised tasks, masked node modeling and adjacent node prediction. We adopt deep bi-directional transformer encoders to train the model, and leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In the fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification, and node clustering. PF-HIN consistently and significantly outperforms state-of-the-art alternatives on each of these tasks, on four datasets.",
                        "Citation Paper Authors": "Authors:Yang Fang, Xiang Zhao, Yifan Chen, Weidong Xiao, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "is the state-of-the-art graph neural network based collaborative filtering\nmodel which simplifies the design of GCN to make it more concise and appropriate for\nrecommendation.\n\u2022HGT ",
                    "Citation Text": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous Graph Transformer. In WWW 2020 .\nACM / IW3C2, 2704\u20132710.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01332",
                        "Citation Paper Title": "Title:Heterogeneous Graph Transformer",
                        "Citation Paper Abstract": "Abstract:Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",
                        "Citation Paper Authors": "Authors:Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "adopts GNN layers on the user-item interaction graph, which exploits the user-\nitem graph structure by propagating embeddings on it to refine user and item representations.\n15Woodstock \u201918, June 03\u201305, 2018, Woodstock, NY Hui and Zhou et al.\n\u2022LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying\nand Powering Graph Convolution Network for Recommendation. In SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "utilize curriculum learning to transmit the acquired knowledge to the target task ",
                    "Citation Text": "Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou, and Ioannis A. Kakadiaris. 2017. Curriculum\nLearning for Multi-task Classification of Visual Attributes. In ICCV Workshops 2017, Venice, Italy, October 22-29, 2017 .\nIEEE Computer Society, 2608\u20132615.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.08728",
                        "Citation Paper Title": "Title:Curriculum Learning for Multi-Task Classification of Visual Attributes",
                        "Citation Paper Abstract": "Abstract:Visual attributes, from simple objects (e.g., backpacks, hats) to soft-biometrics (e.g., gender, height, clothing) have proven to be a powerful representational approach for many applications such as image description and human identification. In this paper, we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework. Individual tasks are grouped based on their correlation so that two groups of strongly and weakly correlated tasks are formed. The two groups of tasks are learned in a curriculum learning setup by transferring the acquired knowledge from the strongly to the weakly correlated. The learning process within each group though, is performed in a multi-task classification setup. The proposed method learns better and converges faster than learning all the tasks in a typical multi-task learning paradigm. We demonstrate the effectiveness of our approach on the publicly available, SoBiR, VIPeR and PETA datasets and report state-of-the-art results across the board.",
                        "Citation Paper Authors": "Authors:Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou, Ioannis A. Kakadiaris"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", pre-training technique has been recently applied to graph datasets for im-\nproving GNNs ",
                    "Citation Text": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020. GPT-GNN: Generative Pre-Training of\nGraph Neural Networks. In KDD 2020 . 1857\u20131867.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15437",
                        "Citation Paper Title": "Title:GPT-GNN: Generative Pre-Training of Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
                        "Citation Paper Authors": "Authors:Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "utilize contrastive learning to capture the universal network topological properties\nacross multiple networks, which empowers graph neural networks to learn the intrinsic and trans-\nferable structural representations. You et al. ",
                    "Citation Text": "Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep Graph Contrastive Representation\nLearning. CoRR abs/2006.04131 (2020).\n26",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04131",
                        "Citation Paper Title": "Title:Deep Graph Contrastive Representation Learning",
                        "Citation Paper Abstract": "Abstract:Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",
                        "Citation Paper Authors": "Authors:Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "propose graph information maximization method to learn node representations,\nwhich is mindful of the global structural properties of the graph. Ren et al. ",
                    "Citation Text": "Yuxiang Ren, Bo Liu, Chao Huang, Peng Dai, Liefeng Bo, and Jiawei Zhang. 2019. Heterogeneous Deep Graph Infomax.\nCoRR abs/1911.08538 (2019). arXiv:1911.08538 http://arxiv.org/abs/1911.08538\n24Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top- \ud835\udc41Recommendation Woodstock \u201918, June 03\u201305, 2018, Woodstock, NY",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.08538",
                        "Citation Paper Title": "Title:Heterogeneous Deep Graph Infomax",
                        "Citation Paper Abstract": "Abstract:Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information theoretic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.",
                        "Citation Paper Authors": "Authors:Yuxiang Ren, Bo Liu, Chao Huang, Peng Dai, Liefeng Bo, Jiawei Zhang"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "utilize graph\nattention network to aggregate features from meta-path based neighbors in a hierarchical manner,\nwhich mainly focuses on semi-supervised classification task. Wang et al. ",
                    "Citation Text": "Yifan Wang, Suyao Tang, Yuntong Lei, Weiping Song, Sheng Wang, and Ming Zhang. 2020. DisenHAN: Disentangled\nHeterogeneous Graph Attention Network for Recommendation. In CIKM \u201920: The 29th ACM International Conference\non Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze,\nClaudia Hauff, Edward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 1605\u20131614. https://doi.org/10.1145/3340531.\n3411996",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10879",
                        "Citation Paper Title": "Title:DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Heterogeneous information network has been widely used to alleviate sparsity and cold start problems in recommender systems since it can model rich context information in user-item interactions. Graph neural network is able to encode this rich context information through propagation on the graph. However, existing heterogeneous graph neural networks neglect entanglement of the latent factors stemming from different aspects. Moreover, meta paths in existing approaches are simplified as connecting paths or side information between node pairs, overlooking the rich semantic information in the paths. In this paper, we propose a novel disentangled heterogeneous graph attention network DisenHAN for top-$N$ recommendation, which learns disentangled user/item representations from different aspects in a heterogeneous information network. In particular, we use meta relations to decompose high-order connectivity between node pairs and propose a disentangled embedding propagation layer which can iteratively identify the major aspect of meta relations. Our model aggregates corresponding aspect features from each meta relation for the target user/item. With different layers of embedding propagation, DisenHAN is able to explicitly capture the collaborative filtering effect semantically. Extensive experiments on three real-world datasets show that DisenHAN consistently outperforms state-of-the-art approaches. We further demonstrate the effectiveness and interpretability of the learned disentangled representations via insightful case studies and visualization.",
                        "Citation Paper Authors": "Authors:Yifan Wang, Suyao Tang, Yuntong Lei, Weiping Song, Sheng Wang, Ming Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.06720v1": {
            "Paper Title": "BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan\n  using text mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06713v1": {
            "Paper Title": "AutoLoss: Automated Loss Function Search in Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "proposed to automatically discover feature\ninteraction architectures for click-through rate (CTR) prediction.\nTsang et al . ",
                    "Citation Text": "Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, and Yan\nLiu. 2020. Feature interaction interpretability: A case for explaining ad-\nrecommendation systems via neural interaction detection. arXiv preprint\narXiv:2006.10966 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10966",
                        "Citation Paper Title": "Title:Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection",
                        "Citation Paper Abstract": "Abstract:Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, e.g., significantly outperforming existing recommender models. What's more, the same approach to interpret interactions can provide new insights into domains even beyond recommendation, such as text and image classification.",
                        "Citation Paper Authors": "Authors:Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, Yan Liu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "proposed to perform\nembedding dimension selection with a soft selection layer, making\nthe dimension selection more flexible. Guo et al . ",
                    "Citation Text": "Huifeng Guo, Bo Chen, Ruiming Tang, Zhenguo Li, and Xiuqiang He. 2020.\nAutoDis: Automatic Discretization for Embedding Numerical Features in CTR\nPrediction. arXiv preprint arXiv:2012.08986 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.08986",
                        "Citation Paper Title": "Title:An Embedding Learning Framework for Numerical Features in CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Click-Through Rate (CTR) prediction is critical for industrial recommender systems, where most deep CTR models follow an Embedding \\& Feature Interaction paradigm. However, the majority of methods focus on designing network architectures to better capture feature interactions while the feature embedding, especially for numerical features, has been overlooked. Existing approaches for numerical features are difficult to capture informative knowledge because of the low capacity or hard discretization based on the offline expertise feature engineering. In this paper, we propose a novel embedding learning framework for numerical features in CTR prediction (AutoDis) with high model capacity, end-to-end training and unique representation properties preserved. AutoDis consists of three core components: meta-embeddings, automatic discretization and aggregation. Specifically, we propose meta-embeddings for each numerical field to learn global knowledge from the perspective of field with a manageable number of parameters. Then the differentiable automatic discretization performs soft discretization and captures the correlations between the numerical features and meta-embeddings. Finally, distinctive and informative embeddings are learned via an aggregation function. Comprehensive experiments on two public and one industrial datasets are conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has been deployed onto a mainstream advertising platform, where online A/B test demonstrates the improvement over the base model by 2.1% and 2.7% in terms of CTR and eCPM, respectively. In addition, the code of our framework is publicly available in MindSpore(this https URL).",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed a multi-granular\nquantized embeddings (MGQE) technique to learn impact embed-\ndings for infrequent items. Cheng et al . ",
                    "Citation Text": "Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural\nInput Search for Recommender Systems. arXiv preprint arXiv:2006.04466 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04466",
                        "Citation Paper Title": "Title:Differentiable Neural Input Search for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Latent factor models are the driving forces of the state-of-the-art recommender systems, with an important insight of vectorizing raw input features into dense embeddings. The dimensions of different feature embeddings are often set to a same value empirically, which limits the predictive performance of latent factor models. Existing works have proposed heuristic or reinforcement learning-based methods to search for mixed feature embedding dimensions. For efficiency concern, these methods typically choose embedding dimensions from a restricted set of candidate dimensions. However, this restriction will hurt the flexibility of dimension selection, leading to suboptimal performance of search results. In this paper, we propose Differentiable Neural Input Search (DNIS), a method that searches for mixed feature embedding dimensions in a more flexible space through continuous relaxation and differentiable optimization. The key idea is to introduce a soft selection layer that controls the significance of each embedding dimension, and optimize this layer according to model's validation performance. DNIS is model-agnostic and thus can be seamlessly incorporated with existing latent factor models for recommendation. We conduct experiments with various architectures of latent factor models on three public real-world datasets for rating prediction, Click-Through-Rate (CTR) prediction, and top-k item recommendation. The results demonstrate that our method achieves the best predictive performance compared with existing neural input search approaches with fewer embedding parameters and less time cost.",
                        "Citation Paper Authors": "Authors:Weiyu Cheng, Yanyan Shen, Linpeng Huang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "proposed\nto use mixed dimension embeddings for users and items based on\ntheir query frequency. Kang et al . ",
                    "Citation Text": "Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin,\nLichan Hong, and Ed H Chi. 2020. Learning Multi-granular Quantized Embed-\ndings for Large-Vocab Categorical Features in Recommender Systems. arXiv\npreprint arXiv:2002.08530 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08530",
                        "Citation Paper Title": "Title:Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system models often represent various sparse features like users, items, and categorical features via embeddings. A standard approach is to map each unique feature value to an embedding vector. The size of the produced embedding table grows linearly with the size of the vocabulary. Therefore, a large vocabulary inevitably leads to a gigantic embedding table, creating two severe problems: (i) making model serving intractable in resource-constrained environments; (ii) causing overfitting problems. In this paper, we seek to learn highly compact embeddings for large-vocab sparse features in recommender systems (recsys). First, we show that the novel Differentiable Product Quantization (DPQ) approach can generalize to recsys problems. In addition, to better handle the power-law data distribution commonly seen in recsys, we propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns more compact embeddings for infrequent items. We seek to provide a new angle to improve recommendation performance with compact model sizes. Extensive experiments on three recommendation tasks and two datasets show that we can achieve on par or better performance, with only ~20% of the original model size.",
                        "Citation Paper Authors": "Authors:Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, Ed H. Chi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "proposed to dynamically\nsearch embedding sizes for users and items based on their popular-\nity in the streaming setting. Similarly, Ginart et al . ",
                    "Citation Text": "Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James\nZou. 2019. Mixed Dimension Embeddings with Application to Memory-Efficient\nRecommendation Systems. arXiv preprint arXiv:1909.11810 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11810",
                        "Citation Paper Title": "Title:Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:Embedding representations power machine intelligence in many applications, including recommendation systems, but they are space intensive -- potentially occupying hundreds of gigabytes in large-scale settings. To help manage this outsized memory consumption, we explore mixed dimension embeddings, an embedding layer architecture in which a particular embedding vector's dimension scales with its query frequency. Through theoretical analysis and systematic experiments, we demonstrate that using mixed dimensions can drastically reduce the memory usage, while maintaining and even improving the ML performance. Empirically, we show that the proposed mixed dimension layers improve accuracy by 0.1% using half as many parameters or maintain it using 16X fewer parameters for click-through rate prediction task on the Criteo Kaggle dataset.",
                        "Citation Paper Authors": "Authors:Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, James Zou"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "composed multiple self-\nsupervised learning tasks to jointly encode multiple sources of\ninformation and produce more generalizable representations, and\ndeveloped two automated frameworks to search the task weights.\nBesides, Li et al . ",
                    "Citation Text": "Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli\nOuyang. 2019. Am-lfs: Automl for loss function search. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision . 8410\u20138419.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07375",
                        "Citation Paper Title": "Title:AM-LFS: AutoML for Loss Function Search",
                        "Citation Paper Abstract": "Abstract:Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",
                        "Citation Paper Authors": "Authors:Chuming Li, Yuan Xin, Chen Lin, Minghao Guo, Wei Wu, Wanli Ouyang, Junjie Yan"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "proposed automatically searching specific sur-\nrogate losses to improve different evaluation metrics in the image\nsemantic segmentation task. Jin et al . ",
                    "Citation Text": "Wei Jin, Xiaorui Liu, Xiangyu Zhao, Yao Ma, Neil Shah, and Jiliang Tang. 2021.\nAutomated Self-Supervised Learning for Graphs. arXiv:2106.05470 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05470",
                        "Citation Paper Title": "Title:Automated Self-Supervised Learning for Graphs",
                        "Citation Paper Abstract": "Abstract:Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for graph self-supervised learning. Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that \"like attracts like,\" as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this search task. Then we propose the AutoSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AutoSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks. Code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Wei Jin, Xiaorui Liu, Xiangyu Zhao, Yao Ma, Neil Shah, Jiliang Tang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed to optimize the stochastic loss function (SLF), where\nthe loss function of an ML model was dynamically selected. During\ntraining, model parameters and the loss parameters are learned\njointly. Li et al . ",
                    "Citation Text": "Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, and Jifeng Dai.\n2020. Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation.\narXiv preprint arXiv:2010.07930 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07930",
                        "Citation Paper Title": "Title:Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released.",
                        "Citation Paper Authors": "Authors:Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "proposed to replace the traditional Softmax\nloss with large margin Softmax (L-Softmax) loss to improve feature\ndiscrimination in classification tasks. Fan et al . ",
                    "Citation Text": "Xing Fan, Wei Jiang, Hao Luo, and Mengjuan Fei. 2019. Spherereid: Deep hy-\npersphere manifold embedding for person re-identification. Journal of Visual\nCommunication and Image Representation 60 (2019), 51\u201358.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00537",
                        "Citation Paper Title": "Title:SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification",
                        "Citation Paper Abstract": "Abstract:Many current successful Person Re-Identification(ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.",
                        "Citation Paper Authors": "Authors:Xing Fan, Wei Jiang, Hao Luo, Mengjuan Fei, "
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "designed losses\nwith larger weights at boundary regions to improve the boundary\nF1 score. Liu et al . ",
                    "Citation Text": "Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. 2016. Large-margin\nsoftmax loss for convolutional neural networks.. In ICML , Vol. 2. 7.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.02295",
                        "Citation Paper Title": "Title:Large-Margin Softmax Loss for Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.",
                        "Citation Paper Authors": "Authors:Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "designed\nloss functions by taking class frequency into consideration to cater\nto the mIoU metric. Caliva et al . ",
                    "Citation Text": "Francesco Caliva, Claudia Iriondo, Alejandro Morales Martinez, Sharmila Ma-\njumdar, and Valentina Pedoia. 2019. Distance map loss penalty term for semantic\nsegmentation. arXiv preprint arXiv:1908.03679 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03679",
                        "Citation Paper Title": "Title:Distance Map Loss Penalty Term for Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks for semantic segmentation suffer from low performance at object boundaries. In medical imaging, accurate representation of tissue surfaces and volumes is important for tracking of disease biomarkers such as tissue morphology and shape features. In this work, we propose a novel distance map derived loss penalty term for semantic segmentation. We propose to use distance maps, derived from ground truth masks, to create a penalty term, guiding the network's focus towards hard-to-segment boundary regions. We investigate the effects of this penalizing factor against cross-entropy, Dice, and focal loss, among others, evaluating performance on a 3D MRI bone segmentation task from the publicly available Osteoarthritis Initiative dataset. We observe a significant improvement in the quality of segmentation, with better shape preservation at bone boundaries and areas affected by partial volume. We ultimately aim to use our loss penalty term to improve the extraction of shape biomarkers and derive metrics to quantitatively evaluate the preservation of shape.",
                        "Citation Paper Authors": "Authors:Francesco Caliva, Claudia Iriondo, Alejandro Morales Martinez, Sharmila Majumdar, Valentina Pedoia"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "argued that\nthe typical cross-entropy loss for semantic segmentation shows\ngreat limitations in aligning with evaluation metrics other than\nglobal accuracy. Ronneberger et al . ",
                    "Citation Text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In International Conference on\nMedical image computing and computer-assisted intervention . Springer, 234\u2013241.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03954v2": {
            "Paper Title": "Evaluating Meta-Feature Selection for the Algorithm Recommendation\n  Problem",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". However, with the rising number of purpose meta-features in the literature ",
                    "Citation Text": "Adriano Rivolli, Lu\u00eds PF Garcia, Carlos Soares, Joaquin Vanschoren, and Andr\u00e9 CPLF\nde Carvalho. Towards reproducible empirical research in meta-learning. arXiv preprint\narXiv:1808.10406 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10406",
                        "Citation Paper Title": "Title:Characterizing classification datasets: a study of meta-features for meta-learning",
                        "Citation Paper Abstract": "Abstract:Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.",
                        "Citation Paper Authors": "Authors:Adriano Rivolli, Lu\u00eds P. F. Garcia, Carlos Soares, Joaquin Vanschoren, Andr\u00e9 C. P. L. F. de Carvalho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.06467v1": {
            "Paper Title": "A Large-Scale Rich Context Query and Recommendation Dataset in Online\n  Knowledge-Sharing",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ": An non-sampling neural recommendation model\nwith efficient neural matrix factorization.\nThe experiment has been done with RecBole ",
                    "Citation Text": "Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Kaiyuan Li, Yushuo Chen,\nYujie Lu, Hui Wang, Changxin Tian, Xingyu Pan, Yingqian Min, Zhichao Feng,\nXinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and\nJi-Rong Wen. 2020. RecBole: Towards a Unified, Comprehensive and Efficient\nFramework for Recommendation Algorithms. arXiv preprint arXiv:2011.01731\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.01731",
                        "Citation Paper Title": "Title:RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms",
                        "Citation Paper Abstract": "Abstract:In recent years, there are a large number of recommendation algorithms proposed in the literature, from traditional collaborative filtering to deep learning algorithms. However, the concerns about how to standardize open source implementation of recommendation algorithms continually increase in the research community. In the light of this challenge, we propose a unified, comprehensive and efficient recommender system library called RecBole, which provides a unified framework to develop and reproduce recommendation algorithms for research purpose. In this library, we implement 73 recommendation models on 28 benchmark datasets, covering the categories of general recommendation, sequential recommendation, context-aware recommendation and knowledge-based recommendation. We implement the RecBole library based on PyTorch, which is one of the most popular deep learning frameworks. Our library is featured in many aspects, including general and extensible data structures, comprehensive benchmark models and datasets, efficient GPU-accelerated execution, and extensive and standard evaluation protocols. We provide a series of auxiliary functions, tools, and scripts to facilitate the use of this library, such as automatic parameter tuning and break-point resume. Such a framework is useful to standardize the implementation and evaluation of recommender systems. The project and documents are released at this https URL.",
                        "Citation Paper Authors": "Authors:Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, Ji-Rong Wen"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ": This method applies the Bayesian Personalized\nRanking objective function to optimize Matrix Factorization.\n\u2022LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for\nRecommendation . Association for Computing Machinery, New York, NY, USA,\n639\u2013648. https://doi.org/10.1145/3397271.3401063",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ": This method selects the top K nearest answer\nneighbors and uses their information to make predictions.\n\u2022BPR ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedingsof the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (Montreal,\nQuebec, Canada) (UAI \u201909) . AUAI Press, Arlington, Virginia, USA, 452\u2013461.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.06420v1": {
            "Paper Title": "A Framework to Enhance Generalization of Deep Metric Learning methods\n  using General Discriminative Feature Learning and Class Adversarial Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06258v1": {
            "Paper Title": "DebiasGAN: Eliminating Position Bias in News Recommendation with\n  Adversarial Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06244v1": {
            "Paper Title": "Predicting Knowledge Gain during Web Search based on Multimedia Resource\n  Consumption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05729v2": {
            "Paper Title": "GRASP: Graph Alignment through Spectral Signatures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06216v1": {
            "Paper Title": "Nested and Balanced Entity Recognition using Multi-Task Learning",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nThe class imbalance within the data is another aspect to treat\nwhen it comes to CR or NER. Most of the samples do not belong\nto any specific class (non-concept or non-named-entity). As showed\nin ",
                    "Citation Text": "Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. 2019.\nDice Loss for Data-imbalanced NLP Tasks. arXiv preprint arXiv:1911.02855\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02855",
                        "Citation Paper Title": "Title:Dice Loss for Data-imbalanced NLP Tasks",
                        "Citation Paper Abstract": "Abstract:Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of background examples (or easy-negative examples) overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",
                        "Citation Paper Authors": "Authors:Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.08976v2": {
            "Paper Title": "Anytime Ranking on Document-Ordered Indexes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09093v2": {
            "Paper Title": "PARADE: Passage Representation Aggregation for Document Reranking",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": ". Turc et al. pre-train a family of compact BERT models\nand explore transferring task knowledge from large fine-tuned mod-\nels ",
                    "Citation Text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-\nRead Students Learn Better: The Impact of Student Initialization on Knowledge\nDistillation. CoRR abs/1908.08962 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.08962",
                        "Citation Paper Title": "Title:Well-Read Students Learn Better: On the Importance of Pre-training Compact Models",
                        "Citation Paper Abstract": "Abstract:Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",
                        "Citation Paper Authors": "Authors:Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". QDS-Transformer tailors Longformer to the rank-\ning task with query-directed sparse attention ",
                    "Citation Text": "Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, and Wei Wang. 2020. Long Doc-\nument Ranking with Query-Directed Sparse Transformer. In EMNLP (Findings) .\nAssociation for Computational Linguistics, 4594\u20134605.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12683",
                        "Citation Paper Title": "Title:Long Document Ranking with Query-Directed Sparse Transformer",
                        "Citation Paper Abstract": "Abstract:The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer, enforces the principle properties desired in ranking: local contextualization, hierarchical representation, and query-oriented proximity matching, while it also enjoys efficiency from sparsity. Experiments on one fully supervised and three few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the computing complexity and demonstrates that our sparse attention with TVM implementation is twice more efficient than the fully-connected self-attention. All source codes, trained model, and predictions of this work are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, Wei Wang"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Hofst\u00e4tter et al. also proposes a cross-architecture\nknowledge distillation framework using a Margin Mean Squared\nError loss in a pairwise training manner ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross-\nArchitecture Knowledge Distillation. CoRR abs/2010.02666 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02666",
                        "Citation Paper Title": "Title:Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Retrieval and ranking models are the backbone of many applications such as web search, open domain QA, or text-based recommender systems. The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT passage ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of distilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves re-ranking effectiveness without compromising their efficiency. Additionally, we show our general distillation method to improve nearest neighbor based index retrieval with the BERT dot product model, offering competitive results with specialized and much more costly training methods. To benefit the community, we publish the teacher-score training files in a ready-to-use package.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": ". Tahami et al. propose a new cross-encoder architecture\nand transfer knowledge from this model to a bi-encoder model for\nfast retrieval ",
                    "Citation Text": "Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. 2020. Distilling\nKnowledge for Fast Retrieval-based Chat-bots. CoRR abs/2004.11045 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11045",
                        "Citation Paper Title": "Title:Distilling Knowledge for Fast Retrieval-based Chat-bots",
                        "Citation Paper Abstract": "Abstract:Response retrieval is a subset of neural ranking in which a model selects a suitable response from a set of candidates given a conversation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as customer support agents. In order to make pairwise comparisons between a conversation history and a candidate response, two approaches are common: cross-encoders performing full self-attention over the pair and bi-encoders encoding the pair separately. The former gives better prediction quality but is too slow for practical use. In this paper, we propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model using distillation. This effectively boosts bi-encoder performance at no cost during inference time. We perform a detailed analysis of this approach on three response retrieval datasets.",
                        "Citation Paper Authors": "Authors:Amir Vakili Tahami, Kamyar Ghajar, Azadeh Shakery"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ". Tang et al. distill knowledge from the BERT model into Bi-\nLSTM ",
                    "Citation Text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy\nLin. 2019. Distilling Task-Specific Knowledge from BERT into Simple Neural\nNetworks. CoRR abs/1903.12136 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.12136",
                        "Citation Paper Title": "Title:Distilling Task-Specific Knowledge from BERT into Simple Neural Networks",
                        "Citation Paper Abstract": "Abstract:In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.",
                        "Citation Paper Authors": "Authors:Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". In the realm of neural networks, HiNTdemonstrated that aggregating representations of passage level rel-\nevance can perform well in the context of pre-BERT models ",
                    "Citation Text": "Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Chengxiang Zhai, and Xueqi\nCheng. 2018. Modeling Diverse Relevance Patterns in Ad-hoc Retrieval. In SIGIR .\nACM, 375\u2013384.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.05737",
                        "Citation Paper Title": "Title:Modeling Diverse Relevance Patterns in Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:Assessing relevance between a query and a document is challenging in ad-hoc retrieval due to its diverse patterns, i.e., a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' need. Such diverse relevance patterns require an ideal retrieval model to be able to assess relevance in the right granularity adaptively. Unfortunately, most existing retrieval models compute relevance at a single granularity, either document-wide or passage-level, or use fixed combination strategy, restricting their ability in capturing diverse relevance patterns. In this work, we propose a data-driven method to allow relevance signals at different granularities to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals by modeling the semantic matching between a query and each passage of a document. The global decision layer accumulates local signals into different granularities and allows them to compete with each other to decide the final relevance score. Experimental results demonstrate that our HiNT model outperforms existing state-of-the-art retrieval models significantly on benchmark ad-hoc retrieval datasets.",
                        "Citation Paper Authors": "Authors:Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Chengxiang Zhai, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", or reducing the\nnumber of Transformer layers [29, 32, 54].\nSeveral works have recently investigated approaches for improv-\ning the Transformer\u2019s efficiency by reducing the computational com-\nplexity of its attention module, e.g., Sparse Transformer ",
                    "Citation Text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\nLong Sequences with Sparse Transformers. CoRR abs/1904.10509 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10509",
                        "Citation Paper Title": "Title:Generating Long Sequences with Sparse Transformers",
                        "Citation Paper Abstract": "Abstract:Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
                        "Citation Paper Authors": "Authors:Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05768v1": {
            "Paper Title": "Linguistically Informed Masking for Representation Learning in the\n  Patent Domain",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "is designed to learn bidirectional\nrepresentations for language and is jointly pre-trained on the tasks\nof masked language modelling (MLM) and next sentence prediction\n(NSP), with two different additional layers based on the output of\nits transformer network ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems 30 , I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.).\nCurran Associates, Inc., 5998\u20136008. http://papers.nips.cc/paper/7181-attention-\nis-all-you-need.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Representation learning. Learning general word representations\ncontinues to be an active research area, from word-level represen-\ntations [ 29,34] up to pretrained language models [ 7,14,35,37]. In\nparticular the BERT language model ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,\n4171\u20134186. https://doi.org/10.18653/v1/N19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05633v1": {
            "Paper Title": "Citation Recommendation for Research Papers via Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05630v1": {
            "Paper Title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01300v2": {
            "Paper Title": "PP-Rec: News Recommendation with Personalized User Interest and\n  Time-aware News Popularity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05147v1": {
            "Paper Title": "Helping results assessment by adding explainable elements to the deep\n  relevance matching model",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ",\nbut our implementation is not identical to the original. Other papers have reported\ndifferent results as well ",
                    "Citation Text": "Ryan McDonald, Georgios-Ioannis Brokos, and Ion Androutsopoulos. 2018. Deep\nrelevance ranking using enhanced document-query interactions. arXiv preprint\narXiv:1809.01682 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01682",
                        "Citation Paper Title": "Title:Deep Relevance Ranking Using Enhanced Document-Query Interactions",
                        "Citation Paper Abstract": "Abstract:We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRR's (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.",
                        "Citation Paper Authors": "Authors:Ryan McDonald, Georgios-Ioannis Brokos, Ion Androutsopoulos"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "developed an explainable search engine designed to aid the\nusers in the search task. They used a modified version of LIME ",
                    "Citation Text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic\ninterpretability of machine learning. arXiv preprint arXiv:1606.05386 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05386",
                        "Citation Paper Title": "Title:Model-Agnostic Interpretability of Machine Learning",
                        "Citation Paper Abstract": "Abstract:Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05144v1": {
            "Paper Title": "Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04873v1": {
            "Paper Title": "AutoFT: Automatic Fine-Tune for Parameters Transfer Learning in\n  Click-Through Rate Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14210v2": {
            "Paper Title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index\n  Sizes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04408v1": {
            "Paper Title": "HieRec: Hierarchical User Interest Modeling for Personalized News\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04441v3": {
            "Paper Title": "Addressing Fairness in Classification with a Model-Agnostic\n  Multi-Objective Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07096v2": {
            "Paper Title": "A Large-Scale Analysis of Mixed Initiative in Information-Seeking\n  Dialogues for Conversational Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04210v1": {
            "Paper Title": "Defining definition: a Text mining Approach to Define Innovative\n  Technological Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00314v2": {
            "Paper Title": "Dual Graph enhanced Embedding Neural Network for CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". GIN utilizes user behaviors to con-\nstruct a co-occurrence commodity graph to mine user intention ",
                    "Citation Text": "Feng Li, Zhenrui Chen, Pengjie Wang, Yi Ren, Di Zhang, and Xiaoyu Zhu. 2019.\nGraph Intention Network for Click-through Rate Prediction in Sponsored Search.\nInSIGIR . 961\u2013964.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.16164",
                        "Citation Paper Title": "Title:Graph Intention Network for Click-through Rate Prediction in Sponsored Search",
                        "Citation Paper Abstract": "Abstract:Estimating click-through rate (CTR) accurately has an essential impact on improving user experience and revenue in sponsored search. For CTR prediction model, it is necessary to make out user real-time search intention. Most of the current work is to mine their intentions based on user real-time behaviors. However, it is difficult to capture the intention when user behaviors are sparse, causing the behavior sparsity problem. Moreover, it is difficult for user to jump out of their specific historical behaviors for possible interest exploration, namely weak generalization problem. We propose a new approach Graph Intention Network (GIN) based on co-occurrence commodity graph to mine user intention. By adopting multi-layered graph diffusion, GIN enriches user behaviors to solve the behavior sparsity problem. By introducing co-occurrence relationship of commodities to explore the potential preferences, the weak generalization problem is also alleviated. To the best of our knowledge, the GIN method is the first to introduce graph learning for user intention mining in CTR prediction and propose end-to-end joint training of graph learning and CTR prediction tasks in sponsored search. At present, GIN has achieved excellent offline results on the real-world data of the e-commerce platform outperforming existing deep learning models, and has been running stable tests online and achieved significant CTR improvements.",
                        "Citation Paper Authors": "Authors:Feng Li, Zhenrui Chen, Pengjie Wang, Yi Ren, Di Zhang, Xiaoyu Zhu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". So it employs self-attention layer and Bi-LSTM\nto model user\u2019s inter-session and intra-session interests. MIND\nlearns multiple vectors for representing user\u2019s interests by using\ncapsule network and dynamic routing architecture ",
                    "Citation Text": "Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,\nGuoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest\nnetwork with dynamic routing for recommendation at Tmall. In CIKM . 2615\u2013\n2623.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08030",
                        "Citation Paper Title": "Title:Multi-Interest Network with Dynamic Routing for Recommendation at Tmall",
                        "Citation Paper Abstract": "Abstract:Industrial recommender systems usually consist of the matching stage and the ranking stage, in order to handle the billion-scale of users and items. The matching stage retrieves candidate items relevant to user interests, while the ranking stage sorts candidate items by user interests. Thus, the most critical ability is to model and represent user interests for either stage. Most of the existing deep learning-based models represent one user as a single vector which is insufficient to capture the varying nature of user's interests. In this paper, we approach this problem from a different view, to represent one user with multiple vectors encoding the different aspects of the user's interests. We propose the Multi-Interest Network with Dynamic routing (MIND) for dealing with user's diverse interests in the matching stage. Specifically, we design a multi-interest extractor layer based on capsule routing mechanism, which is applicable for clustering historical behaviors and extracting diverse interests. Furthermore, we develop a technique named label-aware attention to help learn a user representation with multiple vectors. Through extensive experiments on several public benchmarks and one large-scale industrial dataset from Tmall, we demonstrate that MIND can achieve superior performance than state-of-the-art methods for recommendation. Currently, MIND has been deployed for handling major online traffic at the homepage on Mobile Tmall App.",
                        "Citation Paper Authors": "Authors:Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao, Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". DSIN argues\nthat user behavior sequence are composed of different homoge-\nneous sessions ",
                    "Citation Text": "Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping\nYang. 2019. Deep session interest network for click-through rate prediction.\narXiv preprint arXiv:1905.06482 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06482",
                        "Citation Paper Title": "Title:Deep Session Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-Through Rate (CTR) prediction plays an important role in many industrial applications, such as online advertising and recommender systems. How to capture users' dynamic and evolving interests from their behavior sequences remains a continuous research topic in the CTR prediction. However, most existing studies overlook the intrinsic structure of the sequences: the sequences are composed of sessions, where sessions are user behaviors separated by their occurring time. We observe that user behaviors are highly homogeneous in each session, and heterogeneous cross sessions. Based on this observation, we propose a novel CTR model named Deep Session Interest Network (DSIN) that leverages users' multiple historical sessions in their behavior sequences. We first use self-attention mechanism with bias encoding to extract users' interests in each session. Then we apply Bi-LSTM to model how users' interests evolve and interact among sessions. Finally, we employ the local activation unit to adaptively learn the influences of various session interests on the target item. Experiments are conducted on both advertising and production recommender datasets and DSIN outperforms other state-of-the-art models on both datasets.",
                        "Citation Paper Authors": "Authors:Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, Keping Yang"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "introduce an\ninteresting hybrid architecture, which contain a shallow model and\na DNN model to learn low-order and high-order feature interac-\ntions simultaneously. Deep & Cross Network (DCN) ",
                    "Citation Text": "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network\nfor ad click predictions. In Proceedings of the ADKDD\u201917 . ACM, 12.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "enhances FM with DNN to model non-linear and high-order fea-\nture interactions simultaneously. PNN further introduces a product\nlayer between the embedding layer and DNN to model the featureinteractions ",
                    "Citation Text": "Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng\nGuo, Yong Yu, and Xiuqiang He. 2018. Product-based Neural Networks for\nUser Response Prediction over Multi-field Categorical Data. arXiv preprint\narXiv:1807.00311 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00311",
                        "Citation Paper Title": "Title:Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data",
                        "Citation Paper Abstract": "Abstract:User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this paper, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network (PNN) which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network In Network (PIN) which can generalize previous models. Extensive experiments on 4 industrial datasets and 1 contest dataset demonstrate that our models consistently outperform 8 baselines on both AUC and log loss. Besides, PIN makes great CTR improvement (relatively 34.67%) in online A/B test.",
                        "Citation Paper Authors": "Authors:Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.14616v1": {
            "Paper Title": "ICDAR 2021 Competition on Scientific Literature Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04128v1": {
            "Paper Title": "Conversational Fashion Image Retrieval via Multiturn Natural Language\n  Feedback",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ". Some methods\nimprove the performance by adding more features such as text-\nonly, image-only, attribute-only features, etc [ 22,34]. Notably, ",
                    "Citation Text": "Youngjae Yu, Seunghwan Lee, Yuncheol Choi, and Gunhee Kim. 2020. CurlingNet:\nCompositional Learning Between Images and Text for Fashion IQ Data.\narXiv:2003.12299 [cs.CV]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12299",
                        "Citation Paper Title": "Title:CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data",
                        "Citation Paper Abstract": "Abstract:We present an approach named CurlingNet that can measure the semantic distance of composition of image-text embedding. In order to learn an effective image-text composition for the data in the fashion domain, our model proposes two key components as follows. First, the Delivery makes the transition of a source image in an embedding space. Second, the Sweeping emphasizes query-related components of fashion images in the embedding space. We utilize a channel-wise gating mechanism to make it possible. Our single model outperforms previous state-of-the-art image-text composition models including TIRG and FiLM. We participate in the first fashion-IQ challenge in ICCV 2019, for which ensemble of our model achieves one of the best performances.",
                        "Citation Paper Authors": "Authors:Youngjae Yu, Seunghwan Lee, Yuncheol Choi, Gunhee Kim"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", using a composition clas-\nsifier [ 3,41] or through residual connection ",
                    "Citation Text": "Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays.\n2019. Composing Text and Image for Image Retrieval-an Empirical Odyssey. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n6439\u20136448.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07119",
                        "Citation Paper Title": "Title:Composing Text and Image for Image Retrieval - An Empirical Odyssey",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the task of image retrieval, where the input query is specified in the form of an image plus some text that describes desired modifications to the input image. For example, we may present an image of the Eiffel tower, and ask the system to find images which are visually similar but are modified in small ways, such as being taken at nighttime instead of during the day. To tackle this task, we learn a similarity metric between a target image and a source image plus source text, an embedding and composing function such that target image feature is close to the source image plus text composition feature. We propose a new way to combine image and text using such function that is designed for the retrieval task. We show this outperforms existing approaches on 3 different datasets, namely Fashion-200k, MIT-States and a new synthetic dataset we create based on CLEVR. We also show that our approach can be used to classify input queries, in addition to image retrieval.",
                        "Citation Paper Authors": "Authors:Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". Some seek to fuse the\nimage & text features producing a more precise representation of\nthe image-text pair ranging from simple techniques (e.g. concatena-\ntion, simple feed-forward networks) to advanced techniques such\nas conducting parameter hashing ",
                    "Citation Text": "Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2016. Image Ques-\ntion Answering Using Convolutional Neural Network With Dynamic Parameter\nPrediction. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition . 30\u201338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05756",
                        "Citation Paper Title": "Title:Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction",
                        "Citation Paper Abstract": "Abstract:We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.",
                        "Citation Paper Authors": "Authors:Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.01112v2": {
            "Paper Title": "NaturalProofs: Mathematical Theorem Proving in Natural Language",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "evaluate neural sequence models on symbolic\nintegration problems, while Hendrycks et al. ",
                    "Citation Text": "D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the math dataset, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03874",
                        "Citation Paper Title": "Title:Measuring Mathematical Problem Solving With the MATH Dataset",
                        "Citation Paper Abstract": "Abstract:Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
                        "Citation Paper Authors": "Authors:Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "developed the HOList benchmark based on the HOL Light theorem prover, while\nother benchmark tasks use the Coq [ 18,47], Metamath [ 43,41,29], or Isabelle ",
                    "Citation Text": "W. Li, L. Yu, Y . Wu, and L. C. Paulson. Isarstep: a benchmark for high-level mathematical\nreasoning. In International Conference onLearning Representations , 2021. URL https:\n//openreview.net/forum?id=Pzj6fzU6wkj .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09265",
                        "Citation Paper Title": "Title:IsarStep: a Benchmark for High-level Mathematical Reasoning",
                        "Citation Paper Abstract": "Abstract:A well-defined benchmark is essential for measuring and accelerating research progress of machine learning models. In this paper, we present a benchmark for high-level mathematical reasoning and study the reasoning capabilities of neural sequence-to-sequence models. We build a non-synthetic dataset from the largest repository of proofs written by human experts in a theorem prover. The dataset has a broad coverage of undergraduate and research-level mathematical and computer science theorems. In our defined task, a model is required to fill in a missing intermediate proposition given surrounding proofs. This task provides a starting point for the long-term goal of having machines generate human-readable proofs automatically. Our experiments and analysis reveal that while the task is challenging, neural models can capture non-trivial mathematical reasoning. We further design a hierarchical transformer that outperforms the transformer baseline.",
                        "Citation Paper Authors": "Authors:Wenda Li, Lei Yu, Yuhuai Wu, Lawrence C. Paulson"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "used machine learning for selecting relevant premises in the Mizar mathematical library that\nare passed to an automated theorem prover, which was later explored with deep neural networks ",
                    "Citation Text": "A. A. Alemi, F. Chollet, N. Een, G. Irving, C. Szegedy, and J. Urban. DeepMath - Deep sequence\nmodels for premise selection. In Advances inNeural Information Processing Systems , pages\n2243\u20132251, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.04442",
                        "Citation Paper Title": "Title:DeepMath - Deep Sequence Models for Premise Selection",
                        "Citation Paper Abstract": "Abstract:We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.",
                        "Citation Paper Authors": "Authors:Alex A. Alemi, Francois Chollet, Niklas Een, Geoffrey Irving, Christian Szegedy, Josef Urban"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05996v2": {
            "Paper Title": "Fairness Through Regularization for Learning to Rank",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "for a detailed introduction. In this work we show how to extend three popular group\nfairness notions \u2013 demographic parity, equalized odds and equality of opportunity ",
                    "Citation Text": "M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In Conference\non Neural Information Processing Systems (NeurIPS) , 2016.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02413",
                        "Citation Paper Title": "Title:Equality of Opportunity in Supervised Learning",
                        "Citation Paper Abstract": "Abstract:We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.\nIn line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.\nWe illustrate our notion using a case study of FICO credit scores.",
                        "Citation Paper Authors": "Authors:Moritz Hardt, Eric Price, Nathan Srebro"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "\u2013 to the\nranking setting. In principle, our formalism is applicable to other group fairness notions, as well as\nindividual ",
                    "Citation Text": "C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In\nInnovations in Theoretical Computer Science Conference (ITCS) , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1104.3913",
                        "Citation Paper Title": "Title:Fairness Through Awareness",
                        "Citation Paper Abstract": "Abstract:We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.",
                        "Citation Paper Authors": "Authors:Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Rich Zemel"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". In contrast, our notion of ranking fairness is amortized across queries, similarly to ",
                    "Citation Text": "A. J. Biega, K. P. Gummadi, and G. Weikum. Equity of attention: Amortizing individual\nfairness in rankings. In International Conference on Research and Development in Information\nRetrieval (SIGIR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01788",
                        "Citation Paper Title": "Title:Equity of Attention: Amortizing Individual Fairness in Rankings",
                        "Citation Paper Abstract": "Abstract:Rankings of people and items are at the heart of selection-making, match-making, and recommender systems, ranging from employment sites to sharing economy platforms. As ranking positions influence the amount of attention the ranked subjects receive, biases in rankings can lead to unfair distribution of opportunities and resources, such as jobs or income.\nThis paper proposes new measures and mechanisms to quantify and mitigate unfairness from a bias inherent to all rankings, namely, the position bias, which leads to disproportionately less attention being paid to low-ranked subjects. Our approach differs from recent fair ranking approaches in two important ways. First, existing works measure unfairness at the level of subject groups while our measures capture unfairness at the level of individual subjects, and as such subsume group unfairness. Second, as no single ranking can achieve individual attention fairness, we propose a novel mechanism that achieves amortized fairness, where attention accumulated across a series of rankings is proportional to accumulated relevance.\nWe formulate the challenge of achieving amortized individual fairness subject to constraints on ranking quality as an online optimization problem and show that it can be solved as an integer linear program. Our experimental evaluation reveals that unfair attention distribution in rankings can be substantial, and demonstrates that our method can improve individual fairness while retaining high ranking quality.",
                        "Citation Paper Authors": "Authors:Asia J. Biega, Krishna P. Gummadi, Gerhard Weikum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03819v1": {
            "Paper Title": "A Semi-Personalized System for User Cold Start Recommendation on Music\n  Streaming Apps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03415v1": {
            "Paper Title": "Leveraging Tripartite Interaction Information from Live Stream\n  E-Commerce for Improving Product Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "proposed a dual graph attention network to collabora-\ntively learn user/item representations via a user-specific attention\nweight and a context-aware attention weight for a more accurate so-\ncial recommendation. Fan et al. ",
                    "Citation Text": "Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.\n2019. Graph neural networks for social recommendation. In WWW \u201919 . 417\u2013426.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07243",
                        "Citation Paper Title": "Title:Graph Neural Networks for Social Recommendation",
                        "Citation Paper Abstract": "Abstract:In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec. Our code is available at \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, Dawei Yin"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "proposed the KGAT\nmethod, which can capture the high-order relations from the hybrid\nstructure of KG and user-item graph via attention mechanism. Wu\net al. ",
                    "Citation Text": "Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, PaulWeng, Han Gao, and\nGuihai Chen. 2019. Dual graph attention networks for deep latent representation\nof multifaceted social effects in recommender systems. In WWW \u201919 . 2091\u20132102.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10433",
                        "Citation Paper Title": "Title:Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Social recommendation leverages social information to solve data sparsity and cold-start problems in traditional collaborative filtering methods. However, most existing models assume that social effects from friend users are static and under the forms of constant weights or fixed constraints. To relax this strong assumption, in this paper, we propose dual graph attention networks to collaboratively learn representations for two-fold social effects, where one is modeled by a user-specific attention weight and the other is modeled by a dynamic and context-aware attention weight. We also extend the social effects in user domain to item domain, so that information from related items can be leveraged to further alleviate the data sparsity problem. Furthermore, considering that different social effects in two domains could interact with each other and jointly influence user preferences for items, we propose a new policy-based fusion strategy based on contextual multi-armed bandit to weigh interactions of various social effects. Experiments on one benchmark dataset and a commercial dataset verify the efficacy of the key components in our model. The results show that our model achieves great improvement for recommendation accuracy compared with other state-of-the-art social recommendation methods.",
                        "Citation Paper Authors": "Authors:Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, Guihai Chen"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "proposed the\nknowledge-aware graph neural network with label smoothness\nregularization, which first applies a trainable function to identify\nimportant KG relationships and then applies a GNN to compute\npersonalized item embeddings. Wang et al. ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:\nKnowledge graph attention network for recommendation. In KDD \u201919 . 950\u2013958.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", etc. User activities in online live streaming platforms\nhave been extensively studied including motivation ",
                    "Citation Text": "Zhicong Lu, Haijun Xia, Seongkook Heo, and Daniel Wigdor. 2018. You Watch,\nYou Give, and You Engage: A Study of Live Streaming Practices in China. In CHI\n\u201918.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06032",
                        "Citation Paper Title": "Title:You Watch, You Give, and You Engage: A Study of Live Streaming Practices in China",
                        "Citation Paper Abstract": "Abstract:Despite gaining traction in North America, live streaming has not reached the popularity it has in China, where livestreaming has a tremendous impact on the social behaviors of users. To better understand this socio-technological phenomenon, we conducted a mixed methods study of live streaming practices in China. We present the results of an online survey of 527 live streaming users, focusing on their broadcasting or viewing practices and the experiences they find most engaging. We also interviewed 14 active users to explore their motivations and experiences. Our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers. We also gained insight into the role reward systems and fan group-chat play in engaging users, while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting, gifting, and fan groups that are available today.",
                        "Citation Paper Authors": "Authors:Zhicong Lu, Haijun Xia, Seongkook Heo, Daniel Wigdor"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03399v1": {
            "Paper Title": "Scientific Dataset Discovery via Topic-level Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10784v2": {
            "Paper Title": "Learning to Embed Categorical Features without Embedding Tables for\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02507v2": {
            "Paper Title": "Coarse-to-Fine Entity Representations for Document-level Relation\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03060v1": {
            "Paper Title": "Big-Five, MPTI, Eysenck or HEXACO: The Ideal Personality Model for\n  Personality-aware Recommendation Systems",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "surveyed the recent \nautomatic personality recognition schemes and presented the trends of each recognition technique.  Dhelim \net al . ",
                    "Citation Text": "S. Dhelim, N. Aung, M. A. Bouras, H. Ning, and E. Cambria, \u201cA Survey on Personality -Aware \nRecommendation Systems,\u201d Jan. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.12153",
                        "Citation Paper Title": "Title:A Survey on Personality-Aware Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:With the emergence of personality computing as a new research field related to artificial intelligence and personality psychology, we have witnessed an unprecedented proliferation of personality-aware recommendation systems. Unlike conventional recommendation systems, these new systems solve traditional problems such as the cold start and data sparsity problems. This survey aims to study and systematically classify personality-aware recommendation systems. To the best of our knowledge, this survey is the first that focuses on personality-aware recommendation systems. We explore the different design choices of personality-aware recommendation systems, by comparing their personality modeling methods, as well as their recommendation techniques. Furthermore, we present the commonly used datasets and point out some of the challenges of personality-aware recommendation systems.",
                        "Citation Paper Authors": "Authors:Sahraoui Dhelim, Nyothiri Aung, Mohammed Amine Bouras, Huansheng Ning, Erik Cambria"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03042v1": {
            "Paper Title": "Clone-Seeker: Effective Code Clone Search Using Annotations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02831v1": {
            "Paper Title": "A novel method for recommendation systems using invasive weed\n  optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02771v1": {
            "Paper Title": "PURS: Personalized Unexpected Recommender System for Improving User\n  Satisfaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02768v1": {
            "Paper Title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11601v2": {
            "Paper Title": "Personalized Transformer for Explainable Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12964v9": {
            "Paper Title": "Contrastive Learning for Debiased Candidate Generation in Large-Scale\n  Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ".\nContrastive Learning. Contrastive learning, which aims to learn\nhigh-quality representations via self-supervised pretext tasks, re-\ncently achieves remarkable success in various domains, e.g., speech\nprocessing ",
                    "Citation Text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", even though rich features for\nthe negative samples are demonstrated to be beneficial ",
                    "Citation Text": "Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li, and Jun\nGao. 2019. Personalized Bundle List Recommendation. In WWW . 60\u201371.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01933",
                        "Citation Paper Title": "Title:Personalized Bundle List Recommendation",
                        "Citation Paper Abstract": "Abstract:Product bundling, offering a combination of items to customers, is one of the marketing strategies commonly used in online e-commerce and offline retailers. A high-quality bundle generalizes frequent items of interest, and diversity across bundles boosts the user-experience and eventually increases transaction volume. In this paper, we formalize the personalized bundle list recommendation as a structured prediction problem and propose a bundle generation network (BGN), which decomposes the problem into quality/diversity parts by the determinantal point processes (DPPs). BGN uses a typical encoder-decoder framework with a proposed feature-aware softmax to alleviate the inadequate representation of traditional softmax, and integrates the masked beam search and DPP selection to produce high-quality and diversified bundle list with an appropriate bundle size. We conduct extensive experiments on three public datasets and one industrial dataset, including two generated from co-purchase records and the other two extracted from real-world online bundle services. BGN significantly outperforms the state-of-the-art methods in terms of quality, diversity and response time over all datasets. In particular, BGN improves the precision of the best competitors by 16\\% on average while maintaining the highest diversity on four datasets, and yields a 3.85x improvement of response time over the best competitors in the bundle list recommendation problem.",
                        "Citation Paper Authors": "Authors:Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li, Jun Gao"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". The existing methods\nexplicitly sample negative examples from a pre-defined proposal dis-\ntribution [ 6,26,38]. The proposal distribution not only affects con-\nvergence, but also has a significant impact on the performance ",
                    "Citation Text": "Hugo Caselles-Dupr\u00e9, Florian Lesaint, and Jimena Royo-Letelier. 2018. Word2vec\napplied to recommendation: Hyperparameters matter. In RecSys . ACM, 352\u2013356.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04212",
                        "Citation Paper Title": "Title:Word2Vec applied to Recommendation: Hyperparameters Matter",
                        "Citation Paper Abstract": "Abstract:Skip-gram with negative sampling, a popular variant of Word2vec originally designed and tuned to create word embeddings for Natural Language Processing, has been used to create item embeddings with successful applications in recommendation. While these fields do not share the same type of data, neither evaluate on the same tasks, recommendation applications tend to use the same already tuned hyperparameters values, even if optimal hyperparameters values are often known to be data and task dependent. We thus investigate the marginal importance of each hyperparameter in a recommendation setting through large hyperparameter grid searches on various datasets. Results reveal that optimizing neglected hyperparameters, namely negative sampling distribution, number of epochs, subsampling parameter and window-size, significantly improves performance on a recommendation task, and can increase it by an order of magnitude. Importantly, we find that optimal hyperparameters configurations for Natural Language Processing tasks and Recommendation tasks are noticeably different.",
                        "Citation Paper Authors": "Authors:Hugo Caselles-Dupr\u00e9, Florian Lesaint, Jimena Royo-Letelier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07363v1": {
            "Paper Title": "Cognitive-aware Short-text Understanding for Inferring Professions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02400v1": {
            "Paper Title": "A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "applying attention and transformer compartment to exploit the rela-\ntionship between objects in images and words in texts within themselves.\n\u2022GSMN ",
                    "Citation Text": "Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, and Yongdong Zhang. Graph\nstructured network for image-text matching. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10921\u201310930, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00277",
                        "Citation Paper Title": "Title:Graph Structured Network for Image-Text Matching",
                        "Citation Paper Abstract": "Abstract:Image-text matching has received growing interest since it bridges vision and language. The key challenge lies in how to learn correspondence between image and text. Existing works learn coarse correspondence based on object co-occurrence statistics, while failing to learn fine-grained phrase correspondence. In this paper, we present a novel Graph Structured Matching Network (GSMN) to learn fine-grained correspondence. The GSMN explicitly models object, relation and attribute as a structured phrase, which not only allows to learn correspondence of object, relation and attribute separately, but also benefits to learn fine-grained correspondence of structured phrase. This is achieved by node-level matching and structure-level matching. The node-level matching associates each node with its relevant nodes from another modality, where the node can be object, relation or attribute. The associated nodes then jointly infer fine-grained correspondence by fusing neighborhood associations at structure-level matching. Comprehensive experiments show that GSMN outperforms state-of-the-art methods on benchmarks, with relative Recall@1 improvements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code will be released at: this https URL.",
                        "Citation Paper Authors": "Authors:Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, Yongdong Zhang"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "focused on the local information in images where semantic concepts and\ntheir ordering were extracted by learning with their corresponding sentences. The work\nfrom Wang ",
                    "Citation Text": "Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, and Xin Fan. Position focused\nattention network for image-text matching. arXiv preprint arXiv:1907.09748 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.09748",
                        "Citation Paper Title": "Title:Position Focused Attention Network for Image-Text Matching",
                        "Citation Paper Abstract": "Abstract:Image-text matching tasks have recently attracted a lot of attention in the computer vision field. The key point of this cross-domain problem is how to accurately measure the similarity between the visual and the textual contents, which demands a fine understanding of both modalities. In this paper, we propose a novel position focused attention network (PFAN) to investigate the relation between the visual and the textual views. In this work, we integrate the object position clue to enhance the visual-text joint-embedding learning. We first split the images into blocks, by which we infer the relative position of region in the image. Then, an attention mechanism is proposed to model the relations between the image region and blocks and generate the valuable position feature, which will be further utilized to enhance the region expression and model a more reliable relationship between the visual image and the textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO show the effectiveness of the proposed method. Besides the public datasets, we also conduct experiments on our collected practical large-scale news dataset (Tencent-News) to validate the practical application value of proposed method. As far as we know, this is the first attempt to test the performance on the practical application. Our method achieves the state-of-art performance on all of these three datasets.",
                        "Citation Paper Authors": "Authors:Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, Xin Fan"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", which is one of the most\npopular datasets in this image-text matching \ufb01eld due to its high quality of textual anno-\ntation compared to others ",
                    "Citation Text": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\nmodels. In Proceedings of the IEEE international conference on computer vision , pages 2641\u20132649,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04870",
                        "Citation Paper Title": "Title:Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
                        "Citation Paper Abstract": "Abstract:The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.",
                        "Citation Paper Authors": "Authors:Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "presented a novel structure that iteratively \ufb01nds matching compartments between images\nand sentences and re\ufb01ned them progressively. Meanwhile, the multi-modality cross at-\ntention network employs the compelling Transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "created\na scene concept graph based on a popular scene graph dataset ",
                    "Citation Text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal of computer vision , 123(1):32\u201373,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.07332",
                        "Citation Paper Title": "Title:Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                        "Citation Paper Abstract": "Abstract:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "raised the awareness of the relative position of detected objects within\nan image which could be useful when matching with the caption. Meanwhile, Lee et al. ",
                    "Citation Text": "Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for\nimage-text matching. In Proceedings of the European Conference on Computer Vision (ECCV) , pages\n201\u2013216, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08024",
                        "Citation Paper Title": "Title:Stacked Cross Attention for Image-Text Matching",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: this https URL.",
                        "Citation Paper Authors": "Authors:Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "by entirely embedding each\nimage and sentence into a vector form using two CNN modules separately. In contrast,\nHuang et al. ",
                    "Citation Text": "Yan Huang, Qi Wu, Chunfeng Song, and Liang Wang. Learning semantic concepts and order for im-\nage and sentence matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 6163\u20136171, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02036",
                        "Citation Paper Title": "Title:Learning Semantic Concepts and Order for Image and Sentence Matching",
                        "Citation Paper Abstract": "Abstract:Image and sentence matching has made great progress recently, but it remains challenging due to the large visual-semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.",
                        "Citation Paper Authors": "Authors:Yan Huang, Qi Wu, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "used a Bag-of-Word technique to ex-\ntract the information from semantic data. However, one could also use a CNN network\nto process sentences in a dual-path convolutional model ",
                    "Citation Text": "Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong Shen. Dual-path\nconvolutional image-text embeddings with instance loss. ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM) , 16(2):1\u201323, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05535",
                        "Citation Paper Title": "Title:Dual-Path Convolutional Image-Text Embeddings with Instance Loss",
                        "Citation Paper Abstract": "Abstract:Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, Yi-Dong Shen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07359v1": {
            "Paper Title": "MexPub: Deep Transfer Learning for Metadata Extraction from German\n  Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02361v1": {
            "Paper Title": "Facade-X: an opinionated approach to SPARQL anything",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02293v1": {
            "Paper Title": "Cross-language Sentence Selection via Data Augmentation and Rationale\n  Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02256v1": {
            "Paper Title": "Using Social Media Background to Improve Cold-start Recommendation Deep\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02250v1": {
            "Paper Title": "A General Method for Event Detection on Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02223v1": {
            "Paper Title": "Learning Elastic Embeddings for Customizing On-Device Recommenders",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ", tensor-train decomposition is adopted to\ncompress embeddings, which can also be interpreted as a specific\nquantization operation applied to each element of the full embed-\nding table ",
                    "Citation Text": "Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020.\nCompositional embeddings using complementary partitions for memory-efficient\nrecommendation systems. In SIGKDD . 165\u2013175.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02107",
                        "Citation Paper Title": "Title:Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller memory cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.",
                        "Citation Paper Authors": "Authors:Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, Jiyan Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.01941v1": {
            "Paper Title": "Optimizing Rankings for Recommendation in Matching Markets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01706v1": {
            "Paper Title": "EmoDNN: Understanding emotions from short texts through a deep neural\n  network ensemble",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01674v1": {
            "Paper Title": "JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale\n  Online Inference at Baidu",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00874v2": {
            "Paper Title": "Conversational Question Answering: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04404v1": {
            "Paper Title": "The Struggle with Academic Plagiarism: Approaches based on Semantic\n  Similarity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.10083v3": {
            "Paper Title": "Personalized News Recommendation with Knowledge-aware Interactive\n  Matching",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "adopted a multi-head self-attention\nnetwork to model candidate news from its title and another multi-\nhead self-attention network to model user interest from user\u2019s click\nhistory. Liu et al . ",
                    "Citation Text": "Danyang Liu, Jianxun Lian, Shiyin Wang, Ying Qiao, Jiun-Hung Chen,\nGuangzhong Sun, and Xing Xie. 2020. KRED: Knowledge-aware document\nrepresentation for news recommendations. In RecSys. 200\u2013209.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11494",
                        "Citation Paper Title": "Title:KRED: Knowledge-Aware Document Representation for News Recommendations",
                        "Citation Paper Abstract": "Abstract:News articles usually contain knowledge entities such as celebrities or organizations. Important entities in articles carry key messages and help to understand the content in a more direct way. An industrial news recommender system contains various key applications, such as personalized recommendation, item-to-item recommendation, news category classification, news popularity prediction and local news detection. We find that incorporating knowledge entities for better document understanding benefits these applications consistently. However, existing document understanding models either represent news articles without considering knowledge entities (e.g., BERT) or rely on a specific type of text encoding model (e.g., DKN) so that the generalization ability and efficiency is compromised. In this paper, we propose KRED, which is a fast and effective model to enhance arbitrary document representation with a knowledge graph. KRED first enriches entities' embeddings by attentively aggregating information from their neighborhood in the knowledge graph. Then a context embedding layer is applied to annotate the dynamic context of different entities such as frequency, category and position. Finally, an information distillation layer aggregates the entity embeddings under the guidance of the original document representation and transforms the document vector into a new one. We advocate to optimize the model with a multi-task framework, so that different news recommendation applications can be united and useful information can be shared across different tasks. Experiments on a real-world Microsoft News dataset demonstrate that KRED greatly benefits a variety of news recommendation applications.",
                        "Citation Paper Authors": "Authors:Danyang Liu, Jianxun Lian, Shiyin Wang, Ying Qiao, Jiun-Hung Chen, Guangzhong Sun, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ": learning news representations from news\ntitles, bodies, categories, and sub-categories via multiple attentive\nCNN networks. (5) NPA ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and\nXing Xie. 2019. Npa: Neural news recommendation with personalized attention.\nInKDD . 2576\u20132584.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05559",
                        "Citation Paper Title": "Title:NPA: Neural News Recommendation with Personalized Attention",
                        "Citation Paper Abstract": "Abstract:News recommendation is very important to help users find interested news and alleviate information overload. Different users usually have different interests and the same user may have various interests. Thus, different users may click the same news article with attention on different aspects. In this paper, we propose a neural news recommendation model with personalized attention (NPA). The core of our approach is a news representation model and a user representation model. In the news representation model we use a CNN network to learn hidden representations of news articles based on their titles. In the user representation model we learn the representations of users based on the representations of their clicked news articles. Since different words and different news articles may have different informativeness for representing news and users, we propose to apply both word- and news-level attention mechanism to help our model attend to important words and news articles. In addition, the same news article and the same word may have different informativeness for different users. Thus, we propose a personalized attention network which exploits the embedding of user ID to generate the query vector for the word- and news-level attentions. Extensive experiments are conducted on a real-world news recommendation dataset collected from MSN news, and the results validate the effectiveness of our approach on news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "to train KIM with 5\u00d710\u22125learning rate. All\nhyper-parameters of KIM and other baseline methods were selected\nbased on the validation dataset. Following previous works ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\nand Xing Xie. 2019. Neural news recommendation with attentive multi-view\nlearning. In IJCAI . 3863\u20133869.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.14862v2": {
            "Paper Title": "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01251v1": {
            "Paper Title": "Multilingual Medical Question Answering and Information Retrieval for\n  Rural Health Intelligence Access",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01232v1": {
            "Paper Title": "A weighted unified informetrics based on Scopus and WoS",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06963v2": {
            "Paper Title": "Needle in a Haystack: Label-Efficient Evaluation under Extreme Class\n  Imbalance",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". Recent approaches [ 22,36] are based on a statistical formulation similar to ours,\nhowever they are specialized to IR systems. Within the IR community, stratified sampling and cluster sampling have\nalso been used to efficiently evaluate knowledge graphs ",
                    "Citation Text": "Junyang Gao, Xian Li, Yifan Ethan Xu, Bunyamin Sisman, Xin Luna Dong, and Jun Yang. 2019. Efficient Knowledge Graph Accuracy Evaluation.\nProc. VLDB Endow. 12, 11 (July 2019), 1679\u20131691. https://doi.org/10.14778/3342263.3342642",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.09657",
                        "Citation Paper Title": "Title:Efficient Knowledge Graph Accuracy Evaluation",
                        "Citation Paper Abstract": "Abstract:Estimation of the accuracy of a large-scale knowledge graph (KG) often requires humans to annotate samples from the graph. How to obtain statistically meaningful estimates for accuracy evaluation while keeping human annotation costs low is a problem critical to the development cycle of a KG and its practical applications. Surprisingly, this challenging problem has largely been ignored in prior research. To address the problem, this paper proposes an efficient sampling and evaluation framework, which aims to provide quality accuracy evaluation with strong statistical guarantee while minimizing human efforts. Motivated by the properties of the annotation cost function observed in practice, we propose the use of cluster sampling to reduce the overall cost. We further apply weighted and two-stage sampling as well as stratification for better sampling designs. We also extend our framework to enable efficient incremental evaluation on evolving KG, introducing two solutions based on stratified sampling and a weighted variant of reservoir sampling. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of our proposed solution. Compared to baseline approaches, our best solutions can provide up to 60% cost reduction on static KG evaluation and up to 80% cost reduction on evolving KG evaluation, without loss of evaluation quality.",
                        "Citation Paper Authors": "Authors:Junyang Gao, Xian Li, Yifan Ethan Xu, Bunyamin Sisman, Xin Luna Dong, Jun Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.00882v1": {
            "Paper Title": "Efficient Passage Retrieval with Hashing for Open-domain Question\n  Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05422v2": {
            "Paper Title": "Exploiting Group-level Behavior Pattern forSession-based Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "model the transitions within the session by adding an overall node into graph and employ highway\nnetwork to avoid overfitting problem. Wang et al. ",
                    "Citation Text": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui Qiu. 2020. Global context enhanced graph\nneural networks for session-based recommendation. In Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 169\u2013178.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05081",
                        "Citation Paper Title": "Title:Global Context Enhanced Graph Neural Networks for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation (SBR) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for SBR model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, GCE-GNN learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a GNN on the session graph to learn session-level item embeddings within the current session. Moreover, GCE-GNN aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods consistently.",
                        "Citation Paper Authors": "Authors:Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "develop an additive model of recency to exploit the temporal dynamic of behavior for\nrepeat consumption. Hu et al. ",
                    "Citation Text": "Haoji Hu, Xiangnan He, Jinyang Gao, and Zhi-Li Zhang. 2020. Modeling personalized item frequency information\nfor next-basket recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 1071\u20131080.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.00556",
                        "Citation Paper Title": "Title:Modeling Personalized Item Frequency Information for Next-basket Recommendation",
                        "Citation Paper Abstract": "Abstract:Next-basket recommendation (NBR) is prevalent in e-commerce and retail industry. In this scenario, a user purchases a set of items (a basket) at a time. NBR performs sequential modeling and recommendation based on a sequence of baskets. NBR is in general more complex than the widely studied sequential (session-based) recommendation which recommends the next item based on a sequence of items. Recurrent neural network (RNN) has proved to be very effective for sequential modeling and thus been adapted for NBR. However, we argue that existing RNNs cannot directly capture item frequency information in the recommendation scenario.\nThrough careful analysis of real-world datasets, we find that {\\em personalized item frequency} (PIF) information (which records the number of times that each item is purchased by a user) provides two critical signals for NBR. But, this has been largely ignored by existing methods. Even though existing methods such as RNN based methods have strong representation ability, our empirical results show that they fail to learn and capture PIF. As a result, existing methods cannot fully exploit the critical signals contained in PIF. Given this inherent limitation of RNNs, we propose a simple item frequency based k-nearest neighbors (kNN) method to directly utilize these critical signals. We evaluate our method on four public real-world datasets. Despite its relative simplicity, our method frequently outperforms the state-of-the-art NBR methods -- including deep learning based methods using RNNs -- when patterns associated with PIF play an important role in the data.",
                        "Citation Paper Authors": "Authors:Haoji Hu, Xiangnan He, Jinyang Gao, Zhi-Li Zhang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "propose\nFGNN that applies multi-head attention to learn each item representation, which is then extended ",
                    "Citation Text": "Ruihong Qiu, Zi Huang, Jingjing Li, and Hongzhi Yin. 2020. Exploiting Cross-session Information for Session-based\nRecommendation with Graph Neural Networks. ACM Transactions on Information Systems (TOIS) 38, 3 (2020), 1\u201323.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.00852",
                        "Citation Paper Title": "Title:Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user's recent interest. The existing work on the session-based recommender system mainly relies on mining sequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this paper, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session's representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.",
                        "Citation Paper Authors": "Authors:Ruihong Qiu, Zi Huang, Jingjing Li, Hongzhi Yin"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "employ Gated GNN to learn the item embedding from\nsession graph and use attentions to integrate each learnt item embedding. Qiu et al. ",
                    "Citation Text": "Ruihong Qiu, Jingjing Li, Zi Huang, and Hongzhi Yin. 2019. Rethinking the item order in session-based recommendation\nwith graph neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge\nManagement . 579\u2013588.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11942",
                        "Citation Paper Title": "Title:Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Predicting a user's preference in a short anonymous interaction session instead of long-term history is a challenging problem in the real-life session-based recommendation, e.g., e-commerce and media stream. Recent research of the session-based recommender system mainly focuses on sequential patterns by utilizing the attention mechanism, which is straightforward for the session's natural sequence sorted by time. However, the user's preference is much more complicated than a solely consecutive time pattern in the transition of item choices. In this paper, therefore, we study the item transition pattern by constructing a session graph and propose a novel model which collaboratively considers the sequence order and the latent order in the session graph for a session-based recommender system. We formulate the next item recommendation within the session as a graph classification problem. Specifically, we propose a weighted attention graph layer and a Readout function to learn embeddings of items and sessions for the next item recommendation. Extensive experiments have been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica, and the experimental results show that our model outperforms other state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Ruihong Qiu, Jingjing Li, Zi Huang, Hongzhi Yin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07348v1": {
            "Paper Title": "Is it a click bait? Let's predict using Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.13868v2": {
            "Paper Title": "Learning Relation Alignment for Calibrated Cross-modal Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09458v3": {
            "Paper Title": "MLBiNet: A Cross-Sentence Collective Event Detection Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11149v3": {
            "Paper Title": "Compositional Learning of Image-Text Query for Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". They\ntrain a LSTM to predict the next word in the sequence after\nit has seen the image and previous words. The \ufb01nal state of\nthis LSTM is considered the composed representation. Han\net al. ",
                    "Citation Text": "Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang,\nMenglong Zhu, Yuan Li, Yang Zhao, and Larry S Davis. Au-\ntomatic spatially-aware fashion concept discovery. In Pro-\nceedings of the IEEE International Conference on Computer\nVision , pages 1463\u20131471, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.01311",
                        "Citation Paper Title": "Title:Automatic Spatially-aware Fashion Concept Discovery",
                        "Citation Paper Abstract": "Abstract:This paper proposes an automatic spatially-aware concept discovery approach using weakly labeled image-text data from shopping websites. We first fine-tune GoogleNet by jointly modeling clothing images and their corresponding descriptions in a visual-semantic embedding space. Then, for each attribute (word), we generate its spatially-aware representation by combining its semantic word vector representation with its spatial representation derived from the convolutional maps of the fine-tuned network. The resulting spatially-aware representations are further used to cluster attributes into multiple groups to form spatially-aware concepts (e.g., the neckline concept might consist of attributes like v-neck, round-neck, etc). Finally, we decompose the visual-semantic embedding space into multiple concept-specific subspaces, which facilitates structured browsing and attribute-feedback product retrieval by exploiting multimodal linguistic regularities. We conducted extensive experiments on our newly collected Fashion200K dataset, and results on clustering quality evaluation and attribute-feedback product retrieval task demonstrate the effectiveness of our automatically discovered spatially-aware concepts.",
                        "Citation Paper Authors": "Authors:Xintong Han, Zuxuan Wu, Phoenix X. Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, Larry S. Davis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.14774v2": {
            "Paper Title": "Generating Query Focused Summaries from Query-Free Resources",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00102v1": {
            "Paper Title": "The Cold-start Problem: Minimal Users' Activity Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00062v1": {
            "Paper Title": "Controllable Gradient Item Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", VAE-based unsupervised\nlearning methods fundamentally cannot achieve disentanglement\nwithout model inductive biases. Therefore, a natural question is\ncan our method deliver a disentanglement without the help of\nmodel inductive bias? Shu et al. ",
                    "Citation Text": "Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020.\nWeakly Supervised Disentanglement with Guarantees. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net. https://openreview.net/forum?id=HJgSwyBKvr",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.09772",
                        "Citation Paper Title": "Title:Weakly Supervised Disentanglement with Guarantees",
                        "Citation Paper Abstract": "Abstract:Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.",
                        "Citation Paper Authors": "Authors:Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "proposed a pairwise VAE that tries to\ncapture a binary relationship(similar or not). And Feng et al. ",
                    "Citation Text": "Zunlei Feng, Xinchao Wang, Chenglong Ke, Anxiang Zeng, Dacheng Tao, and\nMingli Song. 2018. Dual Swap Disentangling. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman\nGarnett (Eds.). 5898\u20135908. https://proceedings.neurips.cc/paper/2018/hash/\nfdf1bc5669e8ff5ba45d02fded729feb-Abstract.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10583",
                        "Citation Paper Title": "Title:Dual Swap Disentangling",
                        "Citation Paper Abstract": "Abstract:Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a \"encoding-swap-decoding\" process, where we first swap the parts of their encodings corresponding to the shared attribute and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the \"encoding-swap-decoding\" process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.",
                        "Citation Paper Authors": "Authors:Zunlei Feng, Xinchao Wang, Chenglong Ke, Anxiang Zeng, Dacheng Tao, Mingli Song"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "leveraged data pairs to match\nthem in representation space. To deal with the unparalleled data\nscenario, ",
                    "Citation Text": "Xin Wen, Zhizhong Han, Xinyu Yin, and Yu-Shen Liu. 2019. Adversarial Cross-\nModal Retrieval via Learning and Transferring Single-Modal Similarities. CoRR\nabs/1904.08042 (2019). arXiv:1904.08042 http://arxiv.org/abs/1904.08042",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08042",
                        "Citation Paper Title": "Title:Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities",
                        "Citation Paper Abstract": "Abstract:Cross-modal retrieval aims to retrieve relevant data across different modalities (e.g., texts vs. images). The common strategy is to apply element-wise constraints between manually labeled pair-wise items to guide the generators to learn the semantic relationships between the modalities, so that the similar items can be projected close to each other in the common representation subspace. However, such constraints often fail to preserve the semantic structure between unpaired but semantically similar items (e.g. the unpaired items with the same class label are more similar than items with different labels). To address the above problem, we propose a novel cross-modal similarity transferring (CMST) method to learn and preserve the semantic relationships between unpaired items in an unsupervised way. The key idea is to learn the quantitative similarities in single-modal representation subspace, and then transfer them to the common representation subspace to establish the semantic relationships between unpaired items across modalities. Experiments show that our method outperforms the state-of-the-art approaches both in the class-based and pair-based retrieval tasks.",
                        "Citation Paper Authors": "Authors:Xin Wen, Zhizhong Han, Xinyu Yin, Yu-Shen Liu"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "as initial features for English words and pre-trained\nChinese Word Vectors ",
                    "Citation Text": "Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and Xiaoyong Du. 2018. Analog-\nical Reasoning on Chinese Morphological and Semantic Relations. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers) . Association for Computational Linguistics, Melbourne, Australia,\n138\u2013143. https://doi.org/10.18653/v1/P18-2023",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06504",
                        "Citation Paper Title": "Title:Analogical Reasoning on Chinese Morphological and Semantic Relations",
                        "Citation Paper Abstract": "Abstract:Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.",
                        "Citation Paper Authors": "Authors:Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.15165v1": {
            "Paper Title": "Multimodal Detection of Information Disorder from Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14975v1": {
            "Paper Title": "Privileged Graph Distillation for Cold Start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14909v1": {
            "Paper Title": "Generating Interesting Song-to-Song Segues With Dave",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09112v2": {
            "Paper Title": "An Interpretable and Uncertainty Aware Multi-Task Framework for\n  Multi-Aspect Sentiment Analysis",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "and fixed its weights during training.\nWe also adopted the learning rate warmup heuristic ",
                    "Citation Text": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2019. On the\nVariance of the Adaptive Learning Rate and Beyond. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03265",
                        "Citation Paper Title": "Title:On the Variance of the Adaptive Learning Rate and Beyond",
                        "Citation Paper Abstract": "Abstract:The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", which captures both forward and backward\nsemantic information, with the multi-task learning framework, where different tasks have their\nown classifiers and share the same Bi-LSTM encoder.\n\u2022MBERT is a multi-task version of the BERT classification model ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\n18Multi-Aspect Sentiment Analysis ACM/IMS TDS, New York, NY, USA\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "extracts key features from a review by applying convolution and max-over-time pooling ",
                    "Citation Text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural\nlanguage processing (almost) from scratch. Journal of Machine Learning Research 12, Aug (2011), 2493\u20132537.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.0398",
                        "Citation Paper Title": "Title:Natural Language Processing (almost) from Scratch",
                        "Citation Paper Abstract": "Abstract:We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
                        "Citation Paper Authors": "Authors:Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "proposed incorporating users\u2019 information, overall ratings,\nand hand-crafted aspect keywords into their model to predict ratings, instead of merely using\ntextual reviews. Zeng et al. ",
                    "Citation Text": "Ziqian Zeng, Wenxuan Zhou, Xin Liu, and Yangqiu Song. 2019. A Variational Approach to Weakly Supervised\nDocument-Level Multi-Aspect Sentiment Classification. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) . 386\u2013396.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05055",
                        "Citation Paper Title": "Title:A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment Classification",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a variational approach to weakly supervised document-level multi-aspect sentiment classification. Instead of using user-generated ratings or annotations provided by domain experts, we use target-opinion word pairs as \"supervision.\" These word pairs can be extracted by using dependency parsers and simple rules. Our objective is to predict an opinion word given a target word while our ultimate goal is to learn a sentiment polarity classifier to predict the sentiment polarity of each aspect given a document. By introducing a latent variable, i.e., the sentiment polarity, to the objective function, we can inject the sentiment polarity classifier to the objective via the variational lower bound. We can learn a sentiment polarity classifier by optimizing the lower bound. We show that our method can outperform weakly supervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect.",
                        "Citation Paper Authors": "Authors:Ziqian Zeng, Wenxuan Zhou, Xin Liu, Yangqiu Song"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", so that they do not need to be trained from\nrandom states, which may result in poor embeddings due to the lack of word co-occurrence.\nThen, a single layer highway network ",
                    "Citation Text": "Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. 2015. Highway networks. arXiv preprint\narXiv:1505.00387 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00387",
                        "Citation Paper Title": "Title:Highway Networks",
                        "Citation Paper Abstract": "Abstract:There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.",
                        "Citation Paper Authors": "Authors:Rupesh Kumar Srivastava, Klaus Greff, J\u00fcrgen Schmidhuber"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". This\nmethod has gain popularity in practice [ 10,20] since it is simple to implement and computationally\nmore efficient. Recently, Zhang et al. ",
                    "Citation Text": "Xuchao Zhang, Fanglan Chen, Chang-Tien Lu, and Naren Ramakrishnan. 2019. Mitigating Uncertainty in Document\nClassification. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . 3126\u20133136.\n20",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.07590",
                        "Citation Paper Title": "Title:Mitigating Uncertainty in Document Classification",
                        "Citation Paper Abstract": "Abstract:The uncertainty measurement of classifiers' predictions is especially important in applications such as medical diagnoses that need to ensure limited human resources can focus on the most uncertain predictions returned by machine learning models. However, few existing uncertainty models attempt to improve overall prediction accuracy where human resources are involved in the text classification task. In this paper, we propose a novel neural-network-based model that applies a new dropout-entropy method for uncertainty measurement. We also design a metric learning method on feature representations, which can boost the performance of dropout-based uncertainty methods with smaller prediction variance in accurate prediction trials. Extensive experiments on real-world data sets demonstrate that our method can achieve a considerable improvement in overall prediction accuracy compared to existing approaches. In particular, our model improved the accuracy from 0.78 to 0.92 when 30\\% of the most uncertain predictions were handed over to human experts in \"20NewsGroup\" data.",
                        "Citation Paper Authors": "Authors:Xuchao Zhang, Fanglan Chen, Chang-Tien Lu, Naren Ramakrishnan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2004.13003v4": {
            "Paper Title": "Corpus-level and Concept-based Explanations for Interpretable Document\n  Classification",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ": This dataset contains product reviews in beauty category from Amazon.\nWe converted the original ratings (1-5) to binary (positive or negative) labels and sampled 20,000\nreviews for each label. For all three datasets, we tokenized reviews using BERT tokenizer ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R\u2019emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace\u2019s Transformers: State-of-the-art Natural\nLanguage Processing. ArXiv abs/1910.03771 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ": This model extracts key features from a review by applying convolution and max-\nover-time pooling operations ",
                    "Citation Text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural\nlanguage processing (almost) from scratch. Journal of Machine Learning Research 12, Aug (2011), 2493\u20132537.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.0398",
                        "Citation Paper Title": "Title:Natural Language Processing (almost) from Scratch",
                        "Citation Paper Abstract": "Abstract:We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
                        "Citation Paper Authors": "Authors:Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ".\n4 EXPERIMENTS\n4.1 Datasets\nWe conducted experiments on three publicly available datasets. Newsroom is used for news catego-\nrization, while IMDB and Beauty are used for sentiment analysis. The details of the three datasets\nare as follows: 1) Newsroom ",
                    "Citation Text": "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive\nstrategies. arXiv preprint arXiv:1804.11283 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.11283",
                        "Citation Paper Title": "Title:Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
                        "Citation Paper Abstract": "Abstract:We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.",
                        "Citation Paper Authors": "Authors:Max Grusky, Mor Naaman, Yoav Artzi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". However, these methods usually suffer from the reliability is-\nsues when adversarial perturbations ",
                    "Citation Text": "Amirata Ghorbani, Abubakar Abid, and James Zou. 2019. Interpretation of neural networks is fragile. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , Vol. 33. 3681\u20133688.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10547",
                        "Citation Paper Title": "Title:Interpretation of Neural Networks is Fragile",
                        "Citation Paper Abstract": "Abstract:In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.",
                        "Citation Paper Authors": "Authors:Amirata Ghorbani, Abubakar Abid, James Zou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.15497v1": {
            "Paper Title": "A Bytecode-based Approach for Smart Contract Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14599v1": {
            "Paper Title": "Personalization in E-Grocery: Top-N versus Top-k Rankings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08755v2": {
            "Paper Title": "DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions\n  of Annotators' Labels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14329v1": {
            "Paper Title": "GINA: Neural Relational Inference From Independent Snapshots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09553v2": {
            "Paper Title": "A Sequence-to-Sequence Approach to Dialogue State Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02100v2": {
            "Paper Title": "Deoscillated Graph Collaborative Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14060v1": {
            "Paper Title": "Rethinking Lifelong Sequential Recommendation with Incremental\n  Multi-Interest Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.13881v1": {
            "Paper Title": "CausCF: Causal Collaborative Filtering for RecommendationEffect\n  Estimation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ".\n\u2022WDL (Wide&Deep) . It models the low-order and high-\norder feature interactions simultaneously. The wide side is a\nlinear regression, and the deep side is a neural network ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan\nAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the\n1st Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016,\nBoston, MA, USA, September 15, 2016 . 7\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ".\n\u2022CEVAE . Causal effect variational auto-encoder, who intro-\nduces the latent variables to weaken the assumption on the\ndata generating process and the unobserved confounders ",
                    "Citation Text": "Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and\nMax Welling. 2017. Causal effect inference with deep latent-variable models. In\nNIPS . 6446\u20136456.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.08821",
                        "Citation Paper Title": "Title:Causal Effect Inference with Deep Latent-Variable Models",
                        "Citation Paper Abstract": "Abstract:Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.",
                        "Citation Paper Authors": "Authors:Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, Max Welling"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". When each\ndimension of the input space is heterogeneous and sampled from\ndifferent distribution families, it is hard to guarantee the confidence\nof the results ",
                    "Citation Text": "Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00f6lkopf, and\nAlexander J. Smola. 2012. A Kernel Two-Sample Test. J. Mach. Learn. Res. 13\n(2012), 723\u2013773.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0805.2368",
                        "Citation Paper Title": "Title:A Kernel Method for the Two-Sample Problem",
                        "Citation Paper Abstract": "Abstract:  We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",
                        "Citation Paper Authors": "Authors:Arthur Gretton, Karsten Borgwardt, Malte J. Rasch, Bernhard Scholkopf, Alexander J. Smola"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". And the representation of the\nuser and item can be better encoded via sequential modeling [ 43,53]\nor graph neural network ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for\nRecommendation . Association for Computing Machinery, New York, NY, USA,\n639\u2013648. https://doi.org/10.1145/3397271.3401063",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "2.1 Causal Effect Estimation\nAs summarized by Yao et al . ",
                    "Citation Text": "Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. 2020.\nA Survey on Causal Inference. CoRR abs/2002.02770 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02770",
                        "Citation Paper Title": "Title:A Survey on Causal Inference",
                        "Citation Paper Abstract": "Abstract:Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well known causal inference framework. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.",
                        "Citation Paper Authors": "Authors:Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, Aidong Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.02252v4": {
            "Paper Title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.13066v1": {
            "Paper Title": "Integrating Semantics and Neighborhood Information with Graph-Driven\n  Generative Models for Document Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.13003v1": {
            "Paper Title": "Rethinking InfoNCE: How Many Negative Samples Do You Need?",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ", and we use the ML-1M6version for experiments. We use the same experimental settings\nas ",
                    "Citation Text": "Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.\nBERT4Rec: Sequential recommendation with bidirectional encoder representations from trans-\nformer. In CIKM . 1441\u20131450.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06690",
                        "Citation Paper Title": "Title:BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
                        "Citation Paper Abstract": "Abstract:Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly \"see the target item\". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.",
                        "Citation Paper Authors": "Authors:Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.12989v1": {
            "Paper Title": "A functionality taxonomy for document search engines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12932v1": {
            "Paper Title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12788v1": {
            "Paper Title": "A data-driven strategy to combine word embeddings in information\n  retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06967v2": {
            "Paper Title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". Second, we re-rank our results\nwith the state-of-the-art mono-duo-T5 re-ranking model following\nPradeep et al. ",
                    "Citation Text": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The Expando-Mono-\nDuo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence\nModels. arXiv:2101.05667 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05667",
                        "Citation Paper Title": "Title:The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models",
                        "Citation Paper Abstract": "Abstract:We propose a design pattern for tackling text ranking problems, dubbed \"Expando-Mono-Duo\", that has been empirically validated for a number of ad hoc retrieval tasks in different domains. At the core, our design relies on pretrained sequence-to-sequence models within a standard multi-stage ranking architecture. \"Expando\" refers to the use of document expansion techniques to enrich keyword representations of texts prior to inverted indexing. \"Mono\" and \"Duo\" refer to components in a reranking pipeline based on a pointwise model and a pairwise model that rerank initial candidates retrieved using keyword search. We present experimental results from the MS MARCO passage and document ranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge that validate our design. In all these tasks, we achieve effectiveness that is at or near the state of the art, in some cases using a zero-shot approach that does not exploit any training data from the target task. To support replicability, implementations of our design pattern are open-sourced in the Pyserini IR toolkit and PyGaggle neural reranking library.",
                        "Citation Paper Authors": "Authors:Ronak Pradeep, Rodrigo Nogueira, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "ANCE 8\u00d7V100 32 \u2013 \u2013 \u2713 10K batches +1 BM25-trained checkpoint ",
                    "Citation Text": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. Learning\nTo Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently.\narXiv:2010.10469 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10469",
                        "Citation Paper Title": "Title:Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently",
                        "Citation Paper Abstract": "Abstract:Ranking has always been one of the top concerns in information retrieval research. For decades, lexical matching signal has dominated the ad-hoc retrieval process, but it also has inherent defects, such as the vocabulary mismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to alleviate these limitations by capturing the deep semantic relationship between queries and documents. The training of most existing Dense Retrieval models relies on sampling negative instances from the corpus to optimize a pairwise loss function. Through investigation, we find that this kind of training strategy is biased and fails to optimize full retrieval performance effectively and efficiently. To solve this problem, we propose a Learning To Retrieve (LTRe) training technique. LTRe constructs the document index beforehand. At each training iteration, it performs full retrieval without negative sampling and then updates the query representation model parameters. Through this process, it teaches the DR model how to retrieve relevant documents from the entire corpus instead of how to rerank a potentially biased sample of documents. Experiments in both passage retrieval and document retrieval tasks show that: 1) in terms of effectiveness, LTRe significantly outperforms all competitive sparse and dense baselines. It even gains better performance than the BM25-BERT cascade system under reasonable latency constraints. 2) in terms of training efficiency, compared with the previous state-of-the-art DR method, LTRe provides more than 170x speed-up in the training process. Training with a compressed index further saves computing resources with minor performance loss.",
                        "Citation Paper Authors": "Authors:Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "showed the benefits of an ensemble of pair-\nwise BERT CATteachers; concurrently, Lin et al . ",
                    "Citation Text": "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense\nRepresentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11386",
                        "Citation Paper Title": "Title:Distilling Dense Representations for Ranking using Tightly-Coupled Teachers",
                        "Citation Paper Abstract": "Abstract:We present an approach to ranking with dense representations that applies knowledge distillation to improve the recently proposed late-interaction ColBERT model. Specifically, we distill the knowledge from ColBERT's expressive MaxSim operator for computing relevance scores into a simple dot product, thus enabling single-step ANN search. Our key insight is that during distillation, tight coupling between the teacher model and the student model enables more flexible distillation strategies and yields better learned representations. We empirically show that our approach improves query latency and greatly reduces the onerous storage requirements of ColBERT, while only making modest sacrifices in terms of effectiveness. By combining our dense representations with sparse representations derived from document expansion, we are able to approach the effectiveness of a standard cross-encoder reranker using BERT that is orders of magnitude slower.",
                        "Citation Paper Authors": "Authors:Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "optimizer with a learning rate of 7\u00d710\u22126.\nAs a basis for all our BERT DOT andColBERT instances we use\na 6-layer DistilBERT ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.\narXiv:1910.01108 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "collection with the sparsely-\njudged MSMARCO-DEV query set of 6,980 queries (used in the\nleaderboard) as well as the densely-judged query sets of 43 and 54\nqueries derived from TREC-DL \u201919 ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2019. Overview\nof the TREC 2019 Deep Learning Track. In TREC .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07820",
                        "Citation Paper Title": "Title:Overview of the TREC 2019 deep learning track",
                        "Citation Paper Abstract": "Abstract:The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the first track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs significantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M. Voorhees"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "demonstrated that the sampling policy for negative samples plays\nan important role in the stability of the training, and MacAvaney\net al. ",
                    "Citation Text": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Training Curricula for Open Domain Answer\nRe-Ranking. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14269",
                        "Citation Paper Title": "Title:Training Curricula for Open Domain Answer Re-Ranking",
                        "Citation Paper Abstract": "Abstract:In precision-oriented tasks like answer ranking, it is more important to rank many relevant answers highly than to retrieve all relevant answers. It follows that a good ranking strategy would be to learn how to identify the easiest correct answers first (i.e., assign a high ranking score to answers that have characteristics that usually indicate relevance, and a low ranking score to those with characteristics that do not), before incorporating more complex logic to handle difficult cases (e.g., semantic matching or reasoning). In this work, we apply this idea to the training of neural answer rankers using curriculum learning. We propose several heuristics to estimate the difficulty of a given training sample. We show that the proposed heuristics can be used to build a training curriculum that down-weights difficult samples early in the training process. As the training process progresses, our approach gradually shifts to weighting all samples equally, regardless of difficulty. We present a comprehensive evaluation of our proposed idea on three answer ranking datasets. Results show that our approach leads to superior performance of two leading neural ranking architectures, namely BERT and ConvKNRM, using both pointwise and pairwise losses. When applied to a BERT-based ranker, our method yields up to a 4% improvement in MRR and a 9% improvement in P@1 (compared to the model trained without a curriculum). This results in models that can achieve comparable performance to more expensive state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, Ophir Frieder"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", or changing the image mixture of batches for self-\nsupervised representation learning ",
                    "Citation Text": "Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, and Trevor Darrell.\n2020. Rethinking Image Mixture for Unsupervised Visual Representation Learn-\ning. arXiv:2003.05438 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05438",
                        "Citation Paper Title": "Title:Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:The recently advanced unsupervised learning approaches use the siamese-like framework to compare two \"views\" from the same image for learning representations. Making the two views distinctive is a core to guarantee that unsupervised methods can learn meaningful information. However, such frameworks are sometimes fragile on overfitting if the augmentations used for generating two views are not strong enough, causing the over-confident issue on the training data. This drawback hinders the model from learning subtle variance and fine-grained information. To address this, in this work we aim to involve the distance concept on label space in the unsupervised learning and let the model be aware of the soft degree of similarity between positive or negative pairs through mixing the input data space, to further work collaboratively for the input and loss spaces. Despite its conceptual simplicity, we show empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we can learn subtler, more robust and generalized representations from the transformed input and corresponding new label space. Extensive experiments are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, SwAV, etc. Our proposed image mixture and label assignment strategy can obtain consistent improvement by 1~3% following exactly the same hyperparameters and training procedures of the base methods. Code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, Eric Xing"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nThe composition or sampling of training batches spans all ma-\nchine learning application fields. Many advances were made es-\npecially in computer vision: whether to create synthetic negative\nsamples for contrastive learning ",
                    "Citation Text": "Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and\nDiane Larlus. 2020. Hard Negative Mixing for Contrastive Learning. Advances in\nNeural Information Processing Systems 33 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01028",
                        "Citation Paper Title": "Title:Hard Negative Mixing for Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.",
                        "Citation Paper Authors": "Authors:Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, Diane Larlus"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", reducing the size\nof the BERT CATpassage re-ranking model [ 5,12], to dense keyword\nmatching in sponsored search ",
                    "Citation Text": "Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. TwinBERT: Distilling Knowledge\nto Twin-Structured BERT Models for Efficient Retrieval. arXiv:2002.06275 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06275",
                        "Citation Paper Title": "Title:TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, while the superior performance comes with high demand in computational resources, which hinders the application in low-latency IR systems. We present TwinBERT model for effective and efficient retrieval, which has twin-structured BERT-like encoders to represent query and document respectively and a crossing layer to combine the embeddings and produce a similarity score. Different from BERT, where the two input sentences are concatenated and encoded together, TwinBERT decouples them during encoding and produces the embeddings for query and document independently, which allows document embeddings to be pre-computed offline and cached in memory. Thereupon, the computation left for run-time is from the query encoding and query-document crossing only. This single change can save large amount of computation time and resources, and therefore significantly improve serving efficiency. Moreover, a few well-designed network layers and training strategies are proposed to further reduce computational cost while at the same time keep the performance as remarkable as BERT model. Lastly, we develop two versions of TwinBERT for retrieval and relevance tasks correspondingly, and both of them achieve close or on-par performance to BERT-Base model.\nThe model was trained following the teacher-student framework and evaluated with data from one of the major search engines. Experimental results showed that the inference time was significantly reduced and was firstly controlled around 20ms on CPUs while at the same time the performance gain from fine-tuned BERT-Base model was mostly retained. Integration of the models into production systems also demonstrated remarkable improvements on relevance metrics with negligible influence on latency.",
                        "Citation Paper Authors": "Authors:Wenhao Lu, Jian Jiao, Ruofei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08279v2": {
            "Paper Title": "Learning to Route via Theory-Guided Residual Network",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "begin to learn users\u2019 prefer-\nences by the historical data: ",
                    "Citation Text": "Paolo Campigotto, Christian Rudloff, Maximilian Leodolter,\nand Dietmar Bauer. Personalized and situation-aware\nmultimodal route recommendations: the favour algorithm.\nIEEE Transactions on Intelligent Transportation Systems ,\n18(1):92\u2013102, 2016. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.09076",
                        "Citation Paper Title": "Title:Personalized and situation-aware multimodal route recommendations: the FAVOUR algorithm",
                        "Citation Paper Abstract": "Abstract:Route choice in multimodal networks shows a considerable variation between different individuals as well as the current situational context. Personalization of recommendation algorithms are already common in many areas, e.g., online retail. However, most online routing applications still provide shortest distance or shortest travel-time routes only, neglecting individual preferences as well as the current situation. Both aspects are of particular importance in a multimodal setting as attractivity of some transportation modes such as biking crucially depends on personal characteristics and exogenous factors like the weather. This paper introduces the FAVourite rOUte Recommendation (FAVOUR) approach to provide personalized, situation-aware route proposals based on three steps: first, at the initialization stage, the user provides limited information (home location, work place, mobility options, sociodemographics) used to select one out of a small number of initial profiles. Second, based on this information, a stated preference survey is designed in order to sharpen the profile. In this step a mass preference prior is used to encode the prior knowledge on preferences from the class identified in step one. And third, subsequently the profile is continuously updated during usage of the routing services. The last two steps use Bayesian learning techniques in order to incorporate information from all contributing individuals. The FAVOUR approach is presented in detail and tested on a small number of survey participants. The experimental results on this real-world dataset show that FAVOUR generates better-quality recommendations w.r.t. alternative learning algorithms from the literature. In particular the definition of the mass preference prior for initialization of step two is shown to provide better predictions than a number of alternatives from the literature.",
                        "Citation Paper Authors": "Authors:Paolo Campigotto, Christian Rudloff, Maximilian Leodolter, Dietmar Bauer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.07377v2": {
            "Paper Title": "Set2setRank: Collaborative Set to Set Ranking for Implicit Feedback\n  based Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ". Instead of explicitly picking\nunobserved items as negative items, NCE-PLRec generates negative\ninstances by sampling from the popularity distribution ",
                    "Citation Text": "Ga Wu, Maksims Volkovs, Chee Loong Soon, Scott Sanner, and Himanshu Rai.\n2019. Noise Contrastive Estimation for One-Class Collaborative Filtering. In\nSIGIR . 135\u2013144.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00697",
                        "Citation Paper Title": "Title:Noise Contrastive Estimation for Scalable Linear Models for One-Class Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Previous highly scalable one-class collaborative filtering methods such as Projected Linear Recommendation (PLRec) have advocated using fast randomized SVD to embed items into a latent space, followed by linear regression methods to learn personalized recommendation models per user. Unfortunately, naive SVD embedding methods often exhibit a popularity bias that skews the ability to accurately embed niche items. To address this, we leverage insights from Noise Contrastive Estimation (NCE) to derive a closed-form, efficiently computable \"depopularized\" embedding. While this method is not ideal for direct recommendation using methods like PureSVD since popularity still plays an important role in recommendation, we find that embedding followed by linear regression to learn personalized user models in a novel method we call NCE-PLRec leverages the improved item embedding of NCE while correcting for its popularity unbiasing in final recommendations. An analysis of the recommendation popularity distribution demonstrates that NCE-PLRec uniformly distributes its recommendations over the popularity spectrum while other methods exhibit distinct biases towards specific popularity subranges, thus artificially restricting their recommendations. Empirically, NCE-PLRec outperforms state-of-the-art methods as well as various ablations of itself on a variety of large-scale recommendation datasets.",
                        "Citation Paper Authors": "Authors:Ga Wu, Maksims Volkovs, Chee Loong Soon, Scott Sanner, Himanshu Rai"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". As the observed items are all treated as rating 1,\nSetRank is proposed to use the permutation probability to encour-\nage one observed item ranks in front of multiple unobserved items\nin each list ",
                    "Citation Text": "Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, and Hui Xiong. 2020. SetRank:\nA Setwise Bayesian Approach for Collaborative Ranking from Implicit Feedback..\nInAAAI . 6127\u20136136.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.09841",
                        "Citation Paper Title": "Title:SetRank: A Setwise Bayesian Approach for Collaborative Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:The recent development of online recommender systems has a focus on collaborative ranking from implicit feedback, such as user clicks and purchases. Different from explicit ratings, which reflect graded user preferences, the implicit feedback only generates positive and unobserved labels. While considerable efforts have been made in this direction, the well-known pairwise and listwise approaches have still been limited by various challenges. Specifically, for the pairwise approaches, the assumption of independent pairwise preference is not always held in practice. Also, the listwise approaches cannot efficiently accommodate \"ties\" due to the precondition of the entire list permutation. To this end, in this paper, we propose a novel setwise Bayesian approach for collaborative ranking, namely SetRank, to inherently accommodate the characteristics of implicit feedback in recommender system. Specifically, SetRank aims at maximizing the posterior probability of novel setwise preference comparisons and can be implemented with matrix factorization and neural networks. Meanwhile, we also present the theoretical analysis of SetRank to show that the bound of excess risk can be proportional to $\\sqrt{M/N}$, where $M$ and $N$ are the numbers of items and users, respectively. Finally, extensive experiments on four real-world datasets clearly validate the superiority of SetRank compared with various state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, Hui Xiong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.12371v1": {
            "Paper Title": "Quotient Space-Based Keyword Retrieval in Sponsored Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12261v1": {
            "Paper Title": "Impact of detecting clinical trial elements in exploration of COVID-19\n  literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12190v1": {
            "Paper Title": "Climate Action During COVID-19 Recovery and Beyond: A Twitter Text\n  Mining Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10252v2": {
            "Paper Title": "CoRT: Complementary Rankings from Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02218v3": {
            "Paper Title": "BATS: A Spectral Biclustering Approach to Single Document Topic Modeling\n  and Segmentation",
            "Sentences": [
                {
                    "Sentence ID": 63,
                    "Sentence": ", the case of a checkerboard pattern is considered, with\nimplications to the spectral decomposition. ",
                    "Citation Text": "Karl Rohe, Tai Qin, and Bin Yu. 2012. Co-clustering for directed graphs: the Stochastic co-Blockmodel and spectral\nalgorithm Di-Sim. arXiv:1204.2296 (2012).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1204.2296",
                        "Citation Paper Title": "Title:Co-clustering for directed graphs: the Stochastic co-Blockmodel and spectral algorithm Di-Sim",
                        "Citation Paper Abstract": "Abstract:Directed graphs have asymmetric connections, yet the current graph clustering methodologies cannot identify the potentially global structure of these asymmetries. We give a spectral algorithm called di-sim that builds on a dual measure of similarity that correspond to how a node (i) sends and (ii) receives edges. Using di-sim, we analyze the global asymmetries in the networks of Enron emails, political blogs, and the c elegans neural connectome. In each example, a small subset of nodes have persistent asymmetries; these nodes send edges with one cluster, but receive edges with another cluster. Previous approaches would have assigned these asymmetric nodes to only one cluster, failing to identify their sending/receiving asymmetries. Regularization and \"projection\" are two steps of di-sim that are essential for spectral clustering algorithms to work in practice. The theoretical results show that these steps make the algorithm weakly consistent under the degree corrected Stochastic co-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow for both (i) degree heterogeneity and (ii) the global asymmetries that we intend to detect. The theoretical results make no assumptions on the smallest degree nodes. Instead, the theorem requires that the average degree grows sufficiently fast and that the weak consistency only applies to the subset of the nodes with sufficiently large leverage scores. The results results also apply to bipartite graphs.",
                        "Citation Paper Authors": "Authors:Karl Rohe, Tai Qin, Bin Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.06471v1": {
            "Paper Title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11678v1": {
            "Paper Title": "Hybrid Movie Recommender System based on Resource Allocation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.11410v1": {
            "Paper Title": "From Base Data To Knowledge Discovery -- A Life Cycle Approach -- Using\n  Multilayer Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00743v3": {
            "Paper Title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00573v1": {
            "Paper Title": "One4all User Representation for Recommender Systems in E-commerce",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "train autoencoder-coupled\nTransformer networks that model retention, installation, and unin-\nstallation collectively. They test the user embeddings in three down-\nstream tasks for mobile app management scenarios. Gu et al . ",
                    "Citation Text": "Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, Xiaoxiao Xu, Jingmin Chen, and\nJun Zhang. 2020. Exploiting Behavioral Consistence for Universal User Repre-\nsentation. arXiv preprint arXiv:2012.06146 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.06146",
                        "Citation Paper Title": "Title:Exploiting Behavioral Consistence for Universal User Representation",
                        "Citation Paper Abstract": "Abstract:User modeling is critical for developing personalized services in industry. A common way for user modeling is to learn user representations that can be distinguished by their interests or preferences. In this work, we focus on developing universal user representation model. The obtained universal representations are expected to contain rich information, and be applicable to various downstream applications without further modifications (e.g., user preference prediction and user profiling). Accordingly, we can be free from the heavy work of training task-specific models for every downstream task as in previous works. In specific, we propose Self-supervised User Modeling Network (SUMN) to encode behavior data into the universal representation. It includes two key components. The first one is a new learning objective, which guides the model to fully identify and preserve valuable user information under a self-supervised learning framework. The other one is a multi-hop aggregation layer, which benefits the model capacity in aggregating diverse behaviors. Extensive experiments on benchmark datasets show that our approach can outperform state-of-the-art unsupervised representation methods, and even compete with supervised ones.",
                        "Citation Paper Authors": "Authors:Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, Xiaoxiao Xu, Jingmin Chen, Jun Zhang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "with\ud835\udefd1=0.9,\ud835\udefd2=0.999, L2 weight decay\nof 0.0001, learning rate warmup over the first 30,000 steps, cosine\ndecay of the learning rate. The total training time is 3 days. All\nthe experiments are trained with an automatic mixed-precision\npackage in Pytorch ",
                    "Citation Text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .\n2019. Pytorch: An imperative style, high-performance deep learning library. In\nAdvances in neural information processing systems . 8026\u20138037.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "augments user behav-\nior sequence by cropping, masking, and re-ordering, to construct\npretext tasks. Then it uses a contrastive loss function to pre-train\nthe model. Hu et al . ",
                    "Citation Text": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020.\nGPT-GNN: Generative Pre-Training of Graph Neural Networks. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining (Virtual Event, CA, USA) (KDD \u201920) . Association for Computing\nMachinery, New York, NY, USA, 1857\u20131867. https://doi.org/10.1145/3394486.\n3403237",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15437",
                        "Citation Paper Title": "Title:GPT-GNN: Generative Pre-Training of Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
                        "Citation Paper Authors": "Authors:Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.05186v3": {
            "Paper Title": "GLOW : Global Weighted Self-Attention Network for Web Search",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "presented Birch, a system that applies BERT to document\nretrieval via integration with the open-source Anserini information\nretrieval toolkit to demonstrate end-to-end search over large docu-\nment collections. From the document-query matching perspective,\nDoc2query ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document ex-\npansion by query prediction, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "also proved that in passage ranking task, adding more com-\nponents upon Transformer does not help too much ",
                    "Citation Text": "Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. Understanding the\nbehaviors of bert in ranking. arXiv preprint arXiv:1904.07531 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.07531",
                        "Citation Paper Title": "Title:Understanding the Behaviors of BERT in Ranking",
                        "Citation Paper Abstract": "Abstract:This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.",
                        "Citation Paper Authors": "Authors:Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "is a collection data set for multi-perspective web search\ntasks. The top 10 winners in the leading board all leverage BERT\nas a basis. Typically, Nogueira et al. ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage\ndocument ranking with bert. arXiv preprint arXiv:1910.14424 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14424",
                        "Citation Paper Title": "Title:Multi-Stage Document Ranking with BERT",
                        "Citation Paper Abstract": "Abstract:The advent of deep neural networks pre-trained via language modeling tasks has spurred a number of successful applications in natural language processing. This work explores one such popular model, BERT, in the context of document ranking. We propose two variants, called monoBERT and duoBERT, that formulate the ranking problem as pointwise and pairwise classification, respectively. These two models are arranged in a multi-stage ranking architecture to form an end-to-end search system. One major advantage of this design is the ability to trade off quality against latency by controlling the admission of candidates into each pipeline stage, and by doing so, we are able to find operating points that offer a good balance between these two competing metrics. On two large-scale datasets, MS MARCO and TREC CAR, experiments show that our model produces results that are either at or comparable to the state of the art. Ablation studies show the contributions of each component and characterize the latency/quality tradeoff space.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "upgraded\nDSSM to C-DSSM by using the convolution-max pooling opera-\ntion. Guo et al. ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance\nmatching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\non Conference on Information and Knowledge Management , pages 55\u201364, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08489v2": {
            "Paper Title": "Modeling the Sequential Dependence among Audience Multi-step Conversions\n  with Multi-task Learning in Targeted Display Advertising",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "as\u210e1(\u00b7),\u210e2(\u00b7), and\u210e3(\u00b7). The\nidea of this kind of attention mechanism is similar to self-attention ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". The Neural Multi-\nTask Recommendation (NMTR) [ 2,3] has also been proposed to\nextend the Neural Collaborative Filtering (NCF) ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In TheWebConf . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "to transfer probabilities in\nthe output layers by post-impression click-through rate (CTR) mul-\ntiplying post-click conversion rate (CVR) equals post-impression\nclick-through &conversion rate (CTCVR). Further, more tasks are\ndecomposed for probability transfer in \ud835\udc38\ud835\udc46\ud835\udc402 ",
                    "Citation Text": "Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and\nKeping Yang. 2020. Entire Space Multi-Task Modeling via Post-Click Behavior\nDecomposition for Conversion Rate Prediction. In SIGIR . 2377\u20132386.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07099",
                        "Citation Paper Title": "Title:Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Recommender system, as an essential part of modern e-commerce, consists of two fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate (CVR) prediction. While CVR has a direct impact on the purchasing volume, its prediction is well-known challenging due to the Sample Selection Bias (SSB) and Data Sparsity (DS) issues. Although existing methods, typically built on the user sequential behavior path ``impression$\\to$click$\\to$purchase'', is effective for dealing with SSB issue, they still struggle to address the DS issue due to rare purchase training samples. Observing that users always take several purchase-related actions after clicking, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph ``impression$\\to$click$\\to$D(O)Action$\\to$purchase''. Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervised signals from D(O)Action, which will effectively address the SSB and DS issues together. To this end, we devise a novel deep recommendation model named Elaborated Entire Space Supervised Multi-task Model ($ESM^{2}$). According to the conditional probability rule defined on the graph, it employs multi-task learning to predict some decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR. Extensive experiments on both offline and online environments demonstrate the superiority of $ESM^{2}$ over state-of-the-art models. The source code and dataset will be released.",
                        "Citation Paper Authors": "Authors:Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, Keping Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.12828v3": {
            "Paper Title": "ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language\n  Model for Reading Comprehension of Abstract Meaning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.10866v1": {
            "Paper Title": "Towards Artificial Intelligence Enabled Financial Crime Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07769v2": {
            "Paper Title": "Automated Fact-Checking for Assisting Human Fact-Checkers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07831v2": {
            "Paper Title": "NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable\n  Relaxation of Sorting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.10587v1": {
            "Paper Title": "Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10084v4": {
            "Paper Title": "Context-Aware Learning to Rank with Self-Attention",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "and received more attention after the\nintroduction of the Transformer architecture ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems 30 , I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.).\nCurran Associates, Inc., 5998\u20136008. http://papers.nips.cc/paper/7181-attention-\nis-all-you-need.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10216v1": {
            "Paper Title": "RFID-based Article-to-Fixture Predictions in Real-World Fashion Stores",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06312v2": {
            "Paper Title": "A Non-sequential Approach to Deep User Interest Model for CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "assume the behavior\nsequences are composed of sessions and use self-attention to extract\nusers\u2019 interests in each session and apply Bi-LSTM to capture users\u2019\ncross-session interests. DHAN ",
                    "Citation Text": "Weinan Xu, Hengxu He, Minshi Tan, Yunming Li, Jun Lang, and Dongbai Guo.\n2020. Deep Interest with Hierarchical Attention Network for Click-Through Rate\nPrediction . New York, NY, USA, 1905\u20131908. https://doi.org/10.1145/3397271.\n3401310",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12981",
                        "Citation Paper Title": "Title:Deep Interest with Hierarchical Attention Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Deep Interest Network (DIN) is a state-of-the-art model which uses attention mechanism to capture user interests from historical behaviors. User interests intuitively follow a hierarchical pattern such that users generally show interests from a higher-level then to a lower-level abstraction. Modeling such an interest hierarchy in an attention network can fundamentally improve the representation of user behaviors. We, therefore, propose an improvement over DIN to model arbitrary interest hierarchy: Deep Interest with Hierarchical Attention Network (DHAN). In this model, a multi-dimensional hierarchical structure is introduced on the first attention layer which attends to an individual item, and the subsequent attention layers in the same dimension attend to higher-level hierarchy built on top of the lower corresponding layers. To enable modeling of multiple dimensional hierarchies, an expanding mechanism is introduced to capture one to many hierarchies. This design enables DHAN to attend different importance to different hierarchical abstractions thus can fully capture user interests at different dimensions (e.g. category, price, or brand).To validate our model, a simplified DHAN has applied to Click-Through Rate (CTR) prediction and our experimental results on three public datasets with two levels of the one-dimensional hierarchy only by category. It shows the superiority of DHAN with significant AUC uplift from 12% to 21% over DIN. DHAN is also compared with another state-of-the-art model Deep Interest Evolution Network (DIEN), which models temporal interest. The simplified DHAN also gets slight AUC uplift from 1.0% to 1.7% over DIEN. A potential future work can be a combination of DHAN and DIEN to model both temporal and hierarchical interests.",
                        "Citation Paper Authors": "Authors:Weinan Xu, Hengxu He, Minshi Tan, Yunming Li, Jun Lang, Dongbai Guo"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", etc.\nAmong them, the user interest models [ 6,20,24,25] which use\nattention mechanism to capture user interests from historical be-\nhaviors have been widely proven effective.\nDIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-\nThrough Rate Prediction (KDD \u201918) . New York, NY, USA, 1059\u20131068. https:\n//doi.org/10.1145/3219819.3219823",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". YoutubeNet is a technically designed model\nwhich treats users\u2019 historical behaviors equally and utilizes\naverage pooling operation to obtain user representation.\n\u2022Wide&Deep ",
                    "Citation Text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan\nAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st\nWorkshop on Deep Learning for Recommender Systems (Boston, MA, USA) (DLRS\n2016) . New York, NY, USA, 7\u201310. https://doi.org/10.1145/2988450.2988454",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "first introduces the mechanism of attention to acti-\nvate the historical behaviors w.r.t. given target item, and captures\nthe diverse user interests successfully. ATRANK ",
                    "Citation Text": "Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen,\nand Jun Gao. 2018. ATRank: An Attention-Based User Behavior Modeling Frame-\nwork for Recommendation. Proceedings of the AAAI Conference on Artificial Intel-\nligence 32, 1 (Apr. 2018). https://ojs.aaai.org/index.php/AAAI/article/view/11618",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.06632",
                        "Citation Paper Title": "Title:ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation",
                        "Citation Paper Abstract": "Abstract:A user can be represented as what he/she does along the history. A common way to deal with the user modeling problem is to manually extract all kinds of aggregated features over the heterogeneous behaviors, which may fail to fully represent the data itself due to limited human instinct. Recent works usually use RNN-based methods to give an overall embedding of a behavior sequence, which then could be exploited by the downstream applications. However, this can only preserve very limited information, or aggregated memories of a person. When a downstream application requires to facilitate the modeled user features, it may lose the integrity of the specific highly correlated behavior of the user, and introduce noises derived from unrelated behaviors. This paper proposes an attention based user behavior modeling framework called ATRank, which we mainly use for recommendation tasks. Heterogeneous user behaviors are considered in our model that we project all types of behaviors into multiple latent semantic spaces, where influence can be made among the behaviors via self-attention. Downstream applications then can use the user behavior vectors via vanilla attention. Experiments show that ATRank can achieve better performance and faster training process. We further explore ATRank to use one unified model to predict different types of user behaviors at the same time, showing a comparable performance with the highly optimized individual models.",
                        "Citation Paper Authors": "Authors:Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, Jun Gao"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Deep learning based methods have achieved great success in CTR\nprediction task [ 4,11,14,22]. In early age, models such as DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: A Factorization-Machine Based Neural Network for CTR Prediction.\nInProceedings of the 26th International Joint Conference on Artificial Intelligence\n(Melbourne, Australia) (IJCAI\u201917) . 1725\u20131731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10165v1": {
            "Paper Title": "Have you tried Neural Topic Models? Comparative Analysis of Neural and\n  Non-Neural Topic Models with Application to COVID-19 Twitter Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.10117v1": {
            "Paper Title": "Towards Automatic Comparison of Data Privacy Documents: A Preliminary\n  Experiment on GDPR-like Laws",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "might be required for this task.\nIn order to fully exploit the text and better compare similarities,\nwe use BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,\n4171\u20134186. https://doi.org/10.18653/v1/N19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.01910v2": {
            "Paper Title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question\n  Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09981v1": {
            "Paper Title": "A Load Balanced Recommendation Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09816v1": {
            "Paper Title": "Intra-Document Cascading: Learning to Select Passages for Neural\n  Document Ranking",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". We follow\nthe setup of Hofst\u00e4tter et al . ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross-\nArchitecture Knowledge Distillation. arXiv:cs.IR/2010.02666",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02666",
                        "Citation Paper Title": "Title:Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Retrieval and ranking models are the backbone of many applications such as web search, open domain QA, or text-based recommender systems. The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT passage ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of distilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves re-ranking effectiveness without compromising their efficiency. Additionally, we show our general distillation method to improve nearest neighbor based index retrieval with the BERT dot product model, offering competitive results with specialized and much more costly training methods. To benefit the community, we publish the teacher-score training files in a ready-to-use package.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "that combines convolutional neural networks\n(CNNs) with the kernel-pooling approach of Xiong et al . ",
                    "Citation Text": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\nEnd-to-End Neural Ad-hoc Ranking with Kernel Pooling. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06613",
                        "Citation Paper Title": "Title:End-to-End Neural Ad-hoc Ranking with Kernel Pooling",
                        "Citation Paper Abstract": "Abstract:This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.",
                        "Citation Paper Authors": "Authors:Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "and language model [ 4,22] based retrieval methods\u2014as well as ma-\nchine learning based approaches ",
                    "Citation Text": "Eilon Sheetrit, Anna Shtok, and Oren Kurland. 2020. A passage-based approach\nto learning to rank documents. Information Retrieval Journal (2020), 1\u201328.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02083",
                        "Citation Paper Title": "Title:A Passage-Based Approach to Learning to Rank Documents",
                        "Citation Paper Abstract": "Abstract:According to common relevance-judgments regimes, such as TREC's, a document can be deemed relevant to a query even if it contains a very short passage of text with pertinent information. This fact has motivated work on passage-based document retrieval: document ranking methods that induce information from the document's passages. However, the main source of passage-based information utilized was passage-query similarities. We address the challenge of utilizing richer sources of passage-based information to improve document retrieval effectiveness. Specifically, we devise a suite of learning-to-rank-based document retrieval methods that utilize an effective ranking of passages produced in response to the query; the passage ranking is also induced using a learning-to-rank approach. Some of the methods quantify the ranking of the passages of a document. Others utilize the feature-based representation of passages used for learning a passage ranker. Empirical evaluation attests to the clear merits of our methods with respect to highly effective baselines. Our best performing method is based on learning a document ranking function using document-query features and passage-query features of the document's passage most highly ranked.",
                        "Citation Paper Authors": "Authors:Eilon Sheetrit, Anna Shtok, Oren Kurland"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.09710v1": {
            "Paper Title": "Unified Conversational Recommendation Policy Learning via Graph-based\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00002v2": {
            "Paper Title": "The Graph-Based Behavior-Aware Recommendation for Interactive News",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09613v1": {
            "Paper Title": "FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming\n  Similarity Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09562v1": {
            "Paper Title": "Interactive Query Formulation using Query By Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09137v1": {
            "Paper Title": "TableZa -- A classical Computer Vision approach to Tabular Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13544v2": {
            "Paper Title": "I-AID: Identifying Actionable Information from Disaster-related Tweets",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", there has been a signi\ufb01cant\nadvancement in the \ufb01eld of multi-label text classi\ufb01cation.\nPretrained language models (e.g, BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08872v1": {
            "Paper Title": "Combating Ambiguity for Hash-code Learning in Medical Instance Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "can be grouped into this category using feature embedding\ntailor features from fully-connected layers for hash-codes generating.\nThe representative methods include deep pairwise-supervised hashing\n(DPSH) ",
                    "Citation Text": "W.-J. Li, S. Wang, and W.-C. Kang, \u201cFeature learning based deep super-\nvised hashing with pairwise labels,\u201d arXiv preprint arXiv:1511.03855 ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.03855",
                        "Citation Paper Title": "Title:Feature Learning based Deep Supervised Hashing with Pairwise Labels",
                        "Citation Paper Abstract": "Abstract:Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on hand-crafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing(DPSH), to perform simultaneous feature learning and hash-code learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications.",
                        "Citation Paper Authors": "Authors:Wu-Jun Li, Sheng Wang, Wang-Cheng Kang"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "proposed a non-parametric spatial- and channel-wise\nweighting method for focusing on salient regions. Unlike the spatial\nweighting scheme, class activation maps (CAMs) ",
                    "Citation Text": "A. Jimenez, J. M. Alvarez, and X. Giro-i Nieto, \u201cClass-weighted\nconvolutional features for visual instance search,\u201d arXiv preprint\narXiv:1707.02581 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.02581",
                        "Citation Paper Title": "Title:Class-Weighted Convolutional Features for Visual Instance Search",
                        "Citation Paper Abstract": "Abstract:Image retrieval in realistic scenarios targets large dynamic datasets of unlabeled images. In these cases, training or fine-tuning a model every time new images are added to the database is neither efficient nor scalable. Convolutional neural networks trained for image classification over large datasets have been proven effective feature extractors for image retrieval. The most successful approaches are based on encoding the activations of convolutional layers, as they convey the image spatial information. In this paper, we go beyond this spatial information and propose a local-aware encoding of convolutional features based on semantic information predicted in the target image. To this end, we obtain the most discriminative regions of an image using Class Activation Maps (CAMs). CAMs are based on the knowledge contained in the network and therefore, our approach, has the additional advantage of not requiring external information. In addition, we use CAMs to generate object proposals during an unsupervised re-ranking stage after a first fast search. Our experiments on two public available datasets for instance retrieval, Oxford5k and Paris6k, demonstrate the competitiveness of our approach outperforming the current state-of-the-art when using off-the-shelf models trained on ImageNet. The source code and model used in this paper are publicly available at this http URL.",
                        "Citation Paper Authors": "Authors:Albert Jimenez, Jose M. Alvarez, Xavier Giro-i-Nieto"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "is\ntrained for instance-level retrieval of image- and region-wise\nrepresentations pooled from an object detection CNN. In this\nexperiment, we take advantage of the object proposals learned\nby SOLO ",
                    "Citation Text": "X. Wang, T. Kong, C. Shen, Y . Jiang, and L. Li, \u201cSolo: Segmenting\nobjects by locations,\u201d arXiv preprint arXiv:1912.04488 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04488",
                        "Citation Paper Title": "Title:SOLO: Segmenting Objects by Locations",
                        "Citation Paper Abstract": "Abstract:We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of \"instance categories\", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.",
                        "Citation Paper Authors": "Authors:Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", which\ngenerated a set of regional vectors by performing spatial max-pooling\nwithin a particular region and aggregates features from several local\nregions into a single compact feature. Gordo et al. ",
                    "Citation Text": "A. Gordo, J. Almazan, J. Revaud, and D. Larlus, \u201cEnd-to-end learning\nof deep visual representations for image retrieval,\u201d International Journal\nof Computer Vision , vol. 124, no. 2, pp. 237\u2013254, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.07940",
                        "Citation Paper Title": "Title:End-to-end Learning of Deep Visual Representations for Image Retrieval",
                        "Citation Paper Abstract": "Abstract:While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues.\nFirst, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material, please see this http URL.",
                        "Citation Paper Authors": "Authors:Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "improved\nover the original R-MAC encoding by explicitly learning a region\nproposal network ",
                    "Citation Text": "S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\nobject detection with region proposal networks,\u201d in Advances in neural\ninformation processing systems , pp. 91\u201399, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01497",
                        "Citation Paper Title": "Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08716v1": {
            "Paper Title": "Discovering the Information that is lost in our Databases -- Why bother\n  storing data if you can't find the information?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00404v6": {
            "Paper Title": "Detecting Beneficial Feature Interactions for Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09211v3": {
            "Paper Title": "Multi-Interest-Aware User Modeling for Large-Scale Sequential\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "uses an RNN with GRU cell to generate user representations\nwith user browsing histories as input sequences, which has been\nsuccessfully deployed to a news recommendation service. ",
                    "Citation Text": "Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,\nand Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate\nPrediction. In Thirty-Third AAAI Conference on Artificial Intelligence .Woodstock \u201918, June 03\u201305, 2018, Woodstock, NY Jianxun Lian, Iyad Batal, et al.\nA APPENDIX\nA.1 Dataset Details\nHere we describe the complete setting of datasets.\nDisplay Ads Dataset . The SUM model is originally designed\nto support our online display advertising business in Bing Native\nAds. We collect two weeks\u2019 ads clicking logs as data samples, and\ncollect users\u2019 web behavior history prior to their corresponding ad\nclick behavior for user modeling. The data samples are split into\n70%/15%/15% as training/validation/test dataset by users to avoid\ninformation leakage caused by repeated user behaviors. For more\nefficient offline modeling training, the user behavior sequences are\ntruncated to 100. Some basic data statistics are reported in Table 1.\nFor each positive instance (which is an ad click behavior from one\nuser), we sample 1 negative instance from non-click impression,\nand randomly sample 3 negative instances by item popularity. Each\nweb browsing behavior is represented by its web page title, e.g.,\n\u201cWhy Are People Rushing To Get This Stylish New SmartWatch? The\nHealth Benefits Are Incredible\u201d . We use the CDSSM",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03672",
                        "Citation Paper Title": "Title:Deep Interest Evolution Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate~(CTR) prediction, whose goal is to estimate the probability of the user clicks, has become one of the core tasks in advertising systems. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, few work consider the changing trend of interest. In this paper, we propose a novel model, named Deep Interest Evolution Network~(DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7\\% improvement on CTR.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, Kun Gai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2004.14162v2": {
            "Paper Title": "Conversations with Search Engines: SERP-based Conversational Response\n  Generation",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". However, the studies listed above mostly target fact-centric information needs that\ncan be answered by entity mentions or triples from a knowledge base ( KB) ",
                    "Citation Text": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised\nChallenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (ACL\u201917) . 1601\u20131611.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.03551",
                        "Citation Paper Title": "Title:TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL",
                        "Citation Paper Authors": "Authors:Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "target complex questions in KB-QA, which involve joining multiple\nrelations and multi-hop reasoning in a conversational setting [ 18,49]. Qu et al . ",
                    "Citation Text": "Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. 2020. Open-Retrieval Conversational\nQuestion Answering. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR\u201920) . 539\u2013548.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11364",
                        "Citation Paper Title": "Title:Open-Retrieval Conversational Question Answering",
                        "Citation Paper Abstract": "Abstract:Conversational search is one of the ultimate goals of information retrieval. Recent research approaches conversational search by simplified settings of response ranking and conversational question answering, where an answer is either selected from a given candidate set or extracted from a given passage. These simplifications neglect the fundamental role of retrieval in conversational search. To address this limitation, we introduce an open-retrieval conversational question answering (ORConvQA) setting, where we learn to retrieve evidence from a large collection before extracting answers, as a further step towards building functional conversational search systems. We create a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an end-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader that are all based on Transformers. Our extensive experiments on OR-QuAC demonstrate that a learnable retriever is crucial for ORConvQA. We further show that our system can make a substantial improvement when we enable history modeling in all system components. Moreover, we show that the reranker component contributes to the model performance by providing a regularization effect. Finally, further in-depth analyses are performed to provide new insights into ORConvQA.",
                        "Citation Paper Authors": "Authors:Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, Mohit Iyyer"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", which introduce more challenges related to conversational understanding. For\nexample, Christmann et al . ",
                    "Citation Text": "Philipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, and Gerhard Weikum. 2019. Look\nbefore You Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion. In\nProceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM\u201919) . 729\u2013738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03262",
                        "Citation Paper Title": "Title:Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion",
                        "Citation Paper Abstract": "Abstract:Fact-centric information needs are rarely one-shot; users typically ask follow-up questions to explore a topic. In such a conversational setting, the user's inputs are often incomplete, with entities or predicates left out, and ungrammatical phrases. This poses a huge challenge to question answering (QA) systems that typically rely on cues in full-fledged interrogative sentences. As a solution, we develop CONVEX: an unsupervised method that can answer incomplete questions over a knowledge graph (KG) by maintaining conversation context using entities and predicates seen so far and automatically inferring missing or ambiguous pieces for follow-up questions. The core of our method is a graph exploration algorithm that judiciously expands a frontier to find candidate answers for the current question. To evaluate CONVEX, we release ConvQuestions, a crowdsourced benchmark with 11,200 distinct conversations from five different domains. We show that CONVEX: (i) adds conversational support to any stand-alone QA system, and (ii) outperforms state-of-the-art baselines and question completion strategies.",
                        "Citation Paper Authors": "Authors:Philipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, Gerhard Weikum"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", or extract a span from a given passage as direct answers ",
                    "Citation Text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine\nComprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing\n(EMNLP\u201916) . 2383\u20132392.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "formulate the task of asking clarifying questions in information retrieval. Bi et al . ",
                    "Citation Text": "Keping Bi, Qingyao Ai, Yongfeng Zhang, and W. Bruce Croft. 2019. Conversational Product Search Based on Negative\nFeedback. In Proceedings of the 28th ACM International Conference on Information & Knowledge Management (CIKM\u201919) .\n359\u2013368.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02071",
                        "Citation Paper Title": "Title:Conversational Product Search Based on Negative Feedback",
                        "Citation Paper Abstract": "Abstract:Intelligent assistants change the way people interact with computers and make it possible for people to search for products through conversations when they have purchase needs. During the interactions, the system could ask questions on certain aspects of the ideal products to clarify the users' needs. For example, previous work proposed to ask users the exact characteristics of their ideal items before showing results. However, users may not have clear ideas about what an ideal item looks like, especially when they have not seen any item. So it is more feasible to facilitate the conversational search by showing example items and asking for feedback instead. In addition, when the users provide negative feedback for the presented items, it is easier to collect their detailed feedback on certain properties (aspect-value pairs) of the non-relevant items. By breaking down the item-level negative feedback to fine-grained feedback on aspect-value pairs, more information is available to help clarify users' intents. So in this paper, we propose a conversational paradigm for product search driven by non-relevant items, based on which fine-grained feedback is collected and utilized to show better results in the next iteration. We then propose an aspect-value likelihood model to incorporate both positive and negative feedback on fine-grained aspect-value pairs of the non-relevant items. Experimental results show that our model is significantly better than state-of-the-art product search baselines without using feedback and those baselines using item-level negative feedback.",
                        "Citation Paper Authors": "Authors:Keping Bi, Qingyao Ai, Yongfeng Zhang, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "initiate a path\nbreaking Estimation\u2014Action\u2014Reflection framework to achieve deep interaction between recom-\nmendation and conversation ",
                    "Citation Text": "Xiangnan He Yisong Miao Xiang Wang Liang Chen Wenqiang Lei, Gangyi Zhang and Tat-Seng Chua. 2020. Interactive\nPath Reasoning on Graph for Conversational Recommendation. In Proceedings of the 2020 ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining (KDD\u201920) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00194",
                        "Citation Paper Title": "Title:Interactive Path Reasoning on Graph for Conversational Recommendation",
                        "Citation Paper Abstract": "Abstract:Traditional recommendation systems estimate user preference on items from past interaction history, thus suffering from the limitations of obtaining fine-grained and dynamic user preference. Conversational recommendation system (CRS) brings revolutions to those limitations by enabling the system to directly ask users about their preferred attributes on items. However, existing CRS methods do not make full use of such advantage -- they only use the attribute feedback in rather implicit ways such as updating the latent user representation. In this paper, we propose Conversational Path Reasoning (CPR), a generic framework that models conversational recommendation as an interactive path reasoning problem on a graph. It walks through the attribute vertices by following user feedback, utilizing the user preferred attributes in an explicit way. By leveraging on the graph structure, CPR is able to prune off many irrelevant candidate attributes, leading to better chance of hitting user preferred attributes. To demonstrate how CPR works, we propose a simple yet effective instantiation named SCPR (Simple CPR). We perform empirical studies on the multi-round conversational recommendation scenario, the most realistic CRS setting so far that considers multiple rounds of asking attributes and recommending items. Through extensive experiments on two datasets Yelp and LastFM, we validate the effectiveness of our SCPR, which significantly outperforms the state-of-the-art CRS methods EAR (arXiv:2002.09102) and CRM (arXiv:1806.03277). In particular, we find that the more attributes there are, the more advantages our method can achieve.",
                        "Citation Paper Authors": "Authors:Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "devise a System Ask-User Respond paradigm for CS, and design a memory\nnetwork for product search and recommendation in e-commerce. Lei et al . ",
                    "Citation Text": "Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020.\nEstimation-action-reflection: Towards deep interaction between conversational and recommender systems. In Proceed-\nings of the 13th International Conference on Web Search and Data Mining (WSDM\u201920) . 304\u2013312.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.09102",
                        "Citation Paper Title": "Title:Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems are embracing conversational technologies to obtain user preferences dynamically, and to overcome inherent limitations of their static models. A successful Conversational Recommender System (CRS) requires proper handling of interactions between conversation and recommendation. We argue that three fundamental problems need to be solved: 1) what questions to ask regarding item attributes, 2) when to recommend items, and 3) how to adapt to the users' online feedback. To the best of our knowledge, there lacks a unified framework that addresses these problems.\nIn this work, we fill this missing interaction framework gap by proposing a new CRS framework named Estimation-Action-Reflection, or EAR, which consists of three stages to better converse with users. (1) Estimation, which builds predictive models to estimate user preference on both items and item attributes; (2) Action, which learns a dialogue policy to determine whether to ask attributes or recommend items, based on Estimation stage and conversation history; and (3) Reflection, which updates the recommender model when a user rejects the recommendations made by the Action stage. We present two conversation scenarios on binary and enumerated questions, and conduct extensive experiments on two datasets from Yelp and LastFM, for each scenario, respectively. Our experiments demonstrate significant improvements over the state-of-the-art method CRM [32], corresponding to fewer conversation turns and a higher level of recommendation hits.",
                        "Citation Paper Authors": "Authors:Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.08301v1": {
            "Paper Title": "Wizard of Search Engine: Access to Information Through Conversations\n  with Search Engines",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "re-\nlease the MISC dataset, which is built by recording pairs of a seeker\nand intermediary collaborating on CIS. The work most closely re-\nlated to ours is by Trippas et al . ",
                    "Citation Text": "Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo Joho,\nand Lawrence Cavedon. 2020. Towards a Model for Spoken Conversational\nSearch. Inf. Process. Manag. 57 (2020), 102162.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.13166",
                        "Citation Paper Title": "Title:Towards a Model for Spoken Conversational Search",
                        "Citation Paper Abstract": "Abstract:Conversation is the natural mode for information exchange in daily life, a spoken conversational interaction for search input and output is a logical format for information seeking. However, the conceptualisation of user-system interactions or information exchange in spoken conversational search (SCS) has not been explored. The first step in conceptualising SCS is to understand the conversational moves used in an audio-only communication channel for search. This paper explores conversational actions for the task of search. We define a qualitative methodology for creating conversational datasets, propose analysis protocols, and develop the SCSdata. Furthermore, we use the SCSdata to create the first annotation schema for SCS: the SCoSAS, enabling us to investigate interactivity in SCS. We further establish that SCS needs to incorporate interactivity and pro-activity to overcome the complexity that the information seeking process in an audio-only channel poses. In summary, this exploratory study unpacks the breadth of SCS. Our results highlight the need for integrating discourse in future SCS models and contributes the advancement in the formalisation of SCS models and the design of SCS systems.",
                        "Citation Paper Authors": "Authors:Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo Joho, Lawrence Cavedon"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "release the CAsT dataset, which aims\nto establish a concrete and standard collection of data to make\nCISsystems directly comparable. Ren et al . ",
                    "Citation Text": "Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz,\nand Maarten de Rijke. 2020. Conversations with Search Engines: SERP-based\nConversational Response Generation. arXiv preprint arXiv:2004.14162 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14162",
                        "Citation Paper Title": "Title:Conversations with Search Engines: SERP-based Conversational Response Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of answering complex information needs by conversing conversations with search engines, in the sense that users can express their queries in natural language, and directly receivethe information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs, or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.\nWe pursue two goals in this paper: (1) the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and (2) the development of astate-of-the-art pipeline for conversations with search engines, the Conversations with Search Engines (CaSE), using this dataset. SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and aprior-aware pointer generator, which enables us to generate more accurate responses.\nWe carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.",
                        "Citation Paper Authors": "Authors:Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz, Maarten de Rijke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.07203v2": {
            "Paper Title": "Deep Retrieval: Learning A Retrievable Structure for Large-Scale\n  Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.08246v1": {
            "Paper Title": "Path-based Deep Network for Candidate Item Matching in Recommenders",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "extends user\nfeatures by average pooling user behavior, while BST ",
                    "Citation Text": "Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior\nsequence transformer for e-commerce recommendation in alibaba. In DLP-KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06874",
                        "Citation Paper Title": "Title:Behavior Sequence Transformer for E-commerce Recommendation in Alibaba",
                        "Citation Paper Abstract": "Abstract:Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding&MLP paradigm: raw features are embedded into low-dimensional vectors, which are then fed on to MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users' behaviors. In this paper, we propose to use the powerful Transformer model to capture the sequential signals underlying users' behavior sequences for recommendation in Alibaba. Experimental results demonstrate the superiority of the proposed model, which is then deployed online at Taobao and obtain significant improvements in online Click-Through-Rate (CTR) comparing to two baselines.",
                        "Citation Paper Authors": "Authors:Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "propose NAIS with an attention\nmechanism to distinguish the different importance of historical\nitems in a user profile, which shares a similar idea with DIN ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-\nThrough Rate Prediction. In SIGKDD . Association for Computing Machinery,\nNew York, NY, USA.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "for performance evaluation. The statistics of the\nthree datasets are summarized in Table 1. Following the settings\nof ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In WWW .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.07975v1": {
            "Paper Title": "How Deep is your Learning: the DL-HARD Annotated Deep Learning Dataset",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "constructed a series of QA benchmarks beginning in the late 1990s.\nMore recently, the SQuAD ",
                    "Citation Text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Processing .\nAustin, Texas, 2383\u20132392.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.07826v1": {
            "Paper Title": "TopicsRanksDC: Distance-based Topic Ranking applied on Two-Class Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.07542v1": {
            "Paper Title": "Collaborative Graph Learning with Auxiliary Text for Temporal Event\n  Prediction in Healthcare",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11589v2": {
            "Paper Title": "Recommendation on a Budget: Column Space Recovery from Partially\n  Observed Entries with Random or Active Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04864v3": {
            "Paper Title": "A Survey on Session-based Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 107,
                    "Sentence": ".\nIn addition to the basic GRU4Rec, there are also many variants. To improve GRU4Rec, Tan\net al. ",
                    "Citation Text": "Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural networks for session-based recommen-\ndations. In DLRS . ACM, 17\u201322.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.08117",
                        "Citation Paper Title": "Title:Improved Recurrent Neural Networks for Session-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.",
                        "Citation Paper Authors": "Authors:Yong Kiam Tan, Xinxing Xu, Yong Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.03986v2": {
            "Paper Title": "Advising Agent for Service-Providing Live-Chat Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.09293v1": {
            "Paper Title": "Lessons Learned Addressing Dataset Bias in Model-Based Candidate\n  Generation at Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.06441v1": {
            "Paper Title": "DeepQAMVS: Query-Aware Hierarchical Pointer Networks for Multi-Video\n  Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11530v2": {
            "Paper Title": "Supervised Video Summarization via Multiple Feature Sets with Parallel\n  Attention",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "a\nmulti-concept video self-attention where attention is applied\nto multiple layers of encoder and decoder. The majority of\nprevious work [2, 3, 11] use pre-trained image features from\nGoogleNet ",
                    "Citation Text": "Christian Szegedy, Wei Liu, and Yangqing Jia et al.,\n\u201cGoing deeper with convolutions,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition, CVPR .\n2015, pp. 1\u20139, IEEE Computer Society.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.06323v1": {
            "Paper Title": "Bootstrapping User and Item Representations for One-Class Collaborative\n  Filtering",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ".\nOn the other hand, generative methods [ 1,18,19,31] aim to learn\nthe underlying latent distribution of users, usually represented by\nbinary vectors indicating their interacted items. They employ the\narchitecture of variational autoencoder (VAE) ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.\nVariational autoencoders for collaborative filtering. In WWW . 689\u2013698.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.05571v1": {
            "Paper Title": "\"Alexa, what do you do for fun?\" Characterizing playful requests with\n  virtual assistants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08191v2": {
            "Paper Title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05435v1": {
            "Paper Title": "UIUC_BioNLP at SemEval-2021 Task 11: A Cascade of Neural Models for\n  Structuring Scholarly NLP Contributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.06398v1": {
            "Paper Title": "\"Who can help me?\": Knowledge Infused Matching of Support Seekers and\n  Support Providers during COVID-19 on Reddit",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". Over the annotated dataset, we experimented with various\npipelines, generating contextualized embedding followed by\na classi\ufb01er to predict SP or not SP. For generating embed-\ndings of the posts/comments, we used: ELMo ",
                    "Citation Text": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d arXiv\npreprint arXiv:1802.05365 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.07749v2": {
            "Paper Title": "Boosting the Rating Prediction with Click Data and Textual Contents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05389v1": {
            "Paper Title": "Co-Factorization Model for Collaborative Filtering with Session-based\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05385v1": {
            "Paper Title": "A Statistical Model for Melody Reduction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05326v1": {
            "Paper Title": "Multi-version Tensor Completion for Time-delayed Spatio-temporal Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08542v2": {
            "Paper Title": "ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models\n  with Huge Embedding Table",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01166v3": {
            "Paper Title": "Learning Representations from Product Titles for Modeling Shopping\n  Transactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03299v2": {
            "Paper Title": "Leveraging Multiple Relations for Fashion Trend Forecasting Based on\n  Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04761v1": {
            "Paper Title": "Federated Unbiased Learning to Rank",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "and use Inverse Propensity Score ( IPS) to compen-\nsate the difference between true relevance and clicks [ 1,2,14,25].\nAgarwal et al . ",
                    "Citation Text": "Aman Agarwal, Kenta Takatsu, Ivan Zaitsev, and Thorsten Joachims. 2019. A\nGeneral Framework for Counterfactual Learning-to-Rank. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.00065",
                        "Citation Paper Title": "Title:A General Framework for Counterfactual Learning-to-Rank",
                        "Citation Paper Abstract": "Abstract:Implicit feedback (e.g., click, dwell time) is an attractive source of training data for Learning-to-Rank, but its naive use leads to learning results that are distorted by presentation bias. For the special case of optimizing average rank for linear ranking functions, however, the recently developed SVM-PropRank method has shown that counterfactual inference techniques can be used to provably overcome the distorting effect of presentation bias. Going beyond this special case, this paper provides a general and theoretically rigorous framework for counterfactual learning-to-rank that enables unbiased training for a broad class of additive ranking metrics (e.g., Discounted Cumulative Gain (DCG)) as well as a broad class of models (e.g., deep networks). Specifically, we derive a relaxation for propensity-weighted rank-based metrics which is subdifferentiable and thus suitable for gradient-based optimization. We demonstrate the effectiveness of this general approach by instantiating two new learning methods. One is a new type of unbiased SVM that optimizes DCG -- called SVM PropDCG --, and we show how the resulting optimization problem can be solved via the Convex Concave Procedure (CCP). The other is Deep PropDCG, where the ranking function can be an arbitrary deep network. In addition to the theoretical support, we empirically find that SVM PropDCG significantly outperforms existing linear rankers in terms of DCG. Moreover, the ability to train non-linear ranking functions via Deep PropDCG further improves performance.",
                        "Citation Paper Authors": "Authors:Aman Agarwal, Kenta Takatsu, Ivan Zaitsev, Thorsten Joachims"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "uses FLto solve the\nURL suggestion task for web browsing experience. Anelli et al . ",
                    "Citation Text": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara, and\nFedelucio Narducci. 2021. FedeRank: User Controlled Feedback with Federated\nRecommender Systems. arXiv:2012.11328 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11328",
                        "Citation Paper Title": "Title:FedeRank: User Controlled Feedback with Federated Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems have shown to be a successful representative of how data availability can ease our everyday digital life. However, data privacy is one of the most prominent concerns in the digital era. After several data breaches and privacy scandals, the users are now worried about sharing their data. In the last decade, Federated Learning has emerged as a new privacy-preserving distributed machine learning paradigm. It works by processing data on the user device without collecting data in a central repository. We present FedeRank (this https URL), a federated recommendation algorithm. The system learns a personal factorization model onto every device. The training of the model is a synchronous process between the central server and the federated clients. FedeRank takes care of computing recommendations in a distributed fashion and allows users to control the portion of data they want to share. By comparing with state-of-the-art algorithms, extensive experiments show the effectiveness of FedeRank in terms of recommendation accuracy, even with a small portion of shared user data. Further analysis of the recommendation lists' diversity and novelty guarantees the suitability of the algorithm in real production environments.",
                        "Citation Paper Authors": "Authors:Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara, Fedelucio Narducci"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "studies the federated online LTR\nand proposed FOLtR . Hartmann et al . ",
                    "Citation Text": "Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D Smith, and Ilana\nSegall. 2019. Federated Learning for Ranking Browser History Suggestions. arXiv\npreprint arXiv:1911.11807 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11807",
                        "Citation Paper Title": "Title:Federated Learning for Ranking Browser History Suggestions",
                        "Citation Paper Abstract": "Abstract:Federated Learning is a new subfield of machine learning that allows fitting models without collecting the training data itself. Instead of sharing data, users collaboratively train a model by only sending weight updates to a server. To improve the ranking of suggestions in the Firefox URL bar, we make use of Federated Learning to train a model on user interactions in a privacy-preserving way. This trained model replaces a handcrafted heuristic, and our results show that users now type over half a character less to find what they are looking for. To be able to deploy our system to real users without degrading their experience during training, we design the optimization process to be robust. To this end, we use a variant of Rprop for optimization, and implement additional safeguards. By using a numerical gradient approximation technique, our system is able to optimize anything in Firefox that is currently based on handcrafted heuristics. Our paper shows that Federated Learning can be used successfully to train models in privacy-respecting ways.",
                        "Citation Paper Authors": "Authors:Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D. Smith, Ilana Segall"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.08252v2": {
            "Paper Title": "Entity Summarization: State of the Art and Future Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11384v2": {
            "Paper Title": "APRF-Net: Attentive Pseudo-Relevance Feedback Network for Query\n  Categorization",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ".\n\u2022BiLSTM+SCE: Bidirectional recurrent neural network.\n\u2022FastText: Bag of tricks for efficient text classification ",
                    "Citation Text": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching word vectors with\nsubword information. TACL, ACL , 5:135\u2013146, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". It has been shown that query\ncategorization could play a pivotal role in increasing user satisfac-\ntion by returning more relevant products in e-commerce search ",
                    "Citation Text": "H. Zhang, X. Song, C. Xiong, C. Rosset, P. N. Bennett, N. Craswell, and S. Tiwary.\nGeneric intent representation in web search. In SIGIR , pages 65\u201374. ACM, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10710",
                        "Citation Paper Title": "Title:Generic Intent Representation in Web Search",
                        "Citation Paper Abstract": "Abstract:This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a distributed representation space for user intent in search. Leveraging large scale user clicks from Bing search logs as weak supervision of user intent, GEN Encoder learns to map queries with shared clicks into similar embeddings end-to-end and then finetunes on multiple paraphrase tasks. Experimental results on an intrinsic evaluation task - query intent similarity modeling - demonstrate GEN Encoder's robust and significant advantages over previous representation methods. Ablation studies reveal the crucial role of learning from implicit user feedback in representing user intent and the contributions of multi-task learning in representation generality. We also demonstrate that GEN Encoder alleviates the sparsity of tail search traffic and cuts down half of the unseen queries by using an efficient approximate nearest neighbor search to effectively identify previous queries with the same search intent. Finally, we demonstrate distances between GEN encodings reflect certain information seeking behaviors in search sessions.",
                        "Citation Paper Authors": "Authors:Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N. Bennett, Nick Craswell, Saurabh Tiwary"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.04376v1": {
            "Paper Title": "Recommendations for Item Set Completion: On the Semantics of Item\n  Co-Occurrence With Data Sparsity, Input Size, and Input Modalities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04266v1": {
            "Paper Title": "A Probabilistic Approach to Personalize Type-based Facet Ranking for POI\n  Suggestion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04183v1": {
            "Paper Title": "UGRec: Modeling Directed and Undirected Relations for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ", the item co-occurrence relations\nare modeled as the same as the directed knowledge relations by\nusing TransE. Xin et al. ",
                    "Citation Text": "Xin Xin, Xiangnan He, Yongfeng Zhang, Yongdong Zhang, and Joemon Jose.\n2019. Relational Collaborative Filtering: Modeling Multiple Item Relations for\nRecommendation. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval . ACM, 125\u2013134.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12796",
                        "Citation Paper Title": "Title:Relational Collaborative Filtering:Modeling Multiple Item Relations for Recommendation",
                        "Citation Paper Abstract": "Abstract:Existing item-based collaborative filtering (ICF) methods leverage only the relation of collaborative similarity. Nevertheless, there exist multiple relations between items in real-world scenarios. Distinct from the collaborative similarity that implies co-interact patterns from the user perspective, these relations reveal fine-grained knowledge on items from different perspectives of meta-data, functionality, etc. However, how to incorporate multiple item relations is less explored in recommendation research. In this work, we propose Relational Collaborative Filtering (RCF), a general framework to exploit multiple relations between items in recommender system. We find that both the relation type and the relation value are crucial in inferring user preference. To this end, we develop a two-level hierarchical attention mechanism to model user preference. The first-level attention discriminates which types of relations are more important, and the second-level attention considers the specific relation values to estimate the contribution of a historical item in recommending the target item. To make the item embeddings be reflective of the relational structure between items, we further formulate a task to preserve the item relations, and jointly train it with the recommendation task of preference modeling. Empirical results on two real datasets demonstrate the strong performance of RCF. Furthermore, we also conduct qualitative analyses to show the benefits of explanations brought by the modeling of multiple item relations.",
                        "Citation Paper Authors": "Authors:Xin Xin, Xiangnan He, Yongfeng Zhang, Yongdong Zhang, Joemon Jose"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "combines the knowledge-aware embedding with\nthe word embedding of each word within the news content for\nnews recommendation. Zhang et al. ",
                    "Citation Text": "Yongfeng Zhang, Qingyao Ai, Xu Chen, and Pengfei Wang. 2018. Learning over\nKnowledge-Base Embeddings for Recommendation. In Proceedings of the 41st\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval . ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06540",
                        "Citation Paper Title": "Title:Learning over Knowledge-Base Embeddings for Recommendation",
                        "Citation Paper Abstract": "Abstract:State-of-the-art recommendation algorithms -- especially the collaborative filtering (CF) based approaches with shallow or deep models -- usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely neglected recently due to the availability of vast amount of data, and the learning power of many complex models.\nHowever, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors. A great challenge for using knowledge bases for recommendation is how to integrated large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements on knowledge base embedding sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge. In this work, we propose to reason over knowledge base embeddings for personalized recommendation. Specifically, we propose a knowledge base representation learning approach to embed heterogeneous entities for recommendation. Experimental results on real-world dataset verified the superior performance of our approach compared with state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Yongfeng Zhang, Qingyao Ai, Xu Chen, Pengfei Wang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "combines the advantages\nof the embedding-based and path-based methods and achieves a\nremarkable performance improvement.A more comprehensive review for KG-based recommendation\ncan refer to ",
                    "Citation Text": "Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong,\nand Qing He. 2020. A Survey on Knowledge Graph-Based Recommender Systems.\nCoRR abs/2003.00911 (2020). arXiv:2003.00911 https://arxiv.org/abs/2003.00911",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00911",
                        "Citation Paper Title": "Title:A Survey on Knowledge Graph-Based Recommender Systems",
                        "Citation Paper Abstract": "Abstract:To solve the information explosion problem and enhance user experience in various online applications, recommender systems have been developed to model users preferences. Although numerous efforts have been made toward more personalized recommendations, recommender systems still suffer from several challenges, such as data sparsity and cold start. In recent years, generating recommendations with the knowledge graph as side information has attracted considerable interest. Such an approach can not only alleviate the abovementioned issues for a more accurate recommendation, but also provide explanations for recommended items. In this paper, we conduct a systematical survey of knowledge graph-based recommender systems. We collect recently published papers in this field and summarize them from two perspectives. On the one hand, we investigate the proposed algorithms by focusing on how the papers utilize the knowledge graph for accurate and explainable recommendation. On the other hand, we introduce datasets used in these works. Finally, we propose several potential research directions in this field.",
                        "Citation Paper Authors": "Authors:Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, Qing He"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "reported a knowledge graph attention network (KGAT) to model\nthe high-order connectivities in KG with an end-to-end fashion.\nWu et al. ",
                    "Citation Text": "Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, Yanjie Fu, and Meng Wang.\n2020. Joint Item Recommendation and Attribute Inference: An Adaptive Graph\nConvolutional Network Approach. In Proceedings of the 43rd International ACM\nSIGIR conference on research and development in Information Retrieval . ACM,\n679\u2013688.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12021",
                        "Citation Paper Title": "Title:Joint Item Recommendation and Attribute Inference: An Adaptive Graph Convolutional Network Approach",
                        "Citation Paper Abstract": "Abstract:In many recommender systems, users and items are associated with attributes, and users show preferences to items. The attribute information describes users'(items') characteristics and has a wide range of applications, such as user profiling, item annotation, and feature-enhanced recommendation. As annotating user (item) attributes is a labor intensive task, the attribute values are often incomplete with many missing attribute values. Therefore, item recommendation and attribute inference have become two main tasks in these platforms. Researchers have long converged that user (item) attributes and the preference behavior are highly correlated. Some researchers proposed to leverage one kind of data for the remaining task, and showed to improve performance. Nevertheless, these models either neglected the incompleteness of user (item) attributes or regarded the correlation of the two tasks with simple models, leading to suboptimal performance of these two tasks. To this end, in this paper, we define these two tasks in an attributed user-item bipartite graph, and propose an Adaptive Graph Convolutional Network (AGCN) approach for joint item recommendation and attribute inference. The key idea of AGCN is to iteratively perform two parts: 1) Learning graph embedding parameters with previously learned approximated attribute values to facilitate two tasks; 2) Sending the approximated updated attribute values back to the attributed graph for better graph embedding learning. Therefore, AGCN could adaptively adjust the graph embedding learning parameters by incorporating both the given attributes and the estimated attribute values, in order to provide weakly supervised information to refine the two tasks. Extensive experimental results on three real-world datasets clearly show the effectiveness of the proposed model.",
                        "Citation Paper Authors": "Authors:Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, Yanjie Fu, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "adopted the TransE to\nmodel both the directed knowledge and undirected co-occurrence\nrelations for entity embedding learning. Cao et al. ",
                    "Citation Text": "Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019.\nUnifying Knowledge Graph Learning and Recommendation: Towards a Better\nUnderstanding of User Preferences. In Proceedings of the 28th International Con-\nference on World Wide Web . IW3C2, 151\u2013161.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06236",
                        "Citation Paper Title": "Title:Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences",
                        "Citation Paper Abstract": "Abstract:Incorporating knowledge graph (KG) into recommender system is promising in improving the recommendation accuracy and explainability. However, existing methods largely assume that a KG is complete and simply transfer the \"knowledge\" in KG at the shallow level of entity raw data or embeddings. This may lead to suboptimal performance, since a practical KG can hardly be complete, and it is common that a KG has missing facts, relations, and entities. Thus, we argue that it is crucial to consider the incomplete nature of KG when incorporating it into recommender system.\nIn this paper, we jointly learn the model of recommendation and knowledge graph completion. Distinct from previous KG-based recommendation methods, we transfer the relation information in KG, so as to understand the reasons that a user likes an item. As an example, if a user has watched several movies directed by (relation) the same person (entity), we can infer that the director relation plays a critical role when the user makes the decision, thus help to understand the user's preference at a finer granularity.\nTechnically, we contribute a new translation-based recommendation model, which specially accounts for various preferences in translating a user to an item, and then jointly train it with a KG completion model by combining several transfer schemes. Extensive experiments on two benchmark datasets show that our method outperforms state-of-the-art KG-based recommendation methods. Further analysis verifies the positive effect of joint training on both tasks of recommendation and KG completion, and the advantage of our model in understanding user preference. We publish our project at this https URL.",
                        "Citation Paper Authors": "Authors:Yixin Cao, Xiang Wang, Xiangnan He, Zikun hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "applies TransR to\nlearn the semantic embedding from KG and integrates the learned\nembedding with the latent factor learned from MF for recommen-\ndation. DKE ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nKnowledge-Aware Network for News Recommendation. In Proceedings of the\n2018 World Wide Web Conference on World Wide Web . ACM, 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.11760v2": {
            "Paper Title": "DeepCAT: Deep Category Representation for Query Understanding in\n  E-commerce Search",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "introduce the idea of heterogeneous text net-\nwork embedding to model the word and label interactions. Guoyin\net al. ",
                    "Citation Text": "G. Wang, C. Li, W. Wang, Y. Zhang, D. Shen, X. Zhang, R. Henao, and L. Carin.\nJoint embedding of words and labels for text classification. arXiv preprint\narXiv:1805.04174 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04174",
                        "Citation Paper Title": "Title:Joint Embedding of Words and Labels for Text Classification",
                        "Citation Paper Abstract": "Abstract:Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.",
                        "Citation Paper Authors": "Authors:Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ".\nMethods Compared: We summarize the multi-label classifica-\ntion methods compared in the experimental results.\n\u2022TF-IDF + SVM: One-Vs-Rest SVM with a linear kernel.\n\u2022FastText: Text classification method by Facebook ",
                    "Citation Text": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching word vectors\nwith subword information. Transactions of the Association for Computational\nLinguistics , 5:135\u2013146, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", we propose a joint word-category (label) representation to\nprovide both word and category embeddings. Then, category rep-\nresentations are leveraged to boost the model\u2019s efficiency on both\ntailqueries and the minority classes.\nTang et al. ",
                    "Citation Text": "J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale\nheterogeneous text networks. In Proceedings of the 21th ACM SIGKDD inter-\nnational conference on knowledge discovery and data mining , pages 1165\u20131174,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.00200",
                        "Citation Paper Title": "Title:PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \\textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Qiaozhu Mei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.04021v1": {
            "Paper Title": "MS MARCO: Benchmarking Ranking Models in the Large-Data Regime",
            "Sentences": [
                {
                    "Sentence ID": 92,
                    "Sentence": "Xinyu Zhang, Andrew Yates, and Jimmy Lin. 2020. A Little Bit Is Worse Than\nNone: Ranking with Limited Training Data. In Proceedings of SustaiNLP: Workshop\non Simple and Efficient Natural Language Processing . 107\u2013112. ",
                    "Citation Text": "Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERT-\nQE: contextualized query expansion for document re-ranking. arXiv preprint\narXiv:2009.07258 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07258",
                        "Citation Paper Title": "Title:BERT-QE: Contextualized Query Expansion for Document Re-ranking",
                        "Citation Paper Abstract": "Abstract:Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models.",
                        "Citation Paper Authors": "Authors:Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "Edwin Zhang, Nikhil Gupta, Rodrigo Nogueira, Kyunghyun Cho, and Jimmy Lin.\n2020. Rapidly deploying a neural search engine for the covid-19 open research\ndataset: Preliminary thoughts and lessons learned. arXiv preprint arXiv:2004.05125\n(2020). ",
                    "Citation Text": "Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, Kuang Lu,\nYue Zhang, Rodrigo Nogueira, Kyunghyun Cho, Hui Fang, et al .2020. Covidex:\nNeural ranking models and keyword search infrastructure for the covid-19 open\nresearch dataset. arXiv preprint arXiv:2007.07846 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.07846",
                        "Citation Paper Title": "Title:Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset",
                        "Citation Paper Abstract": "Abstract:We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the ongoing TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the highest-scoring runs in rounds 1, 2, and 3. In round 3, we report the highest-scoring run that takes advantage of previous training data and the second-highest fully automatic run.",
                        "Citation Paper Authors": "Authors:Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, Kuang Lu, Yue Zhang, Rodrigo Nogueira, Kyunghyun Cho, Hui Fang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, and Saurabh Tiwary.\n2018. Neural ranking models with multiple document fields. In Proceedings of the\neleventh ACM international conference on web search and data mining . 700\u2013708. ",
                    "Citation Text": "Edwin Zhang, Nikhil Gupta, Rodrigo Nogueira, Kyunghyun Cho, and Jimmy Lin.\n2020. Rapidly deploying a neural search engine for the covid-19 open research\ndataset: Preliminary thoughts and lessons learned. arXiv preprint arXiv:2004.05125\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05125",
                        "Citation Paper Title": "Title:Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset: Preliminary Thoughts and Lessons Learned",
                        "Citation Paper Abstract": "Abstract:We present the Neural Covidex, a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. This web application exists as part of a suite of tools that we have developed over the past few weeks to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidence-based decision making and insight generation. This paper describes our initial efforts and offers a few thoughts about lessons we have learned along the way.",
                        "Citation Paper Authors": "Authors:Edwin Zhang, Nikhil Gupta, Rodrigo Nogueira, Kyunghyun Cho, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": "Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett, Jianfeng Gao,\nand Zhiyuan Liu. 2020. Few-Shot Generative Conversational Query Rewriting.\nInProceedings of the 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 1933\u20131936. ",
                    "Citation Text": "Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, and Saurabh Tiwary.\n2018. Neural ranking models with multiple document fields. In Proceedings of the\neleventh ACM international conference on web search and data mining . 700\u2013708.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.09174",
                        "Citation Paper Title": "Title:Neural Ranking Models with Multiple Document Fields",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have recently shown promise in the ad-hoc retrieval task. However, such models have often been based on one field of the document, for example considering document title only or document body only. Since in practice documents typically have multiple fields, and given that non-neural ranking models such as BM25F have been developed to take advantage of document structure, this paper investigates how neural models can deal with multiple document fields. We introduce a model that can consume short text fields such as document title and long text fields such as document body. It can also handle multi-instance fields with variable number of instances, for example where each document has zero or more instances of incoming anchor text. Since fields vary in coverage and quality, we introduce a masking method to handle missing field instances, as well as a field-level dropout method to avoid relying too much on any one field. As in the studies of non-neural field weighting, we find it is better for the ranker to score the whole document jointly, rather than generate a per-field score and aggregate. We find that different document fields may match different aspects of the query and therefore benefit from comparing with separate representations of the query text. The combination of techniques introduced here leads to a neural ranker that can take advantage of full document structure, including multiple instance and missing instance data, of variable length. The techniques significantly enhance the performance of the ranker, and also outperform a learning to rank baseline with hand-crafted features.",
                        "Citation Paper Authors": "Authors:Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, Saurabh Tiwary"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\nEnd-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\nInternational ACM SIGIR conference on research and development in information\nretrieval . 55\u201364. ",
                    "Citation Text": "Chenyan Xiong, Zhenghao Liu, Si Sun, Zhuyun Dai, Kaitao Zhang, Shi Yu,\nZhiyuan Liu, Hoifung Poon, Jianfeng Gao, and Paul Bennett. 2020. CMT in\nTREC-COVID Round 2: Mitigating the Generalization Gaps from Web to Special\nDomain Search. arXiv preprint arXiv:2011.01580 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.01580",
                        "Citation Paper Title": "Title:CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web to Special Domain Search",
                        "Citation Paper Abstract": "Abstract:Neural rankers based on deep pretrained language models (LMs) have been shown to improve many information retrieval benchmarks. However, these methods are affected by their the correlation between pretraining domain and target domain and rely on massive fine-tuning relevance labels. Directly applying pretraining methods to specific domains may result in suboptimal search quality because specific domains may have domain adaption problems, such as the COVID domain. This paper presents a search system to alleviate the special domain adaption problem. The system utilizes the domain-adaptive pretraining and few-shot learning technologies to help neural rankers mitigate the domain discrepancy and label scarcity problems. Besides, we also integrate dense retrieval to alleviate traditional sparse retrieval's vocabulary mismatch obstacle. Our system performs the best among the non-manual runs in Round 2 of the TREC-COVID task, which aims to retrieve useful information from scientific literature related to COVID-19. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Chenyan Xiong, Zhenghao Liu, Si Sun, Zhuyun Dai, Kaitao Zhang, Shi Yu, Zhiyuan Liu, Hoifung Poon, Jianfeng Gao, Paul Bennett"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang,\nDarrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill, et al .\n2020. Cord-19: The covid-19 open research dataset. ArXiv (2020). ",
                    "Citation Text": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\nEnd-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\nInternational ACM SIGIR conference on research and development in information\nretrieval . 55\u201364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06613",
                        "Citation Paper Title": "Title:End-to-End Neural Ad-hoc Ranking with Kernel Pooling",
                        "Citation Paper Abstract": "Abstract:This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.",
                        "Citation Paper Authors": "Authors:Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Lucy Lu Wang and Kyle Lo. 2020. Text mining approaches for dealing with the\nrapidly expanding literature on COVID-19. Briefings in Bioinformatics (2020). ",
                    "Citation Text": "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang,\nDarrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill, et al .\n2020. Cord-19: The covid-19 open research dataset. ArXiv (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.10706",
                        "Citation Paper Title": "Title:CORD-19: The COVID-19 Open Research Dataset",
                        "Citation Paper Abstract": "Abstract:The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.",
                        "Citation Paper Authors": "Authors:Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm, Boya Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, Sebastian Kohlmeier"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": "Nikos Voskarides, Dan Li, Andreas Panteli, and Pengjie Ren. 2019. ILPS at TREC\n2019 Conversational Assistant Track.. In TREC . ",
                    "Citation Text": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de\nRijke. 2020. Query resolution for conversational search with limited supervision.\nInProceedings of the 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 921\u2013930.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11723",
                        "Citation Paper Title": "Title:Query Resolution for Conversational Search with Limited Supervision",
                        "Citation Paper Abstract": "Abstract:In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.",
                        "Citation Paper Authors": "Authors:Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie Bao,\nZhiyuan Liu, and Paul Bennett. 2020. Meta Adaptive Neural Ranking with\nContrastive Synthetic Supervision. arXiv preprint arXiv:2012.14862 (2020). ",
                    "Citation Text": "Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. 2016. Learning\nlatent vector spaces for product search. In Proceedings of the 25th ACM interna-\ntional on conference on information and knowledge management . 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.07253",
                        "Citation Paper Title": "Title:Learning Latent Vector Spaces for Product Search",
                        "Citation Paper Abstract": "Abstract:We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.",
                        "Citation Paper Authors": "Authors:Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "Yastil Rughbeer, Anban W Pillay, and Edgar Jembere. 2021. Dataset Selection for\nTransfer Learning in Information Retrieval. In Southern African Conference for\nArtificial Intelligence Research . Springer, 53\u201365. ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al .\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision 115, 3 (2015), 211\u2013252.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al .1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995),\n109. ",
                    "Citation Text": "Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, and Saurabh Tiwary.\n2018. Optimizing query evaluations using reinforcement learning for web search.\nInThe 41st International ACM SIGIR Conference on Research & Development in\nInformation Retrieval . 1193\u20131196.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04410",
                        "Citation Paper Title": "Title:Optimizing Query Evaluations using Reinforcement Learning for Web Search",
                        "Citation Paper Abstract": "Abstract:In web search, typically a candidate generation step selects a small set of documents---from collections containing as many as billions of web pages---that are subsequently ranked and pruned before being presented to the user. In Bing, the candidate generation involves scanning the index using statically designed match plans that prescribe sequences of different match criteria and stopping conditions. In this work, we pose match planning as a reinforcement learning task and observe up to 20% reduction in index blocks accessed, with small or no degradation in the quality of the candidate sets.",
                        "Citation Paper Authors": "Authors:Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, Saurabh Tiwary"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\nits of transfer learning with a unified text-to-text transformer. arXiv preprint\narXiv:1910.10683 (2019). ",
                    "Citation Text": "Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz,\nand Maarten de Rijke. 2020. Conversations with search engines. arXiv preprint\narXiv:2004.14162 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14162",
                        "Citation Paper Title": "Title:Conversations with Search Engines: SERP-based Conversational Response Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of answering complex information needs by conversing conversations with search engines, in the sense that users can express their queries in natural language, and directly receivethe information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs, or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.\nWe pursue two goals in this paper: (1) the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and (2) the development of astate-of-the-art pipeline for conversations with search engines, the Conversations with Search Engines (CaSE), using this dataset. SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and aprior-aware pointer generator, which enables us to generate more accurate responses.\nWe carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.",
                        "Citation Paper Authors": "Authors:Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information\nretrieval metrics. In Proceedings of the 33rd international ACM SIGIR conference\non Research and development in information retrieval . 667\u2013674. ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\nits of transfer learning with a unified text-to-text transformer. arXiv preprint\narXiv:1910.10683 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a\npretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020). ",
                    "Citation Text": "Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document\nRanking with a Pretrained Sequence-to-Sequence Model. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 . Association for Computational\nLinguistics, Online, 708\u2013718. https://doi.org/10.18653/v1/2020.findings-emnlp.63",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06713",
                        "Citation Paper Title": "Title:Document Ranking with a Pretrained Sequence-to-Sequence Model",
                        "Citation Paper Abstract": "Abstract:This work proposes a novel adaptation of a pretrained sequence-to-sequence model to the task of document ranking. Our approach is fundamentally different from a commonly-adopted classification-based formulation of ranking, based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \"target words\", and how the underlying logits of these target words can be interpreted as relevance probabilities for ranking. On the popular MS MARCO passage ranking task, experimental results show that our approach is at least on par with previous classification-based models and can surpass them with larger, more-recent models. On the test collection from the TREC 2004 Robust Track, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-dataset cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only model in a data-poor regime (i.e., with few training examples). We investigate this observation further by varying target words to probe the model's use of latent knowledge.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2019). ",
                    "Citation Text": "Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a\npretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06713",
                        "Citation Paper Title": "Title:Document Ranking with a Pretrained Sequence-to-Sequence Model",
                        "Citation Paper Abstract": "Abstract:This work proposes a novel adaptation of a pretrained sequence-to-sequence model to the task of document ranking. Our approach is fundamentally different from a commonly-adopted classification-based formulation of ranking, based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \"target words\", and how the underlying logits of these target words can be interpreted as relevance probabilities for ranking. On the popular MS MARCO passage ranking task, experimental results show that our approach is at least on par with previous classification-based models and can surpass them with larger, more-recent models. On the test collection from the TREC 2004 Robust Track, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-dataset cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only model in a data-poor regime (i.e., with few training examples). We investigate this observation further by varying target words to probe the model's use of latent knowledge.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Bench-\nmark for complex answer retrieval. ACM, 293\u2013296. ",
                    "Citation Text": "Vincent Nguyen, Maciej Rybinski, Sarvnaz Karimi, and Zhenchang Xing. [n.d.].\nPandemic Literature Search: Finding Information on COVID-19. ALTA 2020\n([n. d.]), 92.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02492",
                        "Citation Paper Title": "Title:Searching Scientific Literature for Answers on COVID-19 Questions",
                        "Citation Paper Abstract": "Abstract:Finding answers related to a pandemic of a novel disease raises new challenges for information seeking and retrieval, as the new information becomes available gradually. TREC COVID search track aims to assist in creating search tools to aid scientists, clinicians, policy makers and others with similar information needs in finding reliable answers from the scientific literature. We experiment with different ranking algorithms as part of our participation in this challenge. We propose a novel method for neural retrieval, and demonstrate its effectiveness on the TREC COVID search.",
                        "Citation Paper Authors": "Authors:Vincent Nguyen, Maciek Rybinski, Sarvnaz Karimi, Zhenchang Xing"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using\nLocal and Distributed Representations of Text for Web Search. 1291\u20131299. ",
                    "Citation Text": "Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Bench-\nmark for complex answer retrieval. ACM, 293\u2013296.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04803",
                        "Citation Paper Title": "Title:Benchmark for Complex Answer Retrieval",
                        "Citation Paper Abstract": "Abstract:Retrieving paragraphs to populate a Wikipedia article is a challenging task. The new TREC Complex Answer Retrieval (TREC CAR) track introduces a comprehensive dataset that targets this retrieval scenario. We present early results from a variety of approaches -- from standard information retrieval methods (e.g., tf-idf) to complex systems that using query expansion using knowledge bases and deep neural networks. The goal is to offer future participants of this track an overview of some promising approaches to tackle this problem.",
                        "Citation Paper Authors": "Authors:Federico Nanni, Bhaskar Mitra, Matt Magnusson, Laura Dietz"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "Ida Mele, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Nicola\nTonellotto, and Ophir Frieder. 2020. Topic Propagation in Conversational Search.\nInProceedings of the 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 2057\u20132060. ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using\nLocal and Distributed Representations of Text for Web Search. 1291\u20131299.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.13724v3": {
            "Paper Title": "One Person, One Model, One World: Learning Continual User Representation\n  without Forgetting",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "for\nreference, where for learning \ud835\udc47\ud835\udc56(\ud835\udc56>1)the interaction sequence in\n\ud835\udc471is used as the outside features. Note we have omitted baselines\nsuch as DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. arXiv\npreprint arXiv:1703.04247 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.03811v1": {
            "Paper Title": "Click-Through Rate Prediction Using Graph Neural Networks and Online\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03775v1": {
            "Paper Title": "NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using\n  the Long Document Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03456v1": {
            "Paper Title": "CASTing a Net: Supporting Teachers with Search Technology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03300v1": {
            "Paper Title": "DA-GCN: A Domain-aware Attentive Graph Convolution Network for\n  Shared-account Cross-domain Sequential Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00290v2": {
            "Paper Title": "MusicTM-Dataset for Joint Representation Learning among Sheet Music,\n  Lyrics, and Musical Audio",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02935v1": {
            "Paper Title": "Text similarity analysis for evaluation of descriptive answers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02898v1": {
            "Paper Title": "Users' Perception of Search Engine Biases and Satisfaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02746v1": {
            "Paper Title": "Introducing Information Retrieval for Biomedical Informatics Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.00166v3": {
            "Paper Title": "OpenMatch: An Open Source Library for Neu-IR Research",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "also focuses on providing effective first-stage retrieval\nwith spare retrieval and dense retrieval. MatchZoo ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2019. MatchZoo: a learning,\npracticing, and developing system for neural text matching. In Proceedings of\nSIGIR . 1297\u20131300.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10289",
                        "Citation Paper Title": "Title:MatchZoo: A Learning, Practicing, and Developing System for Neural Text Matching",
                        "Citation Paper Abstract": "Abstract:Text matching is the core problem in many natural language processing (NLP) tasks, such as information retrieval, question answering, and conversation. Recently, deep leaning technology has been widely adopted for text matching, making neural text matching a new and active research domain. With a large number of neural matching models emerging rapidly, it becomes more and more difficult for researchers, especially those newcomers, to learn and understand these new models. Moreover, it is usually difficult to try these models due to the tedious data pre-processing, complicated parameter configuration, and massive optimization tricks, not to mention the unavailability of public codes sometimes. Finally, for researchers who want to develop new models, it is also not an easy task to implement a neural text matching model from scratch, and to compare with a bunch of existing models. In this paper, therefore, we present a novel system, namely MatchZoo, to facilitate the learning, practicing and designing of neural text matching models. The system consists of a powerful matching library and a user-friendly and interactive studio, which can help researchers: 1) to learn state-of-the-art neural text matching models systematically, 2) to train, test and apply these models with simple configurable steps; and 3) to develop their own models with rich APIs and assistance.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Xiang Ji, Xueqi Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.02414v1": {
            "Paper Title": "Person Retrieval in Surveillance Using Textual Query: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02377v1": {
            "Paper Title": "Towards Content Provider Aware Recommender Systems: A Simulation Study\n  on the Interplay between User and Provider Utilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03737v3": {
            "Paper Title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual\n  Cross-modal Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "is an extension of the canonical\ncorrelation analysis method, which focuses on keeping the function of popular supervised and\nunsupervised feature extraction approaches by developing a quadratic program to get a single\nnonlinear subspace over different feature spaces. This paper ",
                    "Citation Text": "Andrej Karpathy, Armand Joulin, and Fei-Fei Li. 2014. Deep Fragment Embeddings for Bidirectional Image Sentence\nMapping. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing\nSystems 2014, December 8-13 2014, Montreal, Quebec, Canada . pp.1889\u20131897.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.5679",
                        "Citation Paper Title": "Title:Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
                        "Citation Paper Abstract": "Abstract:We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.",
                        "Citation Paper Authors": "Authors:Andrej Karpathy, Armand Joulin, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ", video question and\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 1, No. 1, Article . Publication date: March 2020.4 DH. Zeng et al.\nanswering ",
                    "Citation Text": "Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander G. Hauptmann. 2017. Uncovering the Temporal Context for\nVideo Question Answering. International Journal of Computer Vision Vol.124, 3 (01 Sep 2017), pp.409\u2013421. https:\n//doi.org/10.1007/s11263-017-1033-7\nReceived Oct 2019; revised Jan 2020; accepted March 2020\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 1, No. 1, Article . Publication date: March 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04670",
                        "Citation Paper Title": "Title:Uncovering Temporal Context for Video Question and Answering",
                        "Citation Paper Abstract": "Abstract:In this work, we introduce Video Question Answering in temporal domain to infer the past, describe the present and predict the future. We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions. We explore approaches for finer understanding of video content using question form of \"fill-in-the-blank\", and managed to collect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD, MEDTest 14 datasets, while the corresponding 390,744 questions are generated from annotations. Extensive experiments demonstrate that our approach significantly outperforms the compared baselines.",
                        "Citation Paper Authors": "Authors:Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.15104v2": {
            "Paper Title": "GTN-ED: Event Detection Using Graph Transformer Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03602v3": {
            "Paper Title": "DEAR: Deep Reinforcement Learning for Online Advertising Impression in\n  Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08679v2": {
            "Paper Title": "Check-N-Run: A Checkpointing System for Training Deep Learning\n  Recommendation Models",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", mixed pre-\ncision quantization that adapts to underlying hardware capa-\nbilities ",
                    "Citation Text": "Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song\nHan. Haq: Hardware-aware automated quantization\nwith mixed precision. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pages\n8612\u20138620, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08886",
                        "Citation Paper Title": "Title:HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
                        "Citation Paper Abstract": "Abstract:Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.",
                        "Citation Paper Authors": "Authors:Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", ternary\nquantization for inference on mobile devices [37, 40], per-\nlayer heterogeneous quantization of DNNs ",
                    "Citation Text": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-\nMan Cheung, and Pascal Frossard. Adaptive quan-\ntization for deep neural network. arXiv preprint\narXiv:1712.01048 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01048",
                        "Citation Paper Title": "Title:Adaptive Quantization for Deep Neural Network",
                        "Citation Paper Abstract": "Abstract:In recent years Deep Neural Networks (DNNs) have been rapidly developed in various applications, together with increasingly complex architectures. The performance gain of these DNNs generally comes with high computational costs and large memory consumption, which may not be affordable for mobile platforms. Deep model quantization can be used for reducing the computation and memory costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we propose an optimization framework for deep model quantization. First, we propose a measurement to estimate the effect of parameter quantization errors in individual layers on the overall model prediction accuracy. Then, we propose an optimization process based on this measurement for finding optimal quantization bit-width for each layer. This is the first work that theoretically analyse the relationship between parameter quantization errors of individual layers and model accuracy. Our new quantization algorithm outperforms previous quantization optimization methods, and achieves 20-40% higher compression rate compared to equal bit-width quantization at the same model prediction accuracy.",
                        "Citation Paper Authors": "Authors:Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, Pascal Frossard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.01564v1": {
            "Paper Title": "PreSizE: Predicting Size in E-Commerce using Transformers",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "trainedaclassi\ufb01erfor\ufb01tmentpredictionusingacombinati onofob-\nservable and latent buyer and item features. Latent feature s were\nobtainedusingaskip-grammodellearnedoverbuyerpurchas ehis-\ntories.Thesamemodelwasextendedin ",
                    "Citation Text": "Shreya Singh, G Mohammed Abdulla, Sumit Borar, and Saga r Arora. 2018.\nFootwear Size Recommendation System. arXiv:1806.11423[c s.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.11423",
                        "Citation Paper Title": "Title:Footwear Size Recommendation System",
                        "Citation Paper Abstract": "Abstract:While shopping for fashion products, customers usually prefer to try-out products to examine fit, material, overall look and feel. Due to lack of try out options during online shopping, it becomes pivotal to provide customers with as much of this information as possible to enhance their shopping experience. Also it becomes essential to provide same experience for new customers. Our work here focuses on providing a production ready size recommendation system for shoes and address the challenge of providing recommendation for users with no previous purchases on the platform. In our work, we present a probabilistic approach based on user co-purchase data facilitated by generating a brand-brand relationship graph. Specifically we address two challenges that are commonly faced while implementing such solution. 1. Sparse signals for less popular or new products in the system 2. Extending the solution for new users. Further we compare and contrast this approach with our previous work and show significant improvement both in recommendation precision and coverage.",
                        "Citation Paper Authors": "Authors:Shreya Singh, G Mohammed Abdulla, Sumit Borar, Sagar Arora"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.01064v1": {
            "Paper Title": "Alternate Model Growth and Pruning for Efficient Training of\n  Recommendation Systems",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". Note thatthe proposed scheme can be combined with these orthogonal\nmethods to further improve training ef\ufb01ciency.\nIII. P ROPOSED METHOD\nIn this section, we describe the proposed training \ufb02ow.\nRecommendation models usually employ wide and deep ",
                    "Citation Text": "H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\nG. Anderson, G. Corrado, W. Chai, M. Ispir et al. , \u201cWide & deep\nlearning for recommender systems,\u201d in Proceedings of the 1st workshop\non deep learning for recommender systems , 2016, pp. 7\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "divides the learning concepts into different\ngroups and trains the model with the sequential concepts. In ",
                    "Citation Text": "A. Carta, A. Sperduti, and D. Bacciu, \u201cIncremental training of arecurrent neural network exploiting a multi-scale dynamic memory,\u201d\narXiv preprint arXiv:2006.16800 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16800",
                        "Citation Paper Title": "Title:Incremental Training of a Recurrent Neural Network Exploiting a Multi-Scale Dynamic Memory",
                        "Citation Paper Abstract": "Abstract:The effectiveness of recurrent neural networks can be largely influenced by their ability to store into their dynamical memory information extracted from input sequences at different frequencies and timescales. Such a feature can be introduced into a neural architecture by an appropriate modularization of the dynamic memory. In this paper we propose a novel incrementally trained recurrent architecture targeting explicitly multi-scale learning. First, we show how to extend the architecture of a simple RNN by separating its hidden state into different modules, each subsampling the network hidden activations at different frequencies. Then, we discuss a training algorithm where new modules are iteratively added to the model to learn progressively longer dependencies. Each new module works at a slower frequency than the previous ones and it is initialized to encode the subsampled sequence of hidden activations. Experimental results on synthetic and real-world datasets on speech recognition and handwritten characters show that the modular architecture and the incremental training algorithm improve the ability of recurrent neural networks to capture long-term dependencies.",
                        "Citation Paper Authors": "Authors:Antonio Carta, Alessandro Sperduti, Davide Bacciu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "en-\nlarged network capacity with fresh neurons and evaluated\nrudimentary problems such as XOR problems and multi-layer\nperceptron (MLP), without datasets from a real scenario. ",
                    "Citation Text": "X. Dai, H. Yin, and N. K. Jha, \u201cNest: A neural network synthesis tool\nbased on a grow-and-prune paradigm,\u201d arXiv preprint arXiv:1711.02017 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.02017",
                        "Citation Paper Title": "Title:NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal DNN architecture for large applications is challenging. Common approaches go for deeper and larger DNN architectures but may incur substantial redundancy. To address these problems, we introduce a network growth algorithm that complements network pruning to learn both weights and compact DNN architectures during training. We propose a DNN synthesis tool (NeST) that combines both methods to automate the generation of compact and accurate DNNs. NeST starts with a randomly initialized sparse network called the seed architecture. It iteratively tunes the architecture with gradient-based growth and magnitude-based pruning of neurons and connections. Our experimental results show that NeST yields accurate, yet very compact DNNs, with a wide range of seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we reduce network parameters by 70.2x (74.3x) and floating-point operations (FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce network parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively. NeST's grow-and-prune paradigm delivers significant additional parameter and FLOPs reduction relative to pruning-only methods.",
                        "Citation Paper Authors": "Authors:Xiaoliang Dai, Hongxu Yin, Niraj K. Jha"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.01004v1": {
            "Paper Title": "Automatic Collection Creation and Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00942v1": {
            "Paper Title": "SmoothI: Smooth Rank Indicators for Differentiable IR Metrics",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ", which frames di\u000berentiable sorting as an optimal transport\nproblem; and FastSort11 ",
                    "Citation Text": "Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast Di\u000berentiable Sorting and\nRanking. In Proceedings of the 37th International Conference on Machine Learning , pages 950{959, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08871",
                        "Citation Paper Title": "Title:Fast Differentiable Sorting and Ranking",
                        "Citation Paper Abstract": "Abstract:The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the $O(n \\log n)$ time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with $O(n \\log n)$ time and $O(n)$ space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank correlation coefficient and least trimmed squares.",
                        "Citation Paper Authors": "Authors:Mathieu Blondel, Olivier Teboul, Quentin Berthet, Josip Djolonga"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", which both propose a continuous relaxation of the sorting operator based\non unimodel row-stochastic matrices; OT10 ",
                    "Citation Text": "Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Di\u000berentiable ranking and sorting using optimal\ntransport. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.11885",
                        "Citation Paper Title": "Title:Differentiable Ranks and Sorting using Optimal Transport",
                        "Citation Paper Abstract": "Abstract:Sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (CDFs), quantiles, or to select closest neighbors and labels. The sorting function is however piece-wise constant (the sorting permutation of a vector does not change if the entries of that vector are infinitesimally perturbed) and therefore has no gradient information to back-propagate. We propose a framework to sort elements that is algorithmically differentiable. We leverage the fact that sorting can be seen as a particular instance of the optimal transport (OT) problem on $\\mathbb{R}$, from input values to a predefined array of sorted values (e.g. $1,2,\\dots,n$ if the input array has $n$ elements). Building upon this link , we propose generalized CDFs and quantile operators by varying the size and weights of the target presorted array. Because this amounts to using the so-called Kantorovich formulation of OT, we call these quantities K-sorts, K-CDFs and K-quantiles. We recover differentiable algorithms by adding to the OT problem an entropic regularization, and approximate it using a few Sinkhorn iterations. We call these operators S-sorts, S-CDFs and S-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neuralsort [Grover et al. 2019], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance.",
                        "Citation Paper Authors": "Authors:Marco Cuturi, Olivier Teboul, Jean-Philippe Vert"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". A typical example is the Top- Kloss proposed in ",
                    "Citation Text": "Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Smooth loss functions for deep top-k\nclassi\fcation. In Proceedings of the 6th International Conference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07595",
                        "Citation Paper Title": "Title:Smooth Loss Functions for Deep Top-k Classification",
                        "Citation Paper Abstract": "Abstract:The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na\u00efve algorithm would require $\\mathcal{O}(\\binom{n}{k})$ operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$. Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k=5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.",
                        "Citation Paper Authors": "Authors:Leonard Berrada, Andrew Zisserman, M. Pawan Kumar"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "adopted a cross-entropy loss. ListMLE and its extensions [25, 52]\nintroduced a likelihood loss and a theoretical framework for statistical consistency (extended in [24, 23, 51]),\nwhile ",
                    "Citation Text": "Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. Surrogate functions for maximizing preci-\nsion at the top. In Proceedings of the 32nd International Conference on Machine Learning , pages 189{198,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.06813",
                        "Citation Paper Title": "Title:Surrogate Functions for Maximizing Precision at the Top",
                        "Citation Paper Abstract": "Abstract:The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure.\nThe most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions.\nThese surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k.",
                        "Citation Paper Authors": "Authors:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". For ListMLE and LambdaLoss, we relied\non the TF-Ranking library ",
                    "Citation Text": "Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael Bendersky, Marc Najork,\nJan Pfeifer, Nadav Golbandi, Rohan Anil, and Stephan Wolf. Tf-ranking: Scalable tensor\row library\nfor learning-to-rank. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining , page 2970{2978, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00073",
                        "Citation Paper Title": "Title:TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank",
                        "Citation Paper Abstract": "Abstract:Learning-to-Rank deals with maximizing the utility of a list of examples presented to the user, with items of higher relevance being prioritized. It has several practical applications such as large-scale search, recommender systems, document summarization and question answering. While there is widespread support for classification and regression based learning, support for learning-to-rank in deep learning has been limited. We propose TensorFlow Ranking, the first open source library for solving large-scale ranking problems in a deep learning framework. It is highly configurable and provides easy-to-use APIs to support different scoring mechanisms, loss functions and evaluation metrics in the learning-to-rank setting. Our library is developed on top of TensorFlow and can thus fully leverage the advantages of this platform. For example, it is highly scalable, both in training and in inference, and can be used to learn ranking models over massive amounts of user activity data, which can include heterogeneous dense and sparse features. We empirically demonstrate the effectiveness of our library in learning ranking functions for large-scale search and recommendation applications in Gmail and Google Drive. We also show that ranking models built using our model scale well for distributed training, without significant impact on metrics. The proposed library is available to the open source community, with the hope that it facilitates further academic research and industrial applications in the field of learning-to-rank.",
                        "Citation Paper Authors": "Authors:Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, Stephan Wolf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.00422v2": {
            "Paper Title": "Future-Aware Diverse Trends Framework for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "uses RNN for the collaborative filtering task and con-\nsidered two different objective functions in the RNN model. ",
                    "Citation Text": "Massimiliano Ruocco, Ole Steinar Lillest\u00f8l Skrede, and Helge Langseth. 2017.\nInter-Session Modeling for Session-Based Recommendation. CoRR abs/1706.07506\n(2017). arXiv:1706.07506 http://arxiv.org/abs/1706.07506",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.07506",
                        "Citation Paper Title": "Title:Inter-Session Modeling for Session-Based Recommendation",
                        "Citation Paper Abstract": "Abstract:In recent years, research has been done on applying Recurrent Neural Networks (RNNs) as recommender systems. Results have been promising, especially in the session-based setting where RNNs have been shown to outperform state-of-the-art models. In many of these experiments, the RNN could potentially improve the recommendations by utilizing information about the user's past sessions, in addition to its own interactions in the current session. A problem for session-based recommendation, is how to produce accurate recommendations at the start of a session, before the system has learned much about the user's current interests. We propose a novel approach that extends a RNN recommender to be able to process the user's recent sessions, in order to improve recommendations. This is done by using a second RNN to learn from recent sessions, and predict the user's interest in the current session. By feeding this information to the original RNN, it is able to improve its recommendations. Our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions, compared to a single RNN working only on the current session. The proposed model especially improves recommendations at the start of sessions, and is therefore able to deal with the cold start problem within sessions.",
                        "Citation Paper Authors": "Authors:Massimiliano Ruocco, Ole Steinar Lillest\u00f8l Skrede, Helge Langseth"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "combines the\nsession-based KNNs with GRU4Rec using the methods of switching,\ncascading, and weighted hybrid. ",
                    "Citation Text": "Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin\nWang, and Tie-Yan Liu. 2014. Sequential Click Prediction for Sponsored Search\nwith Recurrent Neural Networks. CoRR abs/1404.5772 (2014). arXiv:1404.5772\nhttp://arxiv.org/abs/1404.5772",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1404.5772",
                        "Citation Paper Title": "Title:Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user's behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user's sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.",
                        "Citation Paper Authors": "Authors:Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, Tie-Yan Liu"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "introduces recurrent neural networks for the rec-\nommender systems firstly. [ 4,27,42,49] models behavior sequence. ",
                    "Citation Text": "Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved Recurrent Neural\nNetworks for Session-based Recommendations. CoRR abs/1606.08117 (2016).\narXiv:1606.08117 http://arxiv.org/abs/1606.08117",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.08117",
                        "Citation Paper Title": "Title:Improved Recurrent Neural Networks for Session-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.",
                        "Citation Paper Authors": "Authors:Yong Kiam Tan, Xinxing Xu, Yong Liu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "proposes the framework GraphRec to jointly capture\ninteractions and opinions in the user-item graph.\nGRU4Rec ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based Recommendations with Recurrent Neural Networks.\narXiv:1511.06939 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "designs a local acti-\nvation unit to adaptively learn the representation of user interests\nfrom past behaviors with respect to a certain ad. ",
                    "Citation Text": "F. Yuan, X. He, Haochuan Jiang, G. Guo, Jian Xiong, Zhezhao Xu, and Yilin Xiong.\n2020. Future Data Helps Training: Modeling Future Contexts for Session-based\nRecommendation. Proceedings of The Web Conference 2020 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04473",
                        "Citation Paper Title": "Title:Future Data Helps Training: Modeling Future Contexts for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommender systems have attracted much attention recently. To capture the sequential dependencies, existing methods resort either to data augmentation techniques or left-to-right style autoregressive training.Since these methods are aimed to model the sequential nature of user behaviors, they ignore the future data of a target interaction when constructing the prediction model for it. However, we argue that the future interactions after a target interaction, which are also available during training, provide valuable signal on user preference and can be used to enhance the recommendation quality.\nProperly integrating future data into model training, however, is non-trivial to achieve, since it disobeys machine learning principles and can easily cause data leakage. To this end, we propose a new encoder-decoder framework named Gap-filling based Recommender (GRec), which trains the encoder and decoder by a gap-filling mechanism. Specifically, the encoder takes a partially-complete session sequence (where some items are masked by purpose) as input, and the decoder predicts these masked items conditioned on the encoded representation. We instantiate the general GRec framework using convolutional neural network with sparse kernels, giving consideration to both accuracy and efficiency. We conduct experiments on two real-world datasets covering short-, medium-, and long-range user sessions, showing that GRec significantly outperforms the state-of-the-art sequential recommendation methods. More empirical studies verify the high utility of modeling future contexts under our GRec framework.",
                        "Citation Paper Authors": "Authors:Fajie Yuan, Xiangnan He, Haochuan Jiang, Guibing Guo, Jian Xiong, Zhezhao Xu, Yilin Xiong"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ", based on Recurrent\nNeural Network (RNN), learns the user\u2019s dynamic representation\nto reveal the user\u2019s dynamic interest. DIN ",
                    "Citation Text": "Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,\nYanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2017. Deep Interest Network for\nClick-Through Rate Prediction. arXiv:1706.06978 [stat.ML]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.00674v1": {
            "Paper Title": "Bias in Knowledge Graphs -- an Empirical Study with Movie Recommendation\n  and Different Language Editions of DBpedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06727v2": {
            "Paper Title": "Joint Constrained Learning for Event-Event Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.14595v1": {
            "Paper Title": "Efficient SPARQL Autocompletion via SPARQL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.14492v1": {
            "Paper Title": "Questioning causality on sex, gender and COVID-19, and identifying bias\n  in large-scale data-driven analyses: the Bias Priority Recommendations and\n  Bias Catalog for Pandemics",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": ".\nWhen comparing the COVID-19 case fatality rate (CFR) between China and\nItaly, the authors in ",
                    "Citation Text": "J. von K\u00fcgelgen, L. Gresele, and B. Sch\u00f6lkopf. Simpson\u2019s paradox in\nCOVID-19 case fatality rates: a mediation analysis of age-related causal\ne\ufb00ects.arXiv preprint arXiv:2005.07180 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.07180",
                        "Citation Paper Title": "Title:Simpson's paradox in Covid-19 case fatality rates: a mediation analysis of age-related causal effects",
                        "Citation Paper Abstract": "Abstract:We point out an instantiation of Simpson's paradox in Covid-19 case fatality rates (CFRs): comparing a large-scale study from China (17 Feb) with early reports from Italy (9 Mar), we find that CFRs are lower in Italy for every age group, but higher overall. This phenomenon is explained by a stark difference in case demographic between the two countries. Using this as a motivating example, we introduce basic concepts from mediation analysis and show how these can be used to quantify different direct and indirect effects when assuming a coarse-grained causal graph involving country, age, and case fatality. We curate an age-stratified CFR dataset with >750k cases and conduct a case study, investigating total, direct, and indirect (age-mediated) causal effects between different countries and at different points in time. This allows us to separate age-related effects from others unrelated to age and facilitates a more transparent comparison of CFRs across countries at different stages of the Covid-19 pandemic. Using longitudinal data from Italy, we discover a sign reversal of the direct causal effect in mid-March which temporally aligns with the reported collapse of the healthcare system in parts of the country. Moreover, we find that direct and indirect effects across 132 pairs of countries are only weakly correlated, suggesting that a country's policy and case demographic may be largely unrelated. We point out limitations and extensions for future work, and, finally, discuss the role of causal reasoning in the broader context of using AI to combat the Covid-19 pandemic.",
                        "Citation Paper Authors": "Authors:Julius von K\u00fcgelgen, Luigi Gresele, Bernhard Sch\u00f6lkopf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.07060v2": {
            "Paper Title": "Template-Based Question Answering over Linked Geospatial Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00867v1": {
            "Paper Title": "Online Product Feature Recommendations with Interpretable Machine\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "A lot of research has been conducted into online recommendation\nsystems from both an academic and industrial setting ",
                    "Citation Text": "Shuai Zhang, Linda Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: a survey and new perspectives. In Journal of ACM Computing\nSurveys (CSUR) . Vol 52, Issue 1, No. 5.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.13841v1": {
            "Paper Title": "Evaluating Document Representations for Content-based Legal Literature\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ". Such structures can also be\nfound in the legal citation graph in the form of different topics or\njurisdictions. For DeepWalk, Walklets, BoostNe, we use the Karate\nClub implementation ",
                    "Citation Text": "Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. 2020. An API Oriented\nOpen-source Python Framework for Unsupervised Learning on Graphs. (2020).\narXiv:2003.04819",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.04819",
                        "Citation Paper Title": "Title:Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs",
                        "Citation Paper Abstract": "Abstract:We present Karate Club a Python framework combining more than 30 state-of-the-art graph mining algorithms which can solve unsupervised machine learning tasks. The primary goal of the package is to make community detection, node and whole graph embedding available to a wide audience of machine learning researchers and practitioners. We designed Karate Club with an emphasis on a consistent application interface, scalability, ease of use, sensible out of the box model behaviour, standardized dataset ingestion, and output generation. This paper discusses the design principles behind this framework with practical examples. We show Karate Club's efficiency with respect to learning performance on a wide range of real world clustering problems, classification tasks and support evidence with regards to its competitive speed.",
                        "Citation Paper Authors": "Authors:Benedek Rozemberczki, Oliver Kiss, Rik Sarkar"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "introduced Poincar\u00e9 embeddings as a method to\nlearn embedding in the hyperbolic space of the Poincar\u00e9 ball model\nrather than the Euclidean space used in the aforementioned methods.\nEmbeddings produced in hyperbolic space are naturally equipped\nto model hierarchical structures ",
                    "Citation Text": "Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and\nMari\u00e1n Bogu\u00f1\u00e1. 2010. Hyperbolic geometry of complex networks. Physical\nReview E - Statistical, Nonlinear, and Soft Matter Physics 82, 3 (2010), 1\u201318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1006.5169",
                        "Citation Paper Title": "Title:Hyperbolic Geometry of Complex Networks",
                        "Citation Paper Abstract": "Abstract:We develop a geometric framework to study the structure and function of complex networks. We assume that hyperbolic geometry underlies these networks, and we show that with this assumption, heterogeneous degree distributions and strong clustering in complex networks emerge naturally as simple reflections of the negative curvature and metric property of the underlying hyperbolic geometry. Conversely, we show that if a network has some metric structure, and if the network degree distribution is heterogeneous, then the network has an effective hyperbolic geometry underneath. We then establish a mapping between our geometric framework and statistical mechanics of complex networks. This mapping interprets edges in a network as non-interacting fermions whose energies are hyperbolic distances between nodes, while the auxiliary fields coupled to edges are linear functions of these energies or distances. The geometric network ensemble subsumes the standard configuration model and classical random graphs as two limiting cases with degenerate geometric structures. Finally, we show that targeted transport processes without global topology knowledge, made possible by our geometric framework, are maximally efficient, according to all efficiency measures, in networks with strongest heterogeneity and clustering, and that this efficiency is remarkably robust with respect to even catastrophic disturbances and damages to the network structure.",
                        "Citation Paper Authors": "Authors:Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, Marian Boguna"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "explicitly encodes multi-scale node relationships to capture\ncommunity structures with the graph embedding. Walklets gener-\nates these multi-scale relationships by sub-sampling short random\nwalks on the graph nodes. BoostNE ",
                    "Citation Text": "Jundong Li, Liang Wu, Ruocheng Guo, Chenghao Liu, and Huan Liu. 2019.\nMulti-level network embedding with boosted low-rank matrix approximation. In\nProceedings of the 2019 IEEE/ACM International Conference on Advances in\nSocial Networks Analysis and Mining . ACM, New York, NY , USA, 49\u201356.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.08627",
                        "Citation Paper Title": "Title:Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation",
                        "Citation Paper Abstract": "Abstract:As opposed to manual feature engineering which is tedious and difficult to scale, network representation learning has attracted a surge of research interests as it automates the process of feature learning on graphs. The learned low-dimensional node vector representation is generalizable and eases the knowledge discovery process on graphs by enabling various off-the-shelf machine learning tools to be directly applied. Recent research has shown that the past decade of network embedding approaches either explicitly factorize a carefully designed matrix to obtain the low-dimensional node vector representation or are closely related to implicit matrix factorization, with the fundamental assumption that the factorized node connectivity matrix is low-rank. Nonetheless, the global low-rank assumption does not necessarily hold especially when the factorized matrix encodes complex node interactions, and the resultant single low-rank embedding matrix is insufficient to capture all the observed connectivity patterns. In this regard, we propose a novel multi-level network embedding framework BoostNE, which can learn multiple network embedding representations of different granularity from coarse to fine without imposing the prevalent global low-rank assumption. The proposed BoostNE method is also in line with the successful gradient boosting method in ensemble learning as multiple weak embeddings lead to a stronger and more effective one. We assess the effectiveness of the proposed BoostNE framework by comparing it with existing state-of-the-art network embedding methods on various datasets, and the experimental results corroborate the superiority of the proposed BoostNE network embedding framework.",
                        "Citation Paper Authors": "Authors:Jundong Li, Liang Wu, Huan Liu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "which is a BERT base model but fine-tuned on the CAP corpus.\nSimilarly, Legal-AUEB-BERT-base from Chalkidis et al . ",
                    "Citation Text": "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras,\nand Ion Androutsopoulos. 2020. LEGAL-BERT: The Muppets straight out of Law\nSchool. In Findings of the Association for Computational Linguistics: EMNLP\n2020 . ACL, Stroudsburg, PA, USA, 2898\u20132904.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02559",
                        "Citation Paper Title": "Title:LEGAL-BERT: The Muppets straight out of Law School",
                        "Citation Paper Abstract": "Abstract:BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.",
                        "Citation Paper Authors": "Authors:Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ".\n6The legal word vectors can be downloaded from our GitHub repository. ",
                    "Citation Text": "Nils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. 2020. A\ndataset for statutory reasoning in tax law entailment and question answering. In\nProceedings of the 2020 Natural Legal Language Processing Workshop . 31\u201338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.05257",
                        "Citation Paper Title": "Title:A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering",
                        "Citation Paper Abstract": "Abstract:Legislation can be viewed as a body of prescriptive rules expressed in natural language. The application of legislation to facts of a case we refer to as statutory reasoning, where those facts are also expressed in natural language. Computational statutory reasoning is distinct from most existing work in machine reading, in that much of the information needed for deciding a case is declared exactly once (a law), while the information needed in much of machine reading tends to be learned through distributional language statistics. To investigate the performance of natural language understanding approaches on statutory reasoning, we introduce a dataset, together with a legal-domain text corpus. Straightforward application of machine reading models exhibits low out-of-the-box performance on our questions, whether or not they have been fine-tuned to the legal domain. We contrast this with a hand-constructed Prolog-based system, designed to fully solve the task. These experiments support a discussion of the challenges facing statutory reasoning moving forward, which we argue is an interesting real-world task that can motivate the development of models able to utilize prescriptive rules specified in natural language.",
                        "Citation Paper Authors": "Authors:Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "as base and large version (pretrained on\nWikipedia and BookCorpus) and two BERT-base models pretrained\non legal corpora. Legal-JHU-BERT-base from Holzenberger et al .\n5We use the TF-IDF implementation from the scikit-learn framework ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M.\nBlondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D.\nCournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Ma-\nchine Learning in Python. Journal of Machine Learning Research 12 (2011),\n2825\u20132830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", fastText\nis pretrained on Wikipedia, UMBC webbase corpus and statmt.org\nnews dataset ",
                    "Citation Text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching Word Vectors with Subword Information. Transactions of the Association\nfor Computational Linguistics 5 (2017), 135\u2013146.\n9GitHub repository: https://github.com/malteos/legal-document-similarityICAIL \u201921, June, 21\u201325, 2021, S\u00e3o Paulo, Brasil Ostendorff et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.02847v2": {
            "Paper Title": "Explainable Depression Detection with Multi-Modalities Using a Hybrid\n  Deep Learning Model on Social Media",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "study the prob-\nlem of creating word embeddings in cases where the data\nis scarce, for instance, depressive language detection from\nuser tweets. The underlying motivation of their work is to\nsimulatearetro\ufb01tting-basedwordembeddingapproach ",
                    "Citation Text": "Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard\nHovy,andNoahASmith. Retro\ufb01ttingwordvectorstosemanticlexi-\ncons.arXiv preprint arXiv:1411.4166 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4166",
                        "Citation Paper Title": "Title:Retrofitting Word Vectors to Semantic Lexicons",
                        "Citation Paper Abstract": "Abstract:Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into the word vector training algorithms.",
                        "Citation Paper Authors": "Authors:Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ", the authors also propose a multi-model RNN-based\nmodel for depression prediction but apply their model on\nonline user forum datasets. Trotzek et al., ",
                    "Citation Text": "Marcel Trotzek, Sven Koitka, and Christoph M Friedrich. Utilizing\nneuralnetworksandlinguisticmetadataforearlydetectionofdepres-\nsion indications in text sequences. IEEE Transactions on Knowledge\nand Data Engineering , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07000",
                        "Citation Paper Title": "Title:Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences",
                        "Citation Paper Abstract": "Abstract:Depression is ranked as the largest contributor to global disability and is also a major reason for suicide. Still, many individuals suffering from forms of depression are not treated for various reasons. Previous studies have shown that depression also has an effect on language usage and that many depressed individuals use social media platforms or the internet in general to get information or discuss their problems. This paper addresses the early detection of depression using machine learning models based on messages on a social platform. In particular, a convolutional neural network based on different word embeddings is evaluated and compared to a classification based on user-level linguistic metadata. An ensemble of both approaches is shown to achieve state-of-the-art results in a current early detection task. Furthermore, the currently popular ERDE score as metric for early detection systems is examined in detail and its drawbacks in the context of shared tasks are illustrated. A slightly modified metric is proposed and compared to the original score. Finally, a new word embedding was trained on a large corpus of the same domain as the described task and is evaluated as well.",
                        "Citation Paper Authors": "Authors:Marcel Trotzek, Sven Koitka, Christoph M. Friedrich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.02069v3": {
            "Paper Title": "Data Cleansing with Contrastive Learning for Vocal Note Event\n  Annotations",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "concept of predicting the \u201ccorrespondence\u201d between video\nframes and short audio clips \u2013 two types of structured data.\nIt is also similar to CleanNet ",
                    "Citation Text": "K.-H. Lee, X. He, L. Zhang, and L. Yang, \u201cCleanNet:\nTransfer learning for scalable image classi\ufb01er training\nwith label noise,\u201d in Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07131",
                        "Citation Paper Title": "Title:CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at this http URL.",
                        "Citation Paper Authors": "Authors:Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", or by\ninferring the probability of each class being corrupted into\nanother ",
                    "Citation Text": "G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, and\nL. Qu, \u201cMaking deep neural networks robust to la-\nbel noise: A loss correction approach,\u201d in Conference\non Computer Vision and Pattern Recognition (CVPR) ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.03683",
                        "Citation Paper Title": "Title:Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach",
                        "Citation Paper Abstract": "Abstract:We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.",
                        "Citation Paper Authors": "Authors:Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, Lizhen Qu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12483v1": {
            "Paper Title": "Represent Items by Items: An Enhanced Representation of the Target Item\n  for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed to learn the complex\ninteraction function by a standard structure of MLP. DeepICF ",
                    "Citation Text": "Feng Xue, Xiangnan He, Xiang Wang, Jiandong Xu, Kai Liu, and Richang Hong.\n2019. Deep item-based collaborative filtering for top-n recommendation. ACM\nTransactions on Information Systems (TOIS) 37, 3 (2019), 1\u201325.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.04392",
                        "Citation Paper Title": "Title:Deep Item-based Collaborative Filtering for Top-N Recommendation",
                        "Citation Paper Abstract": "Abstract:Item-based Collaborative Filtering(short for ICF) has been widely adopted in recommender systems in industry, owing to its strength in user interest modeling and ease in online personalization. By constructing a user's profile with the items that the user has consumed, ICF recommends items that are similar to the user's profile. With the prevalence of machine learning in recent years, significant processes have been made for ICF by learning item similarity (or representation) from data. Nevertheless, we argue that most existing works have only considered linear and shallow relationship between items, which are insufficient to capture the complicated decision-making process of users.\nIn this work, we propose a more expressive ICF solution by accounting for the nonlinear and higher-order relationship among items. Going beyond modeling only the second-order interaction (e.g. similarity) between two items, we additionally consider the interaction among all interacted item pairs by using nonlinear neural networks. Through this way, we can effectively model the higher-order relationship among items, capturing more complicated effects in user decision-making. For example, it can differentiate which historical itemsets in a user's profile are more important in affecting the user to make a purchase decision on an item. We treat this solution as a deep variant of ICF, thus term it as DeepICF. To justify our proposal, we perform empirical studies on two public datasets from MovieLens and Pinterest. Extensive experiments verify the highly positive effect of higher-order item interaction modeling with nonlinear neural networks. Moreover, we demonstrate that by more fine-grained second-order interaction modeling with attention network, the performance of our DeepICF method can be further improved.",
                        "Citation Paper Authors": "Authors:Feng Xue, Xiangnan He, Xiang Wang, Jiandong Xu, Kai Liu, Richang Hong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12471v1": {
            "Paper Title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ", the proposed approach\nis an uni-directional transformer-based language model ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all youneed. arXiv preprint arXiv:1706.03762 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ", an approach was introduced suggesting that these\nrepresentations capture task-agnostic and highly transferable proper-\nties of language. The authors of ",
                    "Citation Text": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representa-\ntions. arXiv preprint arXiv:1802.05365 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "have introduced an approach that em-\nbeds language and visual information into a common space. In ",
                    "Citation Text": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt\nSchiele, and Trevor Darrell. 2016. Generating visual explanations. In European\nConference on Computer Vision . Springer, 3\u201319.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08507",
                        "Citation Paper Title": "Title:Generating Visual Explanations",
                        "Citation Paper Abstract": "Abstract:Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",
                        "Citation Paper Authors": "Authors:Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12333v1": {
            "Paper Title": "Explore BiLSTM-CRF-Based Models for Open Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12302v1": {
            "Paper Title": "A unified Neural Network Approach to E-CommerceRelevance Learning",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "pioneered the application of deep neural networks in end-to-end\nrelevance learning. Further, new models, such as DRMM ",
                    "Citation Text": "J. Guo, Y . Fan, Q. Ai, and W. B. Croft. A deep relevance matching model for\nad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on\nInformation and Knowledge Management , pages 55\u201364, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "provides an excellent\nsurvey. These works are roughly divided into two camps: using\nneural network induced embeddings as features in a 2-stage process,\nor directly in an end-to-end manner.\nIn the first category, ",
                    "Citation Text": "C. Van Gysel, M. de Rijke, and E. Kanoulas. Learning latent vector spaces for\nproduct search. In Proceedings of the 25th ACM International on Conference on\nInformation and Knowledge Management , pages 165\u2013174. ACM, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.07253",
                        "Citation Paper Title": "Title:Learning Latent Vector Spaces for Product Search",
                        "Citation Paper Abstract": "Abstract:We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.",
                        "Citation Paper Authors": "Authors:Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.12269v1": {
            "Paper Title": "A Bi-Encoder LSTM Model For Learning Unstructured Dialogs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12080v1": {
            "Paper Title": "AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "propose a re al-\ntimepersonalizationsolutionwhereembeddingsofitemsth atuser\nmost recently interacted with are combined in an online mann er\nto calculatesimilarities to items that need to beranked. Hu ang et\nal. ",
                    "Citation Text": "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, Dav id Zhang, Philip\nPronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linju n Yang. 2020.\nEmbedding-based retrieval in facebook search. In Proceedings of the 26th ACM\nSIGKDDInternational ConferenceonKnowledge Discovery&D ataMining .2553\u2013\n2561.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11632",
                        "Citation Paper Title": "Title:Embedding-based Retrieval in Facebook Search",
                        "Citation Paper Abstract": "Abstract:Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in eb search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.",
                        "Citation Paper Authors": "Authors:Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.14339v1": {
            "Paper Title": "A Comprehensive Attempt to Research Statement Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12016v1": {
            "Paper Title": "Learning Passage Impacts for Inverted Indexes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11890v1": {
            "Paper Title": "Automatic Description Construction for Math Expression via Topic\n  Relation Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09140v3": {
            "Paper Title": "Learning Fair Representations for Recommendation: A Graph-based\n  Perspective",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", adversarial fair representation models\nhave a feature learning module and an additional discriminator\nto guess the sensitive information. These two parts play a mini-\nmax game, and adversarial upper bounds on group fairness metrics\ncan be achieved ",
                    "Citation Text": "David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. 2018. Learning\nadversarially fair and transferable representations. In ICML . 3381\u20133390.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06309",
                        "Citation Paper Title": "Title:Learning Adversarially Fair and Transferable Representations",
                        "Citation Paper Abstract": "Abstract:In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",
                        "Citation Paper Authors": "Authors:David Madras, Elliot Creager, Toniann Pitassi, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". Current\nsolutions for fairness requirements can be classified into causal\nbased approaches [ 16,19], ranking based models ",
                    "Citation Text": "Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe\nZhao, Lichan Hong, Ed H Chi, et al .2019. Fairness in recommendation ranking\nthrough pairwise comparisons. In SIGKDD . 2212\u20132220.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.00780",
                        "Citation Paper Title": "Title:Fairness in Recommendation Ranking through Pairwise Comparisons",
                        "Citation Paper Abstract": "Abstract:Recommender systems are one of the most pervasive applications of machine learning in industry, with many services using them to match users to products or information. As such it is important to ask: what are the possible fairness risks, how can we quantify them, and how should we address them? In this paper we offer a set of novel metrics for evaluating algorithmic fairness concerns in recommender systems. In particular we show how measuring fairness based on pairwise comparisons from randomized experiments provides a tractable means to reason about fairness in rankings from recommender systems. Building on this metric, we offer a new regularizer to encourage improving this metric during model training and thus improve fairness in the resulting rankings. We apply this pairwise regularization to a large-scale, production recommender system and show that we are able to significantly improve the system's pairwise fairness.",
                        "Citation Paper Authors": "Authors:Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, Cristos Goodrow"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10817v2": {
            "Paper Title": "Using Prolog for Transforming XML Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11486v1": {
            "Paper Title": "Structuring and presenting data for testing of automotive electronics to\n  reduce effort during decision making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11287v1": {
            "Paper Title": "Tablext: A Combined Neural Network And Heuristic Based Table Extractor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.10925v1": {
            "Paper Title": "Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation\n  via Hybrid Transformer Encoding Networks",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ", as long range interactions can be established for the input\nsequence on top of multi-head self-attention. In recent years, multi-\nlayer transformer encoders have become the foundation of almost\nevery large-scale pretrained language model, like BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.10907v1": {
            "Paper Title": "XCrossNet: Feature Structure-Oriented Learning for Click-Through Rate\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.10584v1": {
            "Paper Title": "Deep Learning for Click-Through Rate Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.10129v1": {
            "Paper Title": "Efficient Retrieval Optimized Multi-task Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00951v2": {
            "Paper Title": "Ranking Papers by their Short-Term Scientific Impact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12558v1": {
            "Paper Title": "EduPal leaves no professor behind: Supporting faculty via a peer-powered\n  recommender system",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06429v2": {
            "Paper Title": "Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier\n  from Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09694v1": {
            "Paper Title": "Efficient pre-training objectives for Transformers",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "using the same compute as our RoBERTa-ML M to show that our models are\naligned with the state-of-the-art. The NSP loss, as reporte d in many works, does not provide a noticeable improvement ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Jos hi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly opt imized bert pretraining approach, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", where the au thors proposed a model based solely on the attention\nmechanism ",
                    "Citation Text": "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning . Effective approaches to attention-based neural\nmachine translation, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04025",
                        "Citation Paper Title": "Title:Effective Approaches to Attention-based Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Transformer-based models represent a new milestone for the AI community and, in particular, for NLP applications.\nThese models proved to outperform the state of the art in many Natural Language Understanding tasks. The original\nTransformer architecture was presented in ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.09677v1": {
            "Paper Title": "Large Scale Record Linkage in the Presence of Missing Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09653v1": {
            "Paper Title": "Modeling \"Newsworthiness\" for Lead-Generation Across Corpora",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09632v1": {
            "Paper Title": "Searching for Scientific Evidence in a Pandemic: An Overview of\n  TREC-COVID",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09393v1": {
            "Paper Title": "Improving Transformer-Kernel Ranking Model Using Conformer and Query\n  Term Independence",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", and therefore these models\nare often combined with explicit term matching [40, 43].\nAn alternative approach assumes QTI in the design of the neural\nranking model ",
                    "Citation Text": "Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating Query Term Independence Assumption\nfor Efficient Retrieval and Ranking using Deep Neural Networks (under review).\nInProc. ACL .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03693",
                        "Citation Paper Title": "Title:Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Classical information retrieval (IR) methods, such as query likelihood and BM25, score documents independently w.r.t. each query term, and then accumulate the scores. Assuming query term independence allows precomputing term-document scores using these models---which can be combined with specialized data structures, such as inverted index, for efficient retrieval. Deep neural IR models, in contrast, compare the whole query to the document and are, therefore, typically employed only for late stage re-ranking. We incorporate query term independence assumption into three state-of-the-art neural IR models: BERT, Duet, and CKNRM---and evaluate their performance on a passage ranking task. Surprisingly, we observe no significant loss in result quality for Duet and CKNRM---and a small degradation in the case of BERT. However, by operating on each query term independently, these otherwise computationally intensive models become amenable to offline precomputation---dramatically reducing the cost of query evaluations employing state-of-the-art neural ranking models. This strategy makes it practical to use deep models for retrieval from large collections---and not restrict their usage to late stage re-ranking.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz, Emine Yilmaz"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Precise match-\ning of terms or concepts may be difficult using query-independent\nlatent document representations ",
                    "Citation Text": "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse,\nDense, and Attentional Representations for Text Retrieval. arXiv preprint\narXiv:2005.00181 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00181",
                        "Citation Paper Title": "Title:Sparse, Dense, and Attentional Representations for Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
                        "Citation Paper Authors": "Authors:Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "extended TK to longer text using\nlocal self-attention. Other more general approaches to reducing the\nmemory footprint, such as model parallelization ",
                    "Citation Text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter\nlanguage models using gpu model parallelism. arXiv preprint arXiv:1909.08053\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.08053",
                        "Citation Paper Title": "Title:Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                        "Citation Paper Abstract": "Abstract:Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
                        "Citation Paper Authors": "Authors:Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "Scaling self-attention to long text .The self-attention layer, as\nproposed by Vaswani et al. ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Processing Systems . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.03824v3": {
            "Paper Title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07748v2": {
            "Paper Title": "Variational Inference for Category Recommendation in E-Commerce\n  platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08943v1": {
            "Paper Title": "Reference-based Weak Supervision for Answer Sentence Selection using Web\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08926v1": {
            "Paper Title": "Transitivity, Time Consumption, and Quality of Preference Judgments in\n  Crowdsourcing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08912v1": {
            "Paper Title": "The Simpson's Paradox in the Offline Evaluation of Recommendation\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06375v3": {
            "Paper Title": "EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with\n  Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08716v1": {
            "Paper Title": "Deep Latent Emotion Network for Multi-Task Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06774v4": {
            "Paper Title": "CrisisBench: Benchmarking Crisis-related Social Media Datasets for\n  Humanitarian Information Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08558v1": {
            "Paper Title": "ASBERT: Siamese and Triplet network embedding for open question\n  answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08523v1": {
            "Paper Title": "Co-BERT: A Context-Aware BERT Retrieval Model Incorporating Local and\n  Query-specific Context",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "investigate sev-\neral traditional keyword expansion approaches and find that they\nare not necessarily beneficial to improving BERT\u2019s ranking per-\nformance. Thereafter, Zheng et al. ",
                    "Citation Text": "Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERT-\nQE: Contextualized Query Expansion for Document Re-ranking. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing:\nFindings . 4718\u20134728.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07258",
                        "Citation Paper Title": "Title:BERT-QE: Contextualized Query Expansion for Document Re-ranking",
                        "Citation Paper Abstract": "Abstract:Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models.",
                        "Citation Paper Authors": "Authors:Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "2.1 BERT-based ranking models\nThe contextualized pre-trained language model, namely Bidirec-\ntional Encoder Representations from Transformers (BERT), has\nshown to outperform the state-of-the-art in many natural language\nprocessing tasks including document ranking ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . Association for\nComputational Linguistics, 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "propose\na neural learning-to-rank approach, named SetRank, that directly\nlearns a permutation-invariant ranking model defined on document\nsets of unlimited size. Evaluation on learning to rank datasets shows\nperformance gain of SetRank over strong baselines. Pasumarthi et\nal. ",
                    "Citation Text": "Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, and Marc Najork.\n2019. Self-Attentive Document Interaction Networks for Permutation Equivariant\nRanking. CoRR abs/1910.09676 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.09676",
                        "Citation Paper Title": "Title:Self-Attentive Document Interaction Networks for Permutation Equivariant Ranking",
                        "Citation Paper Abstract": "Abstract:How to leverage cross-document interactions to improve ranking performance is an important topic in information retrieval (IR) research. However, this topic has not been well-studied in the learning-to-rank setting and most of the existing work still treats each document independently while scoring. The recent development of deep learning shows strength in modeling complex relationships across sequences and sets. It thus motivates us to study how to leverage cross-document interactions for learning-to-rank in the deep learning framework. In this paper, we formally define the permutation-equivariance requirement for a scoring function that captures cross-document interactions. We then propose a self-attention based document interaction network and show that it satisfies the permutation-equivariant requirement, and can generate scores for document sets of varying sizes. Our proposed methods can automatically learn to capture document interactions without any auxiliary information, and can scale across large document sets. We conduct experiments on three ranking datasets: the benchmark Web30k, a Gmail search, and a Google Drive Quick Access dataset. Experimental results show that our proposed methods are both more effective and efficient than baselines.",
                        "Citation Paper Authors": "Authors:Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "propose a general framework for multivariate scoring functions, in\nwhich the relevance score of a document is determined by taking\nmultiple other documents in the list into consideration, instead of\nthe traditional pointwise document scoring. Pang et al. ",
                    "Citation Text": "Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Jirong Wen.\n2020. SetRank: Learning a Permutation-Invariant Ranking Model for Information\nRetrieval. In SIGIR . ACM, 499\u2013508.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.05891",
                        "Citation Paper Title": "Title:SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:In learning-to-rank for information retrieval, a ranking model is automatically learned from the data and then utilized to rank the sets of retrieved documents. Therefore, an ideal ranking model would be a mapping from a document set to a permutation on the set, and should satisfy two critical requirements: (1)~it should have the ability to model cross-document interactions so as to capture local context information in a query; (2)~it should be permutation-invariant, which means that any permutation of the inputted documents would not change the output ranking. Previous studies on learning-to-rank either design uni-variate scoring functions that score each document separately, and thus failed to model the cross-document interactions; or construct multivariate scoring functions that score documents sequentially, which inevitably sacrifice the permutation invariance requirement. In this paper, we propose a neural learning-to-rank model called SetRank which directly learns a permutation-invariant ranking model defined on document sets of any size. SetRank employs a stack of (induced) multi-head self attention blocks as its key component for learning the embeddings for all of the retrieved documents jointly. The self-attention mechanism not only helps SetRank to capture the local context information from cross-document interactions, but also to learn permutation-equivariant representations for the inputted documents, which therefore achieving a permutation-invariant ranking model. Experimental results on three large scale benchmarks showed that the SetRank significantly outperformed the baselines include the traditional learning-to-rank models and state-of-the-art Neural IR models.",
                        "Citation Paper Authors": "Authors:Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, Jirong Wen"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "propose BERT-QE that ex-\npands the original query by text snippets, instead of individual\nkeywords, selected by a fine-tuned BERT ranker. Very recently, Yu\net al. ",
                    "Citation Text": "HongChien Yu, Zhuyun Dai, and Jamie Callan. 2021. PGT: Pseudo Relevance\nFeedback Using a Graph-Based Transformer. CoRR abs/2101.07918 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.07918",
                        "Citation Paper Title": "Title:PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer",
                        "Citation Paper Abstract": "Abstract:Most research on pseudo relevance feedback (PRF) has been done in vector space and probabilistic retrieval models. This paper shows that Transformer-based rerankers can also benefit from the extra context that PRF provides. It presents PGT, a graph-based Transformer that sparsifies attention between graph nodes to enable PRF while avoiding the high computational complexity of most Transformer architectures. Experiments show that PGT improves upon non-PRF Transformer reranker, and it is at least as accurate as Transformer PRF models that use full attention, but with lower computational costs.",
                        "Citation Paper Authors": "Authors:HongChien Yu, Zhuyun Dai, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "utilize larger datasets with more training samples by trans-\nferring models across different domains and aggregate sentence-\nlevel evidences to rank documents; Nogueira et al. ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\nDocument Ranking with BERT. CoRR abs/1910.14424 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14424",
                        "Citation Paper Title": "Title:Multi-Stage Document Ranking with BERT",
                        "Citation Paper Abstract": "Abstract:The advent of deep neural networks pre-trained via language modeling tasks has spurred a number of successful applications in natural language processing. This work explores one such popular model, BERT, in the context of document ranking. We propose two variants, called monoBERT and duoBERT, that formulate the ranking problem as pointwise and pairwise classification, respectively. These two models are arranged in a multi-stage ranking architecture to form an end-to-end search system. One major advantage of this design is the ability to trade off quality against latency by controlling the admission of candidates into each pipeline stage, and by doing so, we are able to find operating points that offer a good balance between these two competing metrics. On two large-scale datasets, MS MARCO and TREC CAR, experiments show that our model produces results that are either at or comparable to the state of the art. Ablation studies show the contributions of each component and characterize the latency/quality tradeoff space.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.08443v1": {
            "Paper Title": "A Graph-guided Multi-round Retrieval Method for Conversational\n  Open-domain Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "or\nheuristically select some words or sentences from the context ",
                    "Citation Text": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy\nLiang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context.\nInProceedings of the International Conference on Empirical Methods in Natural\nLanguage Processing . ACL, 2174\u20132184.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.07036",
                        "Citation Paper Title": "Title:QuAC : Question Answering in Context",
                        "Citation Paper Abstract": "Abstract:We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at this http URL.",
                        "Citation Paper Authors": "Authors:Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ": This model uses a TF-IDF retriever and a RNN\nbased reader. The original distantly supervised setting is not\napplied, since the full supervision is allowed in the dataset\nand adopted for all the methods.\n\u2022BERTserini ",
                    "Citation Text": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with\nBERTserini. In Proceedings of the International Conference of the North American\nChapter of the Association for Computational Linguistics . ACL, 72\u201377.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01718",
                        "Citation Paper Title": "Title:End-to-End Open-Domain Question Answering with BERTserini",
                        "Citation Paper Abstract": "Abstract:We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",
                        "Citation Paper Authors": "Authors:Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "based question encoder, we introduce\ntwo special tokens, [CLS] and [SEP]. \ud835\udc5e\u2217\n\ud835\udc58is represented as a text\nsequence \u201c[CLS] \ud835\udc5e1[SEP],...,\ud835\udc5e\ud835\udc58\u22121[SEP]\ud835\udc5e\ud835\udc58[SEP]\u201d. We use AL-\nBERT ",
                    "Citation Text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In International Conference on Learning\nRepresentations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11942",
                        "Citation Paper Title": "Title:ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                        "Citation Paper Abstract": "Abstract:Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "proposed the\nmulti-hop retrieval method, which iteratively retrieved support-\ning paragraphs by forming a joint vector representation of both\nquestion and returned paragraph. Das et al. ",
                    "Citation Text": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. 2018.\nMulti-step Retriever-Reader Interaction for Scalable Open-domain Question\nAnswering. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.05733",
                        "Citation Paper Title": "Title:Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering",
                        "Citation Paper Abstract": "Abstract:This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures DrQA and BiDAF on various large open-domain datasets --- TriviaQA-unfiltered, QuasarT, SearchQA, and SQuAD-Open.",
                        "Citation Paper Authors": "Authors:Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Andrew McCallum"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "utilized the available knowl-\nedge base to build a passage graph and proposed a graph retriever to\nencode passages. In ",
                    "Citation Text": "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caim-\ning Xiong. 2020. Learning to Retrieve Reasoning Paths over Wikipedia Graph for\nQuestion Answering. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.10470",
                        "Citation Paper Title": "Title:Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
                        "Citation Paper Abstract": "Abstract:Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.",
                        "Citation Paper Authors": "Authors:Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "used both retrieval and extractive\nQA tasks to examine the effect of sequence-to-sequence question\nrewriting on the end-to-end conversational QA performance. Dif-\nferently, Voskarides et. al. ",
                    "Citation Text": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke.\n2020. Query Resolution for Conversational Search with Limited Supervision.\nInProceedings of the International Conference on Research and Development in\nInformation Retrieval . ACM, 921\u2013930.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11723",
                        "Citation Paper Title": "Title:Query Resolution for Conversational Search with Limited Supervision",
                        "Citation Paper Abstract": "Abstract:In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.",
                        "Citation Paper Authors": "Authors:Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de Rijke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.08433v1": {
            "Paper Title": "Are Word Embedding Methods Stable and Should We Care About It?",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". The Word Embedding Association Test (WEAT) is pro-\nposed by Caliskan et al. ",
                    "Citation Text": "Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics\nderived automatically from language corpora contain human-like biases.\nScience 356, 6334 (2017), 183\u2013186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.07187",
                        "Citation Paper Title": "Title:Semantics derived automatically from language corpora contain human-like biases",
                        "Citation Paper Abstract": "Abstract:Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the {\\em status quo} for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.",
                        "Citation Paper Authors": "Authors:Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.08405v1": {
            "Paper Title": "LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08340v1": {
            "Paper Title": "An Analysis of a BERT Deep Learning Strategy on a Technology Assisted\n  Review Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01894v4": {
            "Paper Title": "An efficient manifold density estimator for all recommendation systems",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", including a recent state-of-the-art VAE-based neu-\nral model: MultVAE ",
                    "Citation Text": "Dawen Liang, Rahul G. Krishnan, Matthew D. Ho\ufb00-\nman, and Tony Jebara. Variational autoencoders for\ncollaborative \ufb01ltering. WWW \u201918.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "uses the gated recurrent unit (GRU), in a sequential set-\nting. NARM ",
                    "Citation Text": "Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren,\nTao Lian, and Jun Ma. Neural attentive session-based\nrecommendation. In Proceedings of the 2017 ACM on\nConference on Information and Knowledge Manage-\nment, CIKM \u201917, New York, NY, USA, 2017. Associa-\ntion for Computing Machinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.04725",
                        "Citation Paper Title": "Title:Neural Attentive Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Given e-commerce scenarios that user profiles are invisible, session-based recommendation is proposed to generate recommendation results from short sessions. Previous work only considers the user's sequential behavior in the current session, whereas the user's main purpose in the current session is not emphasized. In this paper, we propose a novel neural networks framework, i.e., Neural Attentive Recommendation Machine (NARM), to tackle this problem. Specifically, we explore a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session, which are combined as a unified session representation later. We then compute the recommendation scores for each candidate item with a bi-linear matching scheme based on this unified session representation. We train NARM by jointly learning the item and session representations as well as their matchings. We carried out extensive experiments on two benchmark datasets. Our experimental results show that NARM outperforms state-of-the-art baselines on both datasets. Furthermore, we also find that NARM achieves a significant improvement on long sessions, which demonstrates its advantages in modeling the user's sequential behavior and main purpose simultaneously.",
                        "Citation Paper Authors": "Authors:Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", which have been \ufb01ne-tuned extensively\nby the benchmark\u2019s authors. We include two recent graph\nneural models: SR-GNN ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang,\nXing Xie, and Tieniu Tan. Session-based recommen-\ndation with graph neural networks. Proceedings of the\nAAAI Conference on Arti\ufb01cial Intelligence , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "features a graph neural network (GNN)\nable to exploit additional paths between sessions, still fo-\ncusing on sequentiality. TAGNN ",
                    "Citation Text": "Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang\nWang, and Tieniu Tan. TAGNN: Target Attentive\nGraph Neural Networks for Session-Based Recommen-\ndation. Association for Computing Machinery, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.02844",
                        "Citation Paper Title": "Title:TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", which is the same for repre-\nsenting both text and interaction networks. For embedding\ntextwesimplycreateagraphofitem-tokenedges. Weleave\nexperiments with elaborate embeddings such as BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nNAACL 2017 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "show\nthat all the aforementioned self-supervised objectives cor-\nrespond to InfoNCE, while ",
                    "Citation Text": "Michael Tschannen, Josip Djolonga, Paul K. Ruben-\nstein, Sylvain Gelly, and Mario Lucic. On mutual\ninformation maximization for representation learning.\nInInternational Conference on Learning Representa-\ntions, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.13625",
                        "Citation Paper Title": "Title:On Mutual Information Maximization for Representation Learning",
                        "Citation Paper Abstract": "Abstract:Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",
                        "Citation Paper Authors": "Authors:Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, Mario Lucic"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "introduce RACE - a LSH sketch-based\nmethod for KDE, which does not require sampling to arrive\nat density estimates. ",
                    "Citation Text": "Benjamin Coleman, Anshumali Shrivastava, and\nRichard G. Baraniuk. Race: Sub-linear memory\nsketches for approximate near-neighbor search on\nstreaming data, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06687",
                        "Citation Paper Title": "Title:Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data",
                        "Citation Paper Abstract": "Abstract:We present the first sublinear memory sketch that can be queried to find the nearest neighbors in a dataset. Our online sketching algorithm compresses an N element dataset to a sketch of size $O(N^b \\log^3 N)$ in $O(N^{(b+1)} \\log^3 N)$ time, where $b < 1$. This sketch can correctly report the nearest neighbors of any query that satisfies a stability condition parameterized by $b$. We achieve sublinear memory performance on stable queries by combining recent advances in locality sensitive hash (LSH)-based estimators, online kernel density estimation, and compressed sensing. Our theoretical results shed new light on the memory-accuracy tradeoff for nearest neighbor search, and our sketch, which consists entirely of short integer arrays, has a variety of attractive features in practice. We evaluate the memory-recall tradeoff of our method on a friend recommendation task in the Google Plus social media network. We obtain orders of magnitude better compression than the random projection based alternative while retaining the ability to report the nearest neighbors of practical queries.",
                        "Citation Paper Authors": "Authors:Benjamin Coleman, Richard G. Baraniuk, Anshumali Shrivastava"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "is based on stacked 1D dilated convolutions, instead of an\nRNN. SR-GNN ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang,\nXing Xie, and Tieniu Tan. Session-based Recommen-\ndation with Graph Neural Networks. AAAI \u201919.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "is an attention-based\nnon-RNN model using neural memory modules to represent\nitem long-term and short-term sequences. NextItNet ",
                    "Citation Text": "Fajie Yuan, Alexandros Karatzoglou, Ioannis Ara-\npakis, Joemon M. Jose, and Xiangnan He. A simple\nbut hard-to-beat baseline for session-based recommen-\ndations. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05163",
                        "Citation Paper Title": "Title:A Simple Convolutional Generative Network for Next Item Recommendation",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) have been recently introduced in the domain of session-based next item recommendation. An ordered collection of past items the user has interacted with in a session (or sequence) are embedded into a 2-dimensional latent matrix, and treated as an image. The convolution and pooling operations are then applied to the mapped item embeddings. In this paper, we first examine the typical session-based CNN recommender and show that both the generative model and network architecture are suboptimal when modeling long-range dependencies in the item sequence. To address the issues, we introduce a simple, but very effective generative model that is capable of learning high-level representation from both short- and long-range item dependencies. The network architecture of the proposed model is formed of a stack of \\emph{holed} convolutional layers, which can efficiently increase the receptive fields without relying on the pooling operation. Another contribution is the effective use of residual block structure in recommender systems, which can ease the optimization for much deeper networks. The proposed generative model attains state-of-the-art accuracy with less training time in the next item recommendation task. It accordingly can be used as a powerful recommendation baseline to beat in future, especially when there are long sequences of user feedback.",
                        "Citation Paper Authors": "Authors:Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, Xiangnan He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.08051v1": {
            "Paper Title": "Optimizing Dense Retrieval Model Training with Hard Negatives",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "used BM25 top documents as\nhard negatives. Xiong et al . ",
                    "Citation Text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neigh-\nbor Negative Contrastive Learning for Dense Text Retrieval. arXiv preprint\narXiv:2007.00808 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00808",
                        "Citation Paper Title": "Title:Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                        "Citation Paper Authors": "Authors:Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "found it\nis beneficial to increase the number of random negatives in the\nmini-batch.\nSome works applied hard-negative mining to train DR models.\nGao et al . ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2020. Complementing lexical\nretrieval with semantic residual embedding. arXiv preprint arXiv:2004.13969\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13969",
                        "Citation Paper Title": "Title:Complementing Lexical Retrieval with Semantic Residual Embedding",
                        "Citation Paper Abstract": "Abstract:This paper presents CLEAR, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model. CLEAR explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of CLEAR over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". We introduce them\nin the following.\nSeveral works utilized random negative sampling for training\nDR models. Huang et al . ",
                    "Citation Text": "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,\nJanani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-\nbased Retrieval in Facebook Search. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . 2553\u20132561.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11632",
                        "Citation Paper Title": "Title:Embedding-based Retrieval in Facebook Search",
                        "Citation Paper Abstract": "Abstract:Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in eb search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.",
                        "Citation Paper Authors": "Authors:Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". Researchers mainly use\nnegative sampling methods to train DR models except the recently\nproposed knowledge distillation method ",
                    "Citation Text": "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense\nRepresentations for Ranking using Tightly-Coupled Teachers. arXiv preprint\narXiv:2010.11386 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11386",
                        "Citation Paper Title": "Title:Distilling Dense Representations for Ranking using Tightly-Coupled Teachers",
                        "Citation Paper Abstract": "Abstract:We present an approach to ranking with dense representations that applies knowledge distillation to improve the recently proposed late-interaction ColBERT model. Specifically, we distill the knowledge from ColBERT's expressive MaxSim operator for computing relevance scores into a simple dot product, thus enabling single-step ANN search. Our key insight is that during distillation, tight coupling between the teacher model and the student model enables more flexible distillation strategies and yields better learned representations. We empirically show that our approach improves query latency and greatly reduces the onerous storage requirements of ColBERT, while only making modest sacrifices in terms of effectiveness. By combining our dense representations with sparse representations derived from document expansion, we are able to approach the effectiveness of a standard cross-encoder reranker using BERT that is orders of magnitude slower.",
                        "Citation Paper Authors": "Authors:Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ",\nwhich use BM25 as the first-stage retriever.\n7.3 Implementation Details\nAll DR models use the RoBERTa base ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: a\nrobustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "used random negative samples to\napproximate the recall task. Karpukhin et al . ",
                    "Citation Text": "Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question\nanswering. arXiv preprint arXiv:2004.04906 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04906",
                        "Citation Paper Title": "Title:Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07878v1": {
            "Paper Title": "Histopathology WSI Encoding based on GCNs for Scalable and Efficient\n  Retrieval of Diagnostically Relevant Regions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07572v1": {
            "Paper Title": "Deep Learning-based Online Alternative Product Recommendations at Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06111v3": {
            "Paper Title": "Instance-based Transfer Learning for Multilingual Deep Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16742v2": {
            "Paper Title": "FairRec: Fairness-aware News Recommendation with Decomposed Adversarial\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07414v1": {
            "Paper Title": "Hyperbolic Neural Collaborative Recommender",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07378v1": {
            "Paper Title": "Tracking entities in technical procedures -- a new dataset and baselines",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "an attention-based model that jointly predicts the property\nand the transition of the entities, KG-MRC ",
                    "Citation Text": "Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, and Andrew McCallum. Building dynamic\nknowledge graphs from text using machine reading comprehension, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05682",
                        "Citation Paper Title": "Title:Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:We propose a neural machine-reading model that constructs dynamic knowledge graphs from procedural text. It builds these graphs recurrently for each step of the described procedure, and uses them to track the evolving states of participant entities. We harness and extend a recently proposed machine reading comprehension (MRC) model to query for entity states, since these states are generally communicated in spans of text and MRC models perform well in extracting entity-centric spans. The explicit, structured, and evolving knowledge graph representations that our model constructs can be used in downstream question answering tasks to improve machine comprehension of text, as we demonstrate empirically. On two comprehension tasks from the recently proposed PROPARA dataset (Dalvi et al., 2018), our model achieves state-of-the-art results. We further show that our model is competitive on the RECIPES dataset (Kiddon et al., 2015), suggesting it may be generally applicable. We present some evidence that the model's knowledge graphs help it to impose commonsense constraints on its predictions.",
                        "Citation Paper Authors": "Authors:Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew McCallum"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Procedural text, due to its inherent difference from the factual text, has lead to new datasets and models specially\ndesigned for the comprehension task. bAbI ",
                    "Citation Text": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri\u00ebnboer, Armand Joulin, and\nTomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05698",
                        "Citation Paper Title": "Title:Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
                        "Citation Paper Abstract": "Abstract:One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.",
                        "Citation Paper Authors": "Authors:Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri\u00ebnboer, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07360v1": {
            "Paper Title": "DebiasedRec: Bias-aware User Modeling and Click Prediction for\n  Personalized News Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "proposed to use an attention network to\nselect clicked news for learning user interest representations, and\nthey used the inner product between candidate news and user\nrepresentations to predict click scores. Wu et al . ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019.\nNeural News Recommendation with Multi-Head Self-Attention. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP) . Association for Computational Linguistics, Hong Kong, China, 6389\u2013\n6394. https://doi.org/10.18653/v1/D19-1671",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "proposed to model user interests from clicked news based on\ntheir relevance to candidate news, and predict click scores based\non candidate news and user embeddings via a feed-forward neural\nnetwork. Wu et al . ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\nand Xing Xie. 2019. Neural news recommendation with attentive multi-view\nlearning. arXiv preprint arXiv:1907.05576 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ". Existing news recommendation methods\nusually model user interests from their clicked news and predict\nclick scores based on the relevance between user and candidate\nnews representations [ 1,13,23,40,42,46]. For example, Wang et al . ",
                    "Citation Text": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nknowledge-aware network for news recommendation. In Proceedings of the 2018\nworld wide web conference . 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07345v1": {
            "Paper Title": "ROC: An Ontology for Country Responses towards COVID-19",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07269v1": {
            "Paper Title": "NeuSE: A Neural Snapshot Ensemble Method for Collaborative Filtering",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ".\nHowever, efficiency issue may arise in ensemble methods due to the learning of multiple models, especially on\nlarge datasets ",
                    "Citation Text": "Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. 2017. Snapshot ensembles: Train 1, get m\nfor free. arXiv preprint arXiv:1704.00109 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00109",
                        "Citation Paper Title": "Title:Snapshot Ensembles: Train 1, get M for free",
                        "Citation Paper Abstract": "Abstract:Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.",
                        "Citation Paper Authors": "Authors:Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". In addition, many existing ensemble collaborative\nfiltering methods were proposed for specific CF methods and thus may not be applied to other CF methods,\ne.g., LLORMA ",
                    "Citation Text": "Joonseok Lee, Seungyeon Kim, Guy Lebanon, and Yoram Singer. 2013. Local low-rank matrix approximation. In Proceedings of The 30th\nInternational Conference on Machine Learning . 82\u201390.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3192",
                        "Citation Paper Title": "Title:Matrix Approximation under Local Low-Rank Assumption",
                        "Citation Paper Abstract": "Abstract:Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.",
                        "Citation Paper Authors": "Authors:Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07186v1": {
            "Paper Title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04898v3": {
            "Paper Title": "Open-Domain Question Answering Goes Conversational via Question\n  Rewriting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06969v1": {
            "Paper Title": "Event Detection as Question Answering with Entity Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06892v1": {
            "Paper Title": "Knowledge-driven Answer Generation for Conversational Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06529v1": {
            "Paper Title": "BERT Embeddings Can Track Context in Conversational Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05101v2": {
            "Paper Title": "Setting the Record Straighter on Shadow Banning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.05310v2": {
            "Paper Title": "Generating Code with the Help of Retrieved Template Functions and Stack\n  Overflow Answers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02448v4": {
            "Paper Title": "Parallel Split-Join Networks for Shared-account Cross-domain Sequential\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ".\nContextual information has proved to be very important for\nbehavior modeling in traditional recommendations. Liu et al. ",
                    "Citation Text": "Qiang Liu, Shu Wu, Diyi Wang, Zhaokang Li, and Liang\nWang. Context-aware sequential recommendation. In\nProceedings of the IEEE 16th International Conference\non Data Mining , ICDM \u201916, pages 1053\u20131058, New\nYork, NY , USA, 2016. IEEE.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.05787",
                        "Citation Paper Title": "Title:Context-aware Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for real-world applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive context-specific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CA-RNN model yields significant improvements over state-of-the-art sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.",
                        "Citation Paper Authors": "Authors:Qiang Liu, Shu Wu, Diyi Wang, Zhaokang Li, Liang Wang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\n2) Deep learning-based methods\nRecently, recurrent neural networks (RNNs) have been\ndevised to model variable-length sequential data [83, 20].\nHidasi et al. ",
                    "Citation Text": "Bal \u00b4azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas,\nand Domonkos Tikk. Session-based recommendations\nwith recurrent neural networks. In International Confer-\nence on Learning Representations , ICLR \u201916, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.05796v1": {
            "Paper Title": "On the instability of embeddings for recommender systems: the case of\n  Matrix Factorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10816v2": {
            "Paper Title": "Narrowing Down XML Template Expansion and Schema Validation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.05307v1": {
            "Paper Title": "Personalized Bundle Recommendation in Online Games",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "is proposed to jointly model the user-item and user-list\ninteractions, which combines two types of latent factor models: BPR ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings\nof the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence . 452\u2013461.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.05216v1": {
            "Paper Title": "Contextualized Knowledge-aware Attentive Neural Network: Enhancing\n  Answer Selection with Knowledge",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "develops a simple but effective\nattention mechanism for the purpose of constructing better answer representations according to\nthe input question. dos Santos et al . ",
                    "Citation Text": "C\u00edcero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive Pooling Networks. CoRR\nabs/1602.03609 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.03609",
                        "Citation Paper Title": "Title:Attentive Pooling Networks",
                        "Citation Paper Abstract": "Abstract:In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.",
                        "Citation Paper Authors": "Authors:Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ", exploit the relation as\na translation and measure the distance between the two entities, while semantic matching models,\nsuch as ComplEx ",
                    "Citation Text": "Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings\nfor Simple Link Prediction. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016 .\n2071\u20132080.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06357",
                        "Citation Paper Title": "Title:Complex Embeddings for Simple Link Prediction",
                        "Citation Paper Abstract": "Abstract:In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nNE methods are first developed to embed traditional networks, such as social networks, biological\nnetworks, and information networks. Recently, with the rapid growth in KG construction and\napplication, some NE methods, such as Deepwalk ",
                    "Citation Text": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning of social representations. In The\n20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914 . 701\u2013710.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "develop a model which adopts ELMo as the pre-trained\nlanguage model along with some latent clustering and transfer learning techniques. Li et al . ",
                    "Citation Text": "Dongfang Li, Yifei Yu, Qingcai Chen, and Xinyu Li. 2019. BERTSel: Answer Selection with Pre-trained Models. CoRR\nabs/1905.07588 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07588",
                        "Citation Paper Title": "Title:BERTSel: Answer Selection with Pre-trained Models",
                        "Citation Paper Abstract": "Abstract:Recently, pre-trained models have been the dominant paradigm in natural language processing. They achieved remarkable state-of-the-art performance across a wide range of related tasks, such as textual entailment, natural language inference, question answering, etc. BERT, proposed by Devlin this http URL., has achieved a better marked result in GLUE leaderboard with a deep transformer architecture. Despite its soaring popularity, however, BERT has not yet been applied to answer selection. This task is different from others with a few nuances: first, modeling the relevance and correctness of candidates matters compared to semantic relatedness and syntactic structure; second, the length of an answer may be different from other candidates and questions. In this paper. we are the first to explore the performance of fine-tuning BERT for answer selection. We achieved STOA results across five popular datasets, demonstrating the success of pre-trained models in this task.",
                        "Citation Paper Authors": "Authors:Dongfang Li, Yifei Yu, Qingcai Chen, Xinyu Li"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "to encode matching features directly in the word representations.\n\u2022HyperQA . Tay et al . ",
                    "Citation Text": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Hyperbolic Representation Learning for Fast and Efficient Neural\nQuestion Answering. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018 . 583\u2013591.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07847",
                        "Citation Paper Title": "Title:Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering",
                        "Citation Paper Abstract": "Abstract:The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks.",
                        "Citation Paper Authors": "Authors:Yi Tay, Luu Anh Tuan, Siu Cheung Hui"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "but in\nthe input of the RNN layer.\n\u2022CTRN . Tay et al . ",
                    "Citation Text": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Cross Temporal Recurrent Networks for Ranking Question Answer\nPairs. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18) . 5512\u20135519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07656",
                        "Citation Paper Title": "Title:Cross Temporal Recurrent Networks for Ranking Question Answer Pairs",
                        "Citation Paper Abstract": "Abstract:Temporal gates play a significant role in modern recurrent-based neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates are learned via 1D convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, i.e., pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.",
                        "Citation Paper Authors": "Authors:Yi Tay, Luu Anh Tuan, Siu Cheung Hui"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "proposes a convolutional neural tensor network (CNTN) architecture to\nmodel the interactions between sentences with tensor layers. ",
                    "Citation Text": "Yi Tay, Minh C. Phan, Anh Tuan Luu, and Siu Cheung Hui. 2017. Learning to Rank Question Answer Pairs with\nHolographic Dual LSTM Architecture. In Proceedings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 695\u2013704.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06372",
                        "Citation Paper Title": "Title:Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture",
                        "Citation Paper Abstract": "Abstract:We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural architectures on two popular benchmark QA datasets. Empirical studies confirm the effectiveness of holographic composition over the neural tensor layer.",
                        "Citation Paper Authors": "Authors:Yi Tay, Minh C. Phan, Luu Anh Tuan, Siu Cheung Hui"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "is employed to extract questions with correct answers. Same\nas TREC QA, MAP and MRR are also adopted to evaluate the performance on WikiQA.\nInsuranceQA . A community-based QA dataset, proposed by ",
                    "Citation Text": "Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. 2015. Applying deep learning to answer\nselection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU\n2015. 813\u2013820.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01585",
                        "Citation Paper Title": "Title:Applying Deep Learning to Answer Selection: A Study and An Open Task",
                        "Citation Paper Abstract": "Abstract:We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use.",
                        "Citation Paper Authors": "Authors:Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "proposes a positional attention\nbased RNN model to integrate positional information into attentive representation.\nSome latest works employ pre-trained language models, such as ElMo ",
                    "Citation Text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, Volume 1 (Long\nPapers) . 2227\u20132237.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ", to tackle\nthe answer selection task. Yoon et al . ",
                    "Citation Text": "Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, and Kyomin Jung. 2019. A Compare-Aggregate Model\nwith Latent Clustering for Answer Selection. In Proceedings of the 28th ACM International Conference on Information\nand Knowledge Management, CIKM 2019 . 2093\u20132096.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12897",
                        "Citation Paper Title": "Title:A Compare-Aggregate Model with Latent Clustering for Answer Selection",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel method for a sentence-level answer-selection task that is a fundamental problem in natural language processing. First, we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large-scale corpus. Second, we enhance the compare-aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise. To evaluate the performance of the proposed approaches, experiments are performed with the WikiQA and TREC-QA datasets. The empirical results demonstrate the superiority of our proposed approach, which achieve state-of-the-art performance for both datasets.",
                        "Citation Paper Authors": "Authors:Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Kyomin Jung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.04864v2": {
            "Paper Title": "EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.02981v2": {
            "Paper Title": "Reinforcement Learning with a Disentangled Universal Value Function for\n  Item Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06105v2": {
            "Paper Title": "BCFNet: A Balanced Collaborative Filtering Network with Attention\n  Mechanism",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "adapts LR and MLP to learn\nthe matching function from input continuous features and\ncategorical features of user and item. DeepFM ",
                    "Citation Text": "H. Guo, R. Tang, Y. Ye, Z. Li, and X. He, \u201cDeepFM: A factorization-\nmachine based neural network for CTR prediction,\u201d in IJCAI , 2017,\npp. 1725\u20131731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is a variant of NeuMF that takes user neighbors\nand item neighbors as inputs. Other than NeuMF, there are\nalso many other works attempting to learn the matching\nfunction directly by making full use of auxiliary data. For\nexample, Wide&Deep ",
                    "Citation Text": "H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Arad-\nhye, G. Anderson, G. Corrado, W. Chai, M. Ispir et al. , \u201cWide &\ndeep learning for recommender systems,\u201d in Proceedings of the 1st\nWorkshop on Deep Learning for Recommender Systems , 2016, pp. 7\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.04697v1": {
            "Paper Title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04584v1": {
            "Paper Title": "Text2Chart: A Multi-Staged Chart Generator from Natural Language Text",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". Recently, Bidirectional Encoder Representations from Transformers (BERT) is proposed\nin ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". Word2Vec maps words with similar\nmeaning to adjacent points in a vector space. The embedding is learnt using a neural network\non continuous bag of words or skip-gram model. A character-level word embedding is proposed\nin ",
                    "Citation Text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word\nvectors with subword information. Transactions of the Association for Computational Lin-\nguistics , 5:135{146, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "that\ngenerates excellent infographics from given text. However, their method is limited to single\nentity only. GPT-3 ",
                    "Citation Text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "has\nproposed a pipeline to generate automatic question answering system based on charts.\nAutomated visualizaton has been always a very fascinating area. A survey of Machine\nLearning based visualization methods has been presented in ",
                    "Citation Text": "Qianwen Wang, Zhutian Chen, Yong Wang, and Huamin Qu. Applying machine learning\nadvances to data visualization: A survey on ml4vis. arXiv preprint arXiv:2012.00467 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.00467",
                        "Citation Paper Title": "Title:A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization",
                        "Citation Paper Abstract": "Abstract:Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations:Data Processing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations.Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at this https URL",
                        "Citation Paper Authors": "Authors:Qianwen Wang, Zhutian Chen, Yong Wang, Huamin Qu"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "has proposed an automatic chart description generator. CycleGT has been proposed recently\nthat works on both directions: text to graphs and graphs to text ",
                    "Citation Text": "Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang.\nCyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. arXiv\npreprint arXiv:2006.04702 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04702",
                        "Citation Paper Title": "Title:CycleGT: Unsupervised Graph-to-Text and Text-to-Graph Generation via Cycle Training",
                        "Citation Paper Abstract": "Abstract:Two important tasks at the intersection of knowledge graphs and natural language processing are graph-to-text (G2T) and text-to-graph (T2G) conversion. Due to the difficulty and high cost of data collection, the supervised data available in the two fields are usually on the magnitude of tens of thousands, for example, 18K in the WebNLG~2017 dataset after preprocessing, which is far fewer than the millions of data for other tasks such as machine translation. Consequently, deep learning models for G2T and T2G suffer largely from scarce training data. We present CycleGT, an unsupervised training method that can bootstrap from fully non-parallel graph and text data, and iteratively back translate between the two forms. Experiments on WebNLG datasets show that our unsupervised model trained on the same number of data achieves performance on par with several fully supervised models. Further experiments on the non-parallel GenWiki dataset verify that our method performs the best among unsupervised baselines. This validates our framework as an effective approach to overcome the data scarcity problem in the fields of G2T and T2G. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "that can generate images from text descriptions. Balaji et al. ",
                    "Citation Text": "A. Balaji, T. Ramanathan, and V. Sonathi. Chart-text: A fully automated chart image\ndescriptor. arXiv preprint arXiv:1812.10636 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.10636",
                        "Citation Paper Title": "Title:Chart-Text: A Fully Automated Chart Image Descriptor",
                        "Citation Paper Abstract": "Abstract:Images greatly help in understanding, interpreting and visualizing data. Adding textual description to images is the first and foremost principle of web accessibility. Visually impaired users using screen readers will use these textual descriptions to get better understanding of images present in digital contents. In this paper, we propose Chart-Text a novel fully automated system that creates textual description of chart images. Given a PNG image of a chart, our Chart-Text system creates a complete textual description of it. First, the system classifies the type of chart and then it detects and classifies the labels and texts in the charts. Finally, it uses specific image processing algorithms to extract relevant information from the chart images. Our proposed system achieves an accuracy of 99.72% in classifying the charts and an accuracy of 78.9% in extracting the data and creating the corresponding textual description.",
                        "Citation Paper Authors": "Authors:Abhijit Balaji, Thuvaarakkesh Ramanathan, Venkateshwarlu Sonathi"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", authors have proposed a method for generating ground truth for chart images. Both of\nthe works are limited to bar charts and line charts only. A Generative Adversarial Network,\nAttnGAN is proposed in ",
                    "Citation Text": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and\nXiaodong He. Attngan: Fine-grained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1316{1324, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10485",
                        "Citation Paper Title": "Title:AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",
                        "Citation Paper Authors": "Authors:Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.04556v1": {
            "Paper Title": "A Probabilistic Framework for Lexicon-based Keyword Spotting in\n  Handwritten Text Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04501v1": {
            "Paper Title": "Exploring Current User Web Search Behaviours in Analysis Tasks to be\n  Supported in Conversational Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04415v1": {
            "Paper Title": "Automatic Knowledge Extraction with Human Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07996v2": {
            "Paper Title": "Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective\n  and a Call to Arms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04243v1": {
            "Paper Title": "Incorporating External Knowledge to Enhance Tabular Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.03940v1": {
            "Paper Title": "A Conceptual Framework for Implicit Evaluation of Conversational Search\n  Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07993v3": {
            "Paper Title": "Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based\n  Target Behavior Prediction",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "is used to model both\nbehavior sequences simultaneously.\n\u2022CoAtt. Co-Attention (CoAtt) ",
                    "Citation Text": "Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical\nquestion-image co-attention for visual question answering. In NIPS . 289\u2013297.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.00061",
                        "Citation Paper Title": "Title:Hierarchical Question-Image Co-Attention for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.",
                        "Citation Paper Authors": "Authors:Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposes a\nnovel attention mechanism to capture both the users\u2019 long-term\ninterests in general and their short-term attention. More recently,\nwith the flourish Graph Neural Networks (GNN) methodologies, ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In AAAI . 346\u2013353.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Session-Based Behavior Prediction. In the literature, the pio-\nneering study ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based recommendations with recurrent neural networks. ICLR\n(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "encodes first-\norder and second-order proximity of nodes into a low-dimensional\nspace. Recently, a surge of methods related on graph convolutional\nnetworks (GCN) have been raised. ",
                    "Citation Text": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral\nnetworks and locally connected networks on graphs. ICLR (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6203",
                        "Citation Paper Title": "Title:Spectral Networks and Locally Connected Networks on Graphs",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
                        "Citation Paper Authors": "Authors:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "by proposing a model, DeepWalk, to learn node representations\nbased on sequences sampled from graphs. LINE ",
                    "Citation Text": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\n2015. Line: Large-scale information network embedding. In WWW . 1067\u20131077.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "also considers\nthe assignment different preference levels of various user behav-\niors. Instead of BPR, it incorporates this useful information into\nelement-wise alternating least squares learner. More recently, a\nneural network approach is proposed by ",
                    "Citation Text": "Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-\nSeng Chua, and Depeng Jin. 2019. Neural Multi-task Recommendation from\nMulti-behavior Data. In ICDE . 1554\u20131557.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08161",
                        "Citation Paper Title": "Title:Learning to Recommend with Multiple Cascading Behaviors",
                        "Citation Paper Abstract": "Abstract:Most existing recommender systems leverage user behavior data of one type only, such as the purchase behavior in E-commerce that is directly related to the business KPI (Key Performance Indicator) of conversion rate. Besides the key behavioral data, we argue that other forms of user behaviors also provide valuable signal, such as views, clicks, adding a product to shop carts and so on. They should be taken into account properly to provide quality recommendation for users. In this work, we contribute a new solution named NMTR (short for Neural Multi-Task Recommendation) for learning recommender systems from user multi-behavior data. We develop a neural network model to capture the complicated and multi-type interactions between users and items. In particular, our model accounts for the cascading relationship among different types of behaviors (e.g., a user must click on a product before purchasing it). To fully exploit the signal in the data of multiple types of behaviors, we perform a joint optimization based on the multi-task learning framework, where the optimization on a behavior is treated as a task. Extensive experiments on two real-world datasets demonstrate that NMTR significantly outperforms state-of-the-art recommender systems that are designed to learn from both single-behavior data and multi-behavior data. Further analysis shows that modeling multiple behaviors is particularly useful for providing recommendation for sparse users that have very few interactions.",
                        "Citation Paper Authors": "Authors:Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-Seng Chua, Lina Yao, Yang Song, Depeng Jin"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "in the direction of single-session based recom-\nmendation first adopts a recurrent neural network based approach\nwith past interacted items as the input of different time steps for\nsession-based recommendation. Following that, ",
                    "Citation Text": "Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\nnetworks for session-based recommendations. In Proceedings of the 1st Workshop\non Deep Learning for Recommender Systems . ACM, 17\u201322.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.08117",
                        "Citation Paper Title": "Title:Improved Recurrent Neural Networks for Session-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.",
                        "Citation Paper Authors": "Authors:Yong Kiam Tan, Xinxing Xu, Yong Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.03354v1": {
            "Paper Title": "Prism: Private Verifiable Set Computation over Multi-Owner Outsourced\n  Databases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.02962v1": {
            "Paper Title": "DyGCN: Dynamic Graph Embedding with Graph Convolutional Network",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "ingests dynamic graph information in the form of association and communication events over\ntime and updates the node representations as they appear in these events. DynWalks ",
                    "Citation Text": "Chengbin Hou, Han Zhang, Ke Tang, and Shan He. 2019. DynWalks: Global Topology and Recent Changes Awareness Dynamic Network Embedding.\narXiv preprint arXiv:1907.11968 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11968",
                        "Citation Paper Title": "Title:DynWalks: Global Topology and Recent Changes Awareness Dynamic Network Embedding",
                        "Citation Paper Abstract": "Abstract:Learning topological representation of a network in dynamic environments has recently attracted considerable attention due to the time-evolving nature of many real-world networks i.e. nodes/links might be added/removed as time goes on. Dynamic network embedding aims to learn low dimensional embeddings for unseen and seen nodes by using any currently available snapshots of a dynamic network. For seen nodes, the existing methods either treat them equally important or focus on the $k$ most affected nodes at each time step. However, the former solution is time-consuming, and the later solution that relies on incoming changes may lose the global topology---an important feature for downstream tasks. To address these challenges, we propose a dynamic network embedding method called DynWalks, which includes two key components: 1) An online network embedding framework that can dynamically and efficiently learn embeddings based on the selected nodes; 2) A novel online node selecting scheme that offers the flexible choices to balance global topology and recent changes, as well as to fulfill the real-time constraint if needed. The empirical studies on six real-world dynamic networks under three different slicing ways show that DynWalks significantly outperforms the state-of-the-art methods in graph reconstruction tasks, and obtains comparable results in link prediction tasks. Furthermore, the wall-clock time and complexity analysis demonstrate its excellent time and space efficiency. The source code of DynWalks is available at this https URL",
                        "Citation Paper Authors": "Authors:Chengbin Hou, Han Zhang, Ke Tang, Shan He"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "is a dataset containing abstracts of papers in High Energy Physics Theory conference from 1993 to\n2003. We create a collaboration network for papers published in each month. Likewise, we select 60 consecutive\nsnapshots in the dataset.\n\u2022Facebook ",
                    "Citation Text": "Amanda L Traud, Peter J Mucha, and Mason A Porter. 2012. Social structure of facebook networks. Physica A: Statistical Mechanics and its\nApplications (2012), 4165\u20134180.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1102.2166",
                        "Citation Paper Title": "Title:Social Structure of Facebook Networks",
                        "Citation Paper Abstract": "Abstract:We study the social structure of Facebook \"friendship\" networks at one hundred American colleges and universities at a single point in time, and we examine the roles of user attributes - gender, class year, major, high school, and residence - at these institutions. We investigate the influence of common attributes at the dyad level in terms of assortativity coefficients and regression models. We then examine larger-scale groupings by detecting communities algorithmically and comparing them to network partitions based on the user characteristics. We thereby compare the relative importances of different characteristics at different institutions, finding for example that common high school is more important to the social organization of large institutions and that the importance of common major varies significantly between institutions. Our calculations illustrate how microscopic and macroscopic perspectives give complementary insights on the social organization at universities and suggest future studies to investigate such phenomena further.",
                        "Citation Paper Authors": "Authors:Amanda L. Traud, Peter J. Mucha, Mason A. Porter"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "deals with the computation complexity problem of GCN, it samples node by each layer of aggregation. SGC ",
                    "Citation Text": "Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. 2019. Simplifying graph convolutional\nnetworks. arXiv preprint arXiv:1902.07153 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "first attempts to study\nthe representation ability of different kinds of aggregation part, and gets a more powerful GNN model. FastGCN ",
                    "Citation Text": "Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint\narXiv:1801.10247 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.10247",
                        "Citation Paper Title": "Title:FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
                        "Citation Paper Abstract": "Abstract:The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.",
                        "Citation Paper Authors": "Authors:Jie Chen, Tengfei Ma, Cao Xiao"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "considers from the spatial perspective and introduces an inductive learning method. After that,\nresearchers focus on the effectiveness and efficiency of aggregation and updating part. GIN ",
                    "Citation Text": "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00826",
                        "Citation Paper Title": "Title:How Powerful are Graph Neural Networks?",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "and GraphSAGE use graph\nconvolution network to model information propagation in graph, GAT ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv\npreprint arXiv:1710.10903 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "perform graph convolutions for aggregation and update motivated by spectral\nconvolution. Gated Graph Neural Networks (GGNN) ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493\n(2015).\n20DyGCN: Dynamic Graph Embedding with Graph Convolutional Network Woodstock \u201918, June 03\u201305, 2018, Woodstock, NY",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "uses a Graph Convolutional Networks (GCN) to learn node embedding and feed it into the LSTM\nto learn dynamism. WD-GCN/CD-GCN ",
                    "Citation Text": "Franco Manessi, Alessandro Rozza, and Mario Manzo. 2019. Dynamic graph convolutional networks. Pattern Recognition (2019), 107000.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06199",
                        "Citation Paper Title": "Title:Dynamic Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Many different classification tasks need to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change during time. Our goal is to jointly exploit structured data and temporal information through the use of a neural network model. To the best of our knowledge, this task has not been addressed using these kind of architectures. For this reason, we propose two novel approaches, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The quality of our methods is confirmed by the promising results achieved.",
                        "Citation Paper Authors": "Authors:Franco Manessi, Alessandro Rozza, Mario Manzo"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "proposes a network embedding method in dynamic environment\nwhich updates the node embedding based on the change on adjacency matrix as well as attribute matrix via matrix\nperturbation. DyRep ",
                    "Citation Text": "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2018. Dyrep: Learning representations over dynamic graphs. (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04051",
                        "Citation Paper Title": "Title:Representation Learning over Dynamic Graphs",
                        "Citation Paper Abstract": "Abstract:How can we effectively encode evolving information over dynamic graphs into low-dimensional representations? In this paper, we propose DyRep, an inductive deep representation learning framework that learns a set of functions to efficiently produce low-dimensional node embeddings that evolves over time. The learned embeddings drive the dynamics of two key processes namely, communication and association between nodes in dynamic graphs. These processes exhibit complex nonlinear dynamics that evolve at different time scales and subsequently contribute to the update of node embeddings. We employ a time-scale dependent multivariate point process model to capture these dynamics. We devise an efficient unsupervised learning procedure and demonstrate that our approach significantly outperforms representative baselines on two real-world datasets for the problem of dynamic link prediction and event time prediction.",
                        "Citation Paper Authors": "Authors:Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, Hongyuan Zha"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "proposes an dynamic version of LINE, with only a few gradient descent process to update\nthe representation of current graph. DANE ",
                    "Citation Text": "Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. 2017. Attributed network embedding for learning in a dynamic environment.\nInCIKM . 387\u2013396.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01860",
                        "Citation Paper Title": "Title:Attributed Network Embedding for Learning in a Dynamic Environment",
                        "Citation Paper Abstract": "Abstract:Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network. The learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. Most, if not all, of the existing works, are overwhelmingly performed in the context of plain and static networks. Nonetheless, in reality, network structure often evolves over time with addition/deletion of links and nodes. Also, a vast majority of real-world networks are associated with a rich set of node attributes, and their attribute values are also naturally changing, with the emerging of new content patterns and the fading of old content patterns. These changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which is of fundamental importance for learning in a dynamic environment. To our best knowledge, we are the first to tackle this problem with the following two challenges: (1) the inherently correlated network and node attributes could be noisy and incomplete, it necessitates a robust consensus representation to capture their individual properties and correlations; (2) the embedding learning needs to be performed in an online fashion to adapt to the changes accordingly. In this paper, we tackle this problem by proposing a novel dynamic attributed network embedding framework - DANE. In particular, DANE first provides an offline method for a consensus embedding and then leverages matrix perturbation theory to maintain the freshness of the end embedding results in an online manner. We perform extensive experiments on both synthetic and real attributed networks to corroborate the effectiveness and efficiency of the proposed framework.",
                        "Citation Paper Authors": "Authors:Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "applies a Hawkes process to model the evolution\nof neighborhood formation sequence. Goyal et al [ 9,10] use Auto-Encoder to learn the graph and use the Recurrent\nNeural Network (RNN) to model the relations over time. Sankar et al ",
                    "Citation Text": "Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2018. Dynamic Graph Representation Learning via Self-Attention Networks.\narXiv preprint arXiv:1812.09430 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.09430",
                        "Citation Paper Title": "Title:Dynamic Graph Representation Learning via Self-Attention Networks",
                        "Citation Paper Abstract": "Abstract:Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.",
                        "Citation Paper Authors": "Authors:Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, Hao Yang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nDeepWalk transforms graph structure information into sequences by random walk, and then Skip-Gram ",
                    "Citation Text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint\narXiv:1301.3781 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.00860v2": {
            "Paper Title": "GRN: Generative Rerank Network for Context-wise Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "works by handmade global\nlist-wise features, while it requires much domain knowledge\nand decreases its generalization performance. DLCM ",
                    "Citation Text": "Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep\nListwise Context Model for Ranking Refinement. In ACM Special Interest Group\non Information Retrieval . 135\u2013144.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05936",
                        "Citation Paper Title": "Title:Learning a Deep Listwise Context Model for Ranking Refinement",
                        "Citation Paper Abstract": "Abstract:Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the \\textit{local ranking context}, can provide important information about the query's characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.",
                        "Citation Paper Authors": "Authors:Qingyao Ai, Keping Bi, Jiafeng Guo, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is a well-known tree-based method with the list-\nwise loss function. MIDNN ",
                    "Citation Text": "Tao Zhuang, Wenwu Ou, and Zhirong Wang. 2018. Globally Optimized Mutual\nInfluence Aware Ranking in E-Commerce Search. In International Joint Conference\non Artificial Intelligence . 3725\u20133731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08524",
                        "Citation Paper Title": "Title:Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search",
                        "Citation Paper Abstract": "Abstract:In web search, mutual influences between documents have been studied from the perspective of search result diversification. But the methods in web search is not directly applicable to e-commerce search because of their differences. And little research has been done on the mutual influences between items in e-commerce search. We propose a global optimization framework for mutual influence aware ranking in e-commerce search. Our framework directly optimizes the Gross Merchandise Volume (GMV) for ranking, and decomposes ranking into two tasks. The first task is mutual influence aware purchase probability estimation. We propose a global feature extension method to incorporate mutual influences into the features of an item. We also use Recurrent Neural Network (RNN) to capture influences related to ranking orders in purchase probability estimation. The second task is to find the best ranking order based on the purchase probability estimations. We treat the second task as a sequence generation problem and solved it using the beam search algorithm. We performed online A/B test on a large e-commerce search engine. The results show that our method brings a 5% increase in GMV for the search engine over a strong baseline.",
                        "Citation Paper Authors": "Authors:Tao Zhuang, Wenwu Ou, Zhirong Wang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "is a standard deep learning method in the indus-\ntrial recommender system, which applies MLP for complex\nfeature interaction.\n\u2022DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. In\nInternational Joint Conference on Artificial Intelligence . 1725\u20131731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.00234v4": {
            "Paper Title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.02381v1": {
            "Paper Title": "Scene Graph Embeddings Using Relative Similarity Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06547v2": {
            "Paper Title": "LayoutGMN: Neural Graph Matching for Structural Layout Similarity",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ". The state-of-the-art network for struc-\ntural similarity on UI layouts is a hybrid network comprised\nof an attention-based GCN, similar to the gating mechanism\nin ",
                    "Citation Text": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\nZemel. Gated graph sequence neural networks. 2016. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.04615v2": {
            "Paper Title": "Toward Effective Automated Content Analysis via Crowdsourcing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01628v1": {
            "Paper Title": "Revisiting Indirect Ontology Alignment : New Challenging Issues in\n  Cross-Lingual Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01495v1": {
            "Paper Title": "Towards Self-Adaptive Metric Learning On the Fly",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "uses a regularized online metric learning al-\ngorithm. Recently, Li et al. ",
                    "Citation Text": "Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, and Yinghuan Shi.\n2018. OPML: A one-pass closed-form solution for online metric learning. Pattern\nRecognition 75 (2018), 302\u2013314. https://doi.org/10.1016/j.patcog.2017.03.016",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09178",
                        "Citation Paper Title": "Title:OPML: A One-Pass Closed-Form Solution for Online Metric Learning",
                        "Citation Paper Abstract": "Abstract:To achieve a low computational cost when performing online metric learning for large-scale data, we present a one-pass closed-form solution namely OPML in this paper. Typically, the proposed OPML first adopts a one-pass triplet construction strategy, which aims to use only a very small number of triplets to approximate the representation ability of whole original triplets obtained by batch-manner methods. Then, OPML employs a closed-form solution to update the metric for new coming samples, which leads to a low space (i.e., $O(d)$) and time (i.e., $O(d^2)$) complexity, where $d$ is the feature dimensionality. In addition, an extension of OPML (namely COPML) is further proposed to enhance the robustness when in real case the first several samples come from the same class (i.e., cold start problem). In the experiments, we have systematically evaluated our methods (OPML and COPML) on three typical tasks, including UCI data classification, face verification, and abnormal event detection in videos, which aims to fully evaluate the proposed methods on different sample number, different feature dimensionalities and different feature extraction ways (i.e., hand-crafted and deeply-learned). The results show that OPML and COPML can obtain the promising performance with a very low computational cost. Also, the effectiveness of COPML under the cold start setting is experimentally verified.",
                        "Citation Paper Authors": "Authors:Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.11951v2": {
            "Paper Title": "Demographic Aware Probabilistic Medical Knowledge Graph Embeddings of\n  Electronic Medical Records",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06313v1": {
            "Paper Title": "SetConv: A New Approach for Learning from Imbalanced Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.02061v1": {
            "Paper Title": "Query2Prod2Vec Grounded Word Embeddings for eCommerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01207v1": {
            "Paper Title": "Type Prediction Systems",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "uses additional manual annotations to perform \ufb01ne-grained\ntyping of entity mentions within a sentence. More recently, SeMantic AnsweR Type prediction task (SMART) ",
                    "Citation Text": "MIHINDUKULASOORIYA , N., D UBEY , M., G LIOZZO , A., L EHMANN , J., N GOMO , A.-C. N., AND USBECK ,\nR.SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web Challenge. CoRR/arXiv\nabs/2012.00555 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.00555",
                        "Citation Paper Title": "Title:SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web Challenge",
                        "Citation Paper Abstract": "Abstract:Each year the International Semantic Web Conference accepts a set of Semantic Web Challenges to establish competitions that will advance the state of the art solutions in any given problem domain. The SeMantic AnsweR Type prediction task (SMART) was part of ISWC 2020 challenges. Question type and answer type prediction can play a key role in knowledge base question answering systems providing insights that are helpful to generate correct queries or rank the answer candidates. More concretely, given a question in natural language, the task of SMART challenge is, to predict the answer type using a target ontology (e.g., DBpedia or Wikidata).",
                        "Citation Paper Authors": "Authors:Nandana Mihindukulasooriya, Mohnish Dubey, Alfio Gliozzo, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.12440v2": {
            "Paper Title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12057v2": {
            "Paper Title": "Revisit Recommender System in the Permutation Prospective",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "works by handmade global\nlist-wise features, while it requires much domain knowledge and\ndecreases its generalization performance. DLCM ",
                    "Citation Text": "Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep\nListwise Context Model for Ranking Refinement. In SIGIR . 135\u2013144.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05936",
                        "Citation Paper Title": "Title:Learning a Deep Listwise Context Model for Ranking Refinement",
                        "Citation Paper Abstract": "Abstract:Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the \\textit{local ranking context}, can provide important information about the query's characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.",
                        "Citation Paper Authors": "Authors:Qingyao Ai, Keping Bi, Jiafeng Guo, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "is a well-known tree-based method with\nthe list-wise loss function. MIDNN ",
                    "Citation Text": "Tao Zhuang, Wenwu Ou, and Zhirong Wang. 2018. Globally Optimized Mutual\nInfluence Aware Ranking in E-Commerce Search. In IJCAI . 3725\u20133731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08524",
                        "Citation Paper Title": "Title:Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search",
                        "Citation Paper Abstract": "Abstract:In web search, mutual influences between documents have been studied from the perspective of search result diversification. But the methods in web search is not directly applicable to e-commerce search because of their differences. And little research has been done on the mutual influences between items in e-commerce search. We propose a global optimization framework for mutual influence aware ranking in e-commerce search. Our framework directly optimizes the Gross Merchandise Volume (GMV) for ranking, and decomposes ranking into two tasks. The first task is mutual influence aware purchase probability estimation. We propose a global feature extension method to incorporate mutual influences into the features of an item. We also use Recurrent Neural Network (RNN) to capture influences related to ranking orders in purchase probability estimation. The second task is to find the best ranking order based on the purchase probability estimations. We treat the second task as a sequence generation problem and solved it using the beam search algorithm. We performed online A/B test on a large e-commerce search engine. The results show that our method brings a 5% increase in GMV for the search engine over a strong baseline.",
                        "Citation Paper Authors": "Authors:Tao Zhuang, Wenwu Ou, Zhirong Wang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "is a standard deep learning method in the indus-\ntrial recommender system, which applies MLP for complex\nfeature interaction.\n\u2022DeepFM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. In\nIJCAI . 1725\u20131731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.00782v1": {
            "Paper Title": "\"TL;DR:\" Out-of-Context Adversarial Text Summarization and Hashtag\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06079v2": {
            "Paper Title": "Studying Dishonest Intentions in Brazilian Portuguese Texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06192v3": {
            "Paper Title": "Ranking Clarifying Questions Based on Predicted User Engagement",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ". Our research's scope is\nlimited to \fxed candidate answers, but posing open clari\fcation questions has\nalready been shown to improve search results signi\fcantly ",
                    "Citation Text": "Hashemi, H., Zamani, H., Croft, W.B.: Guided Transformer: Leveraging\nMultiple External Sources for Representation Learning in Conversational\nSearch. arXiv e-prints arXiv:2006.07548 (Jun 2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.07548",
                        "Citation Paper Title": "Title:Guided Transformer: Leveraging Multiple External Sources for Representation Learning in Conversational Search",
                        "Citation Paper Abstract": "Abstract:Asking clarifying questions in response to ambiguous or faceted queries has been recognized as a useful technique for various information retrieval systems, especially conversational search systems with limited bandwidth interfaces. Analyzing and generating clarifying questions have been studied recently but the accurate utilization of user responses to clarifying questions has been relatively less explored. In this paper, we enrich the representations learned by Transformer networks using a novel attention mechanism from external information sources that weights each term in the conversation. We evaluate this Guided Transformer model in a conversational search scenario that includes clarifying questions. In our experiments, we use two separate external sources, including the top retrieved documents and a set of different possible clarifying questions for the query. We implement the proposed representation learning model for two downstream tasks in conversational search; document retrieval and next clarifying question selection. Our experiments use a public dataset for search clarification and demonstrate significant improvements compared to competitive baselines.",
                        "Citation Paper Authors": "Authors:Helia Hashemi, Hamed Zamani, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.07178v2": {
            "Paper Title": "Disentangled Item Representation for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "treats the non-observed interactions as negative samples. After that, Bayesian Personalized Ranking\n(BPR) ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personal-\nized Ranking from Implicit Feedback. In UAI, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial\nIntelligence . 452\u2013461.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "declares that style representation could be obtained\nby splitting category information from visual representation. Following the ideas above, some\nrecent works map items into different attribute spaces to get multiple views of items. For example,\nYu et al ",
                    "Citation Text": "Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin. 2018. Aesthetic-based clothing recom-\nmendation. In Proceedings of the 2018 World Wide Web Conference on World Wide Web . International World Wide Web\nConferences Steering Committee, 649\u2013658.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.05822",
                        "Citation Paper Title": "Title:Aesthetic-based Clothing Recommendation",
                        "Citation Paper Abstract": "Abstract:Recently, product images have gained increasing attention in clothing recommendation since the visual appearance of clothing products has a significant impact on consumers' decision. Most existing methods rely on conventional features to represent an image, such as the visual features extracted by convolutional neural networks (CNN features) and the scale-invariant feature transform algorithm (SIFT features), color histograms, and so on. Nevertheless, one important type of features, the \\emph{aesthetic features}, is seldom considered. It plays a vital role in clothing recommendation since a users' decision depends largely on whether the clothing is in line with her aesthetics, however the conventional image features cannot portray this directly. To bridge this gap, we propose to introduce the aesthetic information, which is highly relevant with user preference, into clothing recommender systems. To achieve this, we first present the aesthetic features extracted by a pre-trained neural network, which is a brain-inspired deep structure trained for the aesthetic assessment task. Considering that the aesthetic preference varies significantly from user to user and by time, we then propose a new tensor factorization model to incorporate the aesthetic features in a personalized manner. We conduct extensive experiments on real-world datasets, which demonstrate that our approach can capture the aesthetic preference of users and significantly outperform several state-of-the-art recommendation methods.",
                        "Citation Paper Authors": "Authors:Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, Zheng Qin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "use the auto-encoder model to disentangle the sentiment information of text. Not only in the\narea about image or text, disentangled representations can capture the descriptive entity ",
                    "Citation Text": "Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner.\n2018. Towards a Definition of Disentangled Representations. arXiv preprint arXiv:1812.02230 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02230",
                        "Citation Paper Title": "Title:Towards a Definition of Disentangled Representations",
                        "Citation Paper Abstract": "Abstract:How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.",
                        "Citation Paper Authors": "Authors:Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, Alexander Lerchner"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "try to split the style information from image by representation disentangling. Vineet et al ",
                    "Citation Text": "Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. 2018. Disentangled Representation Learning for\nText Style Transfer. arXiv preprint arXiv:1808.04339 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04339",
                        "Citation Paper Title": "Title:Disentangled Representation Learning for Non-Parallel Text Style Transfer",
                        "Citation Paper Abstract": "Abstract:This paper tackles the problem of disentangling the latent variables of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for label prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning method is applied to style transfer on non-parallel corpora. We achieve substantially better results in terms of transfer accuracy, content preservation and language fluency, in comparison to previous state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.01115v1": {
            "Paper Title": "Local and Global Topics in Text Modeling of Web Pages Nested in Web\n  Sites",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.16624v1": {
            "Paper Title": "Text Classification Using Hybrid Machine Learning Algorithms on Big Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.15953v1": {
            "Paper Title": "TREC 2020 Podcasts Track Overview",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.15950v1": {
            "Paper Title": "Entity Context Graph: Learning Entity Representations\n  fromSemi-Structured Textual Sources on the Web",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "and hardto build. For example, systems like NELL and DeepDive that are\nproposed to extract KG triples from text documents still require seed\npatterns/ontology rules to guide them. On the other hand, there exists\ncontextual word embedding approaches (e.g., BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ") and KGs\nare used to improve their performances (e.g., ERNIE ",
                    "Citation Text": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019. ERNIE: Enhanced Language Representation with Informative Entities. In\nProceedings of ACL 2019 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07129",
                        "Citation Paper Title": "Title:ERNIE: Enhanced Language Representation with Informative Entities",
                        "Citation Paper Abstract": "Abstract:Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from this https URL.",
                        "Citation Paper Authors": "Authors:Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", etc. Some of the other\nrecent approaches include matrix factorization ",
                    "Citation Text": "Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume\nBouchard. 2016. Complex embeddings for simple link prediction. In International\nConference on Machine Learning . 2071\u20132080.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06357",
                        "Citation Paper Title": "Title:Complex Embeddings for Simple Link Prediction",
                        "Citation Paper Abstract": "Abstract:In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.15581v1": {
            "Paper Title": "Supporting verification of news articles with automated search for\n  semantically similar articles",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "where the authors approached fact-checking with machine learning methods\nand focuse on claim verification.\nAnother related approach is presented in ",
                    "Citation Text": "J. Thorne, A. Vlachos, O. Cocarascu, C. Christodoulopoulos, A. Mittal, The fact extraction\nand VERification (FEVER) shared task, in: Proceedings of the First Workshop on Fact\nExtraction and VERification (FEVER), Association for Computational Linguistics, 2018, pp.\n1\u20139.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.10971",
                        "Citation Paper Title": "Title:The Fact Extraction and VERification (FEVER) Shared Task",
                        "Citation Paper Abstract": "Abstract:We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be Supported or Refuted using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.",
                        "Citation Paper Authors": "Authors:James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, Arpit Mittal"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "point out stylistic biases that exists in text in order to automate\nfake news detection. Source-based approaches assess the credibility of a news source [ 20,21]\nwhile knowledge-based approaches compare news content with known facts ",
                    "Citation Text": "B. Botnevik, E. Sakariassen, V. Setty, Brenda: Browser extension for fake news detection,\nin: Proceedings of the 43rd International ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, SIGIR \u201920, Association for Computing Machinery, New\nYork, NY, USA, 2020, p. 2117\u20132120.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13270",
                        "Citation Paper Title": "Title:BRENDA: Browser Extension for Fake News Detection",
                        "Citation Paper Abstract": "Abstract:Misinformation such as fake news has drawn a lot of attention in recent years. It has serious consequences on society, politics and economy. This has lead to a rise of manually fact-checking websites such as Snopes and Politifact. However, the scale of misinformation limits their ability for verification. In this demonstration, we propose BRENDA a browser extension which can be used to automate the entire process of credibility assessments of false claims. Behind the scenes BRENDA uses a tested deep neural network architecture to automatically identify fact check worthy claims and classifies as well as presents the result along with evidence to the user. Since BRENDA is a browser extension, it facilities fast automated fact checking for the end user without having to leave the Webpage.",
                        "Citation Paper Authors": "Authors:Bjarte Botnevik, Eirik Sakariassen, Vinay Setty"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". For style-based analysis, the writing style is assessed according to\nmalicious intent. Perez et al. ",
                    "Citation Text": "V. P\u00e9rez-Rosas, B. Kleinberg, A. Lefevre, R. Mihalcea, Automatic detection of fake news,\nin: Proceedings of the 27th International Conference on Computational Linguistics, Asso-\nciation for Computational Linguistics, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07104",
                        "Citation Paper Title": "Title:Automatic Detection of Fake News",
                        "Citation Paper Abstract": "Abstract:The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.",
                        "Citation Paper Authors": "Authors:Ver\u00f3nica P\u00e9rez-Rosas, Bennett Kleinberg, Alexandra Lefevre, Rada Mihalcea"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.15514v1": {
            "Paper Title": "Context-aware short-term interest first model for session-based\n  recommendation",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "is a modification of GNN that USES gated recursive units and USES time \nbackpropagation (BPTT) to calculate gradients. In recent years, GNN has been widely applied to \ndifferent tasks, su ch as script event prediction [2 4], scene recognition ",
                    "Citation Text": "Li, R.; Tapaswi, M.; Liao, R.; Jia, J.; Urtasun, R.; and Fidler, S. 2017b. Situation recognition with \ngraph neu ral networks. In 2017 IEEE International Conference on Computer Vision (ICCV), 4183 \u2013\n4192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04320",
                        "Citation Paper Title": "Title:Situation Recognition with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (e.g. attacking has weapon), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5% improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs.",
                        "Citation Paper Authors": "Authors:Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". Previously, in the form \nof recurrent neural networks, graph neu ral networks (CNN) [2 2] proposed to operate on digraph. \nGated GNN ",
                    "Citation Text": "Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. S. 2015. Gated graph sequence neural networks. \nIn Proceedings of the 2015 International Conference on Learning Representations, volume \nabs/15 11.05493 of ICLR \u201915.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.15454v1": {
            "Paper Title": "Proxy Synthesis: Learning with Synthetic Classes for Deep Metric\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.15386v1": {
            "Paper Title": "Large-Scale Approximate k-NN Graph Construction on GPU",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "along with k-selection is proposed to construct the k-NN graph\nconstruction for the large-scale dataset. Whereas extra time costs\nare introduced for codebook training and quantization. Moreover,\nhigh-quality k-NN graph cannot be expected due to the quantiza-\ntion loss. GGNN ",
                    "Citation Text": "Fabian Groh, Lukas Ruppert, Patrick Wieschollek, and Hendrik Lensch.\n2019. GGNN: Graph-based GPU Nearest Neighbor Search. arXiv preprint\narXiv:1912.01059 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01059",
                        "Citation Paper Title": "Title:GGNN: Graph-based GPU Nearest Neighbor Search",
                        "Citation Paper Abstract": "Abstract:Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT, FAISS, and SONG started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today's state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed.",
                        "Citation Paper Authors": "Authors:Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P.A. Lensch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2004.13969v3": {
            "Paper Title": "Complementing Lexical Retrieval with Semantic Residual Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13273v4": {
            "Paper Title": "Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional\n  Similarity-Based Approach",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "contains key column\ninformation. We may also use the SATO method ",
                    "Citation Text": "D. Zhang, Y . Suhara, J. Li, M. Hulsebos, C \u00b8. Demiralp, and W. Tan. Sato:\nContextual semantic type detection in tables. PVLDB , 13(11):1835\u20131848,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.06311",
                        "Citation Paper Title": "Title:Sato: Contextual Semantic Type Detection in Tables",
                        "Citation Paper Abstract": "Abstract:Detecting the semantic types of data columns in relational tables is important for various data preparation and information retrieval tasks such as data cleaning, schema matching, data discovery, and semantic search. However, existing detection approaches either perform poorly with dirty data, support only a limited number of semantic types, fail to incorporate the table context of columns or rely on large sample sizes for training data. We introduce Sato, a hybrid machine learning model to automatically detect the semantic types of columns in tables, exploiting the signals from the context as well as the column values. Sato combines a deep learning model trained on a large-scale table corpus with topic modeling and structured prediction to achieve support-weighted and macro average F1 scores of 0.925 and 0.735, respectively, exceeding the state-of-the-art performance by a significant margin. We extensively analyze the overall and per-type performance of Sato, discussing how individual modeling components, as well as feature categories, contribute to its performance.",
                        "Citation Paper Authors": "Authors:Dan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon Hulsebos, \u00c7a\u011fatay Demiralp, Wang-Chiew Tan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.07436v3": {
            "Paper Title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series\n  Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.10044v2": {
            "Paper Title": "Interest-aware Message-Passing GCN for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ", researchers also introspect the complex de-\nsign in GCN-based recommendation models. He at al. ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-\nwork for Recommendation. In Proceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Information Retrieval . ACM, 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "explicitly incorporates the user-user\nand item-item graphs, which is built upon the user-item bipartite\ngraph, in the embedding learning process. Inspired by the study of\nsimplifying GCN ",
                    "Citation Text": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying Graph Convolutional Networks. PMLR, 6861\u2013\n6871.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "is proposed to directly\ncapture the high-order connectivity between neighboring nodes at\nany order. Multi-GCCF ",
                    "Citation Text": "Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming\nTang, and Xiuqiang He. 2019. Multi-graph convolution collaborative filtering. In\nProceedings of IEEE International Conference on Data Mining . 1306 \u2013 1311.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.00267",
                        "Citation Paper Title": "Title:Multi-Graph Convolution Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Personalized recommendation is ubiquitous, playing an important role in many online services. Substantial research has been dedicated to learning vector representations of users and items with the goal of predicting a user's preference for an item based on the similarity of the representations. Techniques range from classic matrix factorization to more recent deep learning based methods. However, we argue that existing methods do not make full use of the information that is available from user-item interaction data and the similarities between user pairs and item pairs. In this work, we develop a graph convolution-based recommendation framework, named Multi-Graph Convolution Collaborative Filtering (Multi-GCCF), which explicitly incorporates multiple graphs in the embedding learning process. Multi-GCCF not only expressively models the high-order information via a partite user-item interaction graph, but also integrates the proximal information by building and processing user-user and item-item graphs. Furthermore, we consider the intrinsic difference between user nodes and item nodes when performing graph convolution on the bipartite graph. We conduct extensive experiments on four publicly accessible benchmarks, showing significant improvements relative to several state-of-the-art collaborative filtering and graph neural network-based recommendation models. Further experiments quantitatively verify the effectiveness of each component of our proposed model and demonstrate that the learned embeddings capture the important relationship structure.",
                        "Citation Paper Authors": "Authors:Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming Tang, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "utilizes\nmetapath-guided neighbors to exploit rich structure information\nfor intent recommendation; NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Development in Information Retrieval .\nACM, 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "employs one convolution layer to exploit the direct con-\nnections between users and items; PinSage ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining . ACM, 974\u2013983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.14934v1": {
            "Paper Title": "Community-based Cyberreading for Information Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.14748v1": {
            "Paper Title": "Analysing the Effect of Recommendation Algorithms on the Amplification\n  of Misinformation",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": ".  \nNevertheless, some strategies have been found to be effective in correcting \nmisperceptions ",
                    "Citation Text": "D. M. Lazer, M. A. Baum, Y. Benkler, A. J. Berinsky, K. M. Greenhill, F. Menczer, M. J. Metzger, B. \nNyhan, G. Pennycook, D. Rothschild, et al., The science of fake news, Science 359 (6380) (2018) \n1094 \u20131096.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.07903",
                        "Citation Paper Title": "Title:The science of fake news",
                        "Citation Paper Abstract": "Abstract:Fake news emerged as an apparent global problem during the 2016 U.S. Presidential election. Addressing it requires a multidisciplinary effort to define the nature and extent of the problem, detect fake news in real time, and mitigate its potentially harmful effects. This will require a better understanding of how the Internet spreads content, how people process news, and how the two interact. We review the state of knowledge in these areas and discuss two broad potential mitigation strategies: better enabling individuals to identify fake news, and intervention within the platforms to reduce the attention given to fake news. The cooperation of Internet platforms (especially Facebook, Google, and Twitter) with researchers will be critical to understanding the scale of the issue and the effectiveness of possible interventions.",
                        "Citation Paper Authors": "Authors:David M. J. Lazer, Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo Menczer, Miriam J. Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan J. Watts, Jonathan L. Zittrain"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.14371v1": {
            "Paper Title": "A PSO Strategy of Finding Relevant Web Documents using a New Similarity\n  Measure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02033v2": {
            "Paper Title": "Text Data Augmentation: Towards better detection of spear-phishing\n  emails",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.13506v1": {
            "Paper Title": "Hierarchical Hyperedge Embedding-based Representation Learning for Group\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.13235v1": {
            "Paper Title": "Web Mining for Estimating Regulatory Blockchain Readiness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06141v2": {
            "Paper Title": "Operationalizing Framing to Support Multiperspective Recommendations of\n  Opinion Pieces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12537v1": {
            "Paper Title": "A News Recommender System Considering Temporal Dynamics and Diversity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12420v1": {
            "Paper Title": "HSEarch: semantic search system for workplace accident reports",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12404v1": {
            "Paper Title": "Diversity Regularized Interests Modeling for Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12286v1": {
            "Paper Title": "Automated Discovery of Real-Time Network Camera Data From Heterogeneous\n  Web Pages",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "discusses the conditions when privacy can be expected. More specifically, the\npaper classifies the scenarios based on the subject\u2019s location and the observer\u2019s location. Recently,\nthe public facial recognition dataset from Microsoft known as MS Celeb ",
                    "Citation Text": "Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. MS-Celeb-1M: A Dataset and Benchmark\nfor Large-Scale Face Recognition. CoRR abs/1607.08221 (2016). arXiv:1607.08221 http://arxiv.org/abs/1607.08221",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08221",
                        "Citation Paper Title": "Title:MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
                        "Citation Paper Abstract": "Abstract:In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.",
                        "Citation Paper Authors": "Authors:Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.11297v1": {
            "Paper Title": "Insight-centric Visualization Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "extends the rule-\nbased system of Voyager by learning weights for manually defined\nconstraints. More recently, Xin et al. ",
                    "Citation Text": "X. Qian, R. A. Rossi, F. Du, S. Kim, E. Koh, S. Malik, T. Y. Lee, and J. Chan. Ml-based\nvisualization recommendation: Learning to recommend visualizations from data.\narXiv:2009.12316 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.12316",
                        "Citation Paper Title": "Title:ML-based Visualization Recommendation: Learning to Recommend Visualizations from Data",
                        "Citation Paper Abstract": "Abstract:Visualization recommendation seeks to generate, score, and recommend to users useful visualizations automatically, and are fundamentally important for exploring and gaining insights into a new or existing dataset quickly. In this work, we propose the first end-to-end ML-based visualization recommendation system that takes as input a large corpus of datasets and visualizations, learns a model based on this data. Then, given a new unseen dataset from an arbitrary user, the model automatically generates visualizations for that new dataset, derive scores for the visualizations, and output a list of recommended visualizations to the user ordered by effectiveness. We also describe an evaluation framework to quantitatively evaluate visualization recommendation models learned from a large corpus of visualizations and datasets. Through quantitative experiments, a user study, and qualitative analysis, we show that our end-to-end ML-based system recommends more effective and useful visualizations compared to existing state-of-the-art rule-based systems. Finally, we observed a strong preference by the human experts in our user study towards the visualizations recommended by our ML-based system as opposed to the rule-based system (5.92 from a 7-point Likert scale compared to only 3.45).",
                        "Citation Paper Authors": "Authors:Xin Qian, Ryan A. Rossi, Fan Du, Sungchul Kim, Eunyee Koh, Sana Malik, Tak Yeon Lee, Joel Chan"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "helps\npeople explore data through manual view specification. Once users\ncreate a visualization, Voder utilizes a set of predefined heuris-\ntics to generate a list of related data facts (insights). DataSite ",
                    "Citation Text": "Z. Cui, S. K. Badam, A. Yal\u00e7in, and N. Elmqvist. Datasite: Proactive visual\ndata exploration with computation of insight-based recommendations. CoRR ,\nabs/1802.08621, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08621",
                        "Citation Paper Title": "Title:DataSite: Proactive Visual Data Exploration with Computation of Insight-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.",
                        "Citation Paper Authors": "Authors:Zhe Cui, Sriram Karthik Badam, Adil Yal\u00e7in, Niklas Elmqvist"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.11950v3": {
            "Paper Title": "Search4Code: Code Search Intent Classification Using Weak Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.11095v1": {
            "Paper Title": "Social Link Inference via Multi-View Matching Network from\n  Spatio-Temporal Trajectories",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "leverages node features such as text attributes for achieving in-\nductive graph representation learning. For the reason that GAT\ncan automatically determine the weights of user neighbors, we\nadopt it to model social relations.\nMulti-view learning ",
                    "Citation Text": "C. Xu, D. Tao, and C. Xu, \u201cA survey on multi-view learning,\u201d arXiv\npreprint arXiv:1304.5634 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1304.5634",
                        "Citation Paper Title": "Title:A Survey on Multi-view Learning",
                        "Citation Paper Abstract": "Abstract:In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.",
                        "Citation Paper Authors": "Authors:Chang Xu, Dacheng Tao, Chao Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.11685v2": {
            "Paper Title": "Neural Methods for Effective, Efficient, and Exposure-Aware Information\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": ". In addi-\ntion, recent trends suggest that advancements in deep neural networks methods are\nalso fuelling emerging IR scenarios such as proactive recommendations [ 61\u201363],\nconversational IR [ 64,65], and multi-modal retrieval ",
                    "Citation Text": "Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li. Multimodal convolu-\ntional neural networks for matching image and sentence. In Proceedings of\nthe IEEE International Conference on Computer Vision , pages 2623\u20132631,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.06063",
                        "Citation Paper Title": "Title:Multimodal Convolutional Neural Networks for Matching Image and Sentence",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.",
                        "Citation Paper Authors": "Authors:Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "that train ML models over a set\nof hand-crafted features, recent neural models for IR typically accept the raw text\nof a query and document as input. Learning suitable representations of text also\ndemands large-scale datasets for training ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using\nlocal and distributed representations of text for web search. In Proc. WWW ,\npages 1291\u20131299, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.10693v1": {
            "Paper Title": "Adversarial and Contrastive Variational Autoencoder for Sequential\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "is a\nrepresentative method of using VAE for recommendation. Based on\nMult-VAE, SVAE ",
                    "Citation Text": "Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, and Vikram Pudi. 2019. Se-\nquential variational autoencoders for collaborative filtering. In Proceedings of the\nTwelfth ACM International Conference on Web Search and Data Mining . 600\u2013608.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.09975",
                        "Citation Paper Title": "Title:Sequential Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Variational autoencoders were proven successful in domains such as computer vision and speech processing. Their adoption for modeling user preferences is still unexplored, although recently it is starting to gain attention in the current literature. In this work, we propose a model which extends variational autoencoders by exploiting the rich information present in the past preference history. We introduce a recurrent version of the VAE, where instead of passing a subset of the whole history regardless of temporal dependencies, we rather pass the consumption sequence subset through a recurrent neural network. At each time-step of the RNN, the sequence is fed through a series of fully-connected layers, the output of which models the probability distribution of the most likely future preferences. We show that handling temporal information is crucial for improving the accuracy of the VAE: In fact, our model beats the current state-of-the-art by valuable margins because of its ability to capture temporal dependencies among the user-consumption sequence using the recurrent encoder still keeping the fundamentals of variational autoencoders intact.",
                        "Citation Paper Authors": "Authors:Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, Vikram Pudi"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "which is similar to\ndenoising autoencoder. The encoded user preference variables ( i.e.,\nthe latent variables) in variational autoencoder can be used in gen-\nerating the distribution of recommended items. Mult-VAE ",
                    "Citation Text": "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.\nVariational autoencoders for collaborative filtering. In Proceedings of the 2018\nWorld Wide Web Conference . 689\u2013698.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "employs VAE to\nlearn disentangled representations that can enhance robustness.\nRecVAE ",
                    "Citation Text": "Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I\nNikolenko. 2020. RecVAE: A New Variational Autoencoder for Top-N Recommen-\ndations with Implicit Feedback. In Proceedings of the 13th International Conference\non Web Search and Data Mining . 528\u2013536.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11160",
                        "Citation Paper Title": "Title:RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Recent research has shown the advantages of using autoencoders based on deep neural networks for collaborative filtering. In particular, the recently proposed Mult-VAE model, which used the multinomial likelihood variational autoencoders, has shown excellent results for top-N recommendations. In this work, we propose the Recommender VAE (RecVAE) model that originates from our research on regularization techniques for variational autoencoders. RecVAE introduces several novel ideas to improve Mult-VAE, including a novel composite prior distribution for the latent codes, a new approach to setting the $\\beta$ hyperparameter for the $\\beta$-VAE framework, and a new approach to training based on alternating updates. In experimental evaluation, we show that RecVAE significantly outperforms previously proposed autoencoder-based models, including Mult-VAE and RaCT, across classical collaborative filtering datasets, and present a detailed ablation study to assess our new developments. Code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, Sergey I. Nikolenko"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "em-\nploys a recurrent neural network and includes both user and item\nfeatures in variational inference. MacridVAE ",
                    "Citation Text": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-\ning disentangled representations for recommendation. In Advances in Neural\nInformation Processing Systems . 5711\u20135722.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14238",
                        "Citation Paper Title": "Title:Learning Disentangled Representations for Recommendation",
                        "Citation Paper Abstract": "Abstract:User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",
                        "Citation Paper Authors": "Authors:Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "takes in a sequence of items with sequential\ndependencies, and processes it with the GRU network and finally\noutputs probability distribution of candidate items. CVRCF ",
                    "Citation Text": "Qingquan Song, Shiyu Chang, and Xia Hu. 2019. Coupled Variational Recurrent\nCollaborative Filtering. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 335\u2013343.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04386",
                        "Citation Paper Title": "Title:Coupled Variational Recurrent Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We focus on the problem of streaming recommender system and explore novel collaborative filtering algorithms to handle the data dynamicity and complexity in a streaming manner. Although deep neural networks have demonstrated the effectiveness of recommendation tasks, it is lack of explorations on integrating probabilistic models and deep architectures under streaming recommendation settings. Conjoining the complementary advantages of probabilistic models and deep neural networks could enhance both model effectiveness and the understanding of inference uncertainties. To bridge the gap, in this paper, we propose a Coupled Variational Recurrent Collaborative Filtering (CVRCF) framework based on the idea of Deep Bayesian Learning to handle the streaming recommendation problem. The framework jointly combines stochastic processes and deep factorization models under a Bayesian paradigm to model the generation and evolution of users' preferences and items' popularities. To ensure efficient optimization and streaming update, we further propose a sequential variational inference algorithm based on a cross variational recurrent neural network structure. Experimental results on three benchmark datasets demonstrate that the proposed framework performs favorably against the state-of-the-art methods in terms of both temporal dependency modeling and predictive accuracy. The learned latent variables also provide visualized interpretations for the evolution of temporal dynamics.",
                        "Citation Paper Authors": "Authors:Qingquan Song, Shiyu Chang, Xia Hu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.10631v1": {
            "Paper Title": "EXSCLAIM! -- An automated pipeline for the construction of labeled\n  materials imaging datasets from literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.10474v1": {
            "Paper Title": "Dynamic Model for Query-Document Expansion towards Improving Retrieval\n  Relevance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06146v3": {
            "Paper Title": "High Dimensional Similarity Search with Satellite System Graph:\n  Efficiency, Scalability, and Unindexed Query Compatibility",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.09944v1": {
            "Paper Title": "IRLI: Iterative Re-partitioning for Learning to Index",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06815v2": {
            "Paper Title": "Exploring Classic and Neural Lexical Translation Models for Information\n  Retrieval: Interpretability, Effectiveness, and Efficiency Benefits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01105v1": {
            "Paper Title": "Capturing Knowledge of Emerging Entities From Extended Search Snippets",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "introduces the head-word distant\nsupervision approach to predict free-form noun phrases as types that the target\nentity plays in a given sentence. We used this noun-phrase approach in entail-\ning the types of the target emerging entities. Whereas ",
                    "Citation Text": "M. Fan, D. Zhao, Q. Zhou, Z. Liu, T. F. Zheng, E. Y. Chang, Distant\nsupervision for relation extraction with matrix completion, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4455",
                        "Citation Paper Title": "Title:Errata: Distant Supervision for Relation Extraction with Matrix Completion",
                        "Citation Paper Abstract": "Abstract:The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, Edward Y. Chang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.10606v2": {
            "Paper Title": "Automating Outlier Detection via Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.03756v2": {
            "Paper Title": "BOPI: A Programming Interface For Reuse Of Research Data Available On\n  DSpace Repositories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.09442v1": {
            "Paper Title": "Improved Deep Classwise Hashing With Centers Similarity Learning for\n  Image Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", combined\nwith deep feature learning. The two methods are denoted as\nSDH+CNN and FSDH+CNN. For the feature learning part, we\nsimply employ GoogLeNet ",
                    "Citation Text": "C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d\ninProceedings of the IEEE conference on computer vision and pattern\nrecognition , 2015, pp. 1\u20139.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.08458v2": {
            "Paper Title": "Deep Dynamic Neural Network to trade-off between Accuracy and Diversity\n  in a News Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08976v1": {
            "Paper Title": "Dual Side Deep Context-aware Modulation for Social Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "proposed a deep interest network to adaptively learn the user inter-\nests from historical behaviors concerning a candidate item. As for\nthe social recommendation, ",
                    "Citation Text": "Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, and Qing Li. 2019.\nDeep social collaborative filtering. In Proceedings of the 13th ACM Conference on\nRecommender Systems . 305\u2013313.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.06853",
                        "Citation Paper Title": "Title:Deep Social Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Recommender systems are crucial to alleviate the information overload problem in online worlds. Most of the modern recommender systems capture users' preference towards items via their interactions based on collaborative filtering techniques. In addition to the user-item interactions, social networks can also provide useful information to understand users' preference as suggested by the social theories such as homophily and influence. Recently, deep neural networks have been utilized for social recommendations, which facilitate both the user-item interactions and the social network information. However, most of these models cannot take full advantage of the social network information. They only use information from direct neighbors, but distant neighbors can also provide helpful information. Meanwhile, most of these models treat neighbors' information equally without considering the specific recommendations. However, for a specific recommendation case, the information relevant to the specific item would be helpful. Besides, most of these models do not explicitly capture the neighbor's opinions to items for social recommendations, while different opinions could affect the user differently. In this paper, to address the aforementioned challenges, we propose DSCF, a Deep Social Collaborative Filtering framework, which can exploit the social relations with various aspects for recommender systems. Comprehensive experiments on two-real world datasets show the effectiveness of the proposed framework.",
                        "Citation Paper Authors": "Authors:Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, Qing Li"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": ". And the\nbasic context is the candidate item and targeted user. Such as ",
                    "Citation Text": "Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-\nThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 1059\u20131068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "developed a GNN based model\nto simulate both the social influence and user interest diffusion pro-\ncess. And DANSER ",
                    "Citation Text": "Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, and\nGuihai Chen. 2019. Dual graph attention networks for deep latent representation\nof multifaceted social effects in recommender systems. In The World Wide Web\nConference . 2091\u20132102.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10433",
                        "Citation Paper Title": "Title:Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Social recommendation leverages social information to solve data sparsity and cold-start problems in traditional collaborative filtering methods. However, most existing models assume that social effects from friend users are static and under the forms of constant weights or fixed constraints. To relax this strong assumption, in this paper, we propose dual graph attention networks to collaboratively learn representations for two-fold social effects, where one is modeled by a user-specific attention weight and the other is modeled by a dynamic and context-aware attention weight. We also extend the social effects in user domain to item domain, so that information from related items can be leveraged to further alleviate the data sparsity problem. Furthermore, considering that different social effects in two domains could interact with each other and jointly influence user preferences for items, we propose a new policy-based fusion strategy based on contextual multi-armed bandit to weigh interactions of various social effects. Experiments on one benchmark dataset and a commercial dataset verify the efficacy of the key components in our model. The results show that our model achieves great improvement for recommendation accuracy compared with other state-of-the-art social recommendation methods.",
                        "Citation Paper Authors": "Authors:Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao, Guihai Chen"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "pro-\nposed a random-walk graph neural network to learn the node\nembeddings in web-scale graphs. And NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural graph collaborative filtering. In Proceedings of the 42nd international ACM\nSIGIR conference on Research and development in Information Retrieval . 165\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "proposed a graph auto-encoder framework to predict\nunobserved interactions in the user-item matrix. Pinsage ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 974\u2013983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.08819v1": {
            "Paper Title": "A Novel Paper Recommendation Method Empowered by Knowledge Graph: for\n  Research Beginners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08733v1": {
            "Paper Title": "Category Aware Explainable Conversational Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08541v1": {
            "Paper Title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09159v2": {
            "Paper Title": "Misplaced trust? The relationship between trust, ability to identify\n  commercially influenced results, and search engine preference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08115v1": {
            "Paper Title": "Universal Representation Learning of Knowledge Bases by Jointly\n  Embedding Instances and Ontological Concepts",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "to optimize the joint loss\nfunction. We initialize vectors by drawing from a uniform distri-\nbution on the unit spherical surface, and initialize matrices using\nrandom orthogonal initialization ",
                    "Citation Text": "Andrew M Saxe, James L McClelland, and Surya Ganguli. 2013. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks. ICLR (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6120",
                        "Citation Paper Title": "Title:Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
                        "Citation Paper Abstract": "Abstract:Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.",
                        "Citation Paper Authors": "Authors:Andrew M. Saxe, James L. McClelland, Surya Ganguli"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "associates related\nentities using Hadamard product of embeddings, and HolE ",
                    "Citation Text": "Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al .2016. Holographic\nEmbeddings of Knowledge Graphs.. In AAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.04935",
                        "Citation Paper Title": "Title:Holographic Embeddings of Knowledge Graphs",
                        "Citation Paper Abstract": "Abstract:Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.",
                        "Citation Paper Authors": "Authors:Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". A few studies extend embedding techniques\nto general cross-domain ontologies like ConceptNet. Examples of\nsuch include On2Vec ",
                    "Citation Text": "Muhao Chen, Yingtao Tian, Xuelu Chen, Zijun Xue, and Carlo Zaniolo. 2018.\nOn2Vec: Embedding-based Relation Prediction for Ontology Population. In SDM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.02382",
                        "Citation Paper Title": "Title:On2Vec: Embedding-based Relation Prediction for Ontology Population",
                        "Citation Paper Abstract": "Abstract:Populating ontology graphs represents a long-standing problem for the Semantic Web community. Recent advances in translation-based graph embedding methods for populating instance-level knowledge graphs lead to promising new approaching for the ontology population problem. However, unlike instance-level graphs, the majority of relation facts in ontology graphs come with comprehensive semantic relations, which often include the properties of transitivity and symmetry, as well as hierarchical relations. These comprehensive relations are often too complex for existing graph embedding methods, and direct application of such methods is not feasible. Hence, we propose On2Vec, a novel translation-based graph embedding method for ontology population. On2Vec integrates two model components that effectively characterize comprehensive relation facts in ontology graphs. The first is the Component-specific Model that encodes concepts and relations into low-dimensional embedding spaces without a loss of relational properties; the second is the Hierarchy Model that performs focused learning of hierarchical relation facts. Experiments on several well-known ontology graphs demonstrate the promising capabilities of On2Vec in predicting and verifying new relation facts. These promising results also make possible significant improvements in related methods.",
                        "Citation Paper Authors": "Authors:Muhao Chen, Yingtao Tian, Xuelu Chen, Zijun Xue, Carlo Zaniolo"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "sub-\nstitutes Hadamard product with circular correlation to improve the\nencoding of asymmetric relations, and achieves the state-of-the-art\nperformance in KG completion. ComplEx ",
                    "Citation Text": "Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume\nBouchard. 2016. Complex embeddings for simple link prediction. In ICML .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06357",
                        "Citation Paper Title": "Title:Complex Embeddings for Simple Link Prediction",
                        "Citation Paper Abstract": "Abstract:In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.08057v1": {
            "Paper Title": "RecSim NG: Toward Principled Uncertainty Modeling for Recommender\n  Ecosystems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.08465v2": {
            "Paper Title": "Prioritizing Original News on Facebook",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07849v1": {
            "Paper Title": "Fairness-aware Personalized Ranking Recommendation via Adversarial\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05861v4": {
            "Paper Title": "ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08768v2": {
            "Paper Title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07597v1": {
            "Paper Title": "DeepGroup: Representation Learning for Group Recommendation with\n  Implicit Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07052v1": {
            "Paper Title": "Improving Authorship Verification using Linguistic Divergence",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ". In 2017,\na non-Machine Learning model comprised of a compression algorithm, a dissimilarity method,\nand a threshold was proposed for AV tasks, achieving first place in two of four challenges ",
                    "Citation Text": "O. Halvani, C. Winter, L. Graner, On the usefulness of compression models for authorship\nverification, in: Proceedings of the 12th International Conference on Availability, Reliability\nand Security, ACM, 2017, p. 54.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00516",
                        "Citation Paper Title": "Title:Authorship Verification based on Compression-Models",
                        "Citation Paper Abstract": "Abstract:Compression models represent an interesting approach for different classification tasks and have been used widely across many research fields. We adapt compression models to the field of authorship verification (AV), a branch of digital text forensics. The task in AV is to verify if a questioned document and a reference document of a known author are written by the same person. We propose an intrinsic AV method, which yields competitive results compared to a number of current state-of-the-art approaches, based on support vector machines or neural networks. However, in contrast to these approaches our method does not make use of machine learning algorithms, natural language processing techniques, feature engineering, hyperparameter optimization or external documents (a common strategy to transform AV from a one-class to a multi-class classification problem). Instead, the only three key components of our method are a compressing algorithm, a dissimilarity measure and a threshold, needed to accept or reject the authorship of the questioned document. Due to its compactness, our method performs very fast and can be reimplemented with minimal effort. In addition, the method can handle complicated AV cases where both, the questioned and the reference document, are not related to each other in terms of topic or genre. We evaluated our approach against publicly available datasets, which were used in three international AV competitions. Furthermore, we constructed our own corpora, where we evaluated our method against state-of-the-art approaches and achieved, in both cases, promising results.",
                        "Citation Paper Authors": "Authors:Oren Halvani, Christian Winter, Lukas Graner"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". Parallel recurrent neural\nnetwork and transformation auto-encoder approaches produce excellent results for a variety of\nAV problems ",
                    "Citation Text": "M. Hosseinia, A. Mukherjee, Experiments with neural networks for small and large scale\nauthorship verification, CoRR abs/1803.06456 (2018). URL: http://arxiv.org/abs/1803.06456.\narXiv:1803.06456 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06456",
                        "Citation Paper Title": "Title:Experiments with Neural Networks for Small and Large Scale Authorship Verification",
                        "Citation Paper Abstract": "Abstract:We propose two models for a special case of authorship verification problem. The task is to investigate whether the two documents of a given pair are written by the same author. We consider the authorship verification problem for both small and large scale datasets. The underlying small-scale problem has two main challenges: First, the authors of the documents are unknown to us because no previous writing samples are available. Second, the two documents are short (a few hundred to a few thousand words) and may differ considerably in the genre and/or topic. To solve it we propose transformation encoder to transform one document of the pair into the other. This document transformation generates a loss which is used as a recognizable feature to verify if the authors of the pair are identical. For the large scale problem where various authors are engaged and more examples are available with larger length, a parallel recurrent neural network is proposed. It compares the language models of the two documents. We evaluate our methods on various types of datasets including Authorship Identification datasets of PAN competition, Amazon reviews, and machine learning articles. Experiments show that both methods achieve stable and competitive performance compared to the baselines.",
                        "Citation Paper Authors": "Authors:Marjan Hosseinia, Arjun Mukherjee"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ". A spy induction method was proposed to\nleverage the test data during the training step under \"out-of-training\" setting, where the author\nin question is from a closed set of candidates while appearing unknown to the verifier ",
                    "Citation Text": "M. Hosseinia, A. Mukherjee, Detecting sockpuppets in deceptive opinion spam, Computing\nResearch Repository abs/1703.03149 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.03149",
                        "Citation Paper Title": "Title:Detecting Sockpuppets in Deceptive Opinion Spam",
                        "Citation Paper Abstract": "Abstract:This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches. Two methods are explored. The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features. The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors. Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes.",
                        "Citation Paper Authors": "Authors:Marjan Hosseinia, Arjun Mukherjee"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ".\nAV is studied for detecting linguistic traits of sock-puppets to verify the authorship of a pair\nof accounts in online discussion communities ",
                    "Citation Text": "S. Kumar, J. Cheng, J. Leskovec, V. Subrahmanian, An army of me: Sockpuppets in online\ndiscussion communities, in: Proceedings of the 26th International Conference on WorldWide Web, International World Wide Web Conferences Steering Committee, 2017, pp.\n857\u2013866.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.07355",
                        "Citation Paper Title": "Title:An Army of Me: Sockpuppets in Online Discussion Communities",
                        "Citation Paper Abstract": "Abstract:In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as \"I\", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.",
                        "Citation Paper Authors": "Authors:Srijan Kumar, Justin Cheng, Jure Leskovec, V.S. Subrahmanian"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.00917v2": {
            "Paper Title": "An open-source framework for ExpFinder integrating $N$-gram Vector Space\n  Model and $\u03bc$CO-HITS",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.13367v3": {
            "Paper Title": "SAED: Edge-Based Intelligence for Privacy-Preserving Enterprise Search\n  on the Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02230v2": {
            "Paper Title": "Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term\n  Importance Estimation and Neural Query Rewriting",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": ", in contrast, expands queries by teaching models to optimally concatenate previous\nutterances in conversational contexts. Finally, in the third paper, Voskarides et al . ",
                    "Citation Text": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke. 2020. Query Resolution for Conversational Search\nwith Limited Supervision. In Proc. SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11723",
                        "Citation Paper Title": "Title:Query Resolution for Conversational Search with Limited Supervision",
                        "Citation Paper Abstract": "Abstract:In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.",
                        "Citation Paper Authors": "Authors:Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "propose unsupervised query expansion using keywords from conversational contexts, in which keywords\nare extracted according to a word graph built through external data (i.e., word embeddings). The second work\nby Aliannejadi et al . ",
                    "Citation Text": "Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, and Fabio Crestani. 2020. Harnessing Evolution of Multi-Turn\nConversations for Effective Answer Retrieval. In Proc. CHIIR . 33\u201342.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10554",
                        "Citation Paper Title": "Title:Harnessing Evolution of Multi-Turn Conversations for Effective Answer Retrieval",
                        "Citation Paper Abstract": "Abstract:With the improvements in speech recognition and voice generation technologies over the last years, a lot of companies have sought to develop conversation understanding systems that run on mobile phones or smart home devices through natural language interfaces. Conversational assistants, such as Google Assistant and Microsoft Cortana, can help users to complete various types of tasks. This requires an accurate understanding of the user's information need as the conversation evolves into multiple turns. Finding relevant context in a conversation's history is challenging because of the complexity of natural language and the evolution of a user's information need. In this work, we present an extensive analysis of language, relevance, dependency of user utterances in a multi-turn information-seeking conversation. To this aim, we have annotated relevant utterances in the conversations released by the TREC CaST 2019 track. The annotation labels determine which of the previous utterances in a conversation can be used to improve the current one. Furthermore, we propose a neural utterance relevance model based on BERT fine-tuning, outperforming competitive baselines. We study and compare the performance of multiple retrieval models, utilizing different strategies to incorporate the user's context. The experimental results on both classification and retrieval tasks show that our proposed approach can effectively identify and incorporate the conversation context. We show that processing the current utterance using the predicted relevant utterance leads to a 38% relative improvement in terms of nDCG@20. Finally, to foster research in this area, we have released the dataset of the annotations.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, Fabio Crestani"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "seeks to facilitate IR in a conversational context, covering\ntwo main perspectives: information-need elucidation and information presentation. Information-need elucidation\nfocuses on understanding users\u2019 information needs in dialogue contexts, such as query clarification ",
                    "Citation Text": "Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2019. Asking Clarifying Questions in Open-Domain\nInformation-Seeking Conversations. In Proc. SIGIR . 475\u2013484.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.06554",
                        "Citation Paper Title": "Title:Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
                        "Citation Paper Abstract": "Abstract:Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of (often only one) result(s). In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, W. Bruce Croft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.07531v2": {
            "Paper Title": "Simplified TinyBERT: Knowledge Distillation for Document Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01039v3": {
            "Paper Title": "Improving reference mining in patents with BERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.05923v1": {
            "Paper Title": "Improving Sequential Recommendation with Attribute-augmented Graph\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.05457v1": {
            "Paper Title": "Rudder: A Cross Lingual Video and Text Retrieval Dataset",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", (vi) S2VT (Sequence to Sequence Video Text\nretrieval), which is a reimplementation of ",
                    "Citation Text": "Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond\nMooney, and Kate Saenko. 2015. Translating Videos to Natural Language Using\nDeep Recurrent Neural Networks. In NAACL-HLT.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.4729",
                        "Citation Paper Title": "Title:Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.",
                        "Citation Paper Authors": "Authors:Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nWe subject the output of the aggregation step to the text encoding\narchitecture proposed in ",
                    "Citation Text": "Antoine Miech, Ivan Laptev, and Josef Sivic. 2019. Learning a Text-Video Embed-\nding from Incomplete and Heterogeneous Data. In ICCV.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.02516",
                        "Citation Paper Title": "Title:Learning a Text-Video Embedding from Incomplete and Heterogeneous Data",
                        "Citation Paper Abstract": "Abstract:Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks. Code is available at: this https URL",
                        "Citation Paper Authors": "Authors:Antoine Miech, Ivan Laptev, Josef Sivic"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "as our base model. We\nchoose four pretrained models to serve as experts and derived fea-\ntures from the scene [ 3,16], audio (VGG ",
                    "Citation Text": "Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren\nJansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan\nSeybold, et al .2017. CNN architectures for large-scale audio classification. In\nICASSP.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09430",
                        "Citation Paper Title": "Title:CNN Architectures for Large-Scale Audio Classification",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.",
                        "Citation Paper Authors": "Authors:Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", we work with 1 caption per video.\n(3)DiDeMo: Distinct Describable Moments (DiDeMo) ",
                    "Citation Text": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,\nand Bryan Russell. 2018. Localizing Moments in Video with Temporal Language.\nInEMNLP.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.01641",
                        "Citation Paper Title": "Title:Localizing Moments in Video with Natural Language",
                        "Citation Paper Abstract": "Abstract:We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.",
                        "Citation Paper Authors": "Authors:Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan Russell"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "in the sense\nthat we also consider the extended neighborhood between the\npositives and negatives. The approach closest to our partial-order\nbased loss, however, is the work of ",
                    "Citation Text": "K. Karaman, E. Gundogdu, A. Ko\u00e7, and A. A. Alatan. 2019. Quadruplet Selection\nMethods for Deep Embedding Learning. In ICIP.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.09245",
                        "Citation Paper Title": "Title:Quadruplet Selection Methods for Deep Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.",
                        "Citation Paper Authors": "Authors:Kaan Karaman, Erhan Gundogdu, Aykut Koc, A. Aydin Alatan"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "propose a distance-\nbased sampling method to choose stable and informative samples\nthat can be trained with a simple max-margin loss. Likewise, ",
                    "Citation Text": "L. Wang, Y. Li, J. Huang, and S. Lazebnik. 2019. Learning Two-Branch Neural\nNetworks for Image-Text Matching Tasks. TPAMI (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.03470",
                        "Citation Paper Title": "Title:Learning Two-Branch Neural Networks for Image-Text Matching Tasks",
                        "Citation Paper Abstract": "Abstract:Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching a phrase to relevant regions. This paper investigates two-branch neural networks for learning the similarity between these two data modalities. We propose two network structures that produce different output representations. The first one, referred to as an embedding network, learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints. Compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. The second network structure, referred to as a similarity network, fuses the two branches via element-wise product and is trained with regression loss to directly predict a similarity score. Extensive experiments show that our networks achieve high accuracies for phrase localization on the Flickr30K Entities dataset and for bi-directional image-sentence retrieval on Flickr30K and MSCOCO datasets.",
                        "Citation Paper Authors": "Authors:Liwei Wang, Yin Li, Jing Huang, Svetlana Lazebnik"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "propose a Portuguese/English\ndataset with audio, video and textual captions for multi-modal\nlearning. ",
                    "Citation Text": "Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang\nWang. 2019. VATEX: A Large-Scale, High-Quality Multilingual Dataset for\nVideo-and-Language Research. CoRR abs/1904.03493 (2019). arXiv:1904.03493\nhttp://arxiv.org/abs/1904.03493",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03493",
                        "Citation Paper Title": "Title:VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research",
                        "Citation Paper Abstract": "Abstract:We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.",
                        "Citation Paper Authors": "Authors:Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "pro-\nposed a multi-lingual image dataset, FM-IQA, for image question an-\nswering in Chinese and English. ",
                    "Citation Text": "Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo\u00efc Barrault,\nLucia Specia, and Florian Metze. 2018. How2: A Large-scale Dataset for Multi-\nmodal Language Understanding. CoRR abs/1811.00347 (2018). arXiv:1811.00347\nhttp://arxiv.org/abs/1811.00347",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00347",
                        "Citation Paper Title": "Title:How2: A Large-scale Dataset for Multimodal Language Understanding",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.",
                        "Citation Paper Authors": "Authors:Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo\u00efc Barrault, Lucia Specia, Florian Metze"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "MultiLingual Visual Datasets: While vision and language have\nbeen used in conjunction in several datasets [ 10,13,35], multi-\nlingual visual datasets have continued to be under-explored. ",
                    "Citation Text": "Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.\n2015. Are You Talking to a Machine? Dataset and Methods for Multilingual\nImage Question Answering. In Proceedings ofthe29th Conference onNeural\nInformation Processing Systems (NeurIPS).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.05612",
                        "Citation Paper Title": "Title:Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering",
                        "Citation Paper Abstract": "Abstract:In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: this http URL",
                        "Citation Paper Authors": "Authors:Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.05256v1": {
            "Paper Title": "CEQE: Contextualized Embeddings for Query Expansion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06125v1": {
            "Paper Title": "Learning to Generate Music With Sentiment",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "used a\nsingle-layer multiplicative long short-term memory (mL-\nSTM) network ",
                    "Citation Text": "Ben Krause, Iain Murray, Steve Renals, and Liang Lu.\nMultiplicative LSTM for sequence modelling. ICLR\nWorkshop track , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.07959",
                        "Citation Paper Title": "Title:Multiplicative LSTM for sequence modelling",
                        "Citation Paper Abstract": "Abstract:We introduce multiplicative LSTM (mLSTM), a recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. mLSTM is characterised by its ability to have different recurrent transition functions for each possible input, which we argue makes it more expressive for autoregressive density estimation. We demonstrate empirically that mLSTM outperforms standard LSTM and its deep variants for a range of character level language modelling tasks. In this version of the paper, we regularise mLSTM to achieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also apply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a character level entropy of 1.26 bits/char, corresponding to a word level perplexity of 88.8, which is comparable to word level LSTMs regularised in similar ways on the same task.",
                        "Citation Paper Authors": "Authors:Ben Krause, Liang Lu, Iain Murray, Steve Renals"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "use a\ndependency network and a Gibbs-like sampling procedure\nto generate high-quality four-part chorales in the style of\nBach. Roberts et at. ",
                    "Citation Text": "Adam Roberts, Jesse Engel, and Douglas Eck, edi-\ntors. Hierarchical Variational Autoencoders for Music ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05428",
                        "Citation Paper Title": "Title:A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music",
                        "Citation Paper Abstract": "Abstract:The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the \"posterior collapse\" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a \"flat\" baseline model. An implementation of our \"MusicVAE\" is available online at this http URL.",
                        "Citation Paper Authors": "Authors:Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.04808v1": {
            "Paper Title": "Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding\n  Similarity on FPGAs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02360v2": {
            "Paper Title": "Research Progress of News Recommendation Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.04437v1": {
            "Paper Title": "Usability Evaluation for Online Professional Search in the Dutch\n  Archaeology Domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15982v2": {
            "Paper Title": "A Model of Two Tales: Dual Transfer Learning Framework for Improved\n  Long-tail Item Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "learn the distance across different tasks to\nuncover the general prototype. Model-based approaches ",
                    "Citation Text": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy\nLillicrap. 2016. Meta-learning with memory-augmented neural networks. In\nInternational conference on machine learning .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06065",
                        "Citation Paper Title": "Title:One-shot Learning with Memory-Augmented Neural Networks",
                        "Citation Paper Abstract": "Abstract:Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",
                        "Citation Paper Authors": "Authors:Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.04390v1": {
            "Paper Title": "RevDet: Robust and Memory Efficient Event Detection and Tracking in\n  Large News Feeds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.04156v1": {
            "Paper Title": "Improving Zero-Shot Entity Retrieval through Effective Dense\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00380v2": {
            "Paper Title": "LRG at TREC 2020: Document Ranking with XLNet-Based Models",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". BERT is\nused to get a combined representation of the query and the\ndocument with a linear layer on top which is then used for ob-\ntaining a score. XLNet ",
                    "Citation Text": "Z. Yang, Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhut-\ndinov, and Q. V . Le. XLNet: Generalized autoregres-\nsive pretraining for language understanding. CoRR ,\nabs/1906.08237, 2019. 1, 2, 3\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08237",
                        "Citation Paper Title": "Title:XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                        "Citation Paper Abstract": "Abstract:With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
                        "Citation Paper Authors": "Authors:Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.03587v1": {
            "Paper Title": "Graph Convolutional Embeddings for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "resembles the original FM and it adds\na term of concatenated embeddings. Finally, Neural Collaborative\nFiltering ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In Proceedings of the 26th international\nconference on world wide web . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "treated the neigh-\nborhood aggregation as a pre-computing process and accelerated\nthe training of GCN. Veli\u010dkovi\u0107 et al . ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "leverages the flexibility of neural networks to\nreplace dot products of matrix factorization and to learn higher-\norder signals. Deep FM ",
                    "Citation Text": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. arXiv\npreprint arXiv:1703.04247 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.03578v1": {
            "Paper Title": "Non-invasive Self-attention for Side Information Fusion in Sequential\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09268v2": {
            "Paper Title": "Training Large-Scale News Recommenders with Pretrained Language Models\n  in the Loop",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ",\nwhich was originally proposed for document classi\ufb01cation,\nis adapted for the multi-view representation of news articles ",
                    "Citation Text": "C. Wu, F. Wu, M. An, J. Huang, Y. Huang, and X. Xie, \u201cNeural\nnews recommendation with attentive multi-view learning,\u201d arXiv\npreprint arXiv:1907.05576 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05576",
                        "Citation Paper Title": "Title:Neural News Recommendation with Attentive Multi-View Learning",
                        "Citation Paper Abstract": "Abstract:Personalized news recommendation is very important for online news platforms to help users find interested news and improve user experience. News and user representation learning is critical for news recommendation. Existing news recommendation methods usually learn these representations based on single news information, e.g., title, which may be insufficient. In this paper we propose a neural news recommendation approach which can learn informative representations of users and news by exploiting different kinds of news information. The core of our approach is a news encoder and a user encoder. In the news encoder we propose an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. In addition, we apply both word-level and view-level attention mechanism to news encoder to select important words and views for learning informative news representations. In the user encoder we learn the representations of users based on their browsed news and apply attention mechanism to select informative news for user representation learning. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of news recommendation.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "; in recent years, the network structures are\nbeing quickly scaled up: from EMLo ",
                    "Citation Text": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d\narXiv preprint arXiv:1802.05365 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", memory networks\nare utilized to capture the diversity about users\u2019 behaviors.\nSuch technical advancement also inspires the development\nof news recommenders. In DKN ",
                    "Citation Text": "H. Wang, F. Zhang, X. Xie, and M. Guo, \u201cDkn: Deep knowledge-\naware network for news recommendation,\u201d in Proceedings of the\n2018 world wide web conference , 2018, pp. 1835\u20131844.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08284",
                        "Citation Paper Title": "Title:DKN: Deep Knowledge-Aware Network for News Recommendation",
                        "Citation Paper Abstract": "Abstract:Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.",
                        "Citation Paper Authors": "Authors:Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", users\u2019 historical behaviors are aggregated with GRUs\nfor sequential awareness; in DIN ",
                    "Citation Text": "G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin,\nH. Li, and K. Gai, \u201cDeep interest network for click-through rate\nprediction,\u201d in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining , 2018, pp. 1059\u2013\n1068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", users are represented as the\naverages of their interacted items\u2019 embeddings; in GRU4Rec ",
                    "Citation Text": "B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, \u201cSession-\nbased recommendations with recurrent neural networks,\u201d arXiv\npreprint arXiv:1511.06939 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.04276v4": {
            "Paper Title": "NISER: Normalized Item and Session Representations to Handle Popularity\n  Bias",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "as a working example. Furthermore, SR-GNN\ndoes not incorporate the sequential information explicitly to obtain\nthe session-graph representation. We study the effect of incorpo-\nrating position embeddings ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "introduces the\nring loss for soft feature normalization which eventually learns\nto constrain the feature vectors on a unit hypersphere. Normal-\nizing words embeddings is also popular in NLP applications, e.g. ",
                    "Citation Text": "Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2015.\nA comparative study on regularization strategies for embedding-based neural\nnetworks. arXiv preprint arXiv:1508.03721 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.03721",
                        "Citation Paper Title": "Title:A Comparative Study on Regularization Strategies for Embedding-based Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.",
                        "Citation Paper Authors": "Authors:Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Recent results in computer vision literature, e.g. [ 12,16], indicate\nthe effectiveness of normalizing the final image features during\ntraining, and argue in favor of cosine similarity over inner product\nfor learning and comparing feature vectors. ",
                    "Citation Text": "Yutong Zheng, Dipan K Pal, and Marios Savvides. 2018. Ring loss: Convex\nfeature normalization for face recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition . 5089\u20135097.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.00130",
                        "Citation Paper Title": "Title:Ring loss: Convex Feature Normalization for Face Recognition",
                        "Citation Paper Abstract": "Abstract:We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.",
                        "Citation Paper Authors": "Authors:Yutong Zheng, Dipan K. Pal, Marios Savvides"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.01896v3": {
            "Paper Title": "Taxonomy Completion via Triplet Matching Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02900v1": {
            "Paper Title": "The effects of having lists of synonyms on the performance of Afaan\n  Oromo Text Retrieval system",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02866v1": {
            "Paper Title": "IACN: Influence-aware and Attention-based Co-evolutionary Network for\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06130v1": {
            "Paper Title": "Stay on Topic, Please: Aligning User Comments to the Content of a News\n  Article",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02537v1": {
            "Paper Title": "Weakly-Supervised Open-Retrieval Conversational Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13181v3": {
            "Paper Title": "Position-Based Multiple-Play Bandits with Thompson Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02464v1": {
            "Paper Title": "User Preferential Tour Recommendation Based on POI-Embedding Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02462v1": {
            "Paper Title": "University of Copenhagen Participation in TREC Health Misinformation\n  Track 2020",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02259v1": {
            "Paper Title": "Computation Resource Allocation Solution in Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", while other\nwork employs the optimization in numerical calculation ",
                    "Citation Text": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.\n2015. Deep learning with limited numerical precision. In International Conference\non Machine Learning . 1737\u20131746.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.02551",
                        "Citation Paper Title": "Title:Deep Learning with Limited Numerical Precision",
                        "Citation Paper Abstract": "Abstract:Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.",
                        "Citation Paper Authors": "Authors:Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.01986v1": {
            "Paper Title": "Technical Report on Data Integration and Preparation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.01696v1": {
            "Paper Title": "Cross-Domain Recommendation: Challenges, Progress, and Prospects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.01576v1": {
            "Paper Title": "FinMatcher at FinSim-2: Hypernym Detection in the Financial Services\n  Domain using Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.01472v1": {
            "Paper Title": "TweetCOVID: A System for Analyzing Public Sentiments and Discussions\n  about COVID-19 via Twitter Activities",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "for sentiment analysis that allows us to\ndetermine the positivity and negativity of specific tweets, which we further aggregate for the overall COVID-19\ndiscussions. For specific emotions, we use the NRC Word-Emotion Association Lexicon ",
                    "Citation Text": "Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets. In Proc.\nof SemEval .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1308.6242",
                        "Citation Paper Title": "Title:NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets",
                        "Citation Paper Abstract": "Abstract:In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources.",
                        "Citation Paper Authors": "Authors:Saif M. Mohammad, Svetlana Kiritchenko, Xiaodan Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.01453v1": {
            "Paper Title": "Efficient Optimal Selection for Composited Advertising Creatives with\n  Tree Structure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.01432v1": {
            "Paper Title": "TopicTracker: A Platform for Topic Trajectory Identification and\n  Visualisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12793v2": {
            "Paper Title": "Learning to Truncate Ranked Lists for Information Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12256v2": {
            "Paper Title": "NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14395v2": {
            "Paper Title": "Contrastive Learning for Sequential Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". It combines GNN with self-attention mech-\nanism to capture both local and long-range transitions of\nneighbor items hidden in each interaction session.\n\u2022S3-Rec MIP ",
                    "Citation Text": "Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,\nZhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for\nSequential Recommendation with Mutual Information Maximization . Association\nfor Computing Machinery, 1893\u20131902. https://doi.org/10.1145/3340531.3411954",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07873",
                        "Citation Paper Title": "Title:S^3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization",
                        "Citation Paper Abstract": "Abstract:Recently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn model parameters or data representations. However, the model trained with this loss is prone to suffer from data sparsity problem. Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation. To tackle this problem, we propose the model S^3-Rec, which stands for Self-Supervised learning for Sequential Recommendation, based on the self-attentive neural architecture. The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation. For our task, we devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable in our scenario. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art methods, especially when only limited training data is available. Besides, we extend our self-supervised learning method to other recommendation models, which also improve their performance.",
                        "Citation Paper Authors": "Authors:Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.00498v1": {
            "Paper Title": "Topic Modelling Meets Deep Neural Networks: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07177v3": {
            "Paper Title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00199v1": {
            "Paper Title": "COVID-19 Tweets Analysis through Transformer Language Models",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "and situa-\ntion forecasting has been leveraged for surfacing the tech-\nniques of crisis management ",
                    "Citation Text": "E. Gharavi, N. Nazemi, and F. Dadgostari. Early out-\nbreak detection for proactive crisis management using twit-\nter data: Covid-19 a case study in the us. arXiv preprint\narXiv:2005.00475 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00475",
                        "Citation Paper Title": "Title:Early Outbreak Detection for Proactive Crisis Management Using Twitter Data: COVID-19 a Case Study in the US",
                        "Citation Paper Abstract": "Abstract:During a disease outbreak, timely non-medical interventions are critical in preventing the disease from growing into an epidemic and ultimately a pandemic. However, taking quick measures requires the capability to detect the early warning signs of the outbreak. This work collects Twitter posts surrounding the 2020 COVID-19 pandemic expressing the most common symptoms of COVID-19 including cough and fever, geolocated to the United States. Through examining the variation in Twitter activities at the state level, we observed a temporal lag between the rises in the number of symptom reporting tweets and officially reported positive cases which varies between 5 to 19 days.",
                        "Citation Paper Authors": "Authors:Erfaneh Gharavi, Neda Nazemi, Faraz Dadgostari"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", topic modelling has been used for\nthe analysis of the response of politicians ",
                    "Citation Text": "H. Sha, M. A. Hasan, G. Mohler, and P. J. Brantingham.\nDynamic topic modeling of the covid-19 twitter narrative\namong us governors and cabinet executives. arXiv preprint\narXiv:2004.11692 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11692",
                        "Citation Paper Title": "Title:Dynamic topic modeling of the COVID-19 Twitter narrative among U.S. governors and cabinet executives",
                        "Citation Paper Abstract": "Abstract:A combination of federal and state-level decision making has shaped the response to COVID-19 in the United States. In this paper we analyze the Twitter narratives around this decision making by applying a dynamic topic model to COVID-19 related tweets by U.S. Governors and Presidential cabinet members. We use a network Hawkes binomial topic model to track evolving sub-topics around risk, testing and treatment. We also construct influence networks amongst government officials using Granger causality inferred from the network Hawkes process.",
                        "Citation Paper Authors": "Authors:Hao Sha, Mohammad Al Hasan, George Mohler, P. Jeffrey Brantingham"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.00154v1": {
            "Paper Title": "Parallel Algorithms for Densest Subgraph Discovery Using Shared Memory\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05208v2": {
            "Paper Title": "Dynamic Embeddings for Interaction Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06137v1": {
            "Paper Title": "Task-adaptive Neural Process for User Cold-Start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12887v1": {
            "Paper Title": "Significant Improvements over the State of the Art? A Case Study of the\n  MS MARCO Document Ranking Leaderboard",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12865v4": {
            "Paper Title": "Self-supervised Learning for Large-scale Item Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "Self-supervised Learning and Pre-training. Various unsupervised\nand self-supervised learning tasks have been studied in the com-\nputer vision community. The closest line of research is SimCLR ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.\nA Simple Framework for Contrastive Learning of Visual Representations. https:\n//arxiv.org/abs/2002.05709",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.06124v1": {
            "Paper Title": "Semantically Constrained Memory Allocation (SCMA) for Embedding in\n  Efficient Recommendation Systems",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "that signi\ufb01cantly reduce the size\nof the embedding matrix for recommendation and click-\nthrough prediction systems. Namely, hashing trick ",
                    "Citation Text": "Wenlin Chen, James Wilson, Stephen Tyree, Kilian\nWeinberger, and Yixin Chen. Compressing neural\nnetworks with the hashing trick. In International\nconference on machine learning , pages 2285\u20132294.\nPMLR, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.04788",
                        "Citation Paper Title": "Title:Compressing Neural Networks with the Hashing Trick",
                        "Citation Paper Abstract": "Abstract:As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.",
                        "Citation Paper Authors": "Authors:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "and as implemented in their GitHub page. The batch size\nis set to 2,048 as we want to run experiments for a larger\nnumber of epochs. Also, a higher batch size is preferred\nfor CTR datasets ",
                    "Citation Text": "Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang,\nand Xiuqiang He. Fuxictr: An open benchmark\nfor click-through rate prediction. arXiv preprint\narXiv:2009.05794 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.05794",
                        "Citation Paper Title": "Title:BARS-CTR: Open Benchmarking for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is a critical task for many applications, as its accuracy has a direct impact on user experience and platform revenue. In recent years, CTR prediction has been widely studied in both academia and industry, resulting in a wide variety of CTR prediction models. Unfortunately, there is still a lack of standardized benchmarks and uniform evaluation protocols for CTR prediction research. This leads to non-reproducible or even inconsistent experimental results among existing studies, which largely limits the practical value and potential impact of their research. In this work, we aim to perform open benchmarking for CTR prediction and present a rigorous comparison of different models in a reproducible manner. To this end, we ran over 7,000 experiments for more than 12,000 GPU hours in total to re-evaluate 24 existing models on multiple datasets and settings. Surprisingly, our experiments show that with sufficient hyper-parameter search and model tuning, many deep models have smaller differences than expected. The results also reveal that making real progress on the modeling of CTR prediction is indeed a very challenging research task. We believe that our benchmarking work could not only allow researchers to gauge the effectiveness of new models conveniently but also make them fairly compare with the state of the arts. We have publicly released the benchmarking code, evaluation protocols, and hyper-parameter settings of our work to promote reproducible research in this field.",
                        "Citation Paper Authors": "Authors:Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.12413v1": {
            "Paper Title": "Designing Explanations for Group Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12342v1": {
            "Paper Title": "Similarity measure for sparse time course data based on Gaussian\n  processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12998v1": {
            "Paper Title": "Deep NMF Topic Modeling",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". Unsupervised topic modeling projects\ndocuments into a topic embedding space, which promotes\nthe development of document clustering. Recently, many\nworks focused on learning the representations and topic\nassignments of documents simultaneously by deep neural\nnetworks ",
                    "Citation Text": "Junyuan Xie, Ross Girshick, and Ali Farhadi, \u201cUnsupervised deep\nembedding for clustering analysis,\u201d in International conference on\nmachine learning , 2016, pp. 478\u2013487.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06335",
                        "Citation Paper Title": "Title:Unsupervised Deep Embedding for Clustering Analysis",
                        "Citation Paper Abstract": "Abstract:Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Junyuan Xie, Ross Girshick, Ali Farhadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.12188v1": {
            "Paper Title": "Support the Underground: Characteristics of Beyond-Mainstream Music\n  Listeners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06314v4": {
            "Paper Title": "Embracing Domain Differences in Fake News: Cross-domain Fake News\n  Detection using Multi-modal Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.12029v1": {
            "Paper Title": "Theoretical Understandings of Product Embedding for E-commerce Machine\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "), or by the follow-up sam-\npling steps, e.g. sampling the products from a given context window\n(CompProd2vec ",
                    "Citation Text": "Da Xu, Chuanwei Ruan, Jason Cho, Evren Korpeoglu, Sushant Kumar, and Kan-\nnan Achan. 2020. Knowledge-aware Complementary Product Representation\nLearning. In Proceedings of the 13th International Conference on Web Search and\nData Mining . 681\u2013689.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12574",
                        "Citation Paper Title": "Title:Knowledge-aware Complementary Product Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning product representations that reflect complementary relationship plays a central role in e-commerce recommender system. In the absence of the product relationships graph, which existing methods rely on, there is a need to detect the complementary relationships directly from noisy and sparse customer purchase activities. Furthermore, unlike simple relationships such as similarity, complementariness is asymmetric and non-transitive. Standard usage of representation learning emphasizes on only one set of embedding, which is problematic for modelling such properties of complementariness. We propose using knowledge-aware learning with dual product embedding to solve the above challenges. We encode contextual knowledge into product representation by multi-task learning, to alleviate the sparsity issue. By explicitly modelling with user bias terms, we separate the noise of customer-specific preferences from the complementariness. Furthermore, we adopt the dual embedding framework to capture the intrinsic properties of complementariness and provide geometric interpretation motivated by the classic separating hyperplane theory. Finally, we propose a Bayesian network structure that unifies all the components, which also concludes several popular models as special cases. The proposed method compares favourably to state-of-art methods, in downstream classification and recommendation tasks. We also develop an implementation that scales efficiently to a dataset with millions of items and customers.",
                        "Citation Paper Authors": "Authors:Da Xu, Chuanwei Ruan, Jason Cho, Evren Korpeoglu, Sushant Kumar, Kannan Achan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.11603v2": {
            "Paper Title": "SeqNet: Learning Descriptors for Sequence-based Hierarchical Place\n  Recognition",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nSuch localization pipelines require high recall performance\nfor VPR as the following 3D pose estimation modules often\nhave high precision in selecting the best match ",
                    "Citation Text": "P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, \u201cSu-\nperglue: Learning feature matching with graph neural networks,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2020, pp. 4938\u20134947.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11763",
                        "Citation Paper Title": "Title:SuperGlue: Learning Feature Matching with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2006.16977v2": {
            "Paper Title": "Learning Post-Hoc Causal Explanations for Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", etc. With respect to recommendation\ntasks, large amount of work is about how to achieve de-bias ma-\ntrix factorization with causal inference. The probabilistic approach\nExpoMF proposed in ",
                    "Citation Text": "Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Mod-\neling user exposure in recommendation. In Proceedings of the 25th International\nConference on World Wide Web . 951\u2013961.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.07025",
                        "Citation Paper Title": "Title:Modeling User Exposure in Recommendation",
                        "Citation Paper Abstract": "Abstract:Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis, the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model, and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.11314v1": {
            "Paper Title": "Distributed Application of Guideline-Based Decision Support through\n  Mobile Devices: Implementation and Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04219v2": {
            "Paper Title": "Mitigating Bias in Set Selection with Noisy Protected Attributes",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ".\nCounterfactually fair approaches. One could also consider counterfactually fair approaches to mitigate\nbias in selection. (We refer the reader to ",
                    "Citation Text": "Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NIPS ,\npages 4066{4076, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06856",
                        "Citation Paper Title": "Title:Counterfactual Fairness",
                        "Citation Paper Abstract": "Abstract:Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",
                        "Citation Paper Authors": "Authors:Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.10966v1": {
            "Paper Title": "Better Call the Plumber: Orchestrating Dynamic Information Extraction\n  Pipelines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.10962v1": {
            "Paper Title": "Entities of Interest",
            "Sentences": [
                {
                    "Sentence ID": 218,
                    "Sentence": "formalize and analyze the\nspeci\ufb01c challenges and aspects that come with linking emerging entities, while Reinanda\net al. ",
                    "Citation Text": "R. Reinanda, E. Meij, and M. de Rijke. Document \ufb01ltering for long-tail entities. CIKM, pages 771\u2013780.\nACM, 2016. (Cited on page 21.)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.04281",
                        "Citation Paper Title": "Title:Document Filtering for Long-tail Entities",
                        "Citation Paper Abstract": "Abstract:Filtering relevant documents with respect to entities is an essential task in the context of knowledge base construction and maintenance. It entails processing a time-ordered stream of documents that might be relevant to an entity in order to select only those that contain vital information. State-of-the-art approaches to document filtering for popular entities are entity-dependent: they rely on and are also trained on the specifics of differentiating features for each specific entity. Moreover, these approaches tend to use so-called extrinsic information such as Wikipedia page views and related entities which is typically only available only for popular head entities. Entity-dependent approaches based on such signals are therefore ill-suited as filtering methods for long-tail entities. In this paper we propose a document filtering method for long-tail entities that is entity-independent and thus also generalizes to unseen or rarely seen entities. It is based on intrinsic features, i.e., features that are derived from the documents in which the entities are mentioned. We propose a set of features that capture informativeness, entity-saliency, and timeliness. In particular, we introduce features based on entity aspect similarities, relation patterns, and temporal expressions and combine these with standard features for document filtering. Experiments following the TREC KBA 2014 setup on a publicly available dataset show that our model is able to improve the filtering performance for long-tail entities over several baselines. Results of applying the model to unseen entities are promising, indicating that the model is able to learn the general characteristics of a vital document. The overall performance across all entities---i.e., not just long-tail entities---improves upon the state-of-the-art without depending on any entity-specific training data.",
                        "Citation Paper Authors": "Authors:Ridho Reinanda, Edgar Meij, Maarten de Rijke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.12498v2": {
            "Paper Title": "Fake News Data Collection and Classification: Iterative Query Selection\n  for Opaque Search Engines with Pseudo Relevance Feedback",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". Since the detection of fake news is very challenging, many researchers\nsuggested different approaches to confronting this issue. Many of them were based on natural language processing ",
                    "Citation Text": "Zhixuan Zhou, Huankang Guan, Meghana Moorthy Bhat, and Justin Hsu. Fake news detection via nlp is vulnerable\nto adversarial attacks. arXiv preprint arXiv:1901.09657 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.09657",
                        "Citation Paper Title": "Title:Fake News Detection via NLP is Vulnerable to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.",
                        "Citation Paper Authors": "Authors:Zhixuan Zhou, Huankang Guan, Meghana Moorthy Bhat, Justin Hsu"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", etc. Also, a few papers have attempted to detect fake news solely using social\ncontext features ",
                    "Citation Text": "Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. Fake news detection on social media: A data\nmining perspective. ACM SIGKDD Explorations Newsletter , 19(1):22\u201336, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.01967",
                        "Citation Paper Title": "Title:Fake News Detection on Social Media: A Data Mining Perspective",
                        "Citation Paper Abstract": "Abstract:Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \"fake news\", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ineffective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.",
                        "Citation Paper Authors": "Authors:Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "is a supervised model for determining the relevance of a document given a particular\nquery. The proposed model employed a joint deep architecture at the query term level that estimates the query document\nsimilarity. Mitra et al. ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations\nof text for web search. In Proceedings of the 26th International Conference on World Wide Web , pages 1291\u20131299,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "generate multiple types of meta-features from texts\u2019 word embedding to train a supervised learning classi\ufb01er. Later,\nthey used the trained model for predicting the semantic similarity of new, unlabelled pairs of short texts. Deep relevance\nmatching model (DRMM) ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval.\nInProceedings of the 25th ACM International on Conference on Information and Knowledge Management , pages\n55\u201364, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08611",
                        "Citation Paper Title": "Title:A Deep Relevance Matching Model for Ad-hoc Retrieval",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". In most cases, selecting these queries requires reformulation or expansion of an initial query. Several\nmethods suggest analyzing the underlying corpus of the given search engine and used this valuable information for\nexpanding the queries. Dwaipayan et al. ",
                    "Citation Text": "Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. Using word embeddings for automatic query\nexpansion. arXiv preprint arXiv:1606.07608 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07608",
                        "Citation Paper Title": "Title:Using Word Embeddings for Automatic Query Expansion",
                        "Citation Paper Abstract": "Abstract:In this paper a framework for Automatic Query Expansion (AQE) is proposed using distributed neural language model word2vec. Using semantic and contextual relation in a distributed and unsupervised framework, word2vec learns a low dimensional embedding for each vocabulary entry. Using such a framework, we devise a query expansion technique, where related terms to a query are obtained by K-nearest neighbor approach. We explore the performance of the AQE methods, with and without feedback query expansion, and a variant of simple K-nearest neighbor in the proposed framework. Experiments on standard TREC ad-hoc data (Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with query set 451-550) shows significant improvement over standard term-overlapping based retrieval methods. However the proposed method fails to achieve comparable performance with statistical co-occurrence based feedback method such as RM3. We have also found that the word2vec based query expansion methods perform similarly with and without any feedback information.",
                        "Citation Paper Authors": "Authors:Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, Utpal Garain"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.10560v1": {
            "Paper Title": "A Concept Knowledge-Driven Keywords Retrieval Framework for Sponsored\n  Search",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "to better define user needs and\noffer more intelligent shopping experience in their e-commerce\nplatform. ",
                    "Citation Text": "Bang Liu, Weidong Guo, Di Niu, Chaoyue Wang, Shunnan Xu, Jinghong Lin,\nKunfeng Lai, and Yu Xu. 2019. A User-Centered Concept Mining System for\nQuery and Document Understanding at Tencent. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining . 1831\u2013\n1841.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08487",
                        "Citation Paper Title": "Title:A User-Centered Concept Mining System for Query and Document Understanding at Tencent",
                        "Citation Paper Abstract": "Abstract:Concepts embody the knowledge of the world and facilitate the cognitive processes of human beings. Mining concepts from web documents and constructing the corresponding taxonomy are core research problems in text understanding and support many downstream tasks such as query analysis, knowledge base construction, recommendation, and search. However, we argue that most prior studies extract formal and overly general concepts from Wikipedia or static web pages, which are not representing the user perspective. In this paper, we describe our experience of implementing and deploying ConcepT in Tencent QQ Browser. It discovers user-centered concepts at the right granularity conforming to user interests, by mining a large amount of user queries and interactive search click logs. The extracted concepts have the proper granularity, are consistent with user language styles and are dynamically updated. We further present our techniques to tag documents with user-centered concepts and to construct a topic-concept-instance taxonomy, which has helped to improve search as well as news feeds recommendation in Tencent QQ Browser. We performed extensive offline evaluation to demonstrate that our approach could extract concepts of higher quality compared to several other existing methods. Our system has been deployed in Tencent QQ Browser. Results from online A/B testing involving a large number of real users suggest that the Impression Efficiency of feeds users increased by 6.01% after incorporating the user-centered concepts into the recommendation framework of Tencent QQ Browser.",
                        "Citation Paper Authors": "Authors:Bang Liu, Weidong Guo, Di Niu, Chaoyue Wang, Shunnan Xu, Jinghong Lin, Kunfeng Lai, Yu Xu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "was applied to Amazon\u2019s virtual assistant Alexa to\noffer better answers to factual questions. A large-scale cognitive\nconcept net was constructed by ",
                    "Citation Text": "Xusheng Luo, Luxin Liu, Yonghua Yang, Le Bo, Yuanpeng Cao, Jinghang Wu,\nQiang Li, Keping Yang, and Kenny Q Zhu. 2020. AliCoCo: Alibaba E-commerce\nCognitive Concept Net. In Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data . 313\u2013327.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.13230",
                        "Citation Paper Title": "Title:AliCoCo: Alibaba E-commerce Cognitive Concept Net",
                        "Citation Paper Abstract": "Abstract:One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale e-commerce cognitive concept net named \"AliCoCo\", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.",
                        "Citation Paper Authors": "Authors:Xusheng Luo, Luxin Liu, Yonghua Yang, Le Bo, Yuanpeng Cao, Jinhang Wu, Qiang Li, Keping Yang, Kenny Q. Zhu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed a pivot approach for extracting\nparaphrase patterns from bilingual parallel corpora.\nKnowledge Graph is commonly used in information retrieve\nsystems ",
                    "Citation Text": "Colby Wise, Vassilis N Ioannidis, Miguel Romero Calvo, Xiang Song, George\nPrice, Ninad Kulkarni, Ryan Brand, Parminder Bhatia, and George Karypis. 2020.\nCOVID-19 knowledge graph: accelerating information retrieval and discovery\nfor scientific literature. arXiv preprint arXiv:2007.12731 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12731",
                        "Citation Paper Title": "Title:COVID-19 Knowledge Graph: Accelerating Information Retrieval and Discovery for Scientific Literature",
                        "Citation Paper Abstract": "Abstract:The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 6 million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID-19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.",
                        "Citation Paper Authors": "Authors:Colby Wise, Vassilis N. Ioannidis, Miguel Romero Calvo, Xiang Song, George Price, Ninad Kulkarni, Ryan Brand, Parminder Bhatia, George Karypis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.10246v1": {
            "Paper Title": "CDA: a Cost Efficient Content-based Multilingual Web Document Aligner",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.10162v1": {
            "Paper Title": "Clarification of Video Retrieval Query Results by the Automated\n  Insertion of Supporting Shots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05894v2": {
            "Paper Title": "MicroRec: Efficient Recommendation Inference by Hardware and Data\n  Structure Solutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11011v2": {
            "Paper Title": "Disentangling User Interest and Conformity for Recommendation with\n  Causal Embedding",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "proposed to use Variational Auto-Encoder to disen-\ntangle macro-level concept such as intention on different items,\nand disentangle micro-level factors like color or size of an item.\nWang et al. ",
                    "Citation Text": "Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tonog Xu, and Tat-Seng\nChua. 2020. Disentagnled Graph Collaborative Filtering. SIGIR (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.01764",
                        "Citation Paper Title": "Title:Disentangled Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning informative representations of users and items from the interaction data is of crucial importance to collaborative filtering (CF). Present embedding functions exploit user-item relationships to enrich the representations, evolving from a single user-item instance to the holistic interaction graph. Nevertheless, they largely model the relationships in a uniform manner, while neglecting the diversity of user intents on adopting the items, which could be to pass time, for interest, or shopping for others like families. Such uniform approach to model user interests easily results in suboptimal representations, failing to model diverse relationships and disentangle user intents in representations.\nIn this work, we pay special attention to user-item relationships at the finer granularity of user intents. We hence devise a new model, Disentangled Graph Collaborative Filtering (DGCF), to disentangle these factors and yield disentangled representations. Specifically, by modeling a distribution over intents for each user-item interaction, we iteratively refine the intent-aware interaction graphs and representations. Meanwhile, we encourage independence of different intents. This leads to disentangled representations, effectively distilling information pertinent to each intent. We conduct extensive experiments on three benchmark datasets, and DGCF achieves significant improvements over several state-of-the-art models like NGCF, DisenGCN, and MacridVAE. Further analyses offer insights into the advantages of DGCF on the disentanglement of user intents and interpretability of representations. Our codes are available in this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "learns interpretable representations from raw images in\nan unsupervised manner. Disentangled representation learning in\nrecommender systems was not explored until recently [ 14,33,47].Maet al. ",
                    "Citation Text": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-\ning disentangled representations for recommendation. In NeurIPS . 5712\u20135723.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14238",
                        "Citation Paper Title": "Title:Learning Disentangled Representations for Recommendation",
                        "Citation Paper Abstract": "Abstract:User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",
                        "Citation Paper Authors": "Authors:Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.04205v2": {
            "Paper Title": "How Pandemic Spread in News: Text Analysis Using Topic Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09681v1": {
            "Paper Title": "WebRED: Effective Pretraining And Finetuning For Relation Extraction On\n  The Web",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06986v2": {
            "Paper Title": "On the Problem of Underranking in Group-Fair Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06687v2": {
            "Paper Title": "Destination similarity based on implicit user interest",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09395v1": {
            "Paper Title": "Robust PDF Document Conversion Using Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10784v2": {
            "Paper Title": "Semantic Hypergraphs",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": "that has been revived lately with modern ma-\nchine learning methods ",
                    "Citation Text": "Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., Zettlemoyer, L., 2018. Deep contextualized\nword representations. arXiv 1802.05365.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ",\nand more recently a transformer language model has\nbeen proposed to automatically extend common-sense\nknowledge bases ",
                    "Citation Text": "Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyil-\nmaz, A., Choi, Y., 2019. Comet: Commonsense transform-\ners for automatic knowledge graph construction. arXiv\n1906.05317.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05317",
                        "Citation Paper Title": "Title:COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
                        "Citation Paper Abstract": "Abstract:We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
                        "Citation Paper Authors": "Authors:Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.09375v1": {
            "Paper Title": "Hierarchical Similarity Learning for Language-based Product Image\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", Ding et al. also leverage\ntransformer-based encoder to represent images and textual queries,\nbut they formulate the retrieval as a multi-task problem where image\ncaptioning ",
                    "Citation Text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru\nErhan, \u201cShow and tell: A neural image caption generator,\u201d in\nCVPR , 2015, pp. 3156\u20133164.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4555",
                        "Citation Paper Title": "Title:Show and Tell: A Neural Image Caption Generator",
                        "Citation Paper Abstract": "Abstract:Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                        "Citation Paper Authors": "Authors:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.09283v1": {
            "Paper Title": "Truncation-Free Matching System for Display Advertising at Alibaba",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09269v1": {
            "Paper Title": "Dynamic Memory based Attention Network for Sequential Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09267v1": {
            "Paper Title": "Sparse-Interest Network for Sequential Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "introduces a memory-augmented neural\nnetwork with the insights of collaborative filtering for the recom-\nmendation. SDM ",
                    "Citation Text": "Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wil-\nfred Ng. 2019. SDM: Sequential deep matching model for online large-scale\nrecommender system. In CIKM . 2635\u20132643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.00385",
                        "Citation Paper Title": "Title:SDM: Sequential Deep Matching Model for Online Large-scale Recommender System",
                        "Citation Paper Abstract": "Abstract:Capturing users' precise preferences is a fundamental problem in large-scale recommender system. Currently, item-based Collaborative Filtering (CF) methods are common matching approaches in industry. However, they are not effective to model dynamic and evolving preferences of users. In this paper, we propose a new sequential deep matching (SDM) model to capture users' dynamic preferences by combining short-term sessions and long-term behaviors. Compared with existing sequence-aware recommendation methods, we tackle the following two inherent problems in real-world applications: (1) there could exist multiple interest tendencies in one session. (2) long-term preferences may not be effectively fused with current session interests. Long-term behaviors are various and complex, hence those highly related to the short-term session should be kept for fusion. We propose to encode behavior sequences with two corresponding components: multi-head self-attention module to capture multiple types of interests and long-short term gated fusion module to incorporate long-term preferences. Successive items are recommended after matching between sequential user behavior vector and item embedding vectors. Offline experiments on real-world datasets show the superior performance of the proposed SDM. Moreover, SDM has been successfully deployed on online large-scale recommender system at Taobao and achieves improvements in terms of a range of commercial metrics.",
                        "Citation Paper Authors": "Authors:Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, Wilfred Ng"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ",\nwhich treats use\u2019s behavior sequence as an \"image\" and adopts\nConvolutional Neural Network to extract user representation.\nAnother line of works seeks to use a sequential neural module\nto process the user behavior sequence [ 16,21,38,41]. For example,\nGRU4Rec ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". An-\nother range of work targets to replace traditional MF. For example,\nNCF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In WWW . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "aim to model\nall interactions between variables using factorized parameters and\ncan even estimate interactions when facing sparsity problems.\nRecently, inspired by the success of deep learning in computer vi-\nsion and natural language processing ",
                    "Citation Text": "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\nommender system: A survey and new perspectives. ACM Computing Surveys\n(CSUR) 52, 1 (2019), 1\u201338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07435",
                        "Citation Paper Title": "Title:Deep Learning based Recommender System: A Survey and New Perspectives",
                        "Citation Paper Abstract": "Abstract:With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.",
                        "Citation Paper Authors": "Authors:Shuai Zhang, Lina Yao, Aixin Sun, Yi Tay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.04033v2": {
            "Paper Title": "A Hybrid Bandit Model with Visual Priors for Creative Ranking in Display\n  Advertising",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". [8,21,31] extend these meth-\nods by training the CNNs in an end-to-end manner. ",
                    "Citation Text": "Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang,\nWenjie Niu, Xiaokun Zhu, Yongjun Bao, et al .2020. Category-Specific CNN for\nVisual-aware CTR Prediction at JD. com. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . 2686\u20132696.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10337",
                        "Citation Paper Title": "Title:Category-Specific CNN for Visual-aware CTR Prediction at JD.com",
                        "Citation Paper Abstract": "Abstract:As one of the largest B2C e-commerce platforms in China, JD com also powers a leading advertising system, serving millions of advertisers with fingertip connection to hundreds of millions of customers. In our system, as well as most e-commerce scenarios, ads are displayed with images.This makes visual-aware Click Through Rate (CTR) prediction of crucial importance to both business effectiveness and user experience. Existing algorithms usually extract visual features using off-the-shelf Convolutional Neural Networks (CNNs) and late fuse the visual and non-visual features for the finally predicted CTR. Despite being extensively studied, this field still face two key challenges. First, although encouraging progress has been made in offline studies, applying CNNs in real systems remains non-trivial, due to the strict requirements for efficient end-to-end training and low-latency online serving. Second, the off-the-shelf CNNs and late fusion architectures are suboptimal. Specifically, off-the-shelf CNNs were designed for classification thus never take categories as input features. While in e-commerce, categories are precisely labeled and contain abundant visual priors that will help the visual modeling. Unaware of the ad category, these CNNs may extract some unnecessary category-unrelated features, wasting CNN's limited expression ability. To overcome the two challenges, we propose Category-specific CNN (CSCNN) specially for CTR prediction. CSCNN early incorporates the category knowledge with a light-weighted attention-module on each convolutional layer. This enables CSCNN to extract expressive category-specific visual patterns that benefit the CTR prediction. Offline experiments on benchmark and a 10 billion scale real production dataset from JD, together with an Online A/B test show that CSCNN outperforms all compared state-of-the-art algorithms.",
                        "Citation Paper Authors": "Authors:Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang, Wenjie Niu, Xiaokun Zhu, Yongjun Bao, Weipeng Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.08366v1": {
            "Paper Title": "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07575v2": {
            "Paper Title": "User Embedding based Neighborhood Aggregation Method for Inductive\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". In some application setting, knowledge graphs (e.g.,\nexplicit user-user relational graphs) are available, and they are used\nto improve the recommendation quality. See ",
                    "Citation Text": "Yang Gao, Yi-Fan Li, Yu Lin, Hang Gao, and Latifur Khan. 2020. Deep Learning on\nKnowledge Graph for Recommender System: A Survey. arXiv:2004.00387 [cs.IR]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00387",
                        "Citation Paper Title": "Title:Deep Learning on Knowledge Graph for Recommender System: A Survey",
                        "Citation Paper Abstract": "Abstract:Recent advances in research have demonstrated the effectiveness of knowledge graphs (KG) in providing valuable external knowledge to improve recommendation systems (RS). A knowledge graph is capable of encoding high-order relations that connect two objects with one or multiple related attributes. With the help of the emerging Graph Neural Networks (GNN), it is possible to extract both object characteristics and relations from KG, which is an essential factor for successful recommendations. In this paper, we provide a comprehensive survey of the GNN-based knowledge-aware deep recommender systems. Specifically, we discuss the state-of-the-art frameworks with a focus on their core component, i.e., the graph embedding module, and how they address practical recommendation issues such as scalability, cold-start and so on. We further summarize the commonly-used benchmark datasets, evaluation metrics as well as open-source codes. Finally, we conclude the survey and propose potential research directions in this rapidly growing field.",
                        "Citation Paper Authors": "Authors:Yang Gao, Yi-Fan Li, Yu Lin, Hang Gao, Latifur Khan"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", a particular instantia-\ntion of GNNs, has shown to be capable of exploiting graphs to give\nimproved performance. ",
                    "Citation Text": "Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2017. Graph Convolu-\ntional Matrix Completion. CoRR abs/1706.02263 (2017). arXiv:1706.02263",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ": Neural Graph Collaborative Filtering is a graph neural\nnetwork based model that captures high-order information by em-\nbedding propagation using graphs. We utilized the code from this\nrepository2to obtain performance metrics.\nMult-VAE ",
                    "Citation Text": "Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.\nVariational Autoencoders for Collaborative Filtering. In Proceedings of the 2018\nWorld Wide Web Conference .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05814",
                        "Citation Paper Title": "Title:Variational Autoencoders for Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.08113v1": {
            "Paper Title": "Recommender Systems for Configuration Knowledge Engineering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09600v1": {
            "Paper Title": "Within-Document Event Coreference with BERT-Based Contextualized\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07825v1": {
            "Paper Title": "KnowledgeCheckR: Intelligent Techniques for Counteracting Forgetting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09389v1": {
            "Paper Title": "HSR: Hyperbolic Social Recommender",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03062v3": {
            "Paper Title": "Understanding Emails and Drafting Responses -- An Approach Using GPT-3",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14146v2": {
            "Paper Title": "Towards Combating Pandemic-related Misinformation in Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07134v1": {
            "Paper Title": "Automatically Matching Bug Reports With Related App Reviews",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ". Second, we downloaded\nthe replication package of Stanik et al. ",
                    "Citation Text": "C. Stanik, M. Haering, and W. Maalej. Classifying multilingual user\nfeedback using traditional machine learning and deep learning. In 27th\nIEEE International Requirements Engineering Conference Workshops .\nIEEE, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.05504",
                        "Citation Paper Title": "Title:Classifying Multilingual User Feedback using Traditional Machine Learning and Deep Learning",
                        "Citation Paper Abstract": "Abstract:With the rise of social media like Twitter and of software distribution platforms like app stores, users got various ways to express their opinion about software products. Popular software vendors get user feedback thousandfold per day. Research has shown that such feedback contains valuable information for software development teams such as problem reports or feature and support inquires. Since the manual analysis of user feedback is cumbersome and hard to manage many researchers and tool vendors suggested to use automated analyses based on traditional supervised machine learning approaches. In this work, we compare the results of traditional machine learning and deep learning in classifying user feedback in English and Italian into problem reports, inquiries, and irrelevant. Our results show that using traditional machine learning, we can still achieve comparable results to deep learning, although we collected thousands of labels.",
                        "Citation Paper Authors": "Authors:Christoph Stanik, Marlo Haering, Walid Maalej"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". Therefore, we calculated the word embeddings only\nbased on the bug report\u2019s summary.\nRegarding the word embedding technique, we chose Dis-\ntilBERT ",
                    "Citation Text": "V . Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.06989v1": {
            "Paper Title": "Model Synthesis for Communication Traces of System-on-Chip Designs",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "is extended and\nformulated as a graph coloring problem, which is then solved\nby a Boolean satis\ufb01ability solver. Recently, Trace2Model is\nintroduced in ",
                    "Citation Text": "Natasha Jeppu, Tom Melham, Daniel Kroening, and John O\u2019Leary.\nLearning concise models from long execution traces. In DAC \u201920 , June\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.05230",
                        "Citation Paper Title": "Title:Learning Concise Models from Long Execution Traces",
                        "Citation Paper Abstract": "Abstract:Abstract models of system-level behaviour have applications in design exploration, analysis, testing and verification. We describe a new algorithm for automatically extracting useful models, as automata, from execution traces of a HW/SW system driven by software exercising a use-case of interest. Our algorithm leverages modern program synthesis techniques to generate predicates on automaton edges, succinctly describing system behaviour. It employs trace segmentation to tackle complexity for long traces. We learn concise models capturing transaction-level, system-wide behaviour--experimentally demonstrating the approach using traces from a variety of sources, including the x86 QEMU virtual platform and the Real-Time Linux kernel.",
                        "Citation Paper Authors": "Authors:Natasha Yogananda Jeppu, Tom Melham, Daniel Kroening, John O'Leary"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.06762v1": {
            "Paper Title": "Supporting search engines with knowledge and context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11682v2": {
            "Paper Title": "Assessing top-$k$ preferences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06401v1": {
            "Paper Title": "SceneRec: Scene-Based Graph Neural Networks for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "to learn a\nvector embedding for each item or user. The general idea is the\nrepresentation of one graph node can be aggregated and com-\nbined by the representation of its neighbor nodes. NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "extends GNN to multiple depths to capture high-order connectiv-\nities that are included in user-item interactions. KGAT ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019.\nKGAT: Knowledge Graph Attention Network for Recommendation. In KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07854",
                        "Citation Paper Title": "Title:KGAT: Knowledge Graph Attention Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "leverages multi-layer perceptron to learn non-linearities\nbetween user and item interactions in the traditional MF model.\n(3)CMN ",
                    "Citation Text": "Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative Memory Network\nfor Recommendation Systems. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10862",
                        "Citation Paper Title": "Title:Collaborative Memory Network for Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.",
                        "Citation Paper Authors": "Authors:Travis Ebesu, Bin Shen, Yi Fang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Different from existing works\nthat rely on linear predictive function, many recent efforts apply\ndeep learning techniques ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In WWW .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.06343v1": {
            "Paper Title": "Personalized Visualization Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06156v1": {
            "Paper Title": "Personalized Embedding-based e-Commerce Recommendations at eBay",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "proposed two-tower neural networks to embed\nusers and items separately, and applied it to the task of generating\nvideo recommendations. He et al . ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In Proceedings of the 26th international\nconference on world wide web . 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.06021v1": {
            "Paper Title": "Analysing the Requirements for an Open Research Knowledge Graph: Use\n  Cases, Quality Requirements and Construction Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.00826v2": {
            "Paper Title": "Automated Query Reformulation for Efficient Search based on Query Logs\n  From Stack Overflow",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "tried to provide query suggestions\nbased on previous queries in the session by introducing a\nbinary classi\ufb01er and an RNN-based decoder as the query dis-\ncriminator and the query generator. Chen et al. ",
                    "Citation Text": "W. Chen, F. Cai, H. Chen, and M. de Rijke, \u201cAttention-based hierar-\nchical neural query suggestion,\u201d in The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval , 2018,\npp. 1093\u20131096.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.02816",
                        "Citation Paper Title": "Title:Attention-based Hierarchical Neural Query Suggestion",
                        "Citation Paper Abstract": "Abstract:Query suggestions help users of a search engine to refine their queries. Previous work on query suggestion has mainly focused on incorporating directly observable features such as query co-occurrence and semantic similarity. The structure of such features is often set manually, as a result of which hidden dependencies between queries and users may be ignored. We propose an AHNQS model that combines a hierarchical structure with a session-level neural network and a user-level neural network to model the short- and long-term search history of a user. An attention mechanism is used to capture user preferences. We quantify the improvements of AHNQS over state-of-the-art RNN-based query suggestion baselines on the AOL query log dataset, with improvements of up to 21.86% and 22.99% in terms of MRR@10 and Recall@10, respectively, over the state-of-the-art; improvements are especially large for short sessions.",
                        "Citation Paper Authors": "Authors:Wanyu Chen, Fei Cai, Honghui Chen, Maarten de Rijke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05374v1": {
            "Paper Title": "Enhancing Reading Strategies by Exploring A Theme-based Approach to\n  Literature Surveys",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00061v2": {
            "Paper Title": "Unification-based Reconstruction of Multi-hop Explanations for Science\n  Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11910v4": {
            "Paper Title": "Neural Audio Fingerprint for High-specific Audio Retrieval based on\n  Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". The\ntotal lossLaverageslacross all positive pairs, both (i;j)and(j;i):\nL=1\nNNX\nk=1[`(2k\u00001;2k);`(2k;2k\u00001)]: (2)\nUpdating rules are summarized in Algorithm 1.\nIt is worth comparing our approach to SimCLR ",
                    "Citation Text": "T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple\nframework for contrastive learning of visual representations,\u201d\narXiv preprint arXiv:2002.05709 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05260v1": {
            "Paper Title": "SensPick: Sense Picking for Word Sense Disambiguation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.05238v1": {
            "Paper Title": "Concealer: SGX-based Secure, Volume Hiding, and Verifiable Processing of\n  Spatial Time-Series Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.05216v1": {
            "Paper Title": "VINS: Visual Search for Mobile User Interface Design",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ") to infer the UI structure and content [ 2,36,38,\n47]. However, this method is constrained in the identification of\ncomponents that are cluttered with the background and involves the\nadoption of an image classification model to differentiate between\ncomponents ",
                    "Citation Text": "Kevin Moran, Carlos Bernal-C\u00e1rdenas, Michael Curcio, Richard Bonett, and\nDenys Poshyvanyk. 2018. Machine learning-based prototyping of graphical user\ninterfaces for mobile apps. arXiv preprint arXiv:1802.02312 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.02312",
                        "Citation Paper Title": "Title:Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps",
                        "Citation Paper Abstract": "Abstract:It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",
                        "Citation Paper Authors": "Authors:Kevin Moran, Carlos Bernal-C\u00e1rdenas, Michael Curcio, Richard Bonett, Denys Poshyvanyk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.04974v1": {
            "Paper Title": "Content Placement in Networks of Similarity Caches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07654v1": {
            "Paper Title": "A survey of recommender systems for energy efficiency in buildings:\n  Principles, challenges and prospects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10276v2": {
            "Paper Title": "Leveraging the structure of musical preference in content-aware music\n  recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00387v2": {
            "Paper Title": "Fair Multi-Stakeholder News Recommender System with Hypergraph ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.04163v1": {
            "Paper Title": "User Engagement Prediction for Clarification in Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03787v1": {
            "Paper Title": "Improving Accuracy and Diversity in Matching of Recommendation with\n  Diversified Preference Network",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "extend GNN to heterogeneous networks. In recommendation,\nWu et al . ",
                    "Citation Text": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based Recommendation with Graph Neural Networks. In Proceedings of\nAAAI .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "conducts an inductive represen-\ntation learning on large graphs. Graph attention network (GAT) ",
                    "Citation Text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10903",
                        "Citation Paper Title": "Title:Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
                        "Citation Paper Authors": "Authors:Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "have also been explored for diversified recommenda-\ntion. Recently, diversified recommendation is armed with reinforce-\nment learning ",
                    "Citation Text": "Yong Liu, Yinan Zhang, Qiong Wu, Chunyan Miao, Lizhen Cui, Binqiang Zhao,\nYin Zhao, and Lu Guan. 2019. Diversity-Promoting Deep Reinforcement Learning\nfor Interactive Recommendation. arXiv preprint arXiv:1903.07826 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07826",
                        "Citation Paper Title": "Title:Diversity-Promoting Deep Reinforcement Learning for Interactive Recommendation",
                        "Citation Paper Abstract": "Abstract:Interactive recommendation that models the explicit interactions between users and the recommender system has attracted a lot of research attentions in recent years. Most previous interactive recommendation systems only focus on optimizing recommendation accuracy while overlooking other important aspects of recommendation quality, such as the diversity of recommendation results. In this paper, we propose a novel recommendation model, named \\underline{D}iversity-promoting \\underline{D}eep \\underline{R}einforcement \\underline{L}earning (D$^2$RL), which encourages the diversity of recommendation results in interaction recommendations. More specifically, we adopt a Determinantal Point Process (DPP) model to generate diverse, while relevant item recommendations. A personalized DPP kernel matrix is maintained for each user, which is constructed from two parts: a fixed similarity matrix capturing item-item similarity, and the relevance of items dynamically learnt through an actor-critic reinforcement learning framework. We performed extensive offline experiments as well as simulated online experiments with real world datasets to demonstrate the effectiveness of the proposed model.",
                        "Citation Paper Authors": "Authors:Yong Liu, Yinan Zhang, Qiong Wu, Chunyan Miao, Lizhen Cui, Binqiang Zhao, Yin Zhao, Lu Guan"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is specially designed for\ncold-start multi-channel matching. Huang et al . ",
                    "Citation Text": "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,\nJanani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-\nbased retrieval in facebook search. In Proceedings of KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11632",
                        "Citation Paper Title": "Title:Embedding-based Retrieval in Facebook Search",
                        "Citation Paper Abstract": "Abstract:Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in eb search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.",
                        "Citation Paper Authors": "Authors:Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.03749v1": {
            "Paper Title": "Role of Attentive History Selection in Conversational Information\n  Seeking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03692v1": {
            "Paper Title": "What's in a Name? -- Gender Classification of Names with Character Based\n  Machine Learning Models",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "to\nthe contact lists of millions of users, and used the embeddings to\ninfer the nationality and cultural origins ",
                    "Citation Text": "Ye, J., Han, S., Hu, Y ., Coskun, B., Liu, M., Qin, H., Skiena, S.: Nationality\nclassification using name embeddings. In: Proceedings of the 2017 ACM on\nConference on Information and Knowledge Management, CIKM \u201917, pp. 1897\u2013\n1906. ACM, New York, NY , USA (2017). DOI 10.1145/3132847.3133008",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07903",
                        "Citation Paper Title": "Title:Nationality Classification Using Name Embeddings",
                        "Citation Paper Abstract": "Abstract:Nationality identification unlocks important demographic information, with many applications in biomedical and sociological research. Existing name-based nationality classifiers use name substrings as features and are trained on small, unrepresentative sets of labeled names, typically extracted from Wikipedia. As a result, these methods achieve limited performance and cannot support fine-grained classification.\nWe exploit the phenomena of homophily in communication patterns to learn name embeddings, a new representation that encodes gender, ethnicity, and nationality which is readily applicable to building classifiers and other systems. Through our analysis of 57M contact lists from a major Internet company, we are able to design a fine-grained nationality classifier covering 39 groups representing over 90% of the world population. In an evaluation against other published systems over 13 common classes, our F1 score (0.795) is substantial better than our closest competitor Ethnea (0.580). To the best of our knowledge, this is the most accurate, fine-grained nationality classifier available.\nAs a social media application, we apply our classifiers to the followers of major Twitter celebrities over six different domains. We demonstrate stark differences in the ethnicities of the followers of Trump and Obama, and in the sports and entertainments favored by different groups. Finally, we identify an anomalous political figure whose presumably inflated following appears largely incapable of reading the language he posts in.",
                        "Citation Paper Authors": "Authors:Junting Ye, Shuchu Han, Yifan Hu, Baris Coskun, Meizhu Liu, Hong Qin, Steven Skiena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.03674v1": {
            "Paper Title": "Generating Artificial Core Users for Interpretable Condensed Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03577v1": {
            "Paper Title": "Drug Package Recommendation via Interaction-aware Graph Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.12555v2": {
            "Paper Title": "Out-of-Town Recommendation with Travel Intention Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03419v1": {
            "Paper Title": "Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03135v1": {
            "Paper Title": "Graph Attention Collaborative Similarity Embedding for Recommender\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02549v2": {
            "Paper Title": "Dual-embedding based Neural Collaborative Filtering for Recommender\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". This is a state-of-the-art MF method\nwhich learns MF model by optimizing a point-wise\nregression loss that treats all missing data as negative\nfeedback with a smaller weight2.\n\u2022BiasedMF ",
                    "Citation Text": "S. Rendle, W. Krichene, L. Zhang, J. R. Anderson, Neu-\nral collaborative \fltering vs. matrix factorization revisited, in:\nR. L. T. Santos, L. B. Marinho, E. M. Daly, L. Chen, K. Falk,\nN. Koenigstein, E. S. de Moura (Eds.), RecSys 2020: Four-\nteenth ACM Conference on Recommender Systems, Virtual\nEvent, Brazil, September 22-26, 2020, ACM, 2020, pp. 240{\n248. doi:10.1145/3383313.3412488 .\nURL https://doi.org/10.1145/3383313.3412488",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.09683",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering vs. Matrix Factorization Revisited",
                        "Citation Paper Abstract": "Abstract:Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Walid Krichene, Li Zhang, John Anderson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.03089v1": {
            "Paper Title": "Leveraging Review Properties for Effective Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13534v2": {
            "Paper Title": "A Survey of Deep Learning Approaches for OCR and Document Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.04293v1": {
            "Paper Title": "High-level Approaches to Detect Malicious Political Activity on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02922v1": {
            "Paper Title": "Towards a Flexible System Architecture for Automated Knowledge Base\n  Construction Frameworks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02680v1": {
            "Paper Title": "Hierarchical Multi-head Attentive Network for Evidence-aware Fake News\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02335v1": {
            "Paper Title": "Self-Supervised Claim Identification for Automated Fact Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02086v1": {
            "Paper Title": "Focusing Knowledge-based Graph Argument Mining via Topic Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01922v1": {
            "Paper Title": "Session-based Recommendation with Self-Attention Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.12406v2": {
            "Paper Title": "Fairness for Whom? Understanding the Reader's Perception of Fairness in\n  Text Summarization",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ".\n(1) US-Election dataset: This dataset, originally provided\nby Darwish et al. ",
                    "Citation Text": "K. Darwish, W. Magdy, and T. Zanouda, \u201cTrump vs. hillary: What\nwent viral during the 2016 us presidential election,\u201d in International\nconference on social informatics . Springer, 2017, pp. 143\u2013161.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.03375",
                        "Citation Paper Title": "Title:Trump vs. Hillary: What went Viral during the 2016 US Presidential Election",
                        "Citation Paper Abstract": "Abstract:In this paper, we present quantitative and qualitative analysis of the top retweeted tweets (viral tweets) pertaining to the US presidential elections from September 1, 2016 to Election Day on November 8, 2016. For everyday, we tagged the top 50 most retweeted tweets as supporting or attacking either candidate or as neutral/irrelevant. Then we analyzed the tweets in each class for: general trends and statistics; the most frequently used hashtags, terms, and locations; the most retweeted accounts and tweets; and the most shared news and links. In all we analyzed the 3,450 most viral tweets that grabbed the most attention during the US election and were retweeted in total 26.3 million times accounting over 40% of the total tweet volume pertaining to the US election in the aforementioned period. Our analysis of the tweets highlights some of the differences between the social media strategies of both candidates, the penetration of their messages, and the potential effect of attacks on both",
                        "Citation Paper Authors": "Authors:Kareem Darwish, Walid Magdy, Tahar Zanouda"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.00141v2": {
            "Paper Title": "When the Umpire is also a Player: Bias in Private Label Product\n  Recommendations on E-commerce Marketplaces",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ". The nodes in the inner cores are often very\nstrongly connected ",
                    "Citation Text": "Soumya Sarkar, Sanjukta Bhowmick, and Animesh Mukherjee. 2018. On Rich\nClubs of Path-Based Centralities in Networks. In ACM CIKM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.02903",
                        "Citation Paper Title": "Title:On Rich Clubs of Path-Based Centralities in Networks",
                        "Citation Paper Abstract": "Abstract:Many scale-free networks exhibit a rich club structure, where high degree vertices form tightly interconnected subgraphs. In this paper, we explore the emergence of rich clubs in the context of shortest path based centrality metrics. We term these subgraphs of connected high closeness or high betweeness vertices as rich centrality clubs (RCC).\nOur experiments on real world and synthetic networks highlight the inter-relations between RCCs, expander graphs, and the core-periphery structure of the network. We show empirically and theoretically that RCCs exist, if the core-periphery structure of the network is such that each shell is an expander graph, and their density decreases from inner to outer shells.\nThe main contributions of our paper are: (i) we demonstrate that the formation of RCC is related to the core-periphery structure and particularly the expander like properties of each shell, (ii) we show that the RCC property can be used to find effective seed nodes for spreading information and for improving the resilience of the network under perturbation and, finally, (iii) we present a modification algorithm that can insert RCC within networks, while not affecting their other structural properties. Taken together, these contributions present one of the first comprehensive studies of the properties and applications of rich clubs for path based centralities.",
                        "Citation Paper Authors": "Authors:Soumya Sarkar, Animesh Mukherjee, Sanjukta Bhowmick"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", personalization and performance of search\nengines [ 26,43,54], information segregation ",
                    "Citation Text": "Abhijnan Chakraborty, Muhammad Ali, Saptarshi Ghosh, Niloy Ganguly, and\nKrishna P Gummadi. 2017. On quantifying knowledge segregation in society.\narXiv preprint arXiv:1708.00670 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.00670",
                        "Citation Paper Title": "Title:On Quantifying Knowledge Segregation in Society",
                        "Citation Paper Abstract": "Abstract:With rapid increase in online information consumption, especially via social media sites, there have been concerns on whether people are getting selective exposure to a biased subset of the information space, where a user is receiving more of what she already knows, and thereby potentially getting trapped in echo chambers or filter bubbles. Even though such concerns are being debated for some time, it is not clear how to quantify such echo chamber effect. In this position paper, we introduce Information Segregation (or Informational Segregation) measures, which follow the long lines of work on residential segregation. We believe that information segregation nicely captures the notion of exposure to different information by different population in a society, and would help in quantifying the extent of social media sites offering selective (or diverse) information to their users.",
                        "Citation Paper Authors": "Authors:Abhijnan Chakraborty, Muhammad Ali, Saptarshi Ghosh, Niloy Ganguly, Krishna P. Gummadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.11954v2": {
            "Paper Title": "Identifying COVID-19 Fake News in Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01156v1": {
            "Paper Title": "Improving Distantly-Supervised Relation Extraction through BERT-based\n  Label & Instance Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01148v1": {
            "Paper Title": "A comparative study of Bot Detection techniques methods with an\n  application related to Covid-19 discourse on Twitter",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": ". We use this tool in part of our experiments. Then, ",
                    "Citation Text": "Mohsen Sayyadiharikandeh, Onur Varol, Kai-Cheng Yang, Alessandro Flam-\nmini, and Filippo Menczer. Detection of novel social bots by ensembles of\nspecialized classi\fers. arXiv , pages arXiv{2006, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.06867",
                        "Citation Paper Title": "Title:Detection of Novel Social Bots by Ensembles of Specialized Classifiers",
                        "Citation Paper Abstract": "Abstract:Malicious actors create inauthentic social media accounts controlled in part by algorithms, known as social bots, to disseminate misinformation and agitate online discussion. While researchers have developed sophisticated methods to detect abuse, novel bots with diverse behaviors evade detection. We show that different types of bots are characterized by different behavioral features. As a result, supervised learning techniques suffer severe performance deterioration when attempting to detect behaviors not observed in the training data. Moreover, tuning these models to recognize novel bots requires retraining with a significant amount of new annotations, which are expensive to obtain. To address these issues, we propose a new supervised learning method that trains classifiers specialized for each class of bots and combines their decisions through the maximum rule. The ensemble of specialized classifiers (ESC) can better generalize, leading to an average improvement of 56\\% in F1 score for unseen accounts across datasets. Furthermore, novel bot behaviors are learned with fewer labeled examples during retraining. We deployed ESC in the newest version of Botometer, a popular tool to detect social bots in the wild, with a cross-validation AUC of 0.99.",
                        "Citation Paper Authors": "Authors:Mohsen Sayyadiharikandeh, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, Filippo Menczer"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "presents how\nto exploit Social Fingerprinting in both a supervised and unsupervised fashion\nusing Lowest Common Substring(LCS) as a similarity measure between DNA\nstrings. ",
                    "Citation Text": "Stefano Cresci, Fabrizio Lillo, Daniele Regoli, Serena Tardelli, and Maurizio\nTesconi. Cashtag piggybacking: Uncovering spam and bot activity in stock\nmicroblogs on twitter. ACM Transactions on the Web (TWEB) , 13(2):1{27,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04406",
                        "Citation Paper Title": "Title:Cashtag piggybacking: uncovering spam and bot activity in stock microblogs on Twitter",
                        "Citation Paper Abstract": "Abstract:Microblogs are increasingly exploited for predicting prices and traded volumes of stocks in financial markets. However, it has been demonstrated that much of the content shared in microblogging platforms is created and publicized by bots and spammers. Yet, the presence (or lack thereof) and the impact of fake stock microblogs has never systematically been investigated before. Here, we study 9M tweets related to stocks of the 5 main financial markets in the US. By comparing tweets with financial data from Google Finance, we highlight important characteristics of Twitter stock microblogs. More importantly, we uncover a malicious practice - referred to as cashtag piggybacking - perpetrated by coordinated groups of bots and likely aimed at promoting low-value stocks by exploiting the popularity of high-value ones. Among the findings of our study is that as much as 71% of the authors of suspicious financial tweets are classified as bots by a state-of-the-art spambot detection algorithm. Furthermore, 37% of them were suspended by Twitter a few months after our investigation. Our results call for the adoption of spam and bot detection techniques in all studies and applications that exploit user-generated content for predicting the stock market.",
                        "Citation Paper Authors": "Authors:Stefano Cresci, Fabrizio Lillo, Daniele Regoli, Serena Tardelli, Maurizio Tesconi"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": ", it is presented a supervised method\nwith more than 1000 features related to user metadata, friends, network, tem-\nporal, content, and sentiment. This research concluded in the \frst version of\nBotometer, a bot detection service available online. ",
                    "Citation Text": "Kai-Cheng Yang, Onur Varol, Clayton A Davis, Emilio Ferrara, Alessandro\nFlammini, and Filippo Menczer. Arming the public with arti\fcial intelligence\nto counter social bots. Human Behavior and Emerging Technologies , 1(1):48{\n61, 2019.\n35",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00912",
                        "Citation Paper Title": "Title:Arming the public with artificial intelligence to counter social bots",
                        "Citation Paper Abstract": "Abstract:The increased relevance of social media in our daily life has been accompanied by efforts to manipulate online conversations and opinions. Deceptive social bots -- automated or semi-automated accounts designed to impersonate humans -- have been successfully exploited for these kinds of abuse. Researchers have responded by developing AI tools to arm the public in the fight against social bots. Here we review the literature on different types of bots, their impact, and detection methods. We use the case study of Botometer, a popular bot detection tool developed at Indiana University, to illustrate how people interact with AI countermeasures. A user experience survey suggests that bot detection has become an integral part of the social media experience for many users. However, barriers in interpreting the output of AI tools can lead to fundamental misunderstandings. The arms race between machine learning methods to develop sophisticated bots and effective countermeasures makes it necessary to update the training data and features of detection tools. We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations. We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.",
                        "Citation Paper Authors": "Authors:Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara, Alessandro Flammini, Filippo Menczer"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ". The study described and compared the behavior and discussion topics of\nbots and humans. Alternatively, other works analyzed the discourse during the\nCovid-19 pandemic on Online Social Networks(OSN). For instance, ",
                    "Citation Text": "Bennett Kleinberg, Isabelle van der Vegt, and Maximilian Mozes. Mea-\nsuring emotions in the covid-19 real world worry dataset. arXiv preprint\narXiv:2004.04225 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04225",
                        "Citation Paper Title": "Title:Measuring Emotions in the COVID-19 Real World Worry Dataset",
                        "Citation Paper Abstract": "Abstract:The COVID-19 pandemic is having a dramatic impact on societies and economies around the world. With various measures of lockdowns and social distancing in place, it becomes important to understand emotional responses on a large scale. In this paper, we present the first ground truth dataset of emotional responses to COVID-19. We asked participants to indicate their emotions and express these in text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500 short + 2,500 long texts). Our analyses suggest that emotional responses correlated with linguistic measures. Topic modeling further revealed that people in the UK worry about their family and the economic situation. Tweet-sized texts functioned as a call for solidarity, while longer texts shed light on worries and concerns. Using predictive modeling approaches, we were able to approximate the emotional responses of participants from text within 14% of their actual value. We encourage others to use the dataset and improve how we can use automated methods to learn about emotional responses and worries about an urgent problem.",
                        "Citation Paper Authors": "Authors:Bennett Kleinberg, Isabelle van der Vegt, Maximilian Mozes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.00482v1": {
            "Paper Title": "Improving Accountability in Recommender Systems Research Through\n  Reproducibility",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.00426v1": {
            "Paper Title": "A Simple yet Brisk and Efficient Active Learning Platform for Text\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.00201v1": {
            "Paper Title": "Melon Playlist Dataset: a public dataset for audio-based playlist\n  generation and music tagging",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "(MSD) contains audio features\nextracted for one million songs, it was expanded by the MIR\ncommunity with additional metadata, including collaborative\ntagsfromLast.fm. Itwaspreviouslypossibletodownload30-\nsecond audio previews for MSD through the 7digital service,\nbutitisnolongeraccessible. Anotherlimitationofthisdataset\nis the noise in the tags ",
                    "Citation Text": "Keunwoo Choi, Gy\u00f6rgy Fazekas, Kyunghyun Cho, and\nMarkSandler, \u201cThee\ufb00ectsofnoisylabelsondeepcon-\nvolutional neural networks for music tagging,\u201d IEEE\nTransactions on Emerging Topics in Computational In-\ntelligence , vol. 2, no. 2, pp. 139\u2013149, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02361",
                        "Citation Paper Title": "Title:The Effects of Noisy Labels on Deep Convolutional Neural Networks for Music Tagging",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNN) have been successfully applied to music classification including music tagging. However, there are several open questions regarding the training, evaluation, and analysis of DNNs. In this article, we investigate specific aspects of neural networks, the effects of noisy labels, to deepen our understanding of their properties. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation. Using a trained network, we compute label vector similarities which is compared to groundtruth similarity.\nThe results highlight several important aspects of music tagging and neural networks. We show that networks can be effective despite relatively large error rates in groundtruth datasets, while conjecturing that label noise can be the cause of varying tag-wise performance differences. Lastly, the analysis of our trained network provides valuable insight into the relationships between music tags. These results highlight the benefit of using data-driven methods to address automatic music tagging.",
                        "Citation Paper Authors": "Authors:Keunwoo Choi, George Fazekas, Kyunghyun Cho, Mark Sandler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.09442v2": {
            "Paper Title": "Causality-Aware Neighborhood Methods for Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04003v4": {
            "Paper Title": "It Runs in the Family: Searching for Synonyms Using Digitized Family\n  Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04852v2": {
            "Paper Title": "Knowledge-Enhanced Top-K Recommendation in Poincar\u00e9 Ball",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.12549v1": {
            "Paper Title": "Graph Embedding for Recommendation against Attribute Inference Attacks",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "proposes a perturbation method that adds\nor removes items and ratings to minimize privacy risk. Similarly,\nRAPPOR ",
                    "Citation Text": "\u00dalfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Random-\nized aggregatable privacy-preserving ordinal response. In SIGSAC . 1054\u20131067.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.6981",
                        "Citation Paper Title": "Title:RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
                        "Citation Paper Abstract": "Abstract:Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vasyl Pihur, Aleksandra Korolova"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ".\nThe third type of works exploits both friend and behavioral in-\nformation [ 17,18,23]. For example, ",
                    "Citation Text": "Neil Zhenqiang Gong, Ameet Talwalkar, Lester Mackey, Ling Huang, Eui\nChul Richard Shin, Emil Stefanov, Elaine Shi, and Dawn Song. 2014. Joint link\nprediction and attribute inference using a social-attribute network. ACM Trans-\nactions on Intelligent Systems and Technology (2014), 1\u201320.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1112.3265",
                        "Citation Paper Title": "Title:Jointly Predicting Links and Inferring Attributes using a Social-Attribute Network (SAN)",
                        "Citation Paper Abstract": "Abstract:The effects of social influence and homophily suggest that both network structure and node attribute information should inform the tasks of link prediction and node attribute inference. Recently, Yin et al. proposed Social-Attribute Network (SAN), an attribute-augmented social network, to integrate network structure and node attributes to perform both link prediction and attribute inference. They focused on generalizing the random walk with restart algorithm to the SAN framework and showed improved performance. In this paper, we extend the SAN framework with several leading supervised and unsupervised link prediction algorithms and demonstrate performance improvement for each algorithm on both link prediction and attribute inference. Moreover, we make the novel observation that attribute inference can help inform link prediction, i.e., link prediction accuracy is further improved by first inferring missing attributes. We comprehensively evaluate these algorithms and compare them with other existing algorithms using a novel, large-scale Google+ dataset, which we make publicly available.",
                        "Citation Paper Authors": "Authors:Neil Zhenqiang Gong, Ameet Talwalkar, Lester Mackey, Ling Huang, Eui Chul Richard Shin, Emil Stefanov, Elaine (Runting)Shi, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.12506v1": {
            "Paper Title": "Learning User Preferences in Non-Stationary Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.12457v1": {
            "Paper Title": "RetaGNN: Relational Temporal Attentive Graph Neural Networks for\n  Holistic Sequential Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "does not rely on content, but presents a\nfactorized exchangeable autoencoder with permutation-equivariant\nmatrix operations to make RS inductive. Although GraphSage ",
                    "Citation Text": "William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation\nLearning on Large Graphs. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems (NIPS \u201917) . 1025\u20131035.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "further consider rich multimedia content associated with items for\ninductive learning. FEAE ",
                    "Citation Text": "Jason Hartford, Devon R Graham, Kevin Leyton-Brown, and Siamak Ravan-\nbakhsh. 2018. Deep Models of Interactions Across Sets. In Proceedings of the 35th\nInternational Conference on Machine Learning (ICML \u201918) . 1914\u20131923.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02879",
                        "Citation Paper Title": "Title:Deep Models of Interactions Across Sets",
                        "Citation Paper Abstract": "Abstract:We use deep learning to model interactions across two or more sets of objects, such as user-movie ratings, protein-drug bindings, or ternary user-item-tag interactions. The canonical representation of such interactions is a matrix (or a higher-dimensional tensor) with an exchangeability property: the encoding's meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it could not be made any more expressive without violating PE. This scheme yields three benefits. First, we demonstrate state-of-the-art performance on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects, and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. In experiments, our models achieved surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movies).",
                        "Citation Paper Authors": "Authors:Jason Hartford, Devon R Graham, Kevin Leyton-Brown, Siamak Ravanbakhsh"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is a GNN-based SR model that learns item\nembeddings by applying GNN to the graph from item sequences.\nHGN ",
                    "Citation Text": "Chen Ma, Peng Kang, and Xue Liu. 2019. Hierarchical Gating Networks for Se-\nquential Recommendation. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining (KDD \u201919) . 825\u2013833.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.09217",
                        "Citation Paper Title": "Title:Hierarchical Gating Networks for Sequential Recommendation",
                        "Citation Paper Abstract": "Abstract:The chronological order of user-item interactions is a key feature in many recommender systems, where the items that users will interact may largely depend on those items that users just accessed recently. However, with the tremendous increase of users and items, sequential recommender systems still face several challenging problems: (1) the hardness of modeling the long-term user interests from sparse implicit feedback; (2) the difficulty of capturing the short-term user interests given several items the user just accessed. To cope with these challenges, we propose a hierarchical gating network (HGN), integrated with the Bayesian Personalized Ranking (BPR) to capture both the long-term and short-term user interests. Our HGN consists of a feature gating module, an instance gating module, and an item-item product module. In particular, our feature gating and instance gating modules select what item features can be passed to the downstream layers from the feature and instance levels, respectively. Our item-item product module explicitly captures the item relations between the items that users accessed in the past and those items users will access in the future. We extensively evaluate our model with several state-of-the-art methods and different validation metrics on five real-world datasets. The experimental results demonstrate the effectiveness of our model on Top-N sequential recommendation.",
                        "Citation Paper Authors": "Authors:Chen Ma, Peng Kang, Xue Liu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "is a\ndilated convolution-based generative method to learn long-range\ndependencies in the item sequence. JODIE ",
                    "Citation Text": "Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-\nbedding Trajectory in Temporal Interaction Networks. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining\n(KDD \u201919) . 1269\u20131278.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01207",
                        "Citation Paper Title": "Title:Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks",
                        "Citation Paper Abstract": "Abstract:Modeling sequential interactions between users and items/products is crucial in domains such as e-commerce, social networking, and education. Representation learning presents an attractive opportunity to model the dynamic evolution of users and items, where each user/item can be embedded in a Euclidean space and its evolution can be modeled by an embedding trajectory in this space. However, existing dynamic embedding methods generate embeddings only when users take actions and do not explicitly model the future trajectory of the user/item in the embedding space. Here we propose JODIE, a coupled recurrent neural network model that learns the embedding trajectories of users and items. JODIE employs two recurrent neural networks to update the embedding of a user and an item at every interaction. Crucially, JODIE also models the future embedding trajectory of a user/item. To this end, it introduces a novel projection operator that learns to estimate the embedding of the user at any time in the future. These estimated embeddings are then used to predict future user-item interactions. To make the method scalable, we develop a t-Batch algorithm that creates time-consistent batches and leads to 9x faster training. We conduct six experiments to validate JODIE on two prediction tasks---future interaction prediction and state change prediction---using four real-world datasets. We show that JODIE outperforms six state-of-the-art algorithms in these tasks by at least 20% in predicting future interactions and 12% in state change prediction.",
                        "Citation Paper Authors": "Authors:Srijan Kumar, Xikun Zhang, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.11873v2": {
            "Paper Title": "A Graph-based Relevance Matching Model for Ad-hoc Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00482v2": {
            "Paper Title": "Physical Exercise Recommendation and Success Prediction Using\n  Interconnected Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.11556v1": {
            "Paper Title": "Powering COVID-19 community Q&A with Curated Side Information",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "which released queries, documents and manual\nrelevance judgements to power search for COVID related informa-\ntion. Authors in ",
                    "Citation Text": "D. Su, Y. Xu, T. Yu, F. B. Siddique, E. J. Barezi, and P. Fung. CAiRE-COVID: A\nquestion answering and multi-document summarization system for covid-19\nresearch. arXiv 2005.03975 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03975",
                        "Citation Paper Title": "Title:CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management",
                        "Citation Paper Abstract": "Abstract:We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.",
                        "Citation Paper Authors": "Authors:Dan Su, Yan Xu, Tiezheng Yu, Farhad Bin Siddique, Elham J. Barezi, Pascale Fung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.12335v1": {
            "Paper Title": "Personalization and Recommendation Technologies for MaaS",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14503v3": {
            "Paper Title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic\n  Question Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.11446v1": {
            "Paper Title": "A study on information behavior of scholars for article keywords\n  selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05611v2": {
            "Paper Title": "TrNews: Heterogeneous User-Interest Transfer Learning for News\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.11059v1": {
            "Paper Title": "Event-Driven News Stream Clustering using Entity-Aware Contextual\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.10726v1": {
            "Paper Title": "Regulatory Compliance through Doc2Doc Information Retrieval: A case\n  study in EU/UK legislation where text similarity has limitations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00171v2": {
            "Paper Title": "Cross-lingual Entity Alignment with Incidental Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.10554v1": {
            "Paper Title": "pdfPapers: shell-script utilities for frequency-based multi-word phrase\n  extraction from PDF documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06098v4": {
            "Paper Title": "Consistency Analysis of Replication-Based Probabilistic Key-Value Stores",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09244v2": {
            "Paper Title": "Extracting Lifestyle Factors for Alzheimer's Disease from Clinical Notes\n  Using Deep Learning with Weak Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09807v1": {
            "Paper Title": "Estimating the Total Volume of Queries to a Search Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06732v1": {
            "Paper Title": "Towards Robust Visual Information Extraction in Real World: New Dataset\n  and Novel Solution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11588v3": {
            "Paper Title": "Learning Hierarchical Review Graph Representations for Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09662v1": {
            "Paper Title": "Medical Information Retrieval and Interpretation: A Question-Answer\n  based Interaction Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09311v1": {
            "Paper Title": "Drug and Disease Interpretation Learning with Biomedical Entity\n  Representation Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02552v2": {
            "Paper Title": "Re-Assessing the \"Classify and Count\" Quantification Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02666v2": {
            "Paper Title": "Improving Efficient Neural Ranking Models with Cross-Architecture\n  Knowledge Distillation",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". As part of the PARADE\ndocument ranking model Li et al . ",
                    "Citation Text": "Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2020. PA-\nRADE: Passage Representation Aggregation for Document Reranking. arXiv\npreprint arXiv:2008.09093 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.09093",
                        "Citation Paper Title": "Title:PARADE: Passage Representation Aggregation for Document Reranking",
                        "Citation Paper Abstract": "Abstract:Pretrained transformer models, such as BERT and T5, have shown to be highly effective at ad-hoc passage and document ranking. Due to inherent sequence length limits of these models, they need to be run over a document's passages, rather than processing the entire document sequence at once. Although several approaches for aggregating passage-level signals have been proposed, there has yet to be an extensive comparison of these techniques. In this work, we explore strategies for aggregating relevance signals from a document's passages into a final ranking score. We find that passage representation aggregation techniques can significantly improve over techniques proposed in prior work, such as taking the maximum passage score. We call this new approach PARADE. In particular, PARADE can significantly improve results on collections with broad information needs where relevance signals can be spread throughout the document (such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation techniques may work better on collections with an information need that can often be pinpointed to a single passage (such as TREC DL and TREC Genomics). We also conduct efficiency analyses, and highlight several strategies for improving transformer-based aggregation.",
                        "Citation Paper Authors": "Authors:Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, Yingfei Sun"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "study the impact of knowledge\ndistillation on BERT-based retrieval chatbots. Gao et al . ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Understanding BERT Rankers\nUnder Distillation. arXiv preprint arXiv:2007.11088 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11088",
                        "Citation Paper Title": "Title:Understanding BERT Rankers Under Distillation",
                        "Citation Paper Abstract": "Abstract:Deep language models such as BERT pre-trained on large corpus have given a huge performance boost to the state-of-the-art information retrieval ranking systems. Knowledge embedded in such models allows them to pick up complex matching signals between passages and queries. However, the high computation cost during inference limits their deployment in real-world search scenarios. In this paper, we study if and how the knowledge for search within BERT can be transferred to a smaller ranker through distillation. Our experiments demonstrate that it is crucial to use a proper distillation procedure, which produces up to nine times speedup while preserving the state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "distill sequential recom-\nmendation models for recommender systems with one teacher\nmodel. Vakili Tahami et al . ",
                    "Citation Text": "Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. 2020. Distilling\nKnowledge for Fast Retrieval-based Chat-bots. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11045",
                        "Citation Paper Title": "Title:Distilling Knowledge for Fast Retrieval-based Chat-bots",
                        "Citation Paper Abstract": "Abstract:Response retrieval is a subset of neural ranking in which a model selects a suitable response from a set of candidates given a conversation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as customer support agents. In order to make pairwise comparisons between a conversation history and a candidate response, two approaches are common: cross-encoders performing full self-attention over the pair and bi-encoders encoding the pair separately. The former gives better prediction quality but is too slow for practical use. In this paper, we propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model using distillation. This effectively boosts bi-encoder performance at no cost during inference time. We perform a detailed analysis of this approach on three response retrieval datasets.",
                        "Citation Paper Authors": "Authors:Amir Vakili Tahami, Kamyar Ghajar, Azadeh Shakery"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "demonstrate that the\nsampling policy for negative samples plays an important role in the\nstability of the training and the overall performance with respect\nto IR metrics. MacAvaney et al . ",
                    "Citation Text": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Training Curricula for Open Domain Answer\nRe-Ranking. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14269",
                        "Citation Paper Title": "Title:Training Curricula for Open Domain Answer Re-Ranking",
                        "Citation Paper Abstract": "Abstract:In precision-oriented tasks like answer ranking, it is more important to rank many relevant answers highly than to retrieve all relevant answers. It follows that a good ranking strategy would be to learn how to identify the easiest correct answers first (i.e., assign a high ranking score to answers that have characteristics that usually indicate relevance, and a low ranking score to those with characteristics that do not), before incorporating more complex logic to handle difficult cases (e.g., semantic matching or reasoning). In this work, we apply this idea to the training of neural answer rankers using curriculum learning. We propose several heuristics to estimate the difficulty of a given training sample. We show that the proposed heuristics can be used to build a training curriculum that down-weights difficult samples early in the training process. As the training process progresses, our approach gradually shifts to weighting all samples equally, regardless of difficulty. We present a comprehensive evaluation of our proposed idea on three answer ranking datasets. Results show that our approach leads to superior performance of two leading neural ranking architectures, namely BERT and ConvKNRM, using both pointwise and pairwise losses. When applied to a BERT-based ranker, our method yields up to a 4% improvement in MRR and a 9% improvement in P@1 (compared to the model trained without a curriculum). This results in models that can achieve comparable performance to more expensive state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, Ophir Frieder"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "for OpenQA with guided supervision\nby iteratively using ColBERT to extract positive and negative sam-\nples as training data. Similarly Xiong et al . ",
                    "Citation Text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neigh-\nbor Negative Contrastive Learning for Dense Text Retrieval. arXiv preprint\narXiv:2007.00808 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00808",
                        "Citation Paper Title": "Title:Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                        "Citation Paper Authors": "Authors:Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". Examining different weak supervision sources, MacA-\nvaney et al . ",
                    "Citation Text": "Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. Content-Based\nWeak Supervision for Ad-Hoc Re-Ranking. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.00189",
                        "Citation Paper Title": "Title:Content-Based Weak Supervision for Ad-Hoc Re-Ranking",
                        "Citation Paper Abstract": "Abstract:One challenge with neural ranking is the need for a large amount of manually-labeled relevance judgments for training. In contrast with prior work, we examine the use of weak supervision sources for training that yield pseudo query-document pairs that already exhibit relevance (e.g., newswire headline-content pairs and encyclopedic heading-paragraph pairs). We also propose filtering techniques to eliminate training samples that are too far out of domain using two techniques: a heuristic-based approach and novel supervised filter that re-purposes a neural ranker. Using several leading neural ranking architectures and multiple weak supervision datasets, we show that these sources of training pairs are effective on their own (outperforming prior weak supervision techniques), and that filtering can further improve performance.",
                        "Citation Paper Authors": "Authors:Sean MacAvaney, Andrew Yates, Kai Hui, Ophir Frieder"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nAdapted training procedures .In order to tackle the challenge\nof a small annotated training set, Dehghani et al . ",
                    "Citation Text": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, and Jaap Kamps. 2017. Learn-\ning to learn from weak supervision by full supervision. Proc. of NIPS Workshop\non Meta-Learning (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11383",
                        "Citation Paper Title": "Title:Learning to Learn from Weak Supervision by Full Supervision",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.",
                        "Citation Paper Authors": "Authors:Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "or by combining an efficient transformer-kernel model\nwith a conformer layer ",
                    "Citation Text": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani, and Nick Craswell. 2020.\nConformer-Kernel with Query Term Independence for Document Retrieval. arXiv\npreprint arXiv:2007.10434 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.10434",
                        "Citation Paper Title": "Title:Conformer-Kernel with Query Term Independence for Document Retrieval",
                        "Citation Paper Abstract": "Abstract:The Transformer-Kernel (TK) model has demonstrated strong reranking performance on the TREC Deep Learning benchmark---and can be considered to be an efficient (but slightly less effective) alternative to BERT-based ranking models. In this work, we extend the TK architecture to the full retrieval setting by incorporating the query term independence assumption. Furthermore, to reduce the memory complexity of the Transformer layers with respect to the input sequence length, we propose a new Conformer layer. We show that the Conformer's GPU memory requirement scales linearly with input sequence length, making it a more viable option when ranking long documents. Finally, we demonstrate that incorporating explicit term matching signal into the model can be particularly useful in the full retrieval setting. We present preliminary results from our work in this paper.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani, Nick Craswell"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", replacing the self-attention with a local windowed and global\nattention ",
                    "Citation Text": "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05150",
                        "Citation Paper Title": "Title:Longformer: The Long-Document Transformer",
                        "Citation Paper Abstract": "Abstract:Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
                        "Citation Paper Authors": "Authors:Iz Beltagy, Matthew E. Peters, Arman Cohan"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Several\napproaches aim to improve the efficiency of transformer models\nwith windowed self-attention ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan\nHanbury. 2020. Local Self-Attention over Long Text for Efficient Document\nRetrieval. In Proc. of SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04908",
                        "Citation Paper Title": "Title:Local Self-Attention over Long Text for Efficient Document Retrieval",
                        "Citation Paper Abstract": "Abstract:Neural networks, particularly Transformer-based architectures, have achieved significant performance improvements on several retrieval benchmarks. When the items being retrieved are documents, the time and memory cost of employing Transformers over a full sequence of document terms can be prohibitive. A popular strategy involves considering only the first n terms of the document. This can, however, result in a biased system that under retrieves longer documents. In this work, we propose a local self-attention which considers a moving window over the document terms and for each term attends only to other terms in the same window. This local attention incurs a fraction of the compute and memory cost of attention over the whole document. The windowed approach also leads to more compact packing of padded documents in minibatches resulting in additional savings. We also employ a learned saturation function and a two-staged pooling strategy to identify relevant regions of the document. The Transformer-Kernel pooling model with these changes can efficiently elicit relevance information from documents with thousands of tokens. We benchmark our proposed modifications on the document ranking task from the TREC 2019 Deep Learning track and observe significant improvements in retrieval quality as well as increased retrieval of longer documents at moderate increase in compute and memory costs.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, Allan Hanbury"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.09138v1": {
            "Paper Title": "LonelyText: A Short Messaging Based Classification of Loneliness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09018v1": {
            "Paper Title": "Network Clustering for Multi-task Learning",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ". The meta-knowledge from different tasks is extracted and used in the MTL learning in ",
                    "Citation Text": "Junkun Chen, Xipeng Qiu, Pengfei Liu, and Xuanjing Huang. 2018. Meta multi-task learning for sequence modeling. In AAAI'18.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08969",
                        "Citation Paper Title": "Title:Meta Multi-Task Learning for Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:Semantic composition functions have been playing a pivotal role in neural representation learning of text sequences. In spite of their success, most existing models suffer from the underfitting problem: they use the same shared compositional function on all the positions in the sequence, thereby lacking expressive power due to incapacity to capture the richness of compositionality. Besides, the composition functions of different tasks are independent and learned from scratch. In this paper, we propose a new sharing scheme of composition function across multiple tasks. Specifically, we use a shared meta-network to capture the meta-knowledge of semantic composition and generate the parameters of the task-specific semantic composition models. We conduct extensive experiments on two types of tasks, text classification and sequence tagging, which demonstrate the benefits of our approach. Besides, we show that the shared meta-knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.",
                        "Citation Paper Authors": "Authors:Junkun Chen, Xipeng Qiu, Pengfei Liu, Xuanjing Huang"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "to regularize the parameters of all models, while the hard structure is adopted in ",
                    "Citation Text": "Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S. Yu. 2015. Learning Multiple Tasks with Multilinear Relationship Networks. arXiv preprint, arXiv:1506.02117.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02117",
                        "Citation Paper Title": "Title:Learning Multiple Tasks with Multilinear Relationship Networks",
                        "Citation Paper Abstract": "Abstract:Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.",
                        "Citation Paper Authors": "Authors:Mingsheng Long, Zhangjie Cao, Jianmin Wang, Philip S. Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.08962v1": {
            "Paper Title": "Knowledge Graph Completion with Text-aided Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.08769v1": {
            "Paper Title": "Item Recommendation from Implicit Feedback",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "was the \ufb01rst to provide this e\ufb03cient training algorithm for lear ning ma-\ntrix factorization from implicit feedback. Later, ",
                    "Citation Text": "Hidasi, B., and Tikk, D. Fast ALS-based tensor factorization for\ncontext-aware recommendation from implicit feedback. In Joint European\nConference on Machine Learning and Knowledge Discovery in D atabases\n(2012), Springer, pp. 67\u201382.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1204.1259",
                        "Citation Paper Title": "Title:Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback",
                        "Citation Paper Abstract": "Abstract:Albeit, the implicit feedback based recommendation problem - when only the user history is available but there are no ratings - is the most typical setting in real-world applications, it is much less researched than the explicit feedback case. State-of-the-art algorithms that are efficient on the explicit case cannot be straightforwardly transformed to the implicit case if scalability should be maintained. There are few if any implicit feedback benchmark datasets, therefore new ideas are usually experimented on explicit benchmarks. In this paper, we propose a generic context-aware implicit feedback recommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensor factorization learning method that scales linearly with the number of non-zero elements in the tensor. The method also allows us to incorporate diverse context information into the model while maintaining its computational efficiency. In particular, we present two such context-aware implementation variants of iTALS. The first incorporates seasonality and enables to distinguish user behavior in different time intervals. The other views the user history as sequential information and has the ability to recognize usage pattern typical to certain group of items, e.g. to automatically tell apart product types or categories that are typically purchased repetitively (collectibles, grocery goods) or once (household appliances). Experiments performed on three implicit datasets (two proprietary ones and an implicit variant of the Netflix dataset) show that by integrating context-aware information with our factorization framework into the state-of-the-art implicit recommender algorithm the recommendation quality improves significantly.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "have been explored for \u03c0.\n4.3.2 Two-pass Sampler\nAnother sampling strategy is a two stage sampler ",
                    "Citation Text": "Bai, Y., Goldman, S., and Zhang, L. Tapas: Two-pass approximate\nadaptive sampling for softmax, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.03073",
                        "Citation Paper Title": "Title:TAPAS: Two-pass Approximate Adaptive Sampling for Softmax",
                        "Citation Paper Abstract": "Abstract:TAPAS is a novel adaptive sampling method for the softmax model. It uses a two pass sampling strategy where the examples used to approximate the gradient of the partition function are first sampled according to a squashed population distribution and then resampled adaptively using the context and current model. We describe an efficient distributed implementation of TAPAS. We show, on both synthetic data and a large real dataset, that TAPAS has low computational overhead and works well for minimizing the rank loss for multi-class classification problems with a very large label space.",
                        "Citation Paper Authors": "Authors:Yu Bai, Sally Goldman, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.08751v1": {
            "Paper Title": "Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.08729v1": {
            "Paper Title": "Joint Autoregressive and Graph Models for Software and Developer Social\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.08595v1": {
            "Paper Title": "Fast Clustering of Short Text Streams Using Efficient Cluster Indexing\n  and Dynamic Similarity Thresholds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.08370v1": {
            "Paper Title": "Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual\n  Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02639v3": {
            "Paper Title": "Rethinking movie genre classification with fine-grained semantic\n  clustering",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ". Additional features, including text and othermetadata, have been combined using simple pooling in more\nrecent work by Bonilla ",
                    "Citation Text": "Paola Cascante-Bonilla, Kalpathy Sitaraman, Mengjia Luo,\nand Vicente Ordonez. Moviescope: Large-scale analy-\nsis of movies using multiple modalities. arXiv preprint\narXiv:1908.03180 , 2019. 2, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03180",
                        "Citation Paper Title": "Title:Moviescope: Large-scale Analysis of Movies using Multiple Modalities",
                        "Citation Paper Abstract": "Abstract:Film media is a rich form of artistic expression. Unlike photography, and short videos, movies contain a storyline that is deliberately complex and intricate in order to engage its audience. In this paper we present a large scale study comparing the effectiveness of visual, audio, text, and metadata-based features for predicting high-level information about movies such as their genre or estimated budget. We demonstrate the usefulness of content-based methods in this domain in contrast to human-based and metadata-based predictions in the era of deep learning. Additionally, we provide a comprehensive study of temporal feature aggregation methods for representing video and text and find that simple pooling operations are effective in this domain. We also show to what extent different modalities are complementary to each other. To this end, we also introduce Moviescope, a new large-scale dataset of 5,000 movies with corresponding movie trailers (video + audio), movie posters (images), movie plots (text), and metadata.",
                        "Citation Paper Authors": "Authors:Paola Cascante-Bonilla, Kalpathy Sitaraman, Mengjia Luo, Vicente Ordonez"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ",\nproducing a 1\u00021024 embedding. The Audio embeddings\nare obtained with a VGG style model, trained for audio\nclassi\ufb01cation on the YouTube-8m dataset ",
                    "Citation Text": "Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Nat-\nsev, George Toderici, Balakrishnan Varadarajan, and Sudheen-\ndra Vijayanarasimhan. YouTube-8M: A Large-Scale Video\nClassi\ufb01cation Benchmark. 2016. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08675",
                        "Citation Paper Title": "Title:YouTube-8M: A Large-Scale Video Classification Benchmark",
                        "Citation Paper Abstract": "Abstract:Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets.\nIn this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download.\nWe trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.",
                        "Citation Paper Authors": "Authors:Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.08197v1": {
            "Paper Title": "Open-Domain Conversational Search Assistant with Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01287v3": {
            "Paper Title": "Outlier-Resilient Web Service QoS Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11328v3": {
            "Paper Title": "FedeRank: User Controlled Feedback with Federated Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.07918v1": {
            "Paper Title": "PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08919v2": {
            "Paper Title": "Multilingual Evidence Retrieval and Fact Verification to Combat Global\n  Disinformation: The Power of Polyglotism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.07609v1": {
            "Paper Title": "Chronological Citation Recommendation with Time Preference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11405v2": {
            "Paper Title": "Cross-domain Retrieval in the Legal and Patent Domains: a\n  Reproducibility Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.07481v1": {
            "Paper Title": "Density-Ratio Based Personalised Ranking from Implicit Feedback",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "This\nmethod adopts a model based on neural matrix factorisation.\nThe model architecture is originally proposed in the work of\nNeural Collaborative Filtering (NCF) ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International\nConference on World Wide Web .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "; and (2)\nit leverages all the users or items in each training step and can be\ninfeasible for training complex models (e.g. graph neural networks\n(GNNs) ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". We\nutilise the same datasets and train/test splits that are available in\nGitHub1; (a) Gowalla ",
                    "Citation Text": "Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Mod-\neling user exposure in recommendation. In Proceedings of the 25th International\nConference on World Wide Web .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.07025",
                        "Citation Paper Title": "Title:Modeling User Exposure in Recommendation",
                        "Citation Paper Abstract": "Abstract:Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis, the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model, and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates.",
                        "Citation Paper Authors": "Authors:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.07321v1": {
            "Paper Title": "Classification of Pedagogical content using conventional machine\n  learning and deep learning model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.07124v1": {
            "Paper Title": "Tip of the Tongue Known-Item Retrieval: A Case Study in Movie\n  Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15516v3": {
            "Paper Title": "Graph Convolutional Network for Recommendation with Low-pass\n  Collaborative Filters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06984v1": {
            "Paper Title": "Studying Catastrophic Forgetting in Neural Ranking Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06980v1": {
            "Paper Title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06637v1": {
            "Paper Title": "AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06484v1": {
            "Paper Title": "Artificial Intelligence for Emotion-Semantic Trending and People Emotion\n  Detection During COVID-19 Social Isolation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02009v2": {
            "Paper Title": "Visually-aware Recommendation with Aesthetic Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06327v1": {
            "Paper Title": "Controlling the Risk of Conversational Search via Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". In each experiment, we use the\nsame reranker structure for both answer reranking and clarifying\nquestion reranking but with two sets of parameters.\nBi-encoder .The bi-encoder structure uses two transformers ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "create a clarification identification and generation dataset\nwhere they solve clarification identification as a binary classifica-\ntion problem. Most recently, Aliannejadi et al. ",
                    "Citation Text": "Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and\nMikhail Burtsev. 2020. ConvAI3: Generating Clarifying Questions for Open-\nDomain Dialogue Systems (ClariQ). arXiv preprint arXiv:2009.11352 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.11352",
                        "Citation Paper Title": "Title:ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)",
                        "Citation Paper Abstract": "Abstract:This document presents a detailed description of the challenge on clarifying questions for dialogue systems (ClariQ). The challenge is organized as part of the Conversational AI challenge series (ConvAI3) at Search Oriented Conversational AI (SCAI) EMNLP workshop in 2020. The main aim of the conversational systems is to return an appropriate answer in response to the user requests. However, some user requests might be ambiguous. In IR settings such a situation is handled mainly thought the diversification of the search result page. It is however much more challenging in dialogue settings with limited bandwidth. Therefore, in this challenge, we provide a common evaluation framework to evaluate mixed-initiative conversations. Participants are asked to rank clarifying questions in an information-seeking conversations. The challenge is organized in two stages where in Stage 1 we evaluate the submissions in an offline setting and single-turn conversations. Top participants of Stage 1 get the chance to have their model tested by human annotators.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, Mikhail Burtsev"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "also publish an annotated dataset consisting of\nreal-world user interactions. Cho et al. ",
                    "Citation Text": "Woon Sang Cho, Yizhe Zhang, Sudha Rao, Chris Brockett, and Sungjin Lee. 2019.\nGenerating a Common Question from Multiple Documents using Multi-source\nEncoder-Decoder Models. arXiv preprint arXiv:1910.11483 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11483",
                        "Citation Paper Title": "Title:Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models",
                        "Citation Paper Abstract": "Abstract:Ambiguous user queries in search engines result in the retrieval of documents that often span multiple topics. One potential solution is for the search engine to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the 'Distribute' step of our MSQG model predicts target word distributions for each document using the trained model. The 'Aggregate' step aggregates these distributions to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.",
                        "Citation Paper Authors": "Authors:Woon Sang Cho, Yizhe Zhang, Sudha Rao, Chris Brockett, Sungjin Lee"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "solve automatically gen-\neration of clarification questions via multiple approaches. Later,\nZamani et al. ",
                    "Citation Text": "Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick\nCraswell. 2020. Mimics: A large-scale data collection for search clarification.\narXiv preprint arXiv:2006.10174 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10174",
                        "Citation Paper Title": "Title:MIMICS: A Large-Scale Data Collection for Search Clarification",
                        "Citation Paper Abstract": "Abstract:Search clarification has recently attracted much attention due to its applications in search engines. It has also been recognized as a major component in conversational information seeking systems. Despite its importance, the research community still feels the lack of a large-scale data for studying different aspects of search clarification. In this paper, we introduce MIMICS, a collection of search clarification datasets for real web search queries sampled from the Bing query logs. Each clarification in MIMICS is generated by a Bing production algorithm and consists of a clarifying question and up to five candidate answers. MIMICS contains three datasets: (1) MIMICS-Click includes over 400k unique queries, their associated clarification panes, and the corresponding aggregated user interaction signals (i.e., clicks). (2) MIMICS-ClickExplore is an exploration data that includes aggregated user interaction signals for over 60k unique queries, each with multiple clarification panes. (3) MIMICS-Manual includes over 2k unique real search queries. Each query-clarification pair in this dataset has been manually labeled by at least three trained annotators. It contains graded quality labels for the clarifying question, the candidate answer set, and the landing result page for each candidate answer.\nMIMICS is publicly available for research purposes, thus enables researchers to study a number of tasks related to search clarification, including clarification generation and selection, user engagement prediction for clarification, click models for clarification, and analyzing user interactions with search clarification.",
                        "Citation Paper Authors": "Authors:Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, Nick Craswell"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", they\nextend the model to a reinforcement learning model which better\nfits multi-turn settings. Recent works approach asking clarification\nquestions in various ways. Aliannejadi et al. ",
                    "Citation Text": "Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft.\n2019. Asking Clarifying Questions in Open-Domain Information-Seeking Con-\nversations. CoRR abs/1907.06554 (2019). arXiv:1907.06554 http://arxiv.org/abs/\n1907.06554",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.06554",
                        "Citation Paper Title": "Title:Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
                        "Citation Paper Abstract": "Abstract:Users often fail to formulate their complex information needs in a single query. As a consequence, they may need to scan multiple result pages or reformulate their queries, which may be a frustrating experience. Alternatively, systems can improve user satisfaction by proactively asking questions of the users to clarify their information needs. Asking clarifying questions is especially important in conversational systems since they can only return a limited number of (often only one) result(s). In this paper, we formulate the task of asking clarifying questions in open-domain information-seeking conversational systems. To this end, we propose an offline evaluation methodology for the task and collect a dataset, called Qulac, through crowdsourcing. Our dataset is built on top of the TREC Web Track 2009-2012 data and consists of over 10K question-answer pairs for 198 TREC topics with 762 facets. Our experiments on an oracle model demonstrate that asking only one good question leads to over 170% retrieval performance improvement in terms of P@1, which clearly demonstrates the potential impact of the task. We further propose a retrieval framework consisting of three components: question retrieval, question selection, and document retrieval. In particular, our question selection model takes into account the original query and previous question-answer interactions while selecting the next question. Our model significantly outperforms competitive baselines. To foster research in this area, we have made Qulac publicly available.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, W. Bruce Croft"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "propose to suggest query by its\nusefulness. Query auto-completion emphasizes more on additive\nchanges. Voskarides el al. in ",
                    "Citation Text": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke.\n2020. Query Resolution for Conversational Search with Limited Supervision.\narXiv preprint arXiv:2005.11723 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11723",
                        "Citation Paper Title": "Title:Query Resolution for Conversational Search with Limited Supervision",
                        "Citation Paper Abstract": "Abstract:In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.",
                        "Citation Paper Authors": "Authors:Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "build a word proximity network to compute word coherence in ad-\ndition to query-response relevance. Aliannejadi et al. in ",
                    "Citation Text": "Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, and Fabio\nCrestani. 2020. Harnessing evolution of multi-turn conversations for effective\n9WWW \u201921, April 19\u201323, 2021, Ljubljana, Slovenia Trovato and Tobin, et al.\nanswer retrieval. In Proceedings of the 2020 Conference on Human Information\nInteraction and Retrieval . 33\u201342.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10554",
                        "Citation Paper Title": "Title:Harnessing Evolution of Multi-Turn Conversations for Effective Answer Retrieval",
                        "Citation Paper Abstract": "Abstract:With the improvements in speech recognition and voice generation technologies over the last years, a lot of companies have sought to develop conversation understanding systems that run on mobile phones or smart home devices through natural language interfaces. Conversational assistants, such as Google Assistant and Microsoft Cortana, can help users to complete various types of tasks. This requires an accurate understanding of the user's information need as the conversation evolves into multiple turns. Finding relevant context in a conversation's history is challenging because of the complexity of natural language and the evolution of a user's information need. In this work, we present an extensive analysis of language, relevance, dependency of user utterances in a multi-turn information-seeking conversation. To this aim, we have annotated relevant utterances in the conversations released by the TREC CaST 2019 track. The annotation labels determine which of the previous utterances in a conversation can be used to improve the current one. Furthermore, we propose a neural utterance relevance model based on BERT fine-tuning, outperforming competitive baselines. We study and compare the performance of multiple retrieval models, utilizing different strategies to incorporate the user's context. The experimental results on both classification and retrieval tasks show that our proposed approach can effectively identify and incorporate the conversation context. We show that processing the current utterance using the predicted relevant utterance leads to a 38% relative improvement in terms of nDCG@20. Finally, to foster research in this area, we have released the dataset of the annotations.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, Fabio Crestani"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "Conversational search .The problem of the vagueness of user\nquery has been studied by many previous works. One approach is\n1https://github.com/zhenduow/conversationalQAto analyze the links among conversation turns. Kaiser et al. ",
                    "Citation Text": "Magdalena Kaiser, Rishiraj Saha Roy, and Gerhard Weikum. 2020. Conversational\nQuestion Answering over Passages by Leveraging Word Proximity Networks.\narXiv preprint arXiv:2004.13117 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13117",
                        "Citation Paper Title": "Title:Conversational Question Answering over Passages by Leveraging Word Proximity Networks",
                        "Citation Paper Abstract": "Abstract:Question answering (QA) over text passages is a problem of long-standing interest in information retrieval. Recently, the conversational setting has attracted attention, where a user asks a sequence of questions to satisfy her information needs around a topic. While this setup is a natural one and similar to humans conversing with each other, it introduces two key research challenges: understanding the context left implicit by the user in follow-up questions, and dealing with ad hoc question formulations. In this work, we demonstrate CROWN (Conversational passage ranking by Reasoning Over Word Networks): an unsupervised yet effective system for conversational QA with passage responses, that supports several modes of context propagation over multiple turns. To this end, CROWN first builds a word proximity network (WPN) from large corpora to store statistically significant term co-occurrences. At answering time, passages are ranked by a combination of their similarity to the question, and coherence of query terms within: these factors are measured by reading off node and edge weights from the WPN. CROWN provides an interface that is both intuitive for end-users, and insightful for experts for reconfiguration to individual setups. CROWN was evaluated on TREC CAsT data, where it achieved above-median performance in a pool of neural methods.",
                        "Citation Paper Authors": "Authors:Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.05993v1": {
            "Paper Title": "Ensemble Learning Based Classification Algorithm Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07045v2": {
            "Paper Title": "Population-Scale Study of Human Needs During the COVID-19 Pandemic:\n  Analysis and Implications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05667v1": {
            "Paper Title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained\n  Sequence-to-Sequence Models",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ", condition (11), was the best\nmodel just prior to our leaderboard submission. As of January 2021, our submission currently ranks fourth on\nthe leaderboard; the top spot is occupied by RocketQA ",
                    "Citation Text": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. RocketQA: An\nOptimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. arXiv:2010.08191 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.08191",
                        "Citation Paper Title": "Title:RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                        "Citation Paper Authors": "Authors:Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ", and was also referenced in a description\nof our submissions to TREC-COVID ",
                    "Citation Text": "Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, Kuang Lu, Yue Zhang, Rodrigo Nogueira, Kyunghyun Cho, Hui\nFang, and Jimmy Lin. 2020. Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research\nDataset. In Proceedings of the First Workshop on Scholarly Document Processing . 31\u201341.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.07846",
                        "Citation Paper Title": "Title:Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset",
                        "Citation Paper Abstract": "Abstract:We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the ongoing TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the highest-scoring runs in rounds 1, 2, and 3. In round 3, we report the highest-scoring run that takes advantage of previous training data and the second-highest fully automatic run.",
                        "Citation Paper Authors": "Authors:Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, Kuang Lu, Yue Zhang, Rodrigo Nogueira, Kyunghyun Cho, Hui Fang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", although in the context of BERT (contrasting\nwith our shift to sequence-to-sequence models here) and without document expansion. The \u201cExpando\u201d idea\noriginated in Nogueira et al . ",
                    "Citation Text": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by Query Prediction. arXiv:1904.08375 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08375",
                        "Citation Paper Title": "Title:Document Expansion by Query Prediction",
                        "Citation Paper Abstract": "Abstract:One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",
                        "Citation Paper Authors": "Authors:Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "; the literature is too vast for an exhaustive\nreview here, and thus we refer readers to recent overviews [34, 42].\nThe introduction of BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Minneapolis, Minnesota, 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.11213v2": {
            "Paper Title": "Self-Supervised Learning for Visual Summary Identification in Scientific\n  Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00181v2": {
            "Paper Title": "Relation-aware Meta-learning for Market Segment Demand Prediction with\n  Limited Records",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ") have been widely used in time series applica-\ntions. These methods fail to capture complex non-linear temporal\ncorrelations due to the limited expressive capability. With stronger\nexpressive power, deep learning methods, especially recurrent neu-\nral network-based approaches (e.g., GRU ",
                    "Citation Text": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.\nEmpirical evaluation of gated recurrent neural networks on sequence modeling.\narXiv preprint arXiv:1412.3555 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.3555",
                        "Citation Paper Title": "Title:Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
                        "Citation Paper Authors": "Authors:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". Yao et al .incorporates the gradient-based meta-\nlearning with a region functionality based memory ",
                    "Citation Text": "Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, and Zhenhui Li. 2019. Learning\nfrom Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction.\nInWWW . ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08518",
                        "Citation Paper Title": "Title:Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction",
                        "Citation Paper Abstract": "Abstract:Spatial-temporal prediction is a fundamental problem for constructing smart city, which is useful for tasks such as traffic control, taxi dispatching, and environmental policy making. Due to data collection mechanism, it is common to see data collection with unbalanced spatial distributions. For example, some cities may release taxi data for multiple years while others only release a few days of data; some regions may have constant water quality data monitored by sensors whereas some regions only have a small collection of water samples. In this paper, we tackle the problem of spatial-temporal prediction for the cities with only a short period of data collection. We aim to utilize the long-period data from other cities via transfer learning. Different from previous studies that transfer knowledge from one single source city to a target city, we are the first to leverage information from multiple cities to increase the stability of transfer. Specifically, our proposed model is designed as a spatial-temporal network with a meta-learning paradigm. The meta-learning paradigm learns a well-generalized initialization of the spatial-temporal network, which can be effectively adapted to target cities. In addition, a pattern-based spatial-temporal memory is designed to distill long-term temporal information (i.e., periodicity). We conduct extensive experiments on two tasks: traffic (taxi and bike) prediction and water quality prediction. The experiments demonstrate the effectiveness of our proposed model over several competitive baseline models.",
                        "Citation Paper Authors": "Authors:Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, Zhenhui Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.05641v1": {
            "Paper Title": "$C^3DRec$: Cloud-Client Cooperative Deep Learning for Temporal\n  Recommendation in the Post-GDPR Era",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "to train a personalized model on the device and deploy it with the WebView of\nan experimental application ",
                    "Citation Text": "Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, and Xuanzhe Liu. 2019. Moving deep learning into web browser: How far can we\ngo?. In The World Wide Web Conference . 1234\u20131244.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.09388",
                        "Citation Paper Title": "Title:Moving Deep Learning into Web Browser: How Far Can We Go?",
                        "Citation Paper Abstract": "Abstract:Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been supported in browsers so far. Then we measure the performance of different frameworks when running different deep learning tasks. Finally, we dig out the performance gap between deep learning in browsers and on native platforms by comparing the performance of TensorFlow.js and TensorFlow in Python. Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers.",
                        "Citation Paper Authors": "Authors:Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, Xuanzhe Liu"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "to train a global model on the\ncloud and TensorFlow.js ",
                    "Citation Text": "Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David\nSoergel, et al. 2019. Tensorflow. js: Machine learning for the web and beyond. arXiv preprint arXiv:1901.05350 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05350",
                        "Citation Paper Title": "Title:TensorFlow.js: Machine Learning for the Web and Beyond",
                        "Citation Paper Abstract": "Abstract:TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.",
                        "Citation Paper Authors": "Authors:Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David Soergel, Stan Bileschi, Michael Terry, Charles Nicholson, Sandeep N. Gupta, Sarah Sirajuddin, D. Sculley, Rajat Monga, Greg Corrado, Fernanda B. Vi\u00e9gas, Martin Wattenberg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.04099v2": {
            "Paper Title": "Community Detection and Matrix Completion with Social and Item\n  Similarity Graphs",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "(joint recovery of rows\nand columns communities) is similar to ours, but therein, graph\ninformation is not available. Another relevant problem is the\nlabelled or weighted SBM problem ",
                    "Citation Text": "S. Heimlicher, M. Lelarge, and L. Massouli\u00e9, \u201cCommunity detection in\nthe labelled stochastic block model,\u201d arXiv:1209.2910 , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1209.2910",
                        "Citation Paper Title": "Title:Community Detection in the Labelled Stochastic Block Model",
                        "Citation Paper Abstract": "Abstract:We consider the problem of community detection from observed interactions between individuals, in the context where multiple types of interaction are possible. We use labelled stochastic block models to represent the observed data, where labels correspond to interaction types. Focusing on a two-community scenario, we conjecture a threshold for the problem of reconstructing the hidden communities in a way that is correlated with the true partition. To substantiate the conjecture, we prove that the given threshold correctly identifies a transition on the behaviour of belief propagation from insensitive to sensitive. We further prove that the same threshold corresponds to the transition in a related inference problem on a tree model from infeasible to feasible. Finally, numerical results using belief propagation for community detection give further support to the conjecture.",
                        "Citation Paper Authors": "Authors:Simon Heimlicher, Marc Lelarge, Laurent Massouli\u00e9"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.04850v1": {
            "Paper Title": "Heterogeneous Network Embedding for Deep Semantic Relevance Match in\n  E-commerce Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04849v1": {
            "Paper Title": "Probabilistic Metric Learning with Adaptive Margin for Top-K\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "apply a translation emb\nedding to capture more complex relations between users and items,\nwhere the translation embedding is learned from the neighborhood\ninformation of users and items. In ",
                    "Citation Text": "Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based\nRecommendation. In Proc. ACM Conf. Recommender Systems .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.02410",
                        "Citation Paper Title": "Title:Translation-based Recommendation",
                        "Citation Paper Abstract": "Abstract:Modeling the complex interactions between users and items as well as amongst items themselves is at the core of designing successful recommender systems. One classical setting is predicting users' personalized sequential behavior (or `next-item' recommendation), where the challenges mainly lie in modeling `third-order' interactions between a user, her previously visited item(s), and the next item to consume. Existing methods typically decompose these higher-order interactions into a combination of pairwise relationships, by way of which user preferences (user-item interactions) and sequential patterns (item-item interactions) are captured by separate components. In this paper, we propose a unified method, TransRec, to model such third-order relationships for large-scale sequential prediction. Methodologically, we embed items into a `transition space' where users are modeled as translation vectors operating on item sequences. Empirically, this approach outperforms the state-of-the-art on a wide spectrum of real-world datasets. Data and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ruining He, Wang-Cheng Kang, Julian McAuley"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "to explicitly store the user preference\nin external memories. Park et al. in ",
                    "Citation Text": "Chanyoung Park, Donghyun Kim, Xing Xie, and Hwanjo Yu. 2018. Collaborative\nTranslational Metric Learning. In Proc. IEEE Int. Conf. Data Mining .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01637",
                        "Citation Paper Title": "Title:Collaborative Translational Metric Learning",
                        "Citation Paper Abstract": "Abstract:Recently, matrix factorization-based recommendation methods have been criticized for the problem raised by the triangle inequality violation. Although several metric learning-based approaches have been proposed to overcome this issue, existing approaches typically project each user to a single point in the metric space, and thus do not suffice for properly modeling the intensity and the heterogeneity of user-item relationships in implicit feedback. In this paper, we propose TransCF to discover such latent user-item relationships embodied in implicit user-item interactions. Inspired by the translation mechanism popularized by knowledge graph embedding, we construct user-item specific translation vectors by employing the neighborhood information of users and items, and translate each user toward items according to the user's relationships with the items. Our proposed method outperforms several state-of-the-art methods for top-N recommendation on seven real-world data by up to 17% in terms of hit ratio. We also conduct extensive qualitative evaluations on the translation vectors learned by our proposed method to ascertain the benefit of adopting the translation mechanism for implicit feedback-based recommendations.",
                        "Citation Paper Authors": "Authors:Chanyoung Park, Donghyun Kim, Xing Xie, Hwanjo Yu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", Hu et al. propose a weighted regularized matrix\nfactorization (WRMF) model to treat all the missing data as nega-\ntive samples, while heuristically assigning confidence weights to\npositive samples. Rendle et al. adopt a different approach in ",
                    "Citation Text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proc. Conf.\nUncertainty in Artificial Intelligence .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.2618",
                        "Citation Paper Title": "Title:BPR: Bayesian Personalized Ranking from Implicit Feedback",
                        "Citation Paper Abstract": "Abstract:Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.04817v1": {
            "Paper Title": "Discrete Knowledge Graph Embedding based on Discrete Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04617v1": {
            "Paper Title": "AI- and HPC-enabled Lead Generation for SARS-CoV-2: Models and Processes\n  to Extract Druglike Molecules Contained in Natural Language Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04526v1": {
            "Paper Title": "Measuring Recommender System Effects with Simulated Users",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "found that recommenders decrease di-\nversity over time and creates a richer-get-richer effect for popular\nitems; Chaney et al . ",
                    "Citation Text": "Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018. How\nalgorithmic confounding in recommendation systems increases homogeneity\nand decreases utility. In Proceedings of the 12th ACM Conference on Recommender\nSystems . ACM, 224\u2013232.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.11214",
                        "Citation Paper Title": "Title:How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility",
                        "Citation Paper Abstract": "Abstract:Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility.",
                        "Citation Paper Authors": "Authors:Allison J.B. Chaney, Brandon M. Stewart, Barbara E. Engelhardt"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "studies the filter bubble effect in\nterms of content diversity and finds that recommender systems\ncould lead users to a narrowing set of items over time. Dean et al . ",
                    "Citation Text": "Sarah Dean, Sarah Rich, and Benjamin Recht. 2020. Recommendations and user\nagency: the reachability of collaboratively-filtered information. In Proceedings of\nthe 2020 Conference on Fairness, Accountability, and Transparency . 436\u2013445.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10068",
                        "Citation Paper Title": "Title:Recommendations and User Agency: The Reachability of Collaboratively-Filtered Information",
                        "Citation Paper Abstract": "Abstract:Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-$N$ linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.",
                        "Citation Paper Authors": "Authors:Sarah Dean, Sarah Rich, Benjamin Recht"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.04356v1": {
            "Paper Title": "On the Calibration and Uncertainty of Neural Learning to Rank Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04328v1": {
            "Paper Title": "Neural News Recommendation with Negative Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08737v2": {
            "Paper Title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01915v3": {
            "Paper Title": "Origin-Aware Next Destination Recommendation with Personalized\n  Preference Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03626v2": {
            "Paper Title": "Beyond Statistical Relations: Integrating Knowledge Relations into Style\n  Correlations for Multi-Label Music Style Classification",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "proposed\nan approach to learn and combine multi-modal data representations\nfor music style classification.\nThese methods above do not consider the relationships between\nlabels, Zhao et al. ",
                    "Citation Text": "Guangxiang Zhao, Jingjing Xu, Qi Zeng, Xuancheng Ren, and Xu Sun. 2019.\nReview-Driven Multi-Label Music Style Classification by Exploiting Style Corre-\nlations. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) . 2884\u20132891.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.07604",
                        "Citation Paper Title": "Title:Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations",
                        "Citation Paper Abstract": "Abstract:This paper explores a new natural language processing task, review-driven multi-label music style classification. This task requires the system to identify multiple styles of music based on its reviews on websites. The biggest challenge lies in the complicated relations of music styles. It has brought failure to many multi-label classification methods. To tackle this problem, we propose a novel deep learning approach to automatically learn and exploit style correlations. The proposed method consists of two parts: a label-graph based neural network, and a soft training mechanism with correlation-based continuous label representation. Experimental results show that our approach achieves large improvements over the baselines on the proposed dataset. Especially, the micro F1 is improved from 53.9 to 64.5, and the one-error is reduced from 30.5 to 22.6. Furthermore, the visualized analysis shows that our approach performs well in capturing style correlations.",
                        "Citation Paper Authors": "Authors:Guangxiang Zhao, Jingjing Xu, Qi Zeng, Xuancheng Ren"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "74.4 0.083 24.7 41.8 41.3 0.125 35.6 48.4\nClassifier Chains ",
                    "Citation Text": "Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. 2011. Classifier\nchains for multi-label classification. Machine learning 85, 3 (2011), 333.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.13405",
                        "Citation Paper Title": "Title:Classifier Chains: A Review and Perspectives",
                        "Citation Paper Abstract": "Abstract:The family of methods collectively known as classifier chains has become a popular approach to multi-label learning problems. This approach involves linking together off-the-shelf binary classifiers in a chain structure, such that class label predictions become features for other classifiers. Such methods have proved flexible and effective and have obtained state-of-the-art empirical performance across many datasets and multi-label evaluation metrics. This performance led to further studies of how exactly it works, and how it could be improved, and in the recent decade numerous studies have explored classifier chains mechanisms on a theoretical level, and many improvements have been made to the training and inference procedures, such that this method remains among the state-of-the-art options for multi-label learning. Given this past and ongoing interest, which covers a broad range of applications and research themes, the goal of this work is to provide a review of classifier chains, a survey of the techniques and extensions provided in the literature, as well as perspectives for this approach in the domain of multi-label classification in the future. We conclude positively, with a number of recommendations for researchers and practitioners, as well as outlining a number of areas for future research.",
                        "Citation Paper Authors": "Authors:Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "proposed a hyper-graph integrated SVM\nmethod, which can integrate both music contents and music tags\nfor automatic music style classification. Oramas et al. ",
                    "Citation Text": "Sergio Oramas, Francesco Barbieri, Oriol Nieto, and Xavier Serra. 2018. Multi-\nmodal deep learning for music genre classification. Transactions of the Interna-\ntional Society for Music Information Retrieval. 2018; 1 (1): 4-21. (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.04916",
                        "Citation Paper Title": "Title:Multi-label Music Genre Classification from Audio, Text, and Images Using Deep Features",
                        "Citation Paper Abstract": "Abstract:Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results.",
                        "Citation Paper Authors": "Authors:Sergio Oramas, Oriol Nieto, Francesco Barbieri, Xavier Serra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.05626v1": {
            "Paper Title": "Eating Garlic Prevents COVID-19 Infection: Detecting Misinformation on\n  the Arabic Content of Twitter",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ". We investigate the following two types of word embeddings in\nthis work:\n\u2013word2vec ",
                    "Citation Text": "T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of word representations in vector space,\u201d\narXiv preprint arXiv:1301.3781 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.03394v1": {
            "Paper Title": "Context-Aware Target Apps Selection and Recommendation for Enhancing\n  Personal Mobile Assistants",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "did a diary and interview study to understand users\u2019 mobile Web behavior. Aliannejadi\net al. ",
                    "Citation Text": "Mohammad Aliannejadi, Morgan Harvey, Luca Costa, Matthew Pointon, and Fabio Crestani. 2019. Understanding\nMobile Search Task Relevance and User Behaviour in Context. In CHIIR . 143\u2013151.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07081",
                        "Citation Paper Title": "Title:Understanding Mobile Search Task Relevance and User Behaviour in Context",
                        "Citation Paper Abstract": "Abstract:Improvements in mobile technologies have led to a dramatic change in how and when people access and use information, and is having a profound impact on how users address their daily information needs. Smart phones are rapidly becoming our main method of accessing information and are frequently used to perform `on-the-go' search tasks. As research into information retrieval continues to evolve, evaluating search behaviour in context is relatively new. Previous research has studied the effects of context through either self-reported diary studies or quantitative log analysis; however, neither approach is able to accurately capture context of use at the time of searching. In this study, we aim to gain a better understanding of task relevance and search behaviour via a task-based user study (n=31) employing a bespoke Android app. The app allowed us to accurately capture the user's context when completing tasks at different times of the day over the period of a week. Through analysis of the collected data, we gain a better understanding of how using smart phones on the go impacts search behaviour, search performance and task relevance and whether or not the actual context is an important factor.",
                        "Citation Paper Authors": "Authors:Mohammad Aliannejadi, Morgan Harvey, Luca Costa, Matthew Pointon, Fabio Crestani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.03392v1": {
            "Paper Title": "Generate Natural Language Explanations for Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.03303v1": {
            "Paper Title": "An Unsupervised Normalization Algorithm for Noisy Text: A Case Study for\n  Information Retrieval and Stance Detection",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ".\nWe speci\ufb01cally use the popular Louvain graph clustering algorithm ",
                    "Citation Text": "Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communi-\nties in large networks. Journal of Statistical Mechanics: Theory and Experiment , 2008:P10008, 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0476",
                        "Citation Paper Title": "Title:Fast unfolding of communities in large networks",
                        "Citation Paper Abstract": "Abstract:  We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
                        "Citation Paper Authors": "Authors:Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.03207v1": {
            "Paper Title": "Leveraging Multilingual Transformers for Hate Speech Detection",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "have been shown to cap-\nture contextualized embeddings for a sentence. Approaches such as BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional\ntransformers for language understanding, arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "on aggression identification included\nboth English and Hindi Facebook comments. Participants had to detect abusive comments and\ndistinguish between overtly aggressive comments and covertly aggressive comments. OffensEval\n(SemEval-2019 Task 6) ",
                    "Citation Text": "M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal, N. Farra, R. Kumar, SemEval-2019\ntask 6: Identifying and categorizing offensive language in social media (OffensEval), in:\nProceedings of the 13th International Workshop on Semantic Evaluation, Association for\nComputational Linguistics, Minneapolis, Minnesota, USA, 2019, pp. 75\u201386. URL: https:\n//www.aclweb.org/anthology/S19-2010. doi: 10.18653/v1/S19-2010 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.08983",
                        "Citation Paper Title": "Title:SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)",
                        "Citation Paper Abstract": "Abstract:We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets. It featured three sub-tasks. In sub-task A, the goal was to discriminate between offensive and non-offensive posts. In sub-task B, the focus was on the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, about 800 teams signed up to participate in the task, and 115 of them submitted results, which we present and analyze in this report.",
                        "Citation Paper Authors": "Authors:Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "made it possible to better represent long term dependencies. This boosted classification\nscores, with LSTM and CNN-based models significantly outperforming character and word\nbased N-gram models. ",
                    "Citation Text": "P. Badjatiya, S. Gupta, M. Gupta, V. Varma, Deep learning for hate speech detection\nin tweets, in: Proceedings of the 26th International Conference on World Wide Web\nCompanion, WWW \u201917 Companion, International World Wide Web Conferences Steering\nCommittee, Republic and Canton of Geneva, CHE, 2017, p. 759\u2013760. URL: https://doi.org/\n10.1145/3041021.3054223. doi: 10.1145/3041021.3054223 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00188",
                        "Citation Paper Title": "Title:Deep Learning for Hate Speech Detection in Tweets",
                        "Citation Paper Abstract": "Abstract:Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.",
                        "Citation Paper Authors": "Authors:Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, Vasudeva Varma"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "were also incorporated into the classification models leading to an observable rise\nin the prediction scores.\nLater approaches used better representation of words and sentences by utilizing semantic\nvector representations such as word2vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed representations\nof words and phrases and their compositionality, in: Advances in neural information\nprocessing systems, 2013, pp. 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.02930v2": {
            "Paper Title": "Optimizing Multiple Performance Metrics with Deep GSP Auctions for\n  E-commerce Advertising",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.02844v1": {
            "Paper Title": "Dynamic Graph Collaborative Filtering",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "trains\na deep autoencoder based model across snapshots of the\ngraph to learn stable graph embeddings over time. TIMERS ",
                    "Citation Text": "Z. Zhang, P. Cui, J. Pei, X. Wang, and W. Zhu, \u201cTimers: Error-bounded\nsvd restart on dynamic networks,\u201d in AAAI , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.09541",
                        "Citation Paper Title": "Title:TIMERS: Error-Bounded SVD Restart on Dynamic Networks",
                        "Citation Paper Abstract": "Abstract:Singular Value Decomposition (SVD) is a popular approach in various network applications, such as link prediction and network parameter characterization. Incremental SVD approaches are proposed to process newly changed nodes and edges in dynamic networks. However, incremental SVD approaches suffer from serious error accumulation inevitably due to approximation on incremental updates. SVD restart is an effective approach to reset the aggregated error, but when to restart SVD for dynamic networks is not addressed in literature. In this paper, we propose TIMERS, Theoretically Instructed Maximum-Error-bounded Restart of SVD, a novel approach which optimally sets the restart time in order to reduce error accumulation in time. Specifically, we monitor the margin between reconstruction loss of incremental updates and the minimum loss in SVD model. To reduce the complexity of monitoring, we theoretically develop a lower bound of SVD minimum loss for dynamic networks and use the bound to replace the minimum loss in monitoring. By setting a maximum tolerated error as a threshold, we can trigger SVD restart automatically when the margin exceeds this threshold.We prove that the time complexity of our method is linear with respect to the number of local dynamic changes, and our method is general across different types of dynamic networks. We conduct extensive experiments on several synthetic and real dynamic networks. The experimental results demonstrate that our proposed method significantly outperforms the existing methods by reducing 27% to 42% in terms of the maximum error for dynamic network reconstruction when fixing the number of restarts. Our method reduces the number of restarts by 25% to 50% when fixing the maximum error tolerated.",
                        "Citation Paper Authors": "Authors:Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, Wenwu Zhu"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "pro-\nposes an embedding method on dynamic attributed network.\nIt models the variations of the adjacency matrix and attribute\nmatrix based on matrix perturbation. DynGEM ",
                    "Citation Text": "P. Goyal, N. Kamra, X. He, and Y . Liu, \u201cDyngem: Deep embedding\nmethod for dynamic graphs,\u201d arXiv preprint arXiv:1805.11273 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11273",
                        "Citation Paper Title": "Title:DynGEM: Deep Embedding Method for Dynamic Graphs",
                        "Citation Paper Abstract": "Abstract:Embedding large graphs in low dimensional spaces has recently attracted significant interest due to its wide applications such as graph visualization, link prediction and node classification. Existing methods focus on computing the embedding for static graphs. However, many graphs in practical applications are dynamic and evolve constantly over time. Naively applying existing embedding algorithms to each snapshot of dynamic graphs independently usually leads to unsatisfactory performance in terms of stability, flexibility and efficiency. In this work, we present an efficient algorithm DynGEM based on recent advances in deep autoencoders for graph embeddings, to address this problem. The major advantages of DynGEM include: (1) the embedding is stable over time, (2) it can handle growing dynamic graphs, and (3) it has better running time than using static embedding methods on each snapshot of a dynamic graph. We test DynGEM on a variety of tasks including graph visualization, graph reconstruction, link prediction and anomaly detection (on both synthetic and real datasets). Experimental results demonstrate the superior stability and scalability of our approach.",
                        "Citation Paper Authors": "Authors:Palash Goyal, Nitin Kamra, Xinran He, Yan Liu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "leverage spectral convolution over the user-item bipartite graph\nto discover possible connections in the spectral domain. SR-\nGNN ",
                    "Citation Text": "S. Wu, Y . Tang, Y . Zhu, L. Wang, X. Xie, and T. Tan, \u201cSession-based\nrecommendation with graph neural networks,\u201d in AAAI , vol. 33, 2019,\npp. 346\u2013353.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00855",
                        "Citation Paper Title": "Title:Session-based Recommendation with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graph-structured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.",
                        "Citation Paper Authors": "Authors:Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ". The effectiveness of GNNs is also proved\non recommendation problems. GCMC ",
                    "Citation Text": "R. v. d. Berg, T. N. Kipf, and M. Welling, \u201cGraph convolutional matrix\ncompletion,\u201d KDD Deep Learning Day , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02263",
                        "Citation Paper Title": "Title:Graph Convolutional Matrix Completion",
                        "Citation Paper Abstract": "Abstract:We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Rianne van den Berg, Thomas N. Kipf, Max Welling"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.02668v1": {
            "Paper Title": "Towards Meaningful Statements in IR Evaluation. Mapping Evaluation\n  Measures to Interval Scales",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.02655v1": {
            "Paper Title": "Metric Learning for Session-based Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.03054v1": {
            "Paper Title": "Application of Knowledge Graphs to Provide Side Information for Improved\n  Recommendation Accuracy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12284v2": {
            "Paper Title": "Pre-training Graph Transformer with Multimodal Side Information for\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ": This method learns node representa-\ntions by sampling a large number of paths in the graph\nand maximizing the average logarithmic probability of\nall vertex context pairs in sampled paths.\n\u000fLINE ",
                    "Citation Text": "J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \u201cLine:\nLarge-scale information network embedding,\u201d in Proceedings of the\n24th international conference on world wide web , 2015, pp. 1067\u20131077.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "as the base model for item recommen-\ndation task, and Deep & Cross Network (DCN) ",
                    "Citation Text": "R. Wang, B. Fu, G. Fu, and M. Wang, \u201cDeep & cross network for ad\nclick predictions,\u201d in Proceedings of the ADKDD\u201917 , 2017, pp. 1\u20137.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.13915v2": {
            "Paper Title": "SG-Net: Syntax Guided Transformer for Language Representation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "- - 84.8 87.9\nBERT + CLSTM + MTL + V y - - 84.9 88.2\nBERT + NGM + SST y - - 85.2 87.7\nBERT + DAE + AoA y - - 85.9 88.6\nXLNet ",
                    "Citation Text": "Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdi-\nnov, and Q. V . Le, \u201cXLNet: Generalized autoregressive\npretraining for language understanding,\u201d in Advances\nin neural information processing systems , 2019, pp. 5754\u2013\n5764.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08237",
                        "Citation Paper Title": "Title:XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                        "Citation Paper Abstract": "Abstract:With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
                        "Citation Paper Authors": "Authors:Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2002.06987v3": {
            "Paper Title": "DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR\n  Predictions in Ad Serving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.02098v1": {
            "Paper Title": "Investigating the efficacy of music version retrieval systems for\n  setlist identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00844v4": {
            "Paper Title": "DiffNet++: A Neural Influence and Interest Diffusion Network for Social\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": ". By treating the user-item interac-\ntion as a graph structure, GCNs have been applied for\nrecommendation ",
                    "Citation Text": "R. Ying, R. He, K. Chen, P . Eksombatchai, W. L. Hamilton, and\nJ. Leskovec. Graph convolutional neural networks for web-scale\nrecommender systems. In SIGKDD , pages 974\u2013983, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". Earlier works relied on spectral\nGCNs, and suffered from huge time complexity ",
                    "Citation Text": "F. Monti, M. Bronstein, and X. Bresson. Geometric matrix comple-\ntion with recurrent multi-graph neural networks. In NIPS , pages\n3697\u20133707, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06803",
                        "Citation Paper Title": "Title:Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Federico Monti, Michael M. Bronstein, Xavier Bresson"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nRecently, some deep learning based models have been pro-\nposed to tackle the CF problem ",
                    "Citation Text": "X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. Neural\ncollaborative \ufb01ltering. In WWW , pages 173\u2013182, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.01317v1": {
            "Paper Title": "Contrastive Learning for Recommender System",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "is an industrial solution\nthat employs multiple graph convolution layers on the item-item\ngraph for Pinterest image recommendation. NGCF ",
                    "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In the 42nd International ACM SIGIR Con-\nference .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". It learns discriminative\nrepresentations by contrasting positive and negative samples. In\nnatural language processing, the most classic model - Word2vec ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.\n2013. Distributed Representations of Words and Phrases and their Composition-\nality. In NIPS . 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", which we here instantiate as maximizing the mutual\ninformation (MI) between two views ",
                    "Citation Text": "Philip Bachman, R. Devon Hjelm, and William Buchwalter. 2019. Learning\nRepresentations by Maximizing Mutual Information Across Views. In NeurIPS .\n15509\u201315519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00910",
                        "Citation Paper Title": "Title:Learning Representations by Maximizing Mutual Information Across Views",
                        "Citation Paper Abstract": "Abstract:We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events.\nFollowing our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: this https URL.",
                        "Citation Paper Authors": "Authors:Philip Bachman, R Devon Hjelm, William Buchwalter"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "exploits the\nuser-item graph structure by propagating embeddings on it; this\nleads to the expressive modeling of high-order connectivity in the\nuser-item graph. LightGCN ",
                    "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network\nfor Recommendation. In SIGIR . ACM, 639\u2013648.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.02126",
                        "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                        "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "applies\nthe Graph Convolution Network (GCN) on the user-item graph; it\nemploys one convolutional layer to exploit the direct connections\nbetween users and items. PinSage ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In KDD . ACM, 974\u2013983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.00939v1": {
            "Paper Title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.00884v1": {
            "Paper Title": "Coreference Resolution in Research Papers from Multiple Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.00480v1": {
            "Paper Title": "A multi-modal approach towards mining social media data during natural\n  disasters -- a case study of Hurricane Irma",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "other than the Windy tag (poor/chance agreement).\nThis annotated dataset was used to train deep learning models based on con-\nvolutional neural network (CNN) architectures. Convolutional networks have\nbeen widely used in large-scale image and video recognition ",
                    "Citation Text": "K. Simonyan, A. Zisserman, Very Deep Convolutional Networks for Large-\nScale Image Recognition, arXiv:1409.1556 [cs]ArXiv: 1409.1556 (Sep.\n2014).\nURL http://arxiv.org/abs/1409.1556",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.1556",
                        "Citation Paper Title": "Title:Very Deep Convolutional Networks for Large-Scale Image Recognition",
                        "Citation Paper Abstract": "Abstract:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.00430v1": {
            "Paper Title": "Assessing Emoji Use in Modern Text Processing Tools",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09107v2": {
            "Paper Title": "A Simple and Effective Self-Supervised Contrastive Learning Framework\n  for Aspect Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15192v1": {
            "Paper Title": "Simplifying Impact Prediction for Scientific Articles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15151v1": {
            "Paper Title": "Per-Instance Algorithm Selection for Recommender Systems via Instance\n  Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14978v1": {
            "Paper Title": "Few-Shot Named Entity Recognition: A Comprehensive Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14803v1": {
            "Paper Title": "Supporting Human Memory by Reconstructing Personal Episodic Narratives\n  from Digital Traces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14770v1": {
            "Paper Title": "Hybrid Interest Modeling for Long-tailed Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14700v1": {
            "Paper Title": "Image-to-Image Retrieval by Learning Similarity between Scene Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.02411v4": {
            "Paper Title": "Side Information Fusion for Recommender Systems over Heterogeneous\n  Information Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14234v1": {
            "Paper Title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14005v1": {
            "Paper Title": "Neural document expansion for ad-hoc information retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13919v1": {
            "Paper Title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15728v1": {
            "Paper Title": "Multi-Channel Sequential Behavior Networks for User Modeling in Online\n  Advertising",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "uses a product layer to capture interactive\npatterns between the ad and user events, and feeds it as\nadditional input to the network.\n\u2022 DIN ",
                    "Citation Text": "G. Zhou, C. Song, X. Zhu, X. Ma, Y . Yan, X. Dai, H. Zhu,\nJ. Jin, H. Li, and K. Gai. Deep interest network for click-\nthrough rate prediction. In Proceedings of the SIGKDD\nInternational Conference on Knowledge Discovery and Data\nMining , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". RNNs were found particularly suitable\nfor session-based recommendation, in which the task is to pre-\ndict the next item a user will interact with based on his/her pre-vious items. GRU2Rec ",
                    "Citation Text": "B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk.\nSession-based recommendations with recurrent neural net-\nworks. CoRR , abs/1511.06939, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.03590v3": {
            "Paper Title": "Learning 2D Temporal Adjacent Networks for Moment Localization with\n  Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03733v3": {
            "Paper Title": "Toward a Knowledge-based Personalised Recommender System for Mobile App\n  Development",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13569v1": {
            "Paper Title": "Dynamic-K Recommendation with Personalized Decision Boundary",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13538v1": {
            "Paper Title": "Comprehensive Graph-conditional Similarity Preserving Network for\n  Unsupervised Cross-modal Hashing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13529v1": {
            "Paper Title": "Brain-inspired Search Engine Assistant based on Knowledge Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13245v1": {
            "Paper Title": "A Hybrid Bandit Framework for Diversified Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13102v1": {
            "Paper Title": "THUIR@COLIEE-2020: Leveraging Semantic Understanding and Exact Matching\n  for Legal Case Retrieval and Entailment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12065v1": {
            "Paper Title": "Event-Driven Query Expansion",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "proposed to train embeddings on the top retrieved\ndocuments for each query. Rosin et al . ",
                    "Citation Text": "Guy D Rosin, Eytan Adar, and Kira Radinsky. 2017. Learning Word Relatedness\nover Time. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing . 1168\u20131178.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.08081",
                        "Citation Paper Title": "Title:Learning Word Relatedness over Time",
                        "Citation Paper Abstract": "Abstract:Search systems are often focused on providing relevant results for the \"now\", assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.",
                        "Citation Paper Authors": "Authors:Guy D. Rosin, Eytan Adar, Kira Radinsky"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "proposed relevance-based word embeddings\nand thus matched the embedding algorithm\u2019s objective to that\nof IR tasks (i.e., to capture relevance). External data sources (e.g.,\nWikipedia and WordNet) have also been utilized for QE [ 2,27].\nRecently, Imani et al . ",
                    "Citation Text": "Ayyoob Imani, Amir Vakili, Ali Montazer, and Azadeh Shakery. 2019. Deep neural\nnetworks for query expansion using word embeddings. In European Conference\non Information Retrieval . Springer, 203\u2013210.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03514",
                        "Citation Paper Title": "Title:Deep Neural Networks for Query Expansion using Word Embeddings",
                        "Citation Paper Abstract": "Abstract:Query expansion is a method for alleviating the vocabulary mismatch problem present in information retrieval tasks. Previous works have shown that terms selected for query expansion by traditional methods such as pseudo-relevance feedback are not always helpful to the retrieval process. In this paper, we show that this is also true for more recently proposed embedding-based query expansion methods. We then introduce an artificial neural network classifier to predict the usefulness of query expansion terms. This classifier uses term word embeddings as inputs. We perform experiments on four TREC newswire and web collections show that using terms selected by the classifier for expansion significantly improves retrieval performance when compared to competitive baselines. The results are also shown to be more robust than the baselines.",
                        "Citation Paper Authors": "Authors:Ayyoob Imani, Amir Vakili, Ali Montazer, Azadeh Shakery"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", and Zamani and Croft [42,43]used word embeddings\nto select terms semantically related to the query as expansions.\nDiaz et al . ",
                    "Citation Text": "Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query expansion with\nlocally-trained word embeddings. arXiv preprint arXiv:1605.07891 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07891",
                        "Citation Paper Title": "Title:Query Expansion with Locally-Trained Word Embeddings",
                        "Citation Paper Abstract": "Abstract:Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.",
                        "Citation Paper Authors": "Authors:Fernando Diaz, Bhaskar Mitra, Nick Craswell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.11876v1": {
            "Paper Title": "Intelligent Vector-based Customer Segmentation in the Banking Industry",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ".\nThe application of customer segmentation is not limited to sales and marketing goals. There are some researches\non novel methods of measuring customer risks by applying supervised customer segmentation models ",
                    "Citation Text": "A. Namvar, M. Siami, F. Rabhi, and M. Naderpour. Credit risk prediction in an imbalanced social lending\nenvironment FinanceIT Research Group , University of New South Wales , Centre for Arti\ufb01cial Intelligence ,\nUniversity of Technology Sydney ,.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.00801",
                        "Citation Paper Title": "Title:Credit risk prediction in an imbalanced social lending environment",
                        "Citation Paper Abstract": "Abstract:Credit risk prediction is an effective way of evaluating whether a potential borrower will repay a loan, particularly in peer-to-peer lending where class imbalance problems are prevalent. However, few credit risk prediction models for social lending consider imbalanced data and, further, the best resampling technique to use with imbalanced data is still controversial. In an attempt to address these problems, this paper presents an empirical comparison of various combinations of classifiers and resampling techniques within a novel risk assessment methodology that incorporates imbalanced data. The credit predictions from each combination are evaluated with a G-mean measure to avoid bias towards the majority class, which has not been considered in similar studies. The results reveal that combining random forest and random under-sampling may be an effective strategy for calculating the credit risk associated with loan applicants in social lending markets.",
                        "Citation Paper Authors": "Authors:Anahita Namvar, Mohammad Siami, Fethi Rabhi, Mohsen Naderpour"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.11842v1": {
            "Paper Title": "Personalized Adaptive Meta Learning for Cold-start User Preference\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07283v3": {
            "Paper Title": "Dutch General Public Reaction on Governmental COVID-19 Measures and\n  Announcements in Twitter Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11172v1": {
            "Paper Title": "Who will accept my request? Predicting response of link initiation in\n  two-way relation networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13556v2": {
            "Paper Title": "Hierarchical Metadata-Aware Document Categorization under Weak\n  Supervision",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "present a weakly supervised hierarchical classification ap-\nproach to classify GitHub repositories. Later, they also propose a flat\nmetadata-aware text categorization framework ",
                    "Citation Text": "Yu Zhang, Yu Meng, Jiaxin Huang, Frank F. Xu, Xuan Wang, and Jiawei Han.\n2020. Minimally Supervised Categorization of Text with Metadata. In SIGIR\u201920 .\n1231\u20131240.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00624",
                        "Citation Paper Title": "Title:Minimally Supervised Categorization of Text with Metadata",
                        "Citation Paper Abstract": "Abstract:Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1) the presence of metadata: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2) label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.",
                        "Citation Paper Authors": "Authors:Yu Zhang, Yu Meng, Jiaxin Huang, Frank F. Xu, Xuan Wang, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "propose a general framework to\ninject categorical metadata signal into a deep text classifier. How-\never, these models assume a fully supervised setting. Zhang et\nal. ",
                    "Citation Text": "Yu Zhang, Frank F. Xu, Sha Li, Yu Meng, Xuan Wang, Qi Li, and Jiawei Han. 2019.\nHiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories.\nInICDM\u201919 . 876\u2013885.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07115",
                        "Citation Paper Title": "Title:HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories",
                        "Citation Paper Abstract": "Abstract:GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneous information network embedding; keyword enrichment; topic modeling and pseudo document generation. Experimental results on two GitHub repository collections confirm that HiGitClass is superior to existing weakly-supervised and dataless hierarchical classification methods, especially in its ability to integrate both structured and unstructured data for repository classification.",
                        "Citation Paper Authors": "Authors:Yu Zhang, Frank F. Xu, Sha Li, Yu Meng, Xuan Wang, Qi Li, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "encode user\nmobility data and social network information for joint time and\nlocation prediction; Kim et al. ",
                    "Citation Text": "Jihyeok Kim, Reinald Kim Amplayo, Kyungjae Lee, Sua Sung, Minji Seo, and\nSeung-won Hwang. 2019. Categorical Metadata Representation for Customized\nText Classification. TACL 7 (2019), 201\u2013215.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05196",
                        "Citation Paper Title": "Title:Categorical Metadata Representation for Customized Text Classification",
                        "Citation Paper Abstract": "Abstract:The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. These information have been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose to use basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various datasets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.",
                        "Citation Paper Authors": "Authors:Jihyeok Kim, Reinald Kim Amplayo, Kyungjae Lee, Sua Sung, Minji Seo, Seung-won Hwang"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "learn user and product repre-\nsentations for sentiment analysis; Zhang et al. ",
                    "Citation Text": "Yu Zhang, Wei Wei, Binxuan Huang, Kathleen M Carley, and Yan Zhang. 2017.\nRATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location\nEstimation. In CIKM\u201917 . 2423\u20132426.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06515",
                        "Citation Paper Title": "Title:RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation",
                        "Citation Paper Abstract": "Abstract:Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that \"one-hot\" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.",
                        "Citation Paper Authors": "Authors:Yu Zhang, Wei Wei, Binxuan Huang, Kathleen M. Carley, Yan Zhang"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "models topic seman-\ntics in the word2vec embedding space and applies a self-training\nscheme; PCEM ",
                    "Citation Text": "Huiru Xiao, Xin Liu, and Yangqiu Song. 2019. Efficient Path Prediction for Semi-\nSupervised and Weakly Supervised Hierarchical Text Classification. In WWW\u201919 .\n3370\u20133376.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.09347",
                        "Citation Paper Title": "Title:Efficient Path Prediction for Semi-Supervised and Weakly Supervised Hierarchical Text Classification",
                        "Citation Paper Abstract": "Abstract:Hierarchical text classification has many real-world applications. However, labeling a large number of documents is costly. In practice, we can use semi-supervised learning or weakly supervised learning (e.g., dataless classification) to reduce the labeling cost. In this paper, we propose a path cost-sensitive learning algorithm to utilize the structural information and further make use of unlabeled and weakly-labeled data. We use a generative model to leverage the large amount of unlabeled data and introduce path constraints into the learning algorithm to incorporate the structural information of the class hierarchy. The posterior probabilities of both unlabeled and weakly labeled data can be incorporated with path-dependent scores. Since we put a structure-sensitive cost to the learning algorithm to constrain the classification consistent with the class hierarchy and do not need to reconstruct the feature vectors for different structures, we can significantly reduce the computational cost compared to structural output learning. Experimental results on two hierarchical text classification benchmarks show that our approach is not only effective but also efficient to handle the semi-supervised and weakly supervised hierarchical text classification.",
                        "Citation Paper Authors": "Authors:Huiru Xiao, Xin Liu, Yangqiu Song"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "combine the ideas of training a local classifier per level and opti-\nmizing the global classification results to mitigate exposure bias.\nThe global structure of hierarchies is also used in many other mod-\nels, such as meta-learning ",
                    "Citation Text": "Jiawei Wu, Wenhan Xiong, and William Yang Wang. 2019. Learning to Learn and\nPredict: A Meta-Learning Approach for Multi-Label Classification. In EMNLP\u201919 .\n4345\u20134355.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04176",
                        "Citation Paper Title": "Title:Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification",
                        "Citation Paper Abstract": "Abstract:Many tasks in natural language processing can be viewed as multi-label classification problems. However, most of the existing models are trained with the standard cross-entropy loss function and use a fixed prediction policy (e.g., a threshold of 0.5) for all the labels, which completely ignores the complexity and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the training policies and prediction policies for different labels. The training policies are then used to train the classifier with the cross-entropy loss function, and the prediction policies are further implemented for prediction. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.",
                        "Citation Paper Authors": "Authors:Jiawei Wu, Wenhan Xiong, William Yang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.10226v1": {
            "Paper Title": "Should I visit this place? Inclusion and Exclusion Phrase Mining from\n  Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10185v1": {
            "Paper Title": "Recommenders with a mission: assessing diversity in newsrecommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09936v1": {
            "Paper Title": "Named Entity Recognition in the Legal Domain using a Pointer Generator\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06209v2": {
            "Paper Title": "KOSMOS: Knowledge-graph Oriented Social media and Mainstream media\n  Overview System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11321v1": {
            "Paper Title": "Autoregressive Reasoning over Chains of Facts with Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12735v2": {
            "Paper Title": "Learning to Recommend from Sparse Data via Generative User Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09369v1": {
            "Paper Title": "Assessing COVID-19 Impacts on College Students via Automated Processing\n  of Free-form Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09342v1": {
            "Paper Title": "Adaptive Multi-Agent E-Learning Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09263v1": {
            "Paper Title": "Checking Fact Worthiness using Sentence Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "and Universal Sentence Encoder on\nseven Semantic Textual Similarity (STS) tasks.\n3 DATASET\nThe dataset for Task 1 is an extension of the Check That-18\ndataset ",
                    "Citation Text": "Pepa Atanasova, Alberto Barron-Cedeno, Tamer Elsayed, Reem\nSuwaileh, Wajdi Zaghouani, Spas Kyuchukov, Giovanni Martino, and\nPreslav Nakov. 2018. Overview of the CLEF-2018 CheckThat! Lab on\nAutomatic Identification and Verification of Political Claims. Task 1:\nCheck-Worthiness.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05542",
                        "Citation Paper Title": "Title:Overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification of Political Claims. Task 1: Check-Worthiness",
                        "Citation Paper Abstract": "Abstract:We present an overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification of Political Claims, with focus on Task 1: Check-Worthiness. The task asks to predict which claims in a political debate should be prioritized for fact-checking. In particular, given a debate or a political speech, the goal was to produce a ranked list of its sentences based on their worthiness for fact checking. We offered the task in both English and Arabic, based on debates from the 2016 US Presidential Campaign, as well as on some speeches during and after the campaign. A total of 30 teams registered to participate in the Lab and seven teams actually submitted systems for Task~1. The most successful approaches used by the participants relied on recurrent and multi-layer neural networks, as well as on combinations of distributional representations, on matchings claims' vocabulary against lexicons, and on measures of syntactic dependency. The best systems achieved mean average precision of 0.18 and 0.15 on the English and on the Arabic test datasets, respectively. This leaves large room for further improvement, and thus we release all datasets and the scoring scripts, which should enable further research in check-worthiness estimation.",
                        "Citation Paper Authors": "Authors:Pepa Atanasova, Alberto Barron-Cedeno, Tamer Elsayed, Reem Suwaileh, Wajdi Zaghouani, Spas Kyuchukov, Giovanni Da San Martino, Preslav Nakov"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "in their work used neural networks for\nchecking worthiness. They used InferSent ",
                    "Citation Text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and\nAntoine Bordes. 2017. Supervised learning of universal sentence rep-\nresentations from natural language inference data. arXiv preprint\narXiv:1705.02364 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "extended\nClaimBuster\u2019s feature set by including more contextual fea-\ntures, such as the sentence\u2019s position in the debate text,\nand whether the debate opponent is mentioned. Konstanti-\nnovskiy et al. ",
                    "Citation Text": "Lev Konstantinovskiy, Oliver Price, Mevan Babakar, and Arkaitz Zubi-\naga. 2018. Towards automated factchecking: Developing an annota-\ntion schema and benchmark for consistent automated claim detection.\narXiv preprint arXiv:1809.08193 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08193",
                        "Citation Paper Title": "Title:Towards Automated Factchecking: Developing an Annotation Schema and Benchmark for Consistent Automated Claim Detection",
                        "Citation Paper Abstract": "Abstract:In an effort to assist factcheckers in the process of factchecking, we tackle the claim detection task, one of the necessary stages prior to determining the veracity of a claim. It consists of identifying the set of sentences, out of a long text, deemed capable of being factchecked. This paper is a collaborative work between Full Fact, an independent factchecking charity, and academic partners. Leveraging the expertise of professional factcheckers, we develop an annotation schema and a benchmark for automated claim detection that is more consistent across time, topics and annotators than previous approaches. Our annotation schema has been used to crowdsource the annotation of a dataset with sentences from UK political TV shows. We introduce an approach based on universal sentence representations to perform the classification, achieving an F1 score of 0.83, with over 5% relative improvement over the state-of-the-art methods ClaimBuster and ClaimRank. The system was deployed in production and received positive user feedback.",
                        "Citation Paper Authors": "Authors:Lev Konstantinovskiy, Oliver Price, Mevan Babakar, Arkaitz Zubiaga"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "presented a system\ncalled TATHYA based on the similar features, and also in-\ncluded contextual features sentences immediately preceding\nand succeeding the one being assessed, as well as certain\nhand-crafted POS patterns. Gencheva et al. ",
                    "Citation Text": "Pepa Gencheva, Preslav Nakov, Llu\u00eds M\u00e0rquez, Alberto Barr\u00f3n-Cede\u00f1o,\nand Ivan Koychev. 2017. A context-aware approach for detecting worth-\nchecking claims in political debates. In Proceedings of the International\nConference Recent Advances in Natural Language Processing, RANLP\n2017. 267\u2013276.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.08084",
                        "Citation Paper Title": "Title:A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates",
                        "Citation Paper Abstract": "Abstract:In the context of investigative journalism, we address the problem of automatically identifying which claims in a given document are most worthy and should be prioritized for fact-checking. Despite its importance, this is a relatively understudied problem. Thus, we create a new dataset of political debates, containing statements that have been fact-checked by nine reputable sources, and we train machine learning models to predict which claims should be prioritized for fact-checking, i.e., we model the problem as a ranking task. Unlike previous work, which has looked primarily at sentences in isolation, in this paper we focus on a rich input representation modeling the context: relationship between the target statement and the larger context of the debate, interaction between the opponents, and reaction by the moderator and by the public. Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information.",
                        "Citation Paper Authors": "Authors:Pepa Gencheva, Ivan Koychev, Llu\u00eds M\u00e0rquez, Alberto Barr\u00f3n-Cede\u00f1o, Preslav Nakov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.08984v1": {
            "Paper Title": "Batch-Constrained Distributional Reinforcement Learning for\n  Session-based Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ") may be arbitrarily erroneous, affecting\nthe learning process. This overestimation bias ",
                    "Citation Text": "S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo-\nration. In Proceedings of the 36th International Conference on Machine Learning , volume 97,\npages 2052\u20132062, Long Beach, California, USA, 09\u201315 Jun 2019. PMLR.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02900",
                        "Citation Paper Title": "Title:Off-Policy Deep Reinforcement Learning without Exploration",
                        "Citation Paper Abstract": "Abstract:Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.",
                        "Citation Paper Authors": "Authors:Scott Fujimoto, David Meger, Doina Precup"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "), etc. have been proposed for SR. The\nDL approaches provide state-of-the-art performance in next-step interaction prediction task but are\nmyopic in their recommendations and do not take longer-term goals into account. Several RL-based\napproaches based on MDPs ",
                    "Citation Text": "G. Shani, D. Heckerman, and R. I. Brafman. An mdp-based recommender system. Journal of\nMachine Learning Research , 6(Sep):1265\u20131295, 2005.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.0600",
                        "Citation Paper Title": "Title:An MDP-based Recommender System",
                        "Citation Paper Abstract": "Abstract:Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.",
                        "Citation Paper Authors": "Authors:Guy Shani, Ronen I. Brafman, David Heckerman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.08952v1": {
            "Paper Title": "Scenario-aware and Mutual-based approach for Multi-scenario\n  Recommendation in E-Commerce",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "structure to learn high-order representation of\nfeatures.\nBesides, users\u2019 sequential behavior implies the dynamic and\nevolving interests and has been proven effective in tasks of\nuser interest estimation. DIN ",
                    "Citation Text": "G. Zhou, X. Zhu, C. Song, Y . Fan, H. Zhu, X. Ma, Y . Yan, J. Jin, H. Li,\nand K. Gai, \u201cDeep interest network for click-through rate prediction,\u201d\ninProceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining . ACM, 2018, pp. 1059\u20131068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06978",
                        "Citation Paper Title": "Title:Deep Interest Network for Click-Through Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.",
                        "Citation Paper Authors": "Authors:Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, Kun Gai"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "replaces the wide component of Wide&Deepwith factorization machines (FM) to model second-order fea-\nture interactions. DCN ",
                    "Citation Text": "R. Wang, B. Fu, G. Fu, and M. Wang, \u201cDeep & cross network for ad\nclick predictions,\u201d in Proceedings of the ADKDD\u201917 . ACM, 2017,\np. 12.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "combines the bene\ufb01ts of linear and deep\nrepresentations, serving as a good solution for this task.\nDeepFM ",
                    "Citation Text": "H. Guo, R. Tang, Y . Ye, Z. Li, and X. He, \u201cDeepfm: a factorization-\nmachine based neural network for ctr prediction,\u201d arXiv preprint\narXiv:1703.04247 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "divides users\u2019 behaviors into different\nsessions and use self-attention to extract users\u2019 interests in\neach session. Most recently, BST ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances\nin neural information processing systems , 2017, pp. 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "introduces two\nauxiliary networks for CTR and CTCVR tasks, tackling the\nchallenges of sample selection bias and data sparsity prob-\nlems. ESM2 ",
                    "Citation Text": "H. Wen, J. Zhang, Y . Wang, W. Bao, Q. Lin, and K. Yang, \u201cConver-\nsion rate prediction via post-click behaviour modeling,\u201d arXiv preprint\narXiv:1910.07099 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07099",
                        "Citation Paper Title": "Title:Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction",
                        "Citation Paper Abstract": "Abstract:Recommender system, as an essential part of modern e-commerce, consists of two fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate (CVR) prediction. While CVR has a direct impact on the purchasing volume, its prediction is well-known challenging due to the Sample Selection Bias (SSB) and Data Sparsity (DS) issues. Although existing methods, typically built on the user sequential behavior path ``impression$\\to$click$\\to$purchase'', is effective for dealing with SSB issue, they still struggle to address the DS issue due to rare purchase training samples. Observing that users always take several purchase-related actions after clicking, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph ``impression$\\to$click$\\to$D(O)Action$\\to$purchase''. Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervised signals from D(O)Action, which will effectively address the SSB and DS issues together. To this end, we devise a novel deep recommendation model named Elaborated Entire Space Supervised Multi-task Model ($ESM^{2}$). According to the conditional probability rule defined on the graph, it employs multi-task learning to predict some decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR. Extensive experiments on both offline and online environments demonstrate the superiority of $ESM^{2}$ over state-of-the-art models. The source code and dataset will be released.",
                        "Citation Paper Authors": "Authors:Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, Keping Yang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "pro-\nposes a robust and practical representation learning framework,\nwhich learns sharing user representations in an end-to-end\nsetting across multiple e-commerce tasks. Considering the\nsequential pattern of user actions, ESMM ",
                    "Citation Text": "X. Ma, L. Zhao, G. Huang, Z. Wang, Z. Hu, X. Zhu, and K. Gai, \u201cEntire\nspace multi-task model: An effective approach for estimating post-click\nconversion rate,\u201d in The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval . ACM, 2018, pp.\n1137\u20131140.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07931",
                        "Citation Paper Title": "Title:Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate",
                        "Citation Paper Abstract": "Abstract:Estimating post-click conversion rate (CVR) accurately is crucial for ranking systems in industrial applications such as recommendation and advertising. Conventional CVR modeling applies popular deep learning methods and achieves state-of-the-art performance. However it encounters several task-specific problems in practice, making CVR modeling challenging. For example, conventional CVR models are trained with samples of clicked impressions while utilized to make inference on the entire space with samples of all impressions. This causes a sample selection bias problem. Besides, there exists an extreme data sparsity problem, making the model fitting rather difficult. In this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression -> click -> conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy. Experiments on dataset gathered from Taobao's recommender system demonstrate that ESMM significantly outperforms competitive methods. We also release a sampling version of this dataset to enable future research. To the best of our knowledge, this is the first public dataset which contains samples with sequential dependence of click and conversion labels for CVR modeling.",
                        "Citation Paper Authors": "Authors:Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, Kun Gai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.08907v1": {
            "Paper Title": "Information retrieval system for silte language using BM25 weighting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08793v1": {
            "Paper Title": "Session-based k-NNs with Semantic Suggestions for Next-item Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08787v1": {
            "Paper Title": "Query expansion with artificially generated texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08777v1": {
            "Paper Title": "Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs.\n  Identified Customers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11327v1": {
            "Paper Title": "Collaborative residual learners for automatic icd10 prediction using\n  prescribed medications",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Traditionally in \ndesigning NNs , each layer feeds the following  layer in the architecture. Residual net works (ResNets)  are a special \ntype of NNs utilizing  skip connections  that act as shortcuts  to jump over some layers  in the architecture ",
                    "Citation Text": "K. He, X., Zhang, S., Ren,  and J. Sun, J. , Deep residual learning for image recognition , CVPR ( 2016) , 770-778.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.11333v1": {
            "Paper Title": "Ensemble model for pre-discharge icd10 coding prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08695v1": {
            "Paper Title": "DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08575v1": {
            "Paper Title": "Weakly Supervised Label Smoothing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08134v1": {
            "Paper Title": "Distant-Supervised Slot-Filling for E-Commerce Queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08000v1": {
            "Paper Title": "Discovering Airline-Specific Business Intelligence from Online Passenger\n  Reviews: An Unsupervised Text Analytics Approach",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "analyzed online reviews to identify the marketing performance\nof individual airlines. Instead of relying on OCR, Hu et al. ",
                    "Citation Text": "G. Hu, P. Bhargava, S. Fuhrmann, S. Ellinger, N. Spasojevic, Analyzing users' sentiment to-\nwards popular consumer industries and brands on Twitter, IEEE International Conference\non Data Mining Workshops, ICDMW 2017-Novem (2017) 381{388. arXiv:1709 :07434 .\nURL http://arxiv :org/abs/1709 :07434",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07434",
                        "Citation Paper Title": "Title:Analyzing users' sentiment towards popular consumer industries and brands on Twitter",
                        "Citation Paper Abstract": "Abstract:Social media serves as a unified platform for users to express their thoughts on subjects ranging from their daily lives to their opinion on consumer brands and products. These users wield an enormous influence in shaping the opinions of other consumers and influence brand perception, brand loyalty and brand advocacy. In this paper, we analyze the opinion of 19M Twitter users towards 62 popular industries, encompassing 12,898 enterprise and consumer brands, as well as associated subject matter topics, via sentiment analysis of 330M tweets over a period spanning a month. We find that users tend to be most positive towards manufacturing and most negative towards service industries. In addition, they tend to be more positive or negative when interacting with brands than generally on Twitter. We also find that sentiment towards brands within an industry varies greatly and we demonstrate this using two industries as use cases. In addition, we discover that there is no strong correlation between topic sentiments of different industries, demonstrating that topic sentiments are highly dependent on the context of the industry that they are mentioned in. We demonstrate the value of such an analysis in order to assess the impact of brands on social media. We hope that this initial study will prove valuable for both researchers and companies in understanding users' perception of industries, brands and associated topics and encourage more research in this field.",
                        "Citation Paper Authors": "Authors:Guoning Hu, Preeti Bhargava, Saul Fuhrmann, Sarah Ellinger, Nemanja Spasojevic"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.07064v1": {
            "Paper Title": "Pre-Training Graph Neural Networks for Cold-Start Users and Items\n  Representation",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "and model-based\nrecommendation [ 7,20,22,24]. However, few of them capture the\nhigh-order interactions. Another kind of method is GNNs, which\nleverage user-item bipartite graph to capture high-order collabora-\ntive signals for recommendation. The representative models include\nPinsage ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In SIGKDD\"18 . 974\u2013983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.06910v1": {
            "Paper Title": "Learning over no-Preferred and Preferred Sequence of items for Robust\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08762v2": {
            "Paper Title": "Guiding Graph Embeddings using Path-Ranking Methods for Error Detection\n  innoisy Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06901v1": {
            "Paper Title": "GAN-based Recommendation with Positive-Unlabeled Sampling",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "during training due to its\nill-conditioned assumption. To stabilize and improve the model\nperformance, other techniques have been developed to alleviate the\nconvergence issue, such as dynamic negative sampling (DNS) or\ndynamic random negative sampling (DRNS) ",
                    "Citation Text": "Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng\nZhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying Generative\nand Discriminative Information Retrieval Models. In Proceedings of International\nConference on Research and Development in Information Retrieval SIGIR . 515\u2013524.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10513",
                        "Citation Paper Title": "Title:IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models",
                        "Citation Paper Abstract": "Abstract:This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",
                        "Citation Paper Authors": "Authors:Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, Dell Zhang"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". To this end,\nPU learning [ 20,29] can be used to tackle this problem with theoret-\nical guarantees by treating the unobserved user-item tuples directly\nas unlabeled samples. Following ",
                    "Citation Text": "Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama.\n2017. Positive-Unlabeled Learning with Non-Negative Risk Estimator. In Advances\nin Neural Information Processing Systems NeurIPS . 1675\u20131685.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00593",
                        "Citation Paper Title": "Title:Positive-Unlabeled Learning with Non-Negative Risk Estimator",
                        "Citation Paper Abstract": "Abstract:From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.",
                        "Citation Paper Authors": "Authors:Ryuichi Kiryo, Gang Niu, Marthinus C. du Plessis, Masashi Sugiyama"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.06504v2": {
            "Paper Title": "Filling the Gap of Utterance-aware and Speaker-aware Representation for\n  Multi-turn Dialogue",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06690v1": {
            "Paper Title": "Yelp Review Rating Prediction: Machine Learning and Deep Learning Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06683v1": {
            "Paper Title": "Cortex: Harnessing Correlations to Boost Query Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06366v1": {
            "Paper Title": "Limits of PageRank-based ranking methods in sports data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06238v1": {
            "Paper Title": "Query Understanding for Natural Language Enterprise Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09694v4": {
            "Paper Title": "Retrospective Reader for Machine Reading Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06200v1": {
            "Paper Title": "Garment Recommendation with Memory Augmented Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06146v1": {
            "Paper Title": "Exploiting Behavioral Consistence for Universal User Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04682v2": {
            "Paper Title": "Transformer Query-Target Knowledge Discovery (TEND): Drug Discovery from\n  CORD-19",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06025v1": {
            "Paper Title": "Exploring Deep Neural Networks and Transfer Learning for Analyzing\n  Emotions in Tweets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05852v1": {
            "Paper Title": "Social Media Alerts can Improve, but not Replace Hydrological Models for\n  Forecasting Floods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05818v1": {
            "Paper Title": "Bew: Towards Answering Business-Entity-Related Web Questions",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "studies QA over Amazon product reviews. The reviews section\nin the web pages are high text-density document. Other recent\napproaches such as REALM ",
                    "Citation Text": "K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08909",
                        "Citation Paper Title": "Title:REALM: Retrieval-Augmented Language Model Pre-Training",
                        "Citation Paper Abstract": "Abstract:Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                        "Citation Paper Authors": "Authors:Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.05462v1": {
            "Paper Title": "Cold-start Sequential Recommendation via Meta Learner",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07565v1": {
            "Paper Title": "Automating Document Classification with Distant Supervision to Increase\n  the Efficiency of Systematic Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06576v1": {
            "Paper Title": "Learning from User Interactions with Rankings: A Unification of the\n  Field",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ". Over the\nyears, the role of ranking systems has only become more important, as they are now\nused in a wide variety of settings. Users rely on them to search through many large\ncollections of content, including images ",
                    "Citation Text": "A. Gordo, J. Almaz \u00b4an, J. Revaud, and D. Larlus. Deep image retrieval: Learning global representations\nfor image search. In European Conference on Computer Vision , pages 241\u2013257. Springer, 2016. (Cited\non page 1.)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01325",
                        "Citation Paper Title": "Title:Deep Image Retrieval: Learning global representations for image search",
                        "Citation Paper Abstract": "Abstract:We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at this http URL.",
                        "Citation Paper Authors": "Authors:Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.05031v1": {
            "Paper Title": "Improving Knowledge Tracing via Pre-training Question Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04979v1": {
            "Paper Title": "Representation Extraction and Deep Neural Recommendation for\n  Collaborative Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07589v1": {
            "Paper Title": "Incorporating Domain Knowledge To Improve Topic Segmentation Of Long\n  MOOC Lecture Videos",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". It tries to build a universal sen-\ntence encoding by learning the sentence semantics of Stanford\nNatural Language Inference (SNLI) ",
                    "Citation Text": "S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, \u201cA large\nannotated corpus for learning natural language inference,\u201d arXiv preprint\narXiv:1508.05326 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.05326",
                        "Citation Paper Title": "Title:A large annotated corpus for learning natural language inference",
                        "Citation Paper Abstract": "Abstract:Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
                        "Citation Paper Authors": "Authors:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.04388v1": {
            "Paper Title": "Algorithms for finding $k$ in $k$-means",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03975v3": {
            "Paper Title": "CAiRE-COVID: A Question Answering and Query-focused Multi-Document\n  Summarization System for COVID-19 Scholarly Information Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04005v1": {
            "Paper Title": "Improving Clinical Document Understanding on COVID-19 Research with\n  Spark NLP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03704v1": {
            "Paper Title": "Conversational Browsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07672v3": {
            "Paper Title": "Analysis of Word Embeddings Using Fuzzy Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00485v3": {
            "Paper Title": "Mixed Information Flow for Cross-domain Sequential Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13141v4": {
            "Paper Title": "A Differentiable Ranking Metric Using Relaxed Sorting Operation for\n  Top-K Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09180v4": {
            "Paper Title": "How Value-Sensitive Design Can Empower Sustainable Consumption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02476v1": {
            "Paper Title": "Offline Meta-level Model-based Reinforcement Learning Approach for\n  Cold-Start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01953v1": {
            "Paper Title": "Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific\n  Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03257v5": {
            "Paper Title": "Quda: Natural Language Queries for Visual Data Analytics",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "contains over 5,000 static chart images, and over 2,000 are labeled with chart type information. Beagle ",
                    "Citation Text": "Leilani Battle, Peitong Duan, Zachery Miranda, Dana Mukusheva, Remco Chang, and Michael Stonebraker. 2018. Beagle: Automated Extraction and\nInterpretation of Visualizations from the Web. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada)\n(CHI \u201918) . Association for Computing Machinery, New York, NY, USA, Article 594, 8 pages. https://doi.org/10.1145/3173574.3174168",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05962",
                        "Citation Paper Title": "Title:Beagle: Automated Extraction and Interpretation of Visualizations from the Web",
                        "Citation Paper Abstract": "Abstract:\"How common is interactive visualization on the web?\" \"What is the most popular visualization design?\" \"How prevalent are pie charts really?\" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 86% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare in practice. Our findings also indicate that users may prefer tools that emphasize a succinct set of visualization types, and provide diverse expert visualization examples.",
                        "Citation Paper Authors": "Authors:Leilani Battle, Peitong Duan, Zachery Miranda, Dana Mukusheva, Remco Chang, Michael Stonebraker"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", integrate V-NLIs to provide better analytic experience for novice users.\nAnother line of research targets the conversational nature of V-NLI. Fast et al. proposed Iris ",
                    "Citation Text": "Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, and Michael S. Bernstein. 2018. Iris: A Conversational Agent for Complex Tasks. In\nProceedings of the CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI \u201918) . Association for Computing Machinery,\nNew York, NY, USA, Article 473, 12 pages. https://doi.org/10.1145/3173574.3174047",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05015",
                        "Citation Paper Title": "Title:Iris: A Conversational Agent for Complex Tasks",
                        "Citation Paper Abstract": "Abstract:Today's conversational agents are restricted to simple standalone commands. In this paper, we present Iris, an agent that draws on human conversational strategies to combine commands, allowing it to perform more complex tasks that it has not been explicitly designed to support: for example, composing one command to \"plot a histogram\" with another to first \"log-transform the data\". To enable this complexity, we introduce a domain specific language that transforms commands into automata that Iris can compose, sequence, and execute dynamically by interacting with a user through natural language, as well as a conversational type system that manages what kinds of commands can be combined. We have designed Iris to help users with data science tasks, a domain that requires support for command combination. In evaluation, we find that data scientists complete a predictive modeling task significantly faster (2.6 times speedup) with Iris than a modern non-conversational programming environment. Iris supports the same kinds of commands as today's agents, but empowers users to weave together these commands to accomplish complex goals.",
                        "Citation Paper Authors": "Authors:Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, Michael Bernstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.03808v2": {
            "Paper Title": "Diverse Group Formation Based on Multiple Demographic Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00182v5": {
            "Paper Title": "Dual Attention Model for Citation Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14806v2": {
            "Paper Title": "TURL: Table Understanding through Representation Learning",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "learned\nembeddings for heterogeneous data in databases and used it for\ndata integration tasks.\nMore recently, there has been a corpus of work incorporating\nknowledge information into pre-trained language models [ 32,49].\nERNIE ",
                    "Citation Text": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019. ERNIE: Enhanced Language Representation with Informative Entities.\nInProceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics . 1441\u20131451.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07129",
                        "Citation Paper Title": "Title:ERNIE: Enhanced Language Representation with Informative Entities",
                        "Citation Paper Abstract": "Abstract:Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from this https URL.",
                        "Citation Paper Authors": "Authors:Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". These entity and relation embeddings are utilized\nby a variety of tasks, such as KB completion [ 5,40], relation ex-\ntraction [ 34,41], entity resolution ",
                    "Citation Text": "Xavier Glorot, Antoine Bordes, Jason Weston, and Yoshua Bengio. 2013. A seman-\ntic matching energy function for learning with multi-relational data. Machine\nLearning 94 (2013), 233\u2013259.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3485",
                        "Citation Paper Title": "Title:A Semantic Matching Energy Function for Learning with Multi-relational Data",
                        "Citation Paper Abstract": "Abstract:Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",
                        "Citation Paper Authors": "Authors:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.01414v1": {
            "Paper Title": "End-to-End QA on COVID-19: Domain Adaptation with Synthetic Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02629v1": {
            "Paper Title": "Linear Regression Evaluation of Search Engine Automatic Search\n  Performance Based on Hadoop and R",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13574v3": {
            "Paper Title": "Unbiased Learning to Rank: Online or Offline?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02292v1": {
            "Paper Title": "FAST: A Fairness Assured Service Recommendation Strategy Considering\n  Service Capacity Constraint",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07467v2": {
            "Paper Title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad\n  Creative Refinement",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "was limited to only text inputs for a brand\n(e.g., the brand\u2019s Wikipedia page), and the recommendation was\nlimited to single words (keywords). In ",
                    "Citation Text": "Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, and\nWei Wang. 2020. Recommending Themes for Ad Creative Design via Visual-\nLinguistic Representations. In Proceedings of The Web Conference 2020 (WWW\n\u201920). 2521\u20132527.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.07194",
                        "Citation Paper Title": "Title:Recommending Themes for Ad Creative Design via Visual-Linguistic Representations",
                        "Citation Paper Abstract": "Abstract:There is a perennial need in the online advertising industry to refresh ad creatives, i.e., images and text used for enticing online users towards a brand. Such refreshes are required to reduce the likelihood of ad fatigue among online users, and to incorporate insights from other successful campaigns in related product categories. Given a brand, to come up with themes for a new ad is a painstaking and time consuming process for creative strategists. Strategists typically draw inspiration from the images and text used for past ad campaigns, as well as world knowledge on the brands. To automatically infer ad themes via such multimodal sources of information in past ad campaigns, we propose a theme (keyphrase) recommender system for ad creative strategists. The theme recommender is based on aggregating results from a visual question answering (VQA) task, which ingests the following: (i) ad images, (ii) text associated with the ads as well as Wikipedia pages on the brands in the ads, and (iii) questions around the ad. We leverage transformer based cross-modality encoders to train visual-linguistic representations for our VQA task. We study two formulations for the VQA task along the lines of classification and ranking; via experiments on a public dataset, we show that cross-modal representations lead to significantly better classification accuracy and ranking precision-recall metrics. Cross-modal representations show better performance compared to separate image and text representations. In addition, the use of multimodal information shows a significant lift over using only textual or visual information.",
                        "Citation Paper Authors": "Authors:Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, Wei Wang"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "and use it to solve task 1as follows.\nWe use an encoder-decoder architecture with attention ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [n.d.]. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In ICLR 2015 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nIn abstractive summarization, where both the source and target se-\nquences are in the same language, an additional mechanism to copy\ninput tokens to the output sequence has proven to be beneficial ",
                    "Citation Text": "Abigail See, Peter J. Liu, and Christopher D. Manning. [n.d.]. Get To The Point:\nSummarization with Pointer-Generator Networks. In ACL 2017 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.00633v1": {
            "Paper Title": "Meta-Embeddings for Natural Language Inference and Semantic Similarity\n  tasks",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". \n \n2.2 SENTENCE EMBEDDINGS  EVALUATION : \n \nWith continuous developments of word and \nsentence representations, there is a need for a \nstandardized  evaluation of representations. \nSentEval ",
                    "Citation Text": "Conneau, A., Kiela, D .: SentEval: An Evaluation Toolkit for Universal \nSentence Representations. In: Calzolari, N. et al . (eds.) LREC. European \nLanguage Resources Association (ELRA) (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05449",
                        "Citation Paper Title": "Title:SentEval: An Evaluation Toolkit for Universal Sentence Representations",
                        "Citation Paper Abstract": "Abstract:We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ", but they are \nusually improved on supervised  transfer tasks such as \nNatural Language Inference ",
                    "Citation Text": "Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large \nannotated corpus for learning natural language inference. In \nProceedings of EMNLP",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.05326",
                        "Citation Paper Title": "Title:A large annotated corpus for learning natural language inference",
                        "Citation Paper Abstract": "Abstract:Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
                        "Citation Paper Authors": "Authors:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.00600v1": {
            "Paper Title": "Extracting Synonyms from Bilingual Dictionaries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00398v1": {
            "Paper Title": "Introducing Inter-Relatedness between Wikipedia Articles in Explicit\n  Semantic Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00318v1": {
            "Paper Title": "Fast Class-wise Updating for Online Hashing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00056v1": {
            "Paper Title": "Diversifying Relevant Phrases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07636v3": {
            "Paper Title": "Open4Business(O4B): An Open Access Dataset for Summarizing Business\n  Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02291v1": {
            "Paper Title": "Cluster Based Deep Contextual Reinforcement Learning for top-k\n  Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14386v1": {
            "Paper Title": "Google Searches and COVID-19 Cases in Saudi Arabia: A Correlation Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14333v1": {
            "Paper Title": "On Disambiguating Authors: Collaboration Network Reconstruction in a\n  Bottom-up Manner",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.05980v3": {
            "Paper Title": "SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02287v1": {
            "Paper Title": "Coarse-to-Fine Memory Matching for Joint Retrieval and Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14280v1": {
            "Paper Title": "A Novel Sentiment Analysis Engine for Preliminary Depression Status\n  Estimation on Social Media",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "further\noptimized on BERT based pretraining to achieve state of the art results in many\nNLP tasks. In a recent study ",
                    "Citation Text": "Ankit Murarka, Balaji Radhakrishnan, and Sushma Ravichandran. Detec-\ntion and Classi\ufb01cation of mental illnesses on social media using RoBERTa .\n2020. arXiv: 2011.11226 [cs.LG] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.11226",
                        "Citation Paper Title": "Title:Detection and Classification of mental illnesses on social media using RoBERTa",
                        "Citation Paper Abstract": "Abstract:Given the current social distancing regulations across the world, social media has become the primary mode of communication for most people. This has resulted in the isolation of many people suffering from mental illnesses who are unable to receive assistance in person. They have increasingly turned to social media to express themselves and to look for guidance in dealing with their illnesses. Keeping this in mind, we propose a solution to detect and classify mental illness posts on social media thereby enabling users to seek appropriate help. In this work, we detect and classify five prominent kinds of mental illnesses: depression, anxiety, bipolar disorder, ADHD and PTSD by analyzing unstructured user data on social media platforms. In addition, we are sharing a new high-quality dataset to drive research on this topic. We believe that our work is the first multi-class model that uses a Transformer-based architecture such as RoBERTa to analyze people's emotions and psychology. We also demonstrate how we stress-test our model using behavioral testing. With this research, we hope to be able to contribute to the public health system by automating some of the detection and classification process.",
                        "Citation Paper Authors": "Authors:Ankit Murarka, Balaji Radhakrishnan, Sushma Ravichandran"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "formulatedthetaskofMajorDepressiveDisorder(MDD)analysis\nas a text classi\ufb01cation problem solved using statistical methods and were able to\nachieve a peak accuracy of 81% on their data. Neural network based classi\ufb01ers\nwere utilized for early stage depression detection in ",
                    "Citation Text": "M. Trotzek, S. Koitka, and C. M. Friedrich. \u201cUtilizing Neural Networks\nand Linguistic Metadata for Early Detection of Depression Indications in\nText Sequences\u201d. In: IEEE Transactions on Knowledge and Data Engi-\nneering32.3 (2020), pp. 588\u2013601. doi:10.1109/TKDE.2018.2885515 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07000",
                        "Citation Paper Title": "Title:Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences",
                        "Citation Paper Abstract": "Abstract:Depression is ranked as the largest contributor to global disability and is also a major reason for suicide. Still, many individuals suffering from forms of depression are not treated for various reasons. Previous studies have shown that depression also has an effect on language usage and that many depressed individuals use social media platforms or the internet in general to get information or discuss their problems. This paper addresses the early detection of depression using machine learning models based on messages on a social platform. In particular, a convolutional neural network based on different word embeddings is evaluated and compared to a classification based on user-level linguistic metadata. An ensemble of both approaches is shown to achieve state-of-the-art results in a current early detection task. Furthermore, the currently popular ERDE score as metric for early detection systems is examined in detail and its drawbacks in the context of shared tasks are illustrated. A slightly modified metric is proposed and compared to the original score. Finally, a new word embedding was trained on a large corpus of the same domain as the described task and is evaluated as well.",
                        "Citation Paper Authors": "Authors:Marcel Trotzek, Sven Koitka, Christoph M. Friedrich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.15211v2": {
            "Paper Title": "NeuralQA: A Usable Library for Question Answering (Contextual Query\n  Expansion + BERT) on Large Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13802v1": {
            "Paper Title": "Post or Tweet: Lessons from a Study of Facebook and Twitter Usage",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13556v1": {
            "Paper Title": "Eco-Routing Using Open Street Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13482v1": {
            "Paper Title": "Towards real-time population estimates: introducing Twitter daily\n  estimates of residents and non-residents at the county level",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13436v1": {
            "Paper Title": "A Hierarchical Self-attentive Convolution Network for Review Modeling in\n  Recommendation Systems",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nParameter settings . We use 300-dimensional pretrained word\nembeddings from Google News ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, J. Dean Distributed\nrepresentations of words and phrases and their compositionality In NIPS\u201913: Pro-\nceedings of the 26th International Conference on Neural Information Processing\nSystems - Volume 2 December 2013 Pages 3111\u20133119",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". The domains and statistics of these four categories are shown\nin the Table 1. For each dataset, We randomly split user-item pairs\ninto80% training set, 10% validation set, and 10% testing set. And\nwe use NLTK ",
                    "Citation Text": "Edward Loper , Steven Bird NLTK: The Natural Language Toolkit (2002) In\nACL\u20192002",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/0205028",
                        "Citation Paper Title": "Title:NLTK: The Natural Language Toolkit",
                        "Citation Paper Abstract": "Abstract:  NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.",
                        "Citation Paper Authors": "Authors:Edward Loper, Steven Bird"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.12684v2": {
            "Paper Title": "Denmark's Participation in the Search Engine TREC COVID-19 Challenge:\n  Lessons Learned about Searching for Precise Biomedical Scientific Information\n  on COVID-19",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ", any use of advanced machine learning that can\ngeneralise to out of sample data requires larger scale training data.\nMoving forward, one future direction of work would be to combine automatic query generation methods ",
                    "Citation Text": "A. Sordoni, Y. Bengio, H. Vahabi, C. Lioma, J. Grue Simonsen, and J.-Y. Nie. A hierarchical re-\ncurrent encoder-decoder for generative context-aware query suggestion. In Proceedings of the 24th\nACM International on Conference on Information and Knowledge Management , CIKM '15, page\n553{562, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450337946.\ndoi: 10.1145/2806416.2806493. URL https://doi.org/10.1145/2806416.2806493 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.02221",
                        "Citation Paper Title": "Title:A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion",
                        "Citation Paper Abstract": "Abstract:Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications.",
                        "Citation Paper Authors": "Authors:Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob G. Simonsen, Jian-Yun Nie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.13253v1": {
            "Paper Title": "Two Stage Transformer Model for COVID-19 Fake News Detection and Fact\n  Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12349v2": {
            "Paper Title": "Improving Clinical Outcome Predictions Using Convolution over Medical\n  Entities with Multimodal Learning",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "is a two-layer neural network that learns the representations of words in the given\ntext with two ways: as a continuous bag-of-words (CBOW) and as a skip-gram. FastText ",
                    "Citation Text": "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\ne\u000ecient text classi\fcation. arXiv preprint arXiv:1607.01759 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.01759",
                        "Citation Paper Title": "Title:Bag of Tricks for Efficient Text Classification",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
                        "Citation Paper Authors": "Authors:Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "is another research that benchmarked\ntheir results on the MIMIC-III. They used multi-task learning approaches to predict four clinical\nprediction tasks such as risk of mortality, LOS, detecting physiologic decline, and phenotype\nclassi\fcation. MIMIC-Extract ",
                    "Citation Text": "Shirly Wang, Matthew BA McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C\nHughes, and Tristan Naumann. Mimic-extract: A data extraction, preprocessing, and repre-\nsentation pipeline for mimic-iii. In Proceedings of the ACM Conference on Health, Inference,\nand Learning , pages 222{235, 2020.\n18",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.08322",
                        "Citation Paper Title": "Title:MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III",
                        "Citation Paper Abstract": "Abstract:Robust machine learning relies on access to data that can be used with standardized frameworks in important tasks and the ability to develop models whose performance can be reasonably reproduced. In machine learning for healthcare, the community faces reproducibility challenges due to a lack of publicly accessible data and a lack of standardized data processing frameworks. We present MIMIC-Extract, an open-source pipeline for transforming raw electronic health record (EHR) data for critical care patients contained in the publicly-available MIMIC-III database into dataframes that are directly usable in common machine learning pipelines. MIMIC-Extract addresses three primary challenges in making complex health records data accessible to the broader machine learning community. First, it provides standardized data processing functions, including unit conversion, outlier detection, and aggregating semantically equivalent features, thus accounting for duplication and reducing missingness. Second, it preserves the time series nature of clinical data and can be easily integrated into clinically actionable prediction tasks in machine learning for health. Finally, it is highly extensible so that other researchers with related questions can easily use the same pipeline. We demonstrate the utility of this pipeline by showcasing several benchmark tasks and baseline results.",
                        "Citation Paper Authors": "Authors:Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Michael C. Hughes, Tristan Naumann, Marzyeh Ghassemi"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "made uni\fed mortality prediction and try to explore\nhow physiological time series data and clinical notes can be integrated. The study by Jin. et al ",
                    "Citation Text": "Mengqi Jin, Mohammad Taha Bahadori, Aaron Colak, Parminder Bhatia, Busra Celikkaya,\nRam Bhakta, Selvan Senthivel, Mohammed Khalilia, Daniel Navarro, Borui Zhang, et al. Im-\nproving hospital mortality prediction with medical named entities and multimodal learning.\narXiv preprint arXiv:1811.12276 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12276",
                        "Citation Paper Title": "Title:Improving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning",
                        "Citation Paper Abstract": "Abstract:Clinical text provides essential information to estimate the acuity of a patient during hospital stays in addition to structured clinical data. In this study, we explore how clinical text can complement a clinical predictive learning task. We leverage an internal medical natural language processing service to perform named entity extraction and negation detection on clinical notes and compose selected entities into a new text corpus to train document representations. We then propose a multimodal neural network to jointly train time series signals and unstructured clinical text representations to predict the in-hospital mortality risk for ICU patients. Our model outperforms the benchmark by 2% AUC.",
                        "Citation Paper Authors": "Authors:Mengqi Jin, Mohammad Taha Bahadori, Aaron Colak, Parminder Bhatia, Busra Celikkaya, Ram Bhakta, Selvan Senthivel, Mohammed Khalilia, Daniel Navarro, Borui Zhang, Tiberiu Doman, Arun Ravi, Matthieu Liger, Taha Kass-hout"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ". This approach has shown success in image captioning tasks ",
                    "Citation Text": "Andrej Karpathy, Armand Joulin, and Li F Fei-Fei. Deep fragment embeddings for bidi-\nrectional image sentence mapping. In Advances in neural information processing systems ,\npages 1889{1897, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.5679",
                        "Citation Paper Title": "Title:Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
                        "Citation Paper Abstract": "Abstract:We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.",
                        "Citation Paper Authors": "Authors:Andrej Karpathy, Armand Joulin, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "compared di\u000berent embedding\napproaches such as Bag of Words (BoW), Word2Vec and LSTM on clinical note representation\nby evaluating the prediction performance on diagnosis prediction and mortality risk estimation.\nMore recently, transformer-based architectures such as BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "extracts 17 features from the MIMIC-III and works on hospital\nmortality, LOS and ICD-9 code group predictions. They compared their proposed super learner\nmethod with feedforward and recurrent neural network. ",
                    "Citation Text": "Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan.\nMultitask learning and benchmarking with clinical time series data. Scienti\fc data , 6(1):1{\n18, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.07771",
                        "Citation Paper Title": "Title:Multitask learning and benchmarking with clinical time series data",
                        "Citation Paper Abstract": "Abstract:Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.",
                        "Citation Paper Authors": "Authors:Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, Greg Ver Steeg, Aram Galstyan"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "used Convolutional\nneural network to predict long-term mortality risk on the MIMIC-III dataset. More recent work ",
                    "Citation Text": "Huan Song, Deepta Rajan, Jayaraman J Thiagarajan, and Andreas Spanias. Attend and\ndiagnose: Clinical time series analysis using attention models. In Thirty-second AAAI con-\nference on arti\fcial intelligence , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03905",
                        "Citation Paper Title": "Title:Attend and Diagnose: Clinical Time Series Analysis using Attention Models",
                        "Citation Paper Abstract": "Abstract:With widespread adoption of electronic health records, there is an increased emphasis for predictive models that can effectively deal with clinical time-series data. Powered by Recurrent Neural Network (RNN) architectures with Long Short-Term Memory (LSTM) units, deep neural networks have achieved state-of-the-art results in several clinical prediction tasks. Despite the success of RNNs, its sequential nature prohibits parallelized computing, thus making it inefficient particularly when processing long sequences. Recently, architectures which are based solely on attention mechanisms have shown remarkable success in transduction tasks in NLP, while being computationally superior. In this paper, for the first time, we utilize attention models for clinical time-series modeling, thereby dispensing recurrence entirely. We develop the \\textit{SAnD} (Simply Attend and Diagnose) architecture, which employs a masked, self-attention mechanism, and uses positional encoding and dense interpolation strategies for incorporating temporal order. Furthermore, we develop a multi-task variant of \\textit{SAnD} to jointly infer models with multiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we demonstrate that the proposed approach achieves state-of-the-art performance in all tasks, outperforming LSTM models and classical baselines with hand-engineered features.",
                        "Citation Paper Authors": "Authors:Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "used 13 di\u000berent vital measurements to classify\n128 diagnoses using Long Short Term Memory (LSTM) and DoctorAI ",
                    "Citation Text": "Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng\nSun. Doctor ai: Predicting clinical events via recurrent neural networks. In Machine Learning\nfor Healthcare Conference , pages 301{318, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05942",
                        "Citation Paper Title": "Title:Doctor AI: Predicting Clinical Events via Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.",
                        "Citation Paper Authors": "Authors:Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "With the rapid development of deep learning algorithms in the last decade, the number of deep\nlearning models increased substantially for various clinical predictions. Several studies have\nexplored EHRs to solve clinical problems, e.g., ",
                    "Citation Text": "Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose\nwith lstm recurrent neural networks. arXiv preprint arXiv:1511.03677 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.03677",
                        "Citation Paper Title": "Title:Learning to Diagnose with LSTM Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.",
                        "Citation Paper Authors": "Authors:Zachary C. Lipton, David C. Kale, Charles Elkan, Randall Wetzel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.12620v1": {
            "Paper Title": "An Integration of UTAUT and Task-Technology Fit Frameworks for Assessing\n  the Acceptance of Clinical Decision Support Systems in the Context of a\n  Developing Country",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12586v1": {
            "Paper Title": "RRCN: A Reinforced Random Convolutional Network based Reciprocal\n  Recommendation Approach for Online Dating",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09131v2": {
            "Paper Title": "OpenTapioca: Lightweight Entity Linking for Wikidata",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11970v1": {
            "Paper Title": "A Novel Multimodal Music Genre Classifier using Hierarchical Attention\n  and Convolutional Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11938v1": {
            "Paper Title": "DADNN: Multi-Scene CTR Prediction via Domain-Aware Deep Neural Network",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", which are exploited for CTR prediction. Representative\nmodels include Wide&Deep ",
                    "Citation Text": "H. T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\nG. Anderson, G. Corrado, W. Chai, and M. a. Ispir, \u201cWide & deep\nlearning for recommender systems,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07792",
                        "Citation Paper Title": "Title:Wide & Deep Learning for Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
                        "Citation Paper Authors": "Authors:Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.10216v2": {
            "Paper Title": "Sequential Targeting: an incremental learning approach for data\n  imbalance in text classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02066v2": {
            "Paper Title": "University of Washington at TREC 2020 Fairness Ranking Track",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10358v1": {
            "Paper Title": "A Deep Language-independent Network to analyze the impact of COVID-19 on\n  the World via Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10337v1": {
            "Paper Title": "Finding Prerequisite Relations between Concepts using Textbook",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10187v1": {
            "Paper Title": "A Survey on Deep Learning Based Point-Of-Interest (POI) Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": "did\nan experimental evaluation of some of the then state-of-the -\nart traditional POI recommendation models. In another work ,\nZhao et al. ",
                    "Citation Text": "S. Zhao, I. King, M. R. Lyu, A survey of point-of-interes t recommenda-\ntion in location-based social networks, arXiv preprint arX iv:1607.00647\n(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00647",
                        "Citation Paper Title": "Title:A Survey of Point-of-interest Recommendation in Location-based Social Networks",
                        "Citation Paper Abstract": "Abstract:Point-of-interest (POI) recommendation that suggests new places for users to visit arises with the popularity of location-based social networks (LBSNs). Due to the importance of POI recommendation in LBSNs, it has attracted much academic and industrial interest. In this paper, we offer a systematic review of this field, summarizing the contributions of individual efforts and exploring their relations. We discuss the new properties and challenges in POI recommendation, compared with traditional recommendation problems, e.g., movie recommendation. Then, we present a comprehensive review in three aspects: influential factors for POI recommendation, methodologies employed for POI recommendation, and different tasks in POI recommendation. Specifically, we propose three taxonomies to classify POI recommendation systems. First, we categorize the systems by the influential factors check-in characteristics, including the geographical information, social relationship, temporal influence, and content indications. Second, we categorize the systems by the methodology, including systems modeled by fused methods and joint methods. Third, we categorize the systems as general POI recommendation and successive POI recommendation by subtle differences in the recommendation task whether to be bias to the recent check-in. For each category, we summarize the contributions and system features, and highlight the representative work. Moreover, we discuss the available data sets and the popular metrics. Finally, we point out the possible future directions in this area and conclude this survey.",
                        "Citation Paper Authors": "Authors:Shenglin Zhao, Irwin King, Michael R. Lyu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.10106v1": {
            "Paper Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12316v7": {
            "Paper Title": "Towards Persona-Based Empathetic Conversational Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09752v1": {
            "Paper Title": "From Protocol to Screening: A Hybrid Learning Approach for\n  Technology-Assisted Systematic Literature Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09580v1": {
            "Paper Title": "Non-Linear Multiple Field Interactions Neural Document Ranking",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ".\n2.2 Neural Networks forRanking\nMitraetal.haveshownthatnosigni\ufb01cantlossisobservedin mod-\nels thatincorporatethequeryterm independence assumptio n(or-\nder of words does not matter and relevance is only measured if a\nword is in thequery)in web search ",
                    "Citation Text": "Bhaskar Mitra, Corby Rosset, David Hawking, Nick Crasw ell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating query term independe nce assumption\nfor e\ufb03cient retrieval and ranking using deep neural network s.arXiv preprint\narXiv:1907.03693 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03693",
                        "Citation Paper Title": "Title:Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Classical information retrieval (IR) methods, such as query likelihood and BM25, score documents independently w.r.t. each query term, and then accumulate the scores. Assuming query term independence allows precomputing term-document scores using these models---which can be combined with specialized data structures, such as inverted index, for efficient retrieval. Deep neural IR models, in contrast, compare the whole query to the document and are, therefore, typically employed only for late stage re-ranking. We incorporate query term independence assumption into three state-of-the-art neural IR models: BERT, Duet, and CKNRM---and evaluate their performance on a passage ranking task. Surprisingly, we observe no significant loss in result quality for Duet and CKNRM---and a small degradation in the case of BERT. However, by operating on each query term independently, these otherwise computationally intensive models become amenable to offline precomputation---dramatically reducing the cost of query evaluations employing state-of-the-art neural ranking models. This strategy makes it practical to use deep models for retrieval from large collections---and not restrict their usage to late stage re-ranking.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz, Emine Yilmaz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.06807v2": {
            "Paper Title": "Heterogeneous Graph Collaborative Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08598v2": {
            "Paper Title": "Sequential Recommender via Time-aware Attentive Memory Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08606v1": {
            "Paper Title": "Optimizing Offer Sets in Sub-Linear Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14848v2": {
            "Paper Title": "Flexible retrieval with NMSLIB and FlexNeuART",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07680v2": {
            "Paper Title": "An Annotated Corpus of Webtables for Information Extraction Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00357v5": {
            "Paper Title": "Beneath the Tip of the Iceberg: Current Challenges and New Directions in\n  Sentiment Analysis Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12674v3": {
            "Paper Title": "Exploring task-based query expansion at the TREC-COVID track",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07783v1": {
            "Paper Title": "User-based Network Embedding for Collective Opinion Spammer Detection",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "iteratively calculate three quality metrics for spammer detec-\ntion. Wang et al. ",
                    "Citation Text": "Z. Wang, R. Hu, Q. Chen, P. Gao, X. Xu, Collueagle: collusive review\nspammer detection using markov random \felds, Data Mining and Knowl-\nedge Discovery 34 (6) (2020) 1621{1641.\n28",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01690",
                        "Citation Paper Title": "Title:ColluEagle: Collusive review spammer detection using Markov random fields",
                        "Citation Paper Abstract": "Abstract:Product reviews are extremely valuable for online shoppers in providing purchase decisions. Driven by immense profit incentives, fraudsters deliberately fabricate untruthful reviews to distort the reputation of online products. As online reviews become more and more important, group spamming, i.e., a team of fraudsters working collaboratively to attack a set of target products, becomes a new fashion. Previous works use review network effects, i.e. the relationships among reviewers, reviews, and products, to detect fake reviews or review spammers, but ignore time effects, which are critical in characterizing group spamming. In this paper, we propose a novel Markov random field (MRF)-based method (ColluEagle) to detect collusive review spammers, as well as review spam campaigns, considering both network effects and time effects. First we identify co-review pairs, a review phenomenon that happens between two reviewers who review a common product in a similar way, and then model reviewers and their co-review pairs as a pairwise-MRF, and use loopy belief propagation to evaluate the suspiciousness of reviewers. We further design a high quality yet easy-to-compute node prior for ColluEagle, through which the review spammer groups can also be subsequently identified. Experiments show that ColluEagle can not only detect collusive spammers with high precision, significantly outperforming state-of-the-art baselines --- FraudEagle and SpEagle, but also identify highly suspicious review spammer campaigns.",
                        "Citation Paper Authors": "Authors:Zhuo Wang, Runlong Hu, Qian Chen, Pei Gao, Xiaowei Xu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "learn the user embeddings generated\nby tensor decomposition for training a spam review classi\fer. Liu et al. ",
                    "Citation Text": "S. Liu, B. Hooi, C. Faloutsos, Holoscope: Topology-and-spike aware fraud\ndetection, in: Proceedings of the 2017 ACM on Conference on Information\nand Knowledge Management, 2017, pp. 1539{1548.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02505",
                        "Citation Paper Title": "Title:HoloScope: Topology-and-Spike Aware Fraud Detection",
                        "Citation Paper Abstract": "Abstract:As online fraudsters invest more resources, including purchasing large pools of fake user accounts and dedicated IPs, fraudulent attacks become less obvious and their detection becomes increasingly challenging. Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary, resulting in lower accuracy and increased need for manual verification. Hence, we propose HoloScope, which uses information from graph topology and temporal spikes to more accurately detect groups of fraudulent users. In terms of graph topology, we introduce \"contrast suspiciousness,\" a dynamic weighting approach, which allows us to more accurately detect fraudulent blocks, particularly low-density blocks. In terms of temporal spikes, HoloScope takes into account the sudden bursts and drops of fraudsters' attacking patterns. In addition, we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks. Additionally, from the perspective of ratings, HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately. Moreover, HoloScope has a concise framework and sub-quadratic time complexity, making the algorithm reproducible and scalable. Extensive experiments showed that HoloScope achieved significant accuracy improvements on synthetic and real data, compared with state-of-the-art fraud detection methods.",
                        "Citation Paper Authors": "Authors:Shenghua Liu, Bryan Hooi, Christos Faloutsos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.07739v1": {
            "Paper Title": "CoSam: An Efficient Collaborative Adaptive Sampler for Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07546v1": {
            "Paper Title": "Learning Frame Similarity using Siamese networks for Audio-to-Score\n  Alignment",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ", and application of Siamese Neural\nNetworks for content-based audio retrieval ",
                    "Citation Text": "Pranay Manocha, Rohan Badlani, Anurag Kumar, Ankit Shah, Benjamin\nElizalde, and Bhiksha Raj, \u201cContent-based representations of audio\nusing siamese neural networks,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2018, pp. 3136\u2013\n3140.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10974",
                        "Citation Paper Title": "Title:Content-based Representations of audio using Siamese neural networks",
                        "Citation Paper Abstract": "Abstract:In this paper, we focus on the problem of content-based retrieval for audio, which aims to retrieve all semantically similar audio recordings for a given audio clip query. This problem is similar to the problem of query by example of audio, which aims to retrieve media samples from a database, which are similar to the user-provided example. We propose a novel approach which encodes the audio into a vector representation using Siamese Neural Networks. The goal is to obtain an encoding similar for files belonging to the same audio class, thus allowing retrieval of semantically similar audio. Using simple similarity measures such as those based on simple euclidean distance and cosine similarity we show that these representations can be very effectively used for retrieving recordings similar in audio content.",
                        "Citation Paper Authors": "Authors:Pranay Manocha, Rohan Badlani, Anurag Kumar, Ankit Shah, Benjamin Elizalde, Bhiksha Raj"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", however these\nfeatures while being robust to transposition, are sensitive to\nlarge tempo variations and underperform in such situations. ",
                    "Citation Text": "Matthias Dorfer, Florian Henkel, and Gerhard Widmer, \u201cLearning to\nlisten, read, and follow: Score following as a reinforcement learning\ngame,\u201d in International Society for Music Information Retrieval , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06391",
                        "Citation Paper Title": "Title:Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game",
                        "Citation Paper Abstract": "Abstract:Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images.",
                        "Citation Paper Authors": "Authors:Matthias Dorfer, Florian Henkel, Gerhard Widmer"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", whereas recent\nwork in this direction is moving towards the usage of deep\nneural networks ",
                    "Citation Text": "John Thickstun, Zaid Harchaoui, and Sham Kakade, \u201cLearning features\nof music from scratch,\u201d arXiv preprint arXiv:1611.09827 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.09827",
                        "Citation Paper Title": "Title:Learning Features of Music from Scratch",
                        "Citation Paper Abstract": "Abstract:This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions.\nThe paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.",
                        "Citation Paper Authors": "Authors:John Thickstun, Zaid Harchaoui, Sham Kakade"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.02594v1": {
            "Paper Title": "To Schedule or not to Schedule: Extracting Task Specific Temporal\n  Entities and Associated Negation Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07363v1": {
            "Paper Title": "RecTen: A Recursive Hierarchical Low Rank Tensor Factorization Method to\n  Discover Hierarchical Patterns in Multi-modal Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07359v1": {
            "Paper Title": "Analyzing 'Near Me' Services: Potential for Exposure Bias in\n  Location-based Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ".\nThe study by Dwork et al. introduces the notions of individ-\nual and group fairness ",
                    "Citation Text": "C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, \u201cFairness\nthrough awareness,\u201d in ACM ITCS , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1104.3913",
                        "Citation Paper Title": "Title:Fairness Through Awareness",
                        "Citation Paper Abstract": "Abstract:We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.",
                        "Citation Paper Authors": "Authors:Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Rich Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.07307v1": {
            "Paper Title": "Meaningful Answer Generation of E-Commerce Question-Answering",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "to update the parameters with a learning\nrate of 0.1and training batch size of 64. Our model is implemented via TensorFlow ",
                    "Citation Text": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael\nIsard, et al. 2016. Tensorflow: a system for large-scale machine learning.. In OSDI , Vol. 16. 265\u2013283.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "proposed an adversarial learning-based\nmodel which combines product attributes and review information to generate an answer for a given question. Chen\net al. ",
                    "Citation Text": "Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019. Review-Driven Answer Generation for Product-Related Questions in\nE-Commerce. In WSDM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01994",
                        "Citation Paper Title": "Title:Review-Driven Answer Generation for Product-Related Questions in E-Commerce",
                        "Citation Paper Abstract": "Abstract:The users often have many product-related questions before they make a purchase decision in E-commerce. However, it is often time-consuming to examine each user review to identify the desired information. In this paper, we propose a novel review-driven framework for answer generation for product-related questions in E-commerce, named RAGE. We develope RAGE on the basis of the multi-layer convolutional architecture to facilitate speed-up of answer generation with the parallel computation. For each question, RAGE first extracts the relevant review snippets from the reviews of the corresponding product. Then, we devise a mechanism to identify the relevant information from the noise-prone review snippets and incorporate this information to guide the answer generation. The experiments on two real-world E-Commerce datasets show that the proposed RAGE significantly outperforms the existing alternatives in producing more accurate and informative answers in natural language. Moreover, RAGE takes much less time for both model training and answer generation than the existing RNN based generation models.",
                        "Citation Paper Authors": "Authors:Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, Haiqing Chen"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "were the first to propose the\nprototype editing model, where a prototype sentence is sampled from the training data and edited into a new sentence.\nSubsequently, Wu et al . ",
                    "Citation Text": "Yu Ping Wu, Furu Wei, Shaohan Huang, Zhoujun Li, and Ming Zhou. 2018. Response Generation by Context-aware Prototype Editing. CoRR\nabs/1806.07042 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.07042",
                        "Citation Paper Title": "Title:Response Generation by Context-aware Prototype Editing",
                        "Citation Paper Abstract": "Abstract:Open domain response generation has achieved remarkable progress in recent years, but sometimes yields short and uninformative responses. We propose a new paradigm for response generation, that is response generation by editing, which significantly increases the diversity and informativeness of the generation results. Our assumption is that a plausible response can be generated by slightly revising an existing response prototype. The prototype is retrieved from a pre-defined index and provides a good start-point for generation because it is grammatical and informative. We design a response editing model, where an edit vector is formed by considering differences between a prototype context and a current context, and then the edit vector is fed to a decoder to revise the prototype response for the current context. Experiment results on a large scale dataset demonstrate that the response editing model outperforms generative and retrieval-based models on various aspects.",
                        "Citation Paper Authors": "Authors:Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "(including\nEmbedding Average, Embedding Greedy and Embedding Extreme) to compute their semantic similarity. Besides, to\nquantitatively evaluate the safe answer problem, we use the distinct metric ",
                    "Citation Text": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B. Dolan. 2016. A Diversity-Promoting Objective Function for Neural Conversation\nModels. In HLT-NAACL .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.03055",
                        "Citation Paper Title": "Title:A Diversity-Promoting Objective Function for Neural Conversation Models",
                        "Citation Paper Abstract": "Abstract:Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "apply the seq2seq mechanism with attention model to text summarization\nfield. Then See et al. ",
                    "Citation Text": "Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "achieves the state-of-the-art performance in reading comprehension. Cui et al . ",
                    "Citation Text": "Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-over-Attention Neural Networks for Reading\nComprehension. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04423",
                        "Citation Paper Title": "Title:Attention-over-Attention Neural Networks for Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.",
                        "Citation Paper Authors": "Authors:Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, Guoping Hu"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "propose a model to match the question with passage using gated attention-based recurrent networks to\nobtain the question-aware passage representation. Consisting exclusively of convolution and self-attention, QANet ",
                    "Citation Text": "Adams Wei Yu, David Dohan, Quoc Le, Thang Luong, Rui Zhao, and Kai Chen. 2018. Fast and Accurate Reading Comprehension by Combining\nSelf-Attention and Convolution. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09541",
                        "Citation Paper Title": "Title:QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",
                        "Citation Paper Authors": "Authors:Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ", many approaches have been proposed [ 7,14,33,43,60].\nSeoet al. ",
                    "Citation Text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. 2017. Bi-directional attention flow for machine comprehension. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01603",
                        "Citation Paper Title": "Title:Bidirectional Attention Flow for Machine Comprehension",
                        "Citation Paper Abstract": "Abstract:Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
                        "Citation Paper Authors": "Authors:Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "proposed a machine learning model, namely differentiable\nneural computer, which consists of a neural network that can read from and write to an external memory matrix,\nanalogous to the random-access memory in a conventional computer. Subsequently, Le et al . ",
                    "Citation Text": "Hung Le, Truyen Tran, and Svetha Venkatesh. 2019. Learning to Remember More with Less Memorization. arXiv preprint arXiv:1901.01347 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.01347",
                        "Citation Paper Title": "Title:Learning to Remember More with Less Memorization",
                        "Citation Paper Abstract": "Abstract:Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks.",
                        "Citation Paper Authors": "Authors:Hung Le, Truyen Tran, Svetha Venkatesh"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed several improvements to memory and input modules and apply them to\nvisual question-answering. Instead of extractive fact-finding QA, Bauer et al . ",
                    "Citation Text": "Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for Generative Multi-Hop Question Answering Tasks. In EMNLP . 4220\u20134230.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.06309",
                        "Citation Paper Title": "Title:Commonsense for Generative Multi-Hop Question Answering Tasks",
                        "Citation Paper Abstract": "Abstract:Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model's performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo-WikiHop, another multi-hop reasoning dataset.",
                        "Citation Paper Authors": "Authors:Lisa Bauer, Yicheng Wang, Mohit Bansal"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": "proposed a dynamic memory network\nwhere questions trigger an iterative attention process to condition the model\u2019s attention on inputs and the result of\nprevious iterations. Xiong et al . ",
                    "Citation Text": "Caiming Xiong, Stephen Merity, and Richard Socher. 2016. Dynamic memory networks for visual and textual question answering. In ICML .\n2397\u20132406.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01417",
                        "Citation Paper Title": "Title:Dynamic Memory Networks for Visual and Textual Question Answering",
                        "Citation Paper Abstract": "Abstract:Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.",
                        "Citation Paper Authors": "Authors:Caiming Xiong, Stephen Merity, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "have encouraged the development of many advanced, high performing attention-based\nneural models. The ability of reasoning is an important research ingredient in question-answering [ 59,65,74]. Weston\net al. ",
                    "Citation Text": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. 2015. Towards\nai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05698",
                        "Citation Paper Title": "Title:Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
                        "Citation Paper Abstract": "Abstract:One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.",
                        "Citation Paper Authors": "Authors:Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merri\u00ebnboer, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "to predict the answer. However, it can\nonly return simple answers, such as \u201cyes\u201d or \u201cno\u201d. Unfortunately, more often than not there is no proper review that\ncan be used as an answer. Gupta et al . ",
                    "Citation Text": "Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C. Lipton. 2019. AmazonQA: A Review-Based Question\nAnswering Task. ArXiv abs/1908.04364 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.04364",
                        "Citation Paper Title": "Title:AmazonQA: A Review-Based Question Answering Task",
                        "Citation Paper Abstract": "Abstract:Every day, thousands of customers post questions on Amazon product pages. After some time, if they are fortunate, a knowledgeable customer might answer their question. Observing that many questions can be answered based upon the available product reviews, we propose the task of review-based QA. Given a corpus of reviews and a question, the QA system synthesizes an answer. To this end, we introduce a new dataset and propose a method that combines information retrieval techniques for selecting relevant reviews (given a question) and \"reading comprehension\" models for synthesizing an answer (given a question and review). Our dataset consists of 923k questions, 3.6M answers and 14M reviews across 156k products. Building on the well-known Amazon dataset, we collect additional annotations, marking each question as either answerable or unanswerable based on the available reviews. A deployed system could first classify a question as answerable and then attempt to generate an answer. Notably, unlike many popular QA datasets, here, the questions, passages, and answers are all extracted from real human interactions. We evaluate numerous models for answer generation and propose strong baselines, demonstrating the challenging nature of this new task.",
                        "Citation Paper Authors": "Authors:Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, Zachary C Lipton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.08431v1": {
            "Paper Title": "Association Rules Enhanced Knowledge Graph Attention Network",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed a joint model which\njointly encodes the rules into the embedding. In ",
                    "Citation Text": "P. Wang, D. Dou, F. Wu, N. de Silva, and L. Jin,\n\\Logic rules powered knowledge graph embedding,\"\narXiv preprint arXiv:1903.03772 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.03772",
                        "Citation Paper Title": "Title:Logic Rules Powered Knowledge Graph Embedding",
                        "Citation Paper Abstract": "Abstract:Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).",
                        "Citation Paper Authors": "Authors:Pengwei Wang, Dejing Dou, Fangzhao Wu, Nisansa de Silva, Lianwen Jin"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "aim at embedding nodes for node classi\fcation\ngiven the entire graph and thus are inapplicable for\ninductive KG-speci\fc tasks. A graph based neural\nnetwork model called R-GCN ",
                    "Citation Text": "M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van\nDen Berg, I. Titov, and M. Welling, \\Modeling\nrelational data with graph convolutional networks,\" in\nEuropean Semantic Web Conference . Springer, 2018,\npp. 593{607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06103",
                        "Citation Paper Title": "Title:Modeling Relational Data with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
                        "Citation Paper Authors": "Authors:Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "proposed ConvE|the \frst model\napplying CNN for the KB completion task. ConvE\nuses stacked 2D convolutional \flters on reshaping of\nentity and relation representations, thus increasing their\nexpressive power, while remaining parameter e\u000ecient\nat the same time. ConvKB ",
                    "Citation Text": "D. Q. Nguyen, T. D. Nguyen, D. Q. Nguyen,\nand D. Phung, \\A novel embedding model for\nknowledge base completion based on convolutional\nneural network,\" arXiv preprint arXiv:1712.02121 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02121",
                        "Citation Paper Title": "Title:A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vector representing the input triple. The feature vector is multiplied with a weight vector via a dot product to return a score. This score is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.",
                        "Citation Paper Authors": "Authors:Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh Phung"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "use a tri-linear\ndot product to compute the score for each triple.\nRecent research has shown that using relation paths\nbetween entities in the KBs could help to get contextual\ninformation for improving KB completion performance\n( ",
                    "Citation Text": "X. V. Lin, R. Socher, and C. Xiong, \\Multi-hop\nknowledge graph reasoning with reward shaping,\"\narXiv preprint arXiv:1808.10568 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10568",
                        "Citation Paper Title": "Title:Multi-Hop Knowledge Graph Reasoning with Reward Shaping",
                        "Citation Paper Abstract": "Abstract:Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",
                        "Citation Paper Authors": "Authors:Xi Victoria Lin, Richard Socher, Caiming Xiong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.07226v1": {
            "Paper Title": "TenFor: A Tensor-Based Tool to Extract Interesting Events from Security\n  Forums",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07222v1": {
            "Paper Title": "HackerScope: The Dynamics of a Massive Hacker Online Ecosystem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07208v1": {
            "Paper Title": "Utilizing Bidirectional Encoder Representations from Transformers for\n  Answer Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01113v2": {
            "Paper Title": "Short Text Classification Approach to Identify Child Sexual Exploitation\n  Material",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "proposed a framework to identify cyberbullying on Twitter. For text rep-\nresentation, they compared pre-trained language models, like Word2Vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed representa-\ntions of words and phrases and their compositionality, in: Advances in neural\ninformation processing systems, 2013, pp. 3111\u20133119.\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.06490v1": {
            "Paper Title": "Content-based Image Retrieval and the Semantic Gap in the Deep Learning\n  Era",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12612v2": {
            "Paper Title": "A multi-layer approach to disinformation detection on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05928v1": {
            "Paper Title": "J-Recs: Principled and Scalable Recommendation Justification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02084v2": {
            "Paper Title": "Understanding Capacity-Driven Scale-Out Neural Recommendation Inference",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ".\nTechniques speci\ufb01c to large embedding tables have tradi-\ntionally targeted the intuitive characteristics of pre-trained\nword embeddings, not the sparse user- and content-features\nof deep recommendation systems ",
                    "Citation Text": "T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of\nword representations in vector space,\u201d 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.02297v1": {
            "Paper Title": "Active Learning from Crowd in Document Screening",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05742v1": {
            "Paper Title": "Learning User Representations with Hypercuboids for Recommender Systems",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "proposed a rough hypercuboid method for\nrelevant and significant features selection. Vilnis et al. ",
                    "Citation Text": "Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. 2018. Probabilistic\nEmbedding of Knowledge Graphs with Box Lattice Measures. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia,\n263\u2013272. https://doi.org/10.18653/v1/P18-1025",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06627",
                        "Citation Paper Title": "Title:Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures",
                        "Citation Paper Abstract": "Abstract:Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.",
                        "Citation Paper Authors": "Authors:Luke Vilnis, Xiang Li, Shikhar Murty, Andrew McCallum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.05546v1": {
            "Paper Title": "E-commerce Query-based Generation based on User Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05119v1": {
            "Paper Title": "TRSM-RS: A Movie Recommender System Based on Users' Gender and New\n  Weighted Similarity Measure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03796v2": {
            "Paper Title": "Testing the Impact of Semantics and Structure on Recommendation Accuracy\n  and Diversity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05061v1": {
            "Paper Title": "Alleviating Cold-Start Problems in Recommendation through\n  Pseudo-Labelling over Knowledge Graph",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ", these methods leverage unobserved samples only as\nnegative instances during training. Wang et al. proposed a nega-\ntive sampling strategy based on reinforcement learning ",
                    "Citation Text": "Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang, and Tat-Seng\nChua. 2020. Reinforced Negative Sampling over Knowledge Graph for Recom-\nmendation. In Proceedings of The Web Conference 2020 . 99\u2013109.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05753",
                        "Citation Paper Title": "Title:Reinforced Negative Sampling over Knowledge Graph for Recommendation",
                        "Citation Paper Abstract": "Abstract:Properly handling missing data is a fundamental challenge in recommendation. Most present works perform negative sampling from unobserved data to supply the training of recommender models with negative signals. Nevertheless, existing negative sampling strategies, either static or adaptive ones, are insufficient to yield high-quality negative samples --- both informative to model training and reflective of user real needs. In this work, we hypothesize that item knowledge graph (KG), which provides rich relations among items and KG entities, could be useful to infer informative and factual negative samples. Towards this end, we develop a new negative sampling model, Knowledge Graph Policy Network (KGPolicy), which works as a reinforcement learning agent to explore high-quality negatives. Specifically, by conducting our designed exploration operations, it navigates from the target positive interaction, adaptively receives knowledge-aware negative signals, and ultimately yields a potential negative item to train the recommender. We tested on a matrix factorization (MF) model equipped with KGPolicy, and it achieves significant improvements over both state-of-the-art sampling methods like DNS and IRGAN, and KG-enhanced recommender models like KGAT. Further analyses from different angles provide insights of knowledge-aware sampling. We release the codes and datasets at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2005.10322v2": {
            "Paper Title": "A survey on Adversarial Recommender Systems: from Attack/Defense\n  strategies to Generative Adversarial Networks",
            "Sentences": [
                {
                    "Sentence ID": 92,
                    "Sentence": ", for instance based\non the user-centered utility and the vendor-centered utility (e.g., profitability). In the literature, a\nfew research works have exploited the adversarial training procedure to reduce the biased/unfair\nimpact of recommendations.\n[DPR ] Inspired by ",
                    "Citation Text": "Gilles Louppe, Michael Kagan, and Kyle Cranmer. 2017. Learning to Pivot with Adversarial Networks. In Advances\nin Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 981\u2013990.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01046",
                        "Citation Paper Title": "Title:Learning to Pivot with Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.",
                        "Citation Paper Authors": "Authors:Gilles Louppe, Michael Kagan, Kyle Cranmer"
                    }
                },
                {
                    "Sentence ID": 147,
                    "Sentence": "Zhu et al. 2020 \u2713\u2713 \u2713\u2713 \u2713 \u2713 \u2713\u2713\nFAN ",
                    "Citation Text": "Chuhan Wu, Fangzhao Wu, Xiting Wang, Yongfeng Huang, and Xing Xie. 2020. Fairness-aware News Recommendation\nwith Decomposed Adversarial Learning. arXiv (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16742",
                        "Citation Paper Title": "Title:FairRec: Fairness-aware News Recommendation with Decomposed Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:News recommendation is important for online news services. Existing news recommendation models are usually learned from users' news click behaviors. Usually the behaviors of users with the same sensitive attributes (e.g., genders) have similar patterns and news recommendation models can easily capture these patterns. It may lead to some biases related to sensitive user attributes in the recommendation results, e.g., always recommending sports news to male users, which is unfair since users may not receive diverse news information. In this paper, we propose a fairness-aware news recommendation approach with decomposed adversarial learning and orthogonality regularization, which can alleviate unfairness in news recommendation brought by the biases of sensitive user attributes. In our approach, we propose to decompose the user interest model into two components. One component aims to learn a bias-aware user embedding that captures the bias information on sensitive user attributes, and the other aims to learn a bias-free user embedding that only encodes attribute-independent user interest information for fairness-aware news recommendation. In addition, we propose to apply an attribute prediction task to the bias-aware user embedding to enhance its ability on bias modeling, and we apply adversarial learning to the bias-free user embedding to remove the bias information from it. Moreover, we propose an orthogonality regularization method to encourage the bias-free user embeddings to be orthogonal to the bias-aware one to better distinguish the bias-free user embedding from the bias-aware one. For fairness-aware news ranking, we only use the bias-free user embedding. Extensive experiments on benchmark dataset show that our approach can effectively improve fairness in news recommendation with minor performance loss.",
                        "Citation Paper Authors": "Authors:Chuhan Wu, Fangzhao Wu, Xiting Wang, Yongfeng Huang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Resheff et al. 2019 \u2713 \u2713 \u2713 \u2713 \u2713\nRAP ",
                    "Citation Text": "Ghazaleh Beigi, Ahmadreza Mosallanezhad, Ruocheng Guo, Hamidreza Alvari, Alexander Nou, and Huan Liu. 2020.\nPrivacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning. In WSDM \u201920: The\nThirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020 . 34\u201342.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.09872",
                        "Citation Paper Title": "Title:Privacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:Recommendation is one of the critical applications that helps users find information relevant to their interests. However, a malicious attacker can infer users' private information via recommendations. Prior work obfuscates user-item data before sharing it with recommendation system. This approach does not explicitly address the quality of recommendation while performing data obfuscation. Moreover, it cannot protect users against private-attribute inference attacks based on recommendations. This work is the first attempt to build a Recommendation with Attribute Protection (RAP) model which simultaneously recommends relevant items and counters private-attribute inference attacks. The key idea of our approach is to formulate this problem as an adversarial learning problem with two main components: the private attribute inference attacker, and the Bayesian personalized recommender. The attacker seeks to infer users' private-attribute information according to their items list and recommendations. The recommender aims to extract users' interests while employing the attacker to regularize the recommendation process. Experiments show that the proposed model both preserves the quality of recommendation service and protects users against private-attribute inference attacks.",
                        "Citation Paper Authors": "Authors:Ghazaleh Beigi, Ahmadreza Mosallanezhad, Ruocheng Guo, Hamidreza Alvari, Alexander Nou, Huan Liu"
                    }
                },
                {
                    "Sentence ID": 113,
                    "Sentence": "Lin et al. 2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nPrivacy\nPAT ",
                    "Citation Text": "Yehezkel S. Resheff, Yanai Elazar, Moni Shahar, and Oren Sar Shalom. 2019. Privacy and Fairness in Recommender\nSystems via Adversarial Training of User Representations. In ICPRAM . SciTePress, 476\u2013482.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03521",
                        "Citation Paper Title": "Title:Privacy and Fairness in Recommender Systems via Adversarial Training of User Representations",
                        "Citation Paper Abstract": "Abstract:Latent factor models for recommender systems represent users and items as low dimensional vectors. Privacy risks of such systems have previously been studied mostly in the context of recovery of personal information in the form of usage records from the training data. However, the user representations themselves may be used together with external data to recover private user information such as gender and age. In this paper we show that user vectors calculated by a common recommender system can be exploited in this way. We propose the privacy-adversarial framework to eliminate such leakage of private information, and study the trade-off between recommender performance and leakage both theoretically and empirically using a benchmark dataset. An advantage of the proposed method is that it also helps guarantee fairness of results, since all implicit knowledge of a set of attributes is scrubbed from the representations used by the model, and thus can't enter into the decision making. We discuss further applications of this method towards the generation of deeper and more insightful recommendations.",
                        "Citation Paper Authors": "Authors:Yehezkel S. Resheff, Yanai Elazar, Moni Shahar, Oren Sar Shalom"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "privacy-protection laws such as the GDPR, US Congress, and\nother jurisdictions have been proposed to legislate new disclosure laws. Thus, attempts have been\nmade to build machine-learned recommendation models that offer a privacy-by-design architecture,\nsuch as federated learning ",
                    "Citation Text": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, and Antonio Ferrara. 2019. Towards Effective Device-Aware\nFederated Learning. In International Conference of the Italian Association for Artificial Intelligence . Springer, 477\u2013491.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07420",
                        "Citation Paper Title": "Title:Towards Effective Device-Aware Federated Learning",
                        "Citation Paper Abstract": "Abstract:With the wealth of information produced by social networks, smartphones, medical or financial applications, speculations have been raised about the sensitivity of such data in terms of users' personal privacy and data security. To address the above issues, Federated Learning (FL) has been recently proposed as a means to leave data and computational resources distributed over a large number of nodes (clients) where a central coordinating server aggregates only locally computed updates without knowing the original data. In this work, we extend the FL framework by pushing forward the state the art in the field on several dimensions: (i) unlike the original FedAvg approach relying solely on single criteria (i.e., local dataset size), a suite of domain- and client-specific criteria constitute the basis to compute each local client's contribution, (ii) the multi-criteria contribution of each device is computed in a prioritized fashion by leveraging a priority-aware aggregation operator used in the field of information retrieval, and (iii) a mechanism is proposed for online-adjustment of the aggregation operator parameters via a local search strategy with backtracking. Extensive experiments on a publicly available dataset indicate the merits of the proposed approach compared to standard FedAvg baseline.",
                        "Citation Paper Authors": "Authors:Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Manotumruksa et al. 2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nVAR ",
                    "Citation Text": "Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, and Felice Antonio Merra. 2020. An Empirical Study of\nDNNs Robustification Inefficacy in Protecting Visual Recommenders.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.00984",
                        "Citation Paper Title": "Title:An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders",
                        "Citation Paper Abstract": "Abstract:Visual-based recommender systems (VRSs) enhance recommendation performance by integrating users' feedback with the visual features of product images extracted from a deep neural network (DNN). Recently, human-imperceptible images perturbations, defined \\textit{adversarial attacks}, have been demonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking category of products. However, since adversarial training techniques have proven to successfully robustify DNNs in preserving classification accuracy, to the best of our knowledge, two important questions have not been investigated yet: 1) How well can these defensive mechanisms protect the VRSs performance? 2) What are the reasons behind ineffective/effective defenses? To answer these questions, we define a set of defense and attack settings, as well as recommender models, to empirically investigate the efficacy of defensive mechanisms. The results indicate alarming risks in protecting a VRS through the DNN robustification. Our experiments shed light on the importance of visual features in very effective attack scenarios. Given the financial impact of VRSs on many companies, we believe this work might rise the need to investigate how to successfully protect visual-based recommenders. Source code and data are available at https://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.",
                        "Citation Paper Authors": "Authors:Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio Merra"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": ". The authors propose to\nmake more robust neural network-based collaborative filtering models (e.g., NCF ",
                    "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative\nFiltering. In WWW . ACM, 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "Li et al. 2020 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nMSAP ",
                    "Citation Text": "Vito Walter Anelli, Alejandro Bellog\u00edn, Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. 2020. Multi-Step\nAdversarial Perturbations on Recommender Systems Embeddings.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01329",
                        "Citation Paper Title": "Title:Multi-Step Adversarial Perturbations on Recommender Systems Embeddings",
                        "Citation Paper Abstract": "Abstract:Recommender systems (RSs) have attained exceptional performance in learning users' preferences and helping them in finding the most suitable products. Recent advances in adversarial machine learning (AML) in the computer vision domain have raised interests in the security of state-of-the-art model-based recommenders. Recently, worrying deterioration of recommendation accuracy has been acknowledged on several state-of-the-art model-based recommenders (e.g., BPR-MF) when machine-learned adversarial perturbations contaminate model parameters. However, while the single-step fast gradient sign method (FGSM) is the most explored perturbation strategy, multi-step (iterative) perturbation strategies, that demonstrated higher efficacy in the computer vision domain, have been highly under-researched in recommendation tasks.\nIn this work, inspired by the basic iterative method (BIM) and the projected gradient descent (PGD) strategies proposed in the CV domain, we adapt the multi-step strategies for the item recommendation task to study the possible weaknesses of embedding-based recommender models under minimal adversarial perturbations. Letting the magnitude of the perturbation be fixed, we illustrate the highest efficacy of the multi-step perturbation compared to the single-step one with extensive empirical evaluation on two widely adopted recommender datasets. Furthermore, we study the impact of structural dataset characteristics, i.e., sparsity, density, and size, on the performance degradation issued by presented perturbations to support RS designer in interpreting recommendation performance variation due to minimal variations of model parameters. Our implementation and datasets are available at https://anonymous.4open.science/r/9f27f909-93d5-4016-b01c-8976b8c14bc5/.",
                        "Citation Paper Authors": "Authors:Vito Walter Anelli, Alejandro Bellog\u00edn, Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra"
                    }
                },
                {
                    "Sentence ID": 130,
                    "Sentence": "Park et al. 2019 \u2713 \u2713 \u2713\u2713 \u2713 \u2713 \u2713\nAMASR ",
                    "Citation Text": "Thanh Tran, Renee Sweeney, and Kyumin Lee. 2019. Adversarial Mahalanobis Distance-based Attentive Song\nRecommender for Automatic Playlist Continuation. In Proc. of the 42nd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019. 245\u2013254.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03450",
                        "Citation Paper Title": "Title:Adversarial Mahalanobis Distance-based Attentive Song Recommender for Automatic Playlist Continuation",
                        "Citation Paper Abstract": "Abstract:In this paper, we aim to solve the automatic playlist continuation (APC) problem by modeling complex interactions among users, playlists, and songs using only their interaction data. Prior methods mainly rely on dot product to account for similarities, which is not ideal as dot product is not metric learning, so it does not convey the important inequality property. Based on this observation, we propose three novel deep learning approaches that utilize Mahalanobis distance. Our first approach uses user-playlist-song interactions, and combines Mahalanobis distance scores between (i) a target user and a target song, and (ii) between a target playlist and the target song to account for both the user's preference and the playlist's theme. Our second approach measures song-song similarities by considering Mahalanobis distance scores between the target song and each member song (i.e., existing song) in the target playlist. The contribution of each distance score is measured by our proposed memory metric-based attention mechanism. In the third approach, we fuse the two previous models into a unified model to further enhance their performance. In addition, we adopt and customize Adversarial Personalized Ranking (APR) for our three approaches to further improve their robustness and predictive capabilities. Through extensive experiments, we show that our proposed models outperform eight state-of-the-art models in two large-scale real-world datasets.",
                        "Citation Paper Authors": "Authors:Thanh Tran, Renee Sweeney, Kyumin Lee"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": "; the second research direction started in 2016 when the first ML-optimized attack\nwas proposed by Li et al . ",
                    "Citation Text": "Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data Poisoning Attacks on Factorization-Based\nCollaborative Filtering. In NIPS . 1885\u20131893.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.08182",
                        "Citation Paper Title": "Title:Data Poisoning Attacks on Factorization-Based Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the \\emph{alternative minimization} formulation and the \\emph{nuclear norm minimization} method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.",
                        "Citation Paper Authors": "Authors:Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", (ii) attacks on\nunsupervised learning such as clustering and anomaly detection ",
                    "Citation Text": "Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli.\n2018. Poisoning Behavioral Malware Clustering. arXiv (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.09985",
                        "Citation Paper Title": "Title:Poisoning Behavioral Malware Clustering",
                        "Citation Paper Abstract": "Abstract:Clustering algorithms have become a popular tool in computer security to analyze the behavior of malware variants, identify novel malware families, and generate signatures for antivirus systems. However, the suitability of clustering algorithms for security-sensitive settings has been recently questioned by showing that they can be significantly compromised if an attacker can exercise some control over the input data. In this paper, we revisit this problem by focusing on behavioral malware clustering approaches, and investigate whether and to what extent an attacker may be able to subvert these approaches through a careful injection of samples with poisoning behavior. To this end, we present a case study on Malheur, an open-source tool for behavioral malware clustering. Our experiments not only demonstrate that this tool is vulnerable to poisoning attacks, but also that it can be significantly compromised even if the attacker can only inject a very small percentage of attacks into the input data. As a remedy, we discuss possible countermeasures and highlight the need for more secure clustering algorithms.",
                        "Citation Paper Authors": "Authors:Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, Fabio Roli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.09803v2": {
            "Paper Title": "Adversarial Training for Code Retrieval with Question-Description\n  Relevance Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04779v1": {
            "Paper Title": "After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion\n  Functions for Scene Graph Generation",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ".\nThe baseline SGG model \u2014 used in this research \u2014 was imple-\nmented by Tang et al. ",
                    "Citation Text": "Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. 2020.\nUnbiased scene graph generation from biased training. In Proceedings of the\nConference on Computer Vision and Pattern Recognition . IEEE/CVF, Venice, Italy,\n3716\u20133725.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.11949",
                        "Citation Paper Title": "Title:Unbiased Scene Graph Generation from Biased Training",
                        "Citation Paper Abstract": "Abstract:Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse \"human walk on / sit on / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias (e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect (TDE) as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, Hanwang Zhang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The dataset includes over 100K images. With\n18 predicates per image on average, VG is the densest and largest\ndataset for SGG. Of course, one could think about MS-COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Com-\nmon objects in context. In European conference on computer vision . Springer\nInternational Publishing, Cham, 740\u2013755.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.04521v1": {
            "Paper Title": "Automated Discovery of Mathematical Definitions in Text with Deep Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10104v2": {
            "Paper Title": "A Streaming Machine Learning Framework for Online Aggression Detection\n  on Twitter",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "various deep learning architectures were\nevaluated, including Convolutional Neural Networks (CNNs),\nLong Short-Term Memory Networks (LSTMs), and Fast-\nText ",
                    "Citation Text": "A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, \u201cBag of Tricks for\nEf\ufb01cient Text Classi\ufb01cation,\u201d arXiv preprint arXiv:1607.01759 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.01759",
                        "Citation Paper Title": "Title:Bag of Tricks for Efficient Text Classification",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
                        "Citation Paper Authors": "Authors:Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "proposed\na method to detect highly probable users in producing of-\nfensive content in YouTube comments, while authors in ",
                    "Citation Text": "P. Badjatiya, S. Gupta, M. Gupta, and V . Varma, \u201cDeep Learning for\nHate Speech Detection in Tweets,\u201d in WWW . ACM, 2017, pp. 759\u2013760.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00188",
                        "Citation Paper Title": "Title:Deep Learning for Hate Speech Detection in Tweets",
                        "Citation Paper Abstract": "Abstract:Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.",
                        "Citation Paper Authors": "Authors:Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, Vasudeva Varma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.03123v2": {
            "Paper Title": "PubSqueezer: A Text-Mining Web Tool to Transform Unstructured Documents\n  into Structured Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04137v1": {
            "Paper Title": "Automated data extraction of bar chart raster images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02295v1": {
            "Paper Title": "Adversarial Counterfactual Learning and Evaluation for Recommender\n  System",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". Consequently, which player\ngoes \ufb01rst has important implications. Here, we choose to\ntrainf\u0012\ufb01rst because g can then choose the worst candi-\ndate from the uncertainty set in order to undermines f\u0012.\nWe adopt the two-timescale gradient descent ascent (GDA) ",
                    "Citation Text": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium. In Advances in neural information\nprocessing systems , pages 6626\u20136637, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "(see the appendix for detailed discussions).\nRelating the outcome to exposure mechanism has also been found in the recommendation literature ",
                    "Citation Text": "T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations as\ntreatments: Debiasing learning and evaluation. arXiv preprint arXiv:1602.05352 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05352",
                        "Citation Paper Title": "Title:Recommendations as Treatments: Debiasing Learning and Evaluation",
                        "Citation Paper Abstract": "Abstract:Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handling selection biases, adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, finding that it is highly practical and scalable.",
                        "Citation Paper Authors": "Authors:Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, Thorsten Joachims"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". In the\npresence of unobserved factors, Tukey\u2019s factorization suggests that we additionally characterize the\nrelationship between exposure mechanism and outcome ",
                    "Citation Text": "A. Franks, A. D\u2019Amour, and A. Feller. Flexible sensitivity analysis for observational studies\nwithout observable implications. Journal of the American Statistical Association , pages 1\u201333,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00399",
                        "Citation Paper Title": "Title:Flexible sensitivity analysis for observational studies without observable implications",
                        "Citation Paper Abstract": "Abstract:A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey's factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit an unidentified relationship between the treatment assignment indicator and the observed potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data.",
                        "Citation Paper Authors": "Authors:Alexander Franks, Alexander D'Amour, Avi Feller"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.07931v1": {
            "Paper Title": "Do Offline Metrics Predict Online Performance in Recommender Systems?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03371v1": {
            "Paper Title": "Explaining Differences in Classes of Discrete Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03228v1": {
            "Paper Title": "From Dataset Recycling to Multi-Property Extraction and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03080v1": {
            "Paper Title": "EXAMS: A Multi-Subject High School Examinations Dataset for\n  Cross-Lingual and Multilingual Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02690v1": {
            "Paper Title": "Entity Linking in 100 Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02665v1": {
            "Paper Title": "Adversarial Context Aware Network Embeddings for Textual Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06848v2": {
            "Paper Title": "Group-Buying Recommendation for Social E-Commerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02426v1": {
            "Paper Title": "Graph Based Temporal Aggregation for Video Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12417v2": {
            "Paper Title": "An Analysis of the Impact of SEO on University Website Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02248v1": {
            "Paper Title": "Generative Inverse Deep Reinforcement Learning for Online Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15209v3": {
            "Paper Title": "Depressive, Drug Abusive, or Informative: Knowledge-aware Study of News\n  Exposure during COVID-19 Outbreak",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07258v2": {
            "Paper Title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01580v1": {
            "Paper Title": "CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web\n  to Special Domain Search",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". Our experimental results showthat BERT-Large has no obvious advantage over SciBERT-Base and\nConv-KNRM performs the worst. The main reason for the poor\nperformance of Conv-KNRM is that we did not use its subword\nversion ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.\nOn the effect of low-frequency terms on neural-IR models. In Proceedings of\nSIGIR . 1137\u20131140.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12683",
                        "Citation Paper Title": "Title:On the Effect of Low-Frequency Terms on Neural-IR Models",
                        "Citation Paper Abstract": "Abstract:Low-frequency terms are a recurring challenge for information retrieval models, especially neural IR frameworks struggle with adequately capturing infrequently observed words. While these terms are often removed from neural models - mainly as a concession to efficiency demands - they traditionally play an important role in the performance of IR models. In this paper, we analyze the effects of low-frequency terms on the performance and robustness of neural IR models. We conduct controlled experiments on three recent neural IR models, trained on a large-scale passage retrieval collection. We evaluate the neural IR models with various vocabulary sizes for their respective word embeddings, considering different levels of constraints on the available GPU memory. We observe that despite the significant benefits of using larger vocabularies, the performance gap between the vocabularies can be, to a great extent, mitigated by extensive tuning of a related parameter: the number of documents to re-rank. We further investigate the use of subword-token embedding models, and in particular FastText, for neural IR models. Our experiments show that using FastText brings slight improvements to the overall performance of the neural IR models in comparison to models trained on the full vocabulary, while the improvement becomes much more pronounced for queries containing low-frequency terms.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Navid Rekabsaz, Carsten Eickhoff, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "as the base retrieval and adopted the dense retrieval implementation\nprovided by Gao, et al. ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2020. Complement-\ning Lexical Retrieval with Semantic Residual Embedding. arXiv preprint\narXiv:2004.13969 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13969",
                        "Citation Paper Title": "Title:Complementing Lexical Retrieval with Semantic Residual Embedding",
                        "Citation Paper Abstract": "Abstract:This paper presents CLEAR, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model. CLEAR explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of CLEAR over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.00518v2": {
            "Paper Title": "AI Marker-based Large-scale AI Literature Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01453v1": {
            "Paper Title": "Participation in TREC 2020 COVID Track Using Continuous Active Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00377v2": {
            "Paper Title": "Leveraging Natural Language Processing to Mine Issues on Twitter During\n  the COVID-19 Pandemic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01421v1": {
            "Paper Title": "WSL-DS: Weakly Supervised Learning with Distant Supervision for Query\n  Focused Multi-Document Abstractive Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01393v1": {
            "Paper Title": "GAIN: Graph Attention & Interaction Network for Inductive\n  Semi-Supervised Learning over Large-scale Graphs",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ", GraphSAGE can\nonly set the same aggregator for all nodes according to\nmanual selection. Besides, GraphSAGE adapts a uniform\nsampling scheme which can cause important neighbors to\nbe ignored. To address this problem, FastGCN ",
                    "Citation Text": "J. Chen, T. Ma, and C. Xiao, \u201cFastgcn: Fast learning\nwith graph convolutional networks via importance sampling,\u201d\nin6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings , 2018. [Online]. Available:\nhttps://openreview.net/forum?id=rytstxWAW",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.10247",
                        "Citation Paper Title": "Title:FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
                        "Citation Paper Abstract": "Abstract:The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.",
                        "Citation Paper Authors": "Authors:Jie Chen, Tengfei Ma, Cao Xiao"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "suggests that we can extract a\ufb01xed number of neighbors for each node and normalize the\nsubgraph for each node. The normalized subgraph can be\ninput to a convolutional neural network.\nIn ",
                    "Citation Text": "W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d in Advances in Neural Information Pro-\ncessing Systems , 2017, pp. 1024\u20131034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "et al. generate embedding representation for categorical\nfeatures and then learn the feature interactions.\nRecently, generating feature interactions in an explicit\nway has been paid attention especially in recommendation\nproblems. In ",
                    "Citation Text": "R. Wang, B. Fu, G. Fu, and M. Wang, \u201cDeep & cross network for\nad click predictions,\u201d in Proceedings of the ADKDD\u201917 . ACM, 2017,\np. 12.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". They used the normalized graph adjacency\nmatrix as a transition matrix to generate information prop-\nagation in the graph. ",
                    "Citation Text": "M. Niepert, M. Ahmed, and K. Kutzkov, \u201cLearning convolutional\nneural networks for graphs,\u201d in International conference on machine\nlearning , 2016, pp. 2014\u20132023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.05273",
                        "Citation Paper Title": "Title:Learning Convolutional Neural Networks for Graphs",
                        "Citation Paper Abstract": "Abstract:Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.",
                        "Citation Paper Authors": "Authors:Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.04125v2": {
            "Paper Title": "Towards Topic-Guided Conversational Recommender System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02273v1": {
            "Paper Title": "VLEngagement: A Dataset of Scientific Video Lectures for Evaluating\n  Population-based Engagement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00953v1": {
            "Paper Title": "Collaborative Generative Hashing for Marketing and Fast Cold-start\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00944v1": {
            "Paper Title": "Deep Pairwise Hashing for Cold-start Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00866v1": {
            "Paper Title": "An End-to-End ML System for Personalized Conversational Voice Models in\n  Walmart E-Commerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00701v1": {
            "Paper Title": "Cross-Lingual Document Retrieval with Smooth Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00565v1": {
            "Paper Title": "CURE: Collection for Urdu Information Retrieval Evaluation and Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00479v1": {
            "Paper Title": "Cheap IR Evaluation: Fewer Topics, No Relevance Judgements, and\n  Crowdsourced Assessments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06058v2": {
            "Paper Title": "Neural Correction Model for Open-Domain Named Entity Recognition",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ", people implement a similar network that trains on clean and noisy self-labeled data jointly\nby explicitly modeling clean and noisy labels separately. The model in ",
                    "Citation Text": "Lukas Lange, Michael A. Hedderich, and Dietrich Klakow. Feature-dependent confusion matrices for low-resource\nNER labeling with noisy labels. CoRR , abs/1910.06061, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.06061",
                        "Citation Paper Title": "Title:Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels",
                        "Citation Paper Abstract": "Abstract:In low-resource settings, the performance of supervised labeling models can be improved with automatically annotated or distantly supervised data, which is cheap to create but often noisy. Previous works have shown that significant improvements can be reached by injecting information about the confusion between clean and noisy labels in this additional training data into the classifier training. However, for noise estimation, these approaches either do not take the input features (in our case word embeddings) into account, or they need to learn the noise modeling from scratch which can be difficult in a low-resource setting. We propose to cluster the training data using the input features and then compute different confusion matrices for each cluster. To the best of our knowledge, our approach is the first to leverage feature-dependent noise modeling with pre-initialized confusion matrices. We evaluate on low-resource named entity recognition settings in several languages, showing that our methods improve upon other confusion-matrix based methods by up to 9%.",
                        "Citation Paper Authors": "Authors:Lukas Lange, Michael A. Hedderich, Dietrich Klakow"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", authors propose contextual\nstring embeddings which are produced with a pre-trained character language model. BERT is proposed in ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT (1) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "A bidirectional Long Short-Term Memory (LSTM) network with a Conditional Random Field\n(CRF) layer.\nCVT ",
                    "Citation Text": "Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc V . Le. Semi-supervised sequence modeling\nwith cross-view training. In EMNLP , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08370",
                        "Citation Paper Title": "Title:Semi-Supervised Sequence Modeling with Cross-View Training",
                        "Citation Paper Abstract": "Abstract:Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "proposes a de-noising layer and train it on a mixture of\nclean and noisy data. In ",
                    "Citation Text": "Debjit Paul, Mittul Singh, Michael A. Hedderich, and Dietrich Klakow. Handling noisy labels for robustly learning\nfrom self-training data for low-resource sequence labeling. In NAACL-HLT (Student Research Workshop) , pages\n29\u201334. Association for Computational Linguistics, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.12008",
                        "Citation Paper Title": "Title:Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of effectively self-training neural networks in a low-resource setting. Self-training is frequently used to automatically increase the amount of training data. However, in a low-resource scenario, it is less effective due to unreliable annotations created using self-labeling of unlabeled data. We propose to combine self-training with noise handling on the self-labeled data. Directly estimating noise on the combined clean training set and self-labeled data can lead to corruption of the clean data and hence, performs worse. Thus, we propose the Clean and Noisy Label Neural Network which trains on clean and noisy self-labeled data simultaneously by explicitly modelling clean and noisy labels separately. In our experiments on Chunking and NER, this approach performs more robustly than the baselines. Complementary to this explicit approach, noise can also be handled implicitly with the help of an auxiliary learning task. To such a complementary approach, our method is more beneficial than other baseline methods and together provides the best performance overall.",
                        "Citation Paper Authors": "Authors:Debjit Paul, Mittul Singh, Michael A. Hedderich, Dietrich Klakow"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ",\nresearchers propose a two-step named entity recognizer for open-domain search queries and ",
                    "Citation Text": "Kevin Bowden, JiaQi Wu, Shereen Oraby, Amita Misra, and Marilyn A. Walker. Slugnerds: A named entity\nrecognition tool for open domain dialogue systems. In LREC . European Language Resources Association (ELRA),\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03784",
                        "Citation Paper Title": "Title:SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:In dialogue systems, the tasks of named entity recognition (NER) and named entity linking (NEL) are vital preprocessing steps for understanding user intent, especially in open domain interaction where we cannot rely on domain-specific inference. UCSC's effort as one of the funded teams in the 2017 Amazon Alexa Prize Contest has yielded Slugbot, an open domain social bot, aimed at casual conversation. We discovered several challenges specifically associated with both NER and NEL when building Slugbot, such as that the NE labels are too coarse-grained or the entity types are not linked to a useful ontology. Moreover, we have discovered that traditional approaches do not perform well in our context: even systems designed to operate on tweets or other social media data do not work well in dialogue systems. In this paper, we introduce Slugbot's Named Entity Recognition for dialogue Systems (SlugNERDS), a NER and NEL tool which is optimized to address these issues. We describe two new resources that we are building as part of this work: SlugEntityDB and SchemaActuator. We believe these resources will be useful for the research community.",
                        "Citation Paper Authors": "Authors:Kevin K. Bowden, Jiaqi Wu, Shereen Oraby, Amita Misra, Marilyn Walker"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "and it is\npre-trained using the masked language model task and the next sentence prediction task. ",
                    "Citation Text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of\nself-attention networks. CoRR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07785",
                        "Citation Paper Title": "Title:Cloze-driven Pretraining of Self-attention Networks",
                        "Citation Paper Abstract": "Abstract:We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.",
                        "Citation Paper Authors": "Authors:Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.00427v1": {
            "Paper Title": "Efficient Pipelines for Vision-Based Context Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16218v1": {
            "Paper Title": "Biomedical Concept Relatedness -- A large EHR-based benchmark",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16059v1": {
            "Paper Title": "Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot\n  Relational Triple Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16037v1": {
            "Paper Title": "Semantic Labeling Using a Deep Contextualized Language Model",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "is a deep contextualized language model that contains\nmultiple layers of transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems 30 , I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.). 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.16030v1": {
            "Paper Title": "Multimodal Metric Learning for Tag-based Music Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "outlined three main types of music informa-\ntion: editorial ,cultural , and acoustic . Most of the previous\nworks in music tagging ",
                    "Citation Text": "Minz Won, Andres Ferraro, Dmitry Bogdanov, and\nXavier Serra, \u201cEvaluation of cnn-based automatic music\ntagging models,\u201d in Proc. of the 17th Sound and Music\nComputing , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.00751",
                        "Citation Paper Title": "Title:Evaluation of CNN-based Automatic Music Tagging Models",
                        "Citation Paper Abstract": "Abstract:Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.",
                        "Citation Paper Authors": "Authors:Minz Won, Andres Ferraro, Dmitry Bogdanov, Xavier Serra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.15879v1": {
            "Paper Title": "Log(Graph): A Near-Optimal High-Performance Graph Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06226v5": {
            "Paper Title": "Lexical Simplification with Pretrained Encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02733v3": {
            "Paper Title": "Porous Lattice-based Transformer Encoder for Chinese NER",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07964v4": {
            "Paper Title": "Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based\n  Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15670v1": {
            "Paper Title": "Detecting Individuals with Depressive Disorder fromPersonal Google\n  Search and YouTube History Logs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05645v3": {
            "Paper Title": "Hypergraph Clustering for Finding Diverse and Experienced Groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14707v1": {
            "Paper Title": "TopicModel4J: A Java Package for Topic Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08582v3": {
            "Paper Title": "The Capacity of Multi-user Private Information Retrieval for\n  Computationally Limited Databases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14570v1": {
            "Paper Title": "Addressing Purchase-Impression Gap through a Sequential Re-ranker",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". We also measured and validated the\ninfluence of neighborhood on the preference of an item in ecom-\nmerce in our earlier work ",
                    "Citation Text": "Saratchandra Indrakanti, Svetlana Strunjas, Shubhangi Tandon, and Manojku-\nmar Rangasamy Kannadasan. 2019. Influence of Neighborhood on the Preferenceof an Item in eCommerce Search. arXiv:cs.IR/1908.03825",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03825",
                        "Citation Paper Title": "Title:Influence of Neighborhood on the Preference of an Item in eCommerce Search",
                        "Citation Paper Abstract": "Abstract:Surfacing a ranked list of items for a search query to help buyers discover inventory and make purchase decisions is a critical problem in eCommerce search. Typically, items are independently predicted with a probability of sale with respect to a given search query. But in a dynamic marketplace like eBay, even for a single product, there are various different factors distinguishing one item from another which can influence the purchase decision for the user. Users have to make a purchase decision by considering all of these options. Majority of the existing learning to rank algorithms model the relative relevance between labeled items only at the loss functions like pairwise or list-wise losses. But they are limited to point-wise scoring functions where items are ranked independently based on the features of the item itself. In this paper, we study the influence of an item's neighborhood to its purchase decision. Here, we consider the neighborhood as the items ranked above and below the current item in search results. By adding delta features comparing items within a neighborhood and learning a ranking model, we are able to experimentally show that the new ranker with delta features outperforms our baseline ranker in terms of Mean Reciprocal Rank (MRR). The ranking models with proposed delta features result in $3-5\\%$ improvement in MRR over the baseline model. We also study impact of different sizes for neighborhood. Experimental results show that neighborhood size $3$ perform the best based on MRR with an improvement of $4-5\\%$ over the baseline model.",
                        "Citation Paper Authors": "Authors:Saratchandra Indrakanti, Svetlana Strunjas, Shubhangi Tandon, Manojkumar Rangasamy Kannadasan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.14464v1": {
            "Paper Title": "Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14013v1": {
            "Paper Title": "The item selection problem for user cold-start recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13659v1": {
            "Paper Title": "Exploiting Neural Query Translation into Cross Lingual Information\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "architecture\nachieves the best translation quality so far which relies on a self-\nattention mechanism ",
                    "Citation Text": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang,\nBowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.03130",
                        "Citation Paper Title": "Title:A Structured Self-attentive Sentence Embedding",
                        "Citation Paper Abstract": "Abstract:This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
                        "Citation Paper Authors": "Authors:Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "proposed attention mechanism into\nencoder-decoder network and significantly improved the transla-\ntion quality. The recently proposed Transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Processing Systems . 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "utilized clickthrough data\nfrom the query log of the search engine in connection with the log\nof links the users clicked to train the ranking algorithm. Kreutzer\net al. ",
                    "Citation Text": "Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler. 2018. Can\nNeural Machine Translation be Improved with User Feedback? arXiv preprint\narXiv:1804.05958 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05958",
                        "Citation Paper Title": "Title:Can Neural Machine Translation be Improved with User Feedback?",
                        "Citation Paper Abstract": "Abstract:We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay e-commerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments---five-star ratings of translation quality---and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.",
                        "Citation Paper Authors": "Authors:Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.13658v1": {
            "Paper Title": "Constraint Translation Candidates: A Bridge between Neural Query\n  Translation and Cross-lingual Information Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13352v1": {
            "Paper Title": "The Age-related Differences in Web Information Search Process",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15711v1": {
            "Paper Title": "5W1H-based Expression for the Effective Sharing of Information in\n  Digital Forensic Investigations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13128v1": {
            "Paper Title": "ExplanationLP: Abductive Reasoning for Explainable Science Question\n  Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13100v1": {
            "Paper Title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized\n  Recommendation Training",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "A. Embedding Layers in Recommendations\nThe ability to \u201clearn\u201d a semantically meaningful representa-\ntion of a target feature is one of the biggest strengths of ML.\nRather than using one-hot encoded vectors for representing\ncategorical features, recent work ",
                    "Citation Text": "J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding,\u201d in\narxiv.org , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.14057v1": {
            "Paper Title": "An Intermediate Data-driven Methodology for Scientific Workflow\n  Management System to Support Reusability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12647v1": {
            "Paper Title": "Extracting Body Text from Academic PDF Documents for Text Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02340v4": {
            "Paper Title": "Enhancing Social Recommendation with Adversarial Graph Convolutional\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12370v1": {
            "Paper Title": "EventKG+Click: A Dataset of Language-specific Event-centric User\n  Interaction Traces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12363v1": {
            "Paper Title": "Regret in Online Recommendation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10421v2": {
            "Paper Title": "Multimodal Analytics for Real-world News using Measures of Cross-modal\n  Entity Consistency",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "is applied to extract all nouns. They can contain general\nconcepts, such as politics or sports, as well as specific scenes or\nactions. Subsequently, we calculate the word embedding for each\ncandidate using fastText ",
                    "Citation Text": "Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas\nMikolov. 2018. Learning Word Vectors for 157 Languages. In International Con-\nference on Language Resources and Evaluation .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06893",
                        "Citation Paper Title": "Title:Learning Word Vectors for 157 Languages",
                        "Citation Paper Abstract": "Abstract:Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",
                        "Citation Paper Authors": "Authors:Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2004.14652v3": {
            "Paper Title": "Question Rewriting for Conversational Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "train a binary clas-\nsification model on CANARD dataset that learns to pick the terms\nfrom the conversation history for query expansion. Yu et al . ",
                    "Citation Text": "Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett, Jianfeng Gao,\nand Zhiyuan Liu. 2020. Few-Shot Generative Conversational Query Rewriting. In\nProceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval . 1933\u20131936.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05009",
                        "Citation Paper Title": "Title:Few-Shot Generative Conversational Query Rewriting",
                        "Citation Paper Abstract": "Abstract:Conversational query rewriting aims to reformulate a concise conversational query to a fully specified, context-independent query that can be effectively handled by existing information retrieval systems. This paper presents a few-shot generative approach to conversational query rewriting. We develop two methods, based on rules and self-supervised learning, to generate weak supervision data using large amounts of ad hoc search sessions, and to fine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational Assistance Track, our weakly supervised GPT-2 rewriter improves the state-of-the-art ranking accuracy by 12%, only using very limited amounts of manual query rewrites. In the zero-shot learning setting, the rewriter still gives a comparable result to previous state-of-the-art systems. Our analyses reveal that GPT-2 effectively picks up the task syntax and learns to capture context dependencies, even for hard cases that involve group references and long-turn dependencies.",
                        "Citation Paper Authors": "Authors:Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett, Jianfeng Gao, Zhiyuan Liu"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". Several recent studies that are concurrent to our work\nproposed alternative approaches for the conversational QA task and\nevaluated them on the TREC CAsT dataset. Mele et al . ",
                    "Citation Text": "Ida Mele, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Nicola\nTonellotto, and Ophir Frieder. 2020. Topic Propagation in Conversational Search.\nInProceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval . 2057\u20132060.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14054",
                        "Citation Paper Title": "Title:Topic Propagation in Conversational Search",
                        "Citation Paper Abstract": "Abstract:In a conversational context, a user expresses her multi-faceted information need as a sequence of natural-language questions, i.e., utterances. Starting from a given topic, the conversation evolves through user utterances and system replies. The retrieval of documents relevant to a given utterance in a conversation is challenging due to ambiguity of natural language and to the difficulty of detecting possible topic shifts and semantic relationships among utterances. We adopt the 2019 TREC Conversational Assistant Track (CAsT) framework to experiment with a modular architecture performing: (i) topic-aware utterance rewriting, (ii) retrieval of candidate passages for the rewritten utterances, and (iii) neural-based re-ranking of candidate passages. We present a comprehensive experimental evaluation of the architecture assessed in terms of traditional IR metrics at small cutoffs. Experimental results show the effectiveness of our techniques that achieve an improvement up to 0.28 (+93%) for P@1 and 0.19 (+89.9%) for nDCG@3 w.r.t. the CAsT baseline.",
                        "Citation Paper Authors": "Authors:I. Mele, C. I. Muntean, F. M. Nardini, R. Perego, N. Tonellotto, O. Frieder"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nCopyTransformer. The Transformer decoder, which, similar to\npointer-generator model, uses one of the attention heads as a pointer ",
                    "Citation Text": "Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up\nAbstractive Summarization. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing . 4098\u20134109.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10792",
                        "Citation Paper Title": "Title:Bottom-Up Abstractive Summarization",
                        "Citation Paper Abstract": "Abstract:Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain.",
                        "Citation Paper Authors": "Authors:Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "is a standard metric of lexical\noverlap, which is often used in text summarization and other text\ngeneration tasks. We also calculate question similarity scores with\nthe Universal Sentence Encoder ( USE) model ",
                    "Citation Text": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.\nJohn, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian\nStrope, and Ray Kurzweil. 2018. Universal Sentence Encoder for English. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing . 169\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.11175",
                        "Citation Paper Title": "Title:Universal Sentence Encoder",
                        "Citation Paper Abstract": "Abstract:We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.",
                        "Citation Paper Authors": "Authors:Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.12175v1": {
            "Paper Title": "Estimation of Groundwater Storage Variations in Indus River Basin using\n  GRACE Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12174v1": {
            "Paper Title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for\n  Kinyarwanda and Kirundi",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07959v1": {
            "Paper Title": "Text Mining to Identify and Extract Novel Disease Treatments From\n  Unstructured Datasets",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", for the modular nature of its code and its ease of use. \nFurther, the work on BioFLAIR ",
                    "Citation Text": "S. Sharma, R. Daniel Jr,  Bioflair: Pretrained pooled contextualized  embeddings for \nbiomedical sequence labeling tasks,  arXiv preprint  arXiv:1908.05760 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.05760",
                        "Citation Paper Title": "Title:BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks",
                        "Citation Paper Abstract": "Abstract:Biomedical Named Entity Recognition (NER) is a challenging problem in biomedical information processing due to the widespread ambiguity of out of context terms and extensive lexical variations. Performance on bioNER benchmarks continues to improve due to advances like BERT, GPT, and XLNet. FLAIR (1) is an alternative embedding model which is less computationally intensive than the others mentioned. We test FLAIR and its pretrained PubMed embeddings (which we term BioFLAIR) on a variety of bio NER tasks and compare those with results from BERT-type networks. We also investigate the effects of a small amount of additional pretraining on PubMed content, and of combining FLAIR and ELMO models. We find that with the provided embeddings, FLAIR performs on-par with the BERT networks - even establishing a new state of the art on one benchmark. Additional pretraining did not provide a clear benefit, although this might change with even more pretraining being done. Stacking the FLAIR embeddings with others typically does provide a boost in the benchmark results.",
                        "Citation Paper Authors": "Authors:Shreyas Sharma, Ron Daniel Jr"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.11930v1": {
            "Paper Title": "Scientific Claim Verification with VERT5ERINI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11786v1": {
            "Paper Title": "Spikyball sampling: Exploring large networks via an inhomogeneous\n  filtered diffusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11386v1": {
            "Paper Title": "Distilling Dense Representations for Ranking using Tightly-Coupled\n  Teachers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14827v2": {
            "Paper Title": "Memory-efficient Embedding for Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ", Neural Input Search [ 9,18], Multi-granular Quan-\ntized Embedding ",
                    "Citation Text": "Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin,\nLichan Hong, and Ed H Chi. 2020. Learning Multi-granular Quantized Embed-\ndings for Large-Vocab Categorical Features in Recommender Systems. arXiv\npreprint arXiv:2002.08530 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08530",
                        "Citation Paper Title": "Title:Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system models often represent various sparse features like users, items, and categorical features via embeddings. A standard approach is to map each unique feature value to an embedding vector. The size of the produced embedding table grows linearly with the size of the vocabulary. Therefore, a large vocabulary inevitably leads to a gigantic embedding table, creating two severe problems: (i) making model serving intractable in resource-constrained environments; (ii) causing overfitting problems. In this paper, we seek to learn highly compact embeddings for large-vocab sparse features in recommender systems (recsys). First, we show that the novel Differentiable Product Quantization (DPQ) approach can generalize to recsys problems. In addition, to better handle the power-law data distribution commonly seen in recsys, we propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns more compact embeddings for infrequent items. We seek to provide a new angle to improve recommendation performance with compact model sizes. Extensive experiments on three recommendation tasks and two datasets show that we can achieve on par or better performance, with only ~20% of the original model size.",
                        "Citation Paper Authors": "Authors:Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin, Lichan Hong, Ed H. Chi"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "introduce\nMNAS. They propose to search hierarchical convolution cell blocks\nin an independent manner, so that a deep network can be built based\non them. Mixed Dimension Embedding ",
                    "Citation Text": "Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James\nZou. 2019. Mixed Dimension Embeddings with Application to Memory-Efficient\nRecommendation Systems. arXiv preprint arXiv:1909.11810 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11810",
                        "Citation Paper Title": "Title:Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems",
                        "Citation Paper Abstract": "Abstract:Embedding representations power machine intelligence in many applications, including recommendation systems, but they are space intensive -- potentially occupying hundreds of gigabytes in large-scale settings. To help manage this outsized memory consumption, we explore mixed dimension embeddings, an embedding layer architecture in which a particular embedding vector's dimension scales with its query frequency. Through theoretical analysis and systematic experiments, we demonstrate that using mixed dimensions can drastically reduce the memory usage, while maintaining and even improving the ML performance. Empirically, we show that the proposed mixed dimension layers improve accuracy by 0.1% using half as many parameters or maintain it using 16X fewer parameters for click-through rate prediction task on the Criteo Kaggle dataset.",
                        "Citation Paper Authors": "Authors:Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, James Zou"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "propose a transfer learning\nframework called NASNet, which train convolution cells on smaller\ndatasets and apply them on larger datasets. Tan et al. ",
                    "Citation Text": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew\nHoward, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture\nsearch for mobile. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition . 2820\u20132828.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11626",
                        "Citation Paper Title": "Title:MnasNet: Platform-Aware Neural Architecture Search for Mobile",
                        "Citation Paper Abstract": "Abstract:Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at this https URL",
                        "Citation Paper Authors": "Authors:Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "formu-\nlate the problem of network architecture search in a differentiable\nmanner and solve it using gradient descent. Luo et al. ",
                    "Citation Text": "Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. Neural\narchitecture optimization. In Proceedings of the 32nd International Conference on\nNeural Information Processing Systems . 7827\u20137838.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.07233",
                        "Citation Paper Title": "Title:Neural Architecture Optimization",
                        "Citation Paper Abstract": "Abstract:Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.",
                        "Citation Paper Authors": "Authors:Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "introduce a framework\nnamed SMASH, in which a hyper-network is developed to generate\nweights for sampled networks. DARTS ",
                    "Citation Text": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable\narchitecture search. arXiv preprint arXiv:1806.09055 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.09055",
                        "Citation Paper Title": "Title:DARTS: Differentiable Architecture Search",
                        "Citation Paper Abstract": "Abstract:This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",
                        "Citation Paper Authors": "Authors:Hanxiao Liu, Karen Simonyan, Yiming Yang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "propose ENAS, where the controller learns to search a\nsubgraph from a large computational graph to form an optimal\nneural network architecture. Brock et al. ",
                    "Citation Text": "Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. 2017. Smash:\none-shot model architecture search through hypernetworks. arXiv preprint\narXiv:1708.05344 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05344",
                        "Citation Paper Title": "Title:SMASH: One-Shot Model Architecture Search through HyperNetworks",
                        "Citation Paper Abstract": "Abstract:Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", which first utilizes an RNN based controller\nto design neural networks and proposes a reinforcement learningalgorithm to optimize the framework. After that, many endeavors\nare conducted on reducing the high training cost of NAS. Pham\net al. ",
                    "Citation Text": "Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient\nNeural Architecture Search via Parameters Sharing. In International Conference\non Machine Learning . 4095\u20134104.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.03268",
                        "Citation Paper Title": "Title:Efficient Neural Architecture Search via Parameter Sharing",
                        "Citation Paper Abstract": "Abstract:We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.",
                        "Citation Paper Authors": "Authors:Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "present an AutoEncoder based model named AutoRec. In their work,\nboth item-based and user-based AutoRec are introduced. They are\ndesigned to capture the low-dimension feature embeddings of users\nand items, respectively. Hidasi et al. ",
                    "Citation Text": "Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.10932v1": {
            "Paper Title": "Deep learning-based citation recommendation system for patents",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "employed graph convolutional network layers and bidirectional encoder representations from \ntransformers to represent the citation graph and c ontext, respectively. Zhang and Ma ",
                    "Citation Text": "Y. Zhang, Q. Ma, Citation Recommendations Considering Content and Structural Context \nEmbedding,  2020 IEEE International Conference on Big Data and Smart Computing (BigComp), \nIEEE, 2020, pp. 1 -7.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.02344",
                        "Citation Paper Title": "Title:Citation Recommendations Considering Content and Structural Context Embedding",
                        "Citation Paper Abstract": "Abstract:The number of academic papers being published is increasing exponentially in recent years, and recommending adequate citations to assist researchers in writing papers is a non-trivial task. Conventional approaches may not be optimal, as the recommended papers may already be known to the users, or be solely relevant to the surrounding context but not other ideas discussed in the manuscript. In this work, we propose a novel embedding algorithm DocCit2Vec, along with the new concept of ``structural context'', to tackle the aforementioned issues. The proposed approach demonstrates superior performances to baseline models in extensive experiments designed to simulate practical usage scenarios.",
                        "Citation Paper Authors": "Authors:Yang Zhang, Qiang Ma"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "incorporated author information, venue information , and contents to \nrepresent papers, thereby increasing the recommendation performance. Jeong, Jang, Park and Choi ",
                    "Citation Text": "C. Jeong, S. Jang, E. Park, S. Choi, A context -aware citation recommendation model with BERT \nand graph convolutional networks, Scientometrics, (2020) 1 -16.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06464",
                        "Citation Paper Title": "Title:A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:With the tremendous growth in the number of scientific papers being published, searching for references while writing a scientific paper is a time-consuming process. A technique that could add a reference citation at the appropriate place in a sentence will be beneficial. In this perspective, context-aware citation recommendation has been researched upon for around two decades. Many researchers have utilized the text data called the context sentence, which surrounds the citation tag, and the metadata of the target paper to find the appropriate cited research. However, the lack of well-organized benchmarking datasets and no model that can attain high performance has made the research difficult.\nIn this paper, we propose a deep learning based model and well-organized dataset for context-aware paper citation recommendation. Our model comprises a document encoder and a context encoder, which uses Graph Convolutional Networks (GCN) layer and Bidirectional Encoder Representations from Transformers (BERT), which is a pre-trained model of textual data. By modifying the related PeerRead dataset, we propose a new dataset called FullTextPeerRead containing context sentences to cited references and paper metadata. To the best of our knowledge, This dataset is the first well-organized dataset for context-aware paper recommendation. The results indicate that the proposed model with the proposed datasets can attain state-of-the-art performance and achieve a more than 28% improvement in mean average precision (MAP) and recall@k.",
                        "Citation Paper Authors": "Authors:Chanwoo Jeong, Sion Jang, Hyuna Shin, Eunjeong Park, Sungchul Choi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.10789v1": {
            "Paper Title": "ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models\n  in Sponsored Search Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10469v1": {
            "Paper Title": "Learning To Retrieve: How to Train a Dense Retrieval Model Effectively\n  and Efficiently",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "utilized a trick called in-batch negatives to reuse computation and reduce the computational cost. Xiong et\nal. ",
                    "Citation Text": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint\narXiv:2007.00808 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00808",
                        "Citation Paper Title": "Title:Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                        "Citation Paper Authors": "Authors:Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "used random negative samples because they believed it approximates the recall\noptimization task. Gao et al. ",
                    "Citation Text": "Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. Complementing lexical retrieval with semantic residual\nembedding. arXiv preprint arXiv:2004.13969 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13969",
                        "Citation Paper Title": "Title:Complementing Lexical Retrieval with Semantic Residual Embedding",
                        "Citation Paper Abstract": "Abstract:This paper presents CLEAR, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model. CLEAR explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of CLEAR over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
                        "Citation Paper Authors": "Authors:Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", such as maximum inner\nproduct search, is then used to ef\ufb01ciently retrieve the vectors that are similar to the query vector. Section 3 introduces\nits architecture and inference procedure in details. Though early research ",
                    "Citation Text": "Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W Bruce Croft, and\nXueqi Cheng. A deep look into neural ranking models for information retrieval. Information Processing &\nManagement , page 102067, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06902",
                        "Citation Paper Title": "Title:A Deep Look into Neural Ranking Models for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:Ranking models lie at the heart of research on information retrieval (IR). During the past decades, different techniques have been proposed for constructing ranking models, from traditional heuristic methods, probabilistic methods, to modern machine learning methods. Recently, with the advance of deep learning technology, we have witnessed a growing body of work in applying shallow or deep neural networks to the ranking problem in IR, referred to as neural ranking models in this paper. The power of neural ranking models lies in the ability to learn from the raw text inputs for the ranking problem to avoid many limitations of hand-crafted features. Neural networks have sufficient capacity to model complicated tasks, which is needed to handle the complexity of relevance estimation in ranking. Since there have been a large variety of neural ranking models proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we will take a deep look into the neural ranking models from different dimensions to analyze their underlying assumptions, major design principles, and learning strategies. We compare these models through benchmark tasks to obtain a comprehensive empirical understanding of the existing techniques. We will also discuss what is missing in the current literature and what are the promising and desired future directions.",
                        "Citation Paper Authors": "Authors:Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". It uses the highest scored negatives in\nbatch. Luan et al. ",
                    "Citation Text": "Yi Luan, Jacob Eisenstein, Kristina Toutanove, and Michael Collins. Sparse, dense, and attentional representa-\ntions for text retrieval. arXiv preprint arXiv:2005.00181 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00181",
                        "Citation Paper Title": "Title:Sparse, Dense, and Attentional Representations for Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
                        "Citation Paper Authors": "Authors:Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "used the BM25 top documents as negatives. Karpukhin et al. ",
                    "Citation Text": "Vladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense\npassage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04906",
                        "Citation Paper Title": "Title:Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "replaced the term frequency\n\ufb01eld in BM25 with term weights predicted by BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies , pages 4171\u20134186,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.10386v1": {
            "Paper Title": "A Benchmark for Lease Contract Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01888v3": {
            "Paper Title": "Adversarial Item Promotion: Vulnerabilities at the Core of Top-N\n  Recommenders that Use Images to Address Cold Start",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ", we show that the problem of adversarial examples in rec-\nommender system goes beyond the problem of classifier-targeted\nadversarial examples.\nAdversarial training is a promising techniques to tackle adver-\nsarial examples ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks.\nInICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". Recently, with the advances in computational\nresources, learning-based neural frameworks were proposed and\nachieve state of the art performance on fashion recommendation\n(DVBPR ",
                    "Citation Text": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian McAuley. 2017.\nVisually-aware fashion recommendation and design with generative image mod-\nels. In ICDM . 207\u2013216.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.02231",
                        "Citation Paper Title": "Title:Visually-Aware Fashion Recommendation and Design with Generative Image Models",
                        "Citation Paper Abstract": "Abstract:Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using `off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning `fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pre-trained visual features. Furthermore, we show that our model can be used \\emph{generatively}, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.",
                        "Citation Paper Authors": "Authors:Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "propose Targeted Adversarial Attack against\nMultimedia Recommender Systems (TAaMR), and they use two\nclassification-based adversarial attacks, namely Fast Gradient Sign\nMethod (FGSM) ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.\nInICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "gather a large-scale dataset, Fashion-136K,\nwith detailed annotations and propose several retrieval-based ap-\nproaches to recommend the missing part based on query image.\nBeyond image retrieval-based recommendation approaches, user-\nitem interactions are leveraged in visually-aware recommenders.\nIBR ",
                    "Citation Text": "Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.\n2015. Image-based recommendations on styles and substitutes. In SIGIR . 43\u201352.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.04757",
                        "Citation Paper Title": "Title:Image-based Recommendations on Styles and Substitutes",
                        "Citation Paper Abstract": "Abstract:Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.",
                        "Citation Paper Authors": "Authors:Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "gather segmentation parts and retrieve visually similar items\nwithin each of the predicted classes. Later, semantic information\nof images is also incorporated to improve retrieval performance.\nJagadeesh et al. ",
                    "Citation Text": "Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, and Neel\nSundaresan. 2014. Large scale visual recommendations from street fashion images.\nInKDD . 1925\u20131934.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1401.1778",
                        "Citation Paper Title": "Title:Large Scale Visual Recommendations From Street Fashion Images",
                        "Citation Paper Abstract": "Abstract:We describe a completely automated large scale visual recommendation system for fashion. Our focus is to efficiently harness the availability of large quantities of online fashion images and their rich meta-data. Specifically, we propose four data driven models in the form of Complementary Nearest Neighbor Consensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov Chain LDA for solving this problem. We analyze relative merits and pitfalls of these algorithms through extensive experimentation on a large-scale data set and baseline them against existing ideas from color science. We also illustrate key fashion insights learned through these experiments and show how they can be employed to design better recommendation systems. Finally, we also outline a large-scale annotated data set of fashion images (Fashion-136K) that can be exploited for future vision research.",
                        "Citation Paper Authors": "Authors:Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, Neel Sundaresan"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ", then propose\nto generate fake user-item interactions based on influence func-\ntion ",
                    "Citation Text": "Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. 2020. Influence function based\ndata poisoning attacks to top-n recommender systems. In WWW . 3019\u20133025.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08025",
                        "Citation Paper Title": "Title:Influence Function based Data Poisoning Attacks to Top-N Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-$N$ items that match the best with a user's preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods.",
                        "Citation Paper Authors": "Authors:Minghong Fang, Neil Zhenqiang Gong, Jia Liu"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "propose a generative approach to generate fake user profiles\n1Code available at: https://github .com/liuzrcc/AIP\n2to mount profile injection attack. Fang et al. first propose poisoning\nattacks to graph-based recommender systems ",
                    "Citation Text": "Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. 2018. Poisoning\nattacks to graph-based recommender systems. In ACSAC . 381\u2013392.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.04127",
                        "Citation Paper Title": "Title:Poisoning Attacks to Graph-Based Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning attacks, in which an attacker injects fake data to a given system such that the system makes recommendations as the attacker desires. However, these poisoning attacks are either agnostic to recommendation algorithms or optimized to recommender systems that are not graph-based. Like association-rule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store. However, how to design optimized poisoning attacks for graph-based recommender systems is still an open problem. In this work, we perform a systematic study on poisoning attacks to graph-based recommender systems. Due to limited resources and to avoid detection, we assume the number of fake users that can be injected into the system is bounded. The key challenge is how to assign rating scores to the fake users such that the target item is recommended to as many normal users as possible. To address the challenge, we formulate the poisoning attacks as an optimization problem, solving which determines the rating scores for the fake users. We also propose techniques to solve the optimization problem. We evaluate our attacks and compare them with existing attacks under white-box (recommendation algorithm and its parameters are known), gray-box (recommendation algorithm is known but its parameters are unknown), and black-box (recommendation algorithm is unknown) settings using two real-world datasets. Our results show that our attack is effective and outperforms existing attacks for graph-based recommender systems. For instance, when 1% fake users are injected, our attack can make a target item recommended to 580 times more normal users in certain scenarios.",
                        "Citation Paper Authors": "Authors:Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, Jia Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.10176v1": {
            "Paper Title": "Individual corpora predict fast memory retrieval during reading",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09927v1": {
            "Paper Title": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09797v1": {
            "Paper Title": "Surprise: Result List Truncation via Extreme Value Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09426v1": {
            "Paper Title": "LANNS: A Web-Scale Approximate Nearest Neighbor Lookup System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08547v1": {
            "Paper Title": "A Unified Model for Recommendation with Selective Neighborhood Modeling",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "as a\nuser usually give few ratings compared to the large item set. To this end, many research [41, 42, 43] propose to exploit\nadditional knowledge about the users/items to mitigate data sparseness. For example, Zhang et al. ",
                    "Citation Text": "L. Zheng, V . Noroozi, P. S. Yu, Joint deep modeling of users and items using reviews for recommendation, in: Proceedings of the Tenth ACM\nInternational Conference on Web Search and Data Mining, ACM, 2017, pp. 425\u2013434.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.04783",
                        "Citation Paper Title": "Title:Joint Deep Modeling of Users and Items Using Reviews for Recommendation",
                        "Citation Paper Abstract": "Abstract:A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets.",
                        "Citation Paper Authors": "Authors:Lei Zheng, Vahid Noroozi, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "propose to\nboost recommendation performance by exploiting different neural network structures. He et al. ",
                    "Citation Text": "X. He, L. Liao, H. Zhang, L. Nie, X. Hu, T.-S. Chua, Neural collaborative \ufb01ltering, in: Proceedings of the 26th international conference on\nworld wide web, International World Wide Web Conferences Steering Committee, 2017, pp. 173\u2013182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.09189v1": {
            "Paper Title": "Knowledge-guided Open Attribute Value Extraction with Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09139v1": {
            "Paper Title": "We Need to Rethink How We Describe and Organize Spatial Information:\n  Instrumenting and Observing the Community of Users to Improve Data\n  Description and Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09167v3": {
            "Paper Title": "CheXbert: Combining Automatic Labelers and Expert Annotations for\n  Accurate Radiology Report Labeling Using BERT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12539v1": {
            "Paper Title": "Dynamically Tie the Right Offer to the Right Customer in\n  Telecommunications Industry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08874v1": {
            "Paper Title": "Visualization of Contributions to Open-Source Projects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08865v1": {
            "Paper Title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06414v2": {
            "Paper Title": "Cannot Predict Comment Volume of a News Article before (a few) Users\n  Read It",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02623v2": {
            "Paper Title": "Information Theoretic Counterfactual Learning from Missing-Not-At-Random\n  Feedback",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", and it has been used in explaining and guiding deep learning practice\n[3,31]. The IB garners interests in its application for learning disentangled [ 2,14], informative\n[7,15], or compressed representation ",
                    "Citation Text": "Bin Dai, Chen Zhu, and David Wipf. Compressing neural networks using the variational information\nbottleneck. arXiv preprint arXiv:1802.10399 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.10399",
                        "Citation Paper Title": "Title:Compressing Neural Networks using the Variational Information Bottleneck",
                        "Citation Paper Abstract": "Abstract:Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.",
                        "Citation Paper Authors": "Authors:Bin Dai, Chen Zhu, David Wipf"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", while all of\nthese methods require additional RCTs for calculating propensity scores. Besides, recent works\nproposed to balance factual and counterfactual domains based on RCTs with domain adaptation ",
                    "Citation Text": "Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In\nInternational Conference on Machine Learning , pages 3020\u20133029, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03661",
                        "Citation Paper Title": "Title:Learning Representations for Counterfactual Inference",
                        "Citation Paper Abstract": "Abstract:Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:Fredrik D. Johansson, Uri Shalit, David Sontag"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.08777v1": {
            "Paper Title": "Active Testing: An Unbiased Evaluation Method for Distantly Supervised\n  Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07717v2": {
            "Paper Title": "Wasserstein Distance Regularized Sequence Representation for Text\n  Matching in Asymmetrical Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08652v1": {
            "Paper Title": "Cross-Lingual Relation Extraction with Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08593v1": {
            "Paper Title": "Deep Submodular Networks for Extractive Data Summarization",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "achieves state-of-the-art results for\nquery-focused document summarization by de\ufb01ning a joint diversity and query relevance term. In video summarization, ",
                    "Citation Text": "Arun Balajee Vasudevan, Michael Gygli, Anna V olokitin, and Luc Van Gool. Query-adaptive video summarization\nvia quality-aware relevance estimation. In Proceedings of the 25th ACM international conference on Multimedia ,\npages 582\u2013590, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.00581",
                        "Citation Paper Title": "Title:Query-adaptive Video Summarization via Quality-aware Relevance Estimation",
                        "Citation Paper Abstract": "Abstract:Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.",
                        "Citation Paper Authors": "Authors:Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin, Luc Van Gool"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.08591v1": {
            "Paper Title": "A Conglomerate of Multiple OCR Table Detection and Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08319v1": {
            "Paper Title": "Detecting ESG topics using domain-specific language models and data\n  augmentation approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08187v1": {
            "Paper Title": "PrivNet: Safeguarding Private Attributes in Transfer Learning for\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07628v1": {
            "Paper Title": "Hierarchical Text Interaction for Rating Prediction",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "are mainly based on user-item interactions.\nTherefore, they have two major limitations, namely data sparsity and cold-start problem, as users usually give few\nratings given the larger number of items ",
                    "Citation Text": "Y . Han, L. Zhu, Z. Cheng, J. Li, X. Liu, Discrete optimal graph clustering, IEEE Trans. Cybern. 50 (4) (2020) 1697\u20131710.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11266",
                        "Citation Paper Title": "Title:Discrete Optimal Graph Clustering",
                        "Citation Paper Abstract": "Abstract:Graph based clustering is one of the major clustering methods. Most of it work in three separate steps: similarity graph construction, clustering label relaxing and label discretization with k-means. Such common practice has three disadvantages: 1) the predefined similarity graph is often fixed and may not be optimal for the subsequent clustering. 2) the relaxing process of cluster labels may cause significant information loss. 3) label discretization may deviate from the real clustering result since k-means is sensitive to the initialization of cluster centroids. To tackle these problems, in this paper, we propose an effective discrete optimal graph clustering (DOGC) framework. A structured similarity graph that is theoretically optimal for clustering performance is adaptively learned with a guidance of reasonable rank constraint. Besides, to avoid the information loss, we explicitly enforce a discrete transformation on the intermediate continuous label, which derives a tractable optimization problem with discrete solution. Further, to compensate the unreliability of the learned labels and enhance the clustering accuracy, we design an adaptive robust module that learns prediction function for the unseen data based on the learned discrete cluster labels. Finally, an iterative optimization strategy guaranteed with convergence is developed to directly solve the clustering results. Extensive experiments conducted on both real and synthetic datasets demonstrate the superiority of our proposed methods compared with several state-of-the-art clustering approaches.",
                        "Citation Paper Authors": "Authors:Yudong Han, Lei Zhu, Zhiyong Cheng, Jingjing Li, Xiaobai Liu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Our work is related to recommendation with reviews, attention-based recommendation and hierarchical represen-\ntation learning. We brie\ufb02y review the recent advances in these areas.\n2.1. Recommendation with Reviews\nTraditional collaborative \ufb01ltering recommenders ",
                    "Citation Text": "X. Wang, X. He, M. Wang, F. Feng, T.-S. Chua, Neural graph collaborative \ufb01ltering, in: Proceedings of the 42th International ACM SIGIR\nconference on Research and Development in Information Retrieval, ACM, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08108",
                        "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.09851v2": {
            "Paper Title": "The Music Streaming Sessions Dataset",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". This can allow tests of new algorithms to be carried out\nmore quickly and efficiently than what is currently possible. To the\nbest of our knowledge there are no other public datasets except\nthe counterfactual test-bed from ",
                    "Citation Text": "Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and\nMaarten de Rijke. Large-scale validation of counterfactual learning methods: A\ntest-bed. arXiv preprint arXiv:1612.00367 , 2016.The Music Streaming Sessions Dataset WWW \u201919, May 13\u201317, 2019, San Francisco, CA, USA",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00367",
                        "Citation Paper Title": "Title:Large-scale Validation of Counterfactual Learning Methods: A Test-Bed",
                        "Citation Paper Abstract": "Abstract:The ability to perform effective off-policy learning would revolutionize the process of building better interactive systems, such as search engines and recommendation systems for e-commerce, computational advertising and news. Recent approaches for off-policy evaluation and learning in these settings appear promising. With this paper, we provide real-world data and a standardized test-bed to systematically investigate these algorithms using data from display advertising. In particular, we consider the problem of filling a banner ad with an aggregate of multiple products the user may want to purchase. This paper presents our test-bed, the sanity checks we ran to ensure its validity, and shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization, POEM, and reductions to supervised learning using regression baselines. Our results show experimental evidence that recent off-policy learning methods can improve upon state-of-the-art supervised learning techniques on a large-scale real-world data set.",
                        "Citation Paper Authors": "Authors:Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, Maarten de Rijke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.07100v1": {
            "Paper Title": "Re-evaluating Evaluation in Text Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07075v1": {
            "Paper Title": "AutoADR: Automatic Model Design for Ad Relevance",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "compress BERT into\nshallow structures by distilling information during fine-tuning and\npre-training phase respectively. TinyBERT ",
                    "Citation Text": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.10351",
                        "Citation Paper Title": "Title:TinyBERT: Distilling BERT for Natural Language Understanding",
                        "Citation Paper Abstract": "Abstract:Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT.\nTinyBERT with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.",
                        "Citation Paper Authors": "Authors:Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "is a well-known learning-to-matchFigure 1: Simplified framework of online ads serving\npipeline\nparadigm, which leverages a convolutional neural architecture to\ncapture the query intent. Jointly modeling query content as well\nas its context, ",
                    "Citation Text": "Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob\nGrue Simonsen, and Jian-Yun Nie. 2015. A hierarchical recurrent encoder-decoder\nfor generative context-aware query suggestion. In Proceedings of the 24th ACM\nInternational on Conference on Information and Knowledge Management . 553\u2013562.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.02221",
                        "Citation Paper Title": "Title:A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion",
                        "Citation Paper Abstract": "Abstract:Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications.",
                        "Citation Paper Authors": "Authors:Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob G. Simonsen, Jian-Yun Nie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.06952v1": {
            "Paper Title": "Consumer Behaviour in Retail: Next Logical Purchase using Deep Neural\n  Network",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "which out-\nperform the gradient boosting type of models like Xgboost ",
                    "Citation Text": "Chen, T.; and Guestrin, C. 2016. Xgboost: A scalable\ntree boosting system. In Proceedings of the 22nd acm\nsigkdd international conference on knowledge discov-\nery and data mining , 785\u2013794.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.02754",
                        "Citation Paper Title": "Title:XGBoost: A Scalable Tree Boosting System",
                        "Citation Paper Abstract": "Abstract:Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
                        "Citation Paper Authors": "Authors:Tianqi Chen, Carlos Guestrin"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", and are trained on GCP instance con-Figure 5: TCN-LSTM\ntaining 6 CPUs and a single GPU. Scikit-learn ",
                    "Citation Text": "Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel,\nV .; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer,\nP.; Weiss, R.; Dubourg, V .; et al. 2011. Scikit-learn:\nMachine learning in Python. the Journal of machine\nLearning research 12: 2825\u20132830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "along with neural network architec-\ntures like Multi Layer Perceptron, Long Short Term Memory\n(LSTM), Temporal Convolutional Networks (TCN) ",
                    "Citation Text": "Lea, C.; Vidal, R.; Reiter, A.; and Hager, G. D. 2016.\nTemporal convolutional networks: A uni\ufb01ed approach\nto action segmentation. In European Conference on\nComputer Vision , 47\u201354. Springer.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.08242",
                        "Citation Paper Title": "Title:Temporal Convolutional Networks: A Unified Approach to Action Segmentation",
                        "Citation Paper Abstract": "Abstract:The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.",
                        "Citation Paper Authors": "Authors:Colin Lea, Rene Vidal, Austin Reiter, Gregory D. Hager"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nFrom Neural Network architectures perspective, close to\nour work is Deep Neural Network Ensembles for Time Se-\nries Classi\ufb01cation ",
                    "Citation Text": "Fawaz, H. I.; Forestier, G.; Weber, J.; Idoumghar, L.;\nand Muller, P.-A. 2019. Deep neural network ensem-\nbles for time series classi\ufb01cation. In 2019 Interna-\ntional Joint Conference on Neural Networks (IJCNN) ,\n1\u20136. IEEE.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06602",
                        "Citation Paper Title": "Title:Deep Neural Network Ensembles for Time Series Classification",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have revolutionized many fields such as computer vision and natural language processing. Inspired by this recent success, deep learning started to show promising results for Time Series Classification (TSC). However, neural networks are still behind the state-of-the-art TSC algorithms, that are currently composed of ensembles of 37 non deep learning based classifiers. We attribute this gap in performance due to the lack of neural network ensembles for TSC. Therefore in this paper, we show how an ensemble of 60 deep learning models can significantly improve upon the current state-of-the-art performance of neural networks for TSC, when evaluated over the UCR/UEA archive: the largest publicly available benchmark for time series analysis. Finally, we show how our proposed Neural Network Ensemble (NNE) is the first time series classifier to outperform COTE while reaching similar performance to the current state-of-the-art ensemble HIVE-COTE.",
                        "Citation Paper Authors": "Authors:Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2006.08108v2": {
            "Paper Title": "Expertise and Dynamics within Crowdsourced Musical Knowledge Curation: A\n  Case Study of the Genius Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09474v1": {
            "Paper Title": "It's the Best Only When It Fits You Most: Finding Related Models for\n  Serving Based on Dynamic Locality Sensitive Hashing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06705v1": {
            "Paper Title": "Weakly-Supervised Aspect-Based Sentiment Analysis via Joint\n  Aspect-Sentiment Topic Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06444v1": {
            "Paper Title": "Automatic Extraction of Urban Outdoor Perception from Geolocated\n  Free-Texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06395v1": {
            "Paper Title": "Aspect-based Document Similarity for Research Papers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06392v1": {
            "Paper Title": "Projection techniques to update the truncated SVD of evolving matrices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06328v1": {
            "Paper Title": "Assessing the Helpfulness of Review Content for Explaining\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "proposed\na matrix factorization based model to leverage sentiment analysis\non aspects addressed in reviews, to provide both recommendations\nand textual explanations based on templates. ",
                    "Citation Text": "Lei Zheng, Vahid Noroozi, and Philip S. Yu. 2017. Joint deep modeling of users\nand items using reviews for recommendation. In Proceedings of the Tenth ACM\nInternational Conference on Web Search and Data Mining . ACM, 425\u2013434.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.04783",
                        "Citation Paper Title": "Title:Joint Deep Modeling of Users and Items Using Reviews for Recommendation",
                        "Citation Paper Abstract": "Abstract:A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets.",
                        "Citation Paper Authors": "Authors:Lei Zheng, Vahid Noroozi, Philip S. Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.07050v1": {
            "Paper Title": "Modurec: Recommender Systems with Feature and Time Modulation",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "for multiple features and n-wise low-rank in-\nteractions. The authors in ",
                    "Citation Text": "Steffen Rendle, Li Zhang, and Yehuda Koren, \u201cOn the Dif\ufb01-\nculty of Evaluating Baselines: A Study on Recommender Sys-\ntems,\u201d arXiv:1905.01395 [cs] , May 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01395",
                        "Citation Paper Title": "Title:On the Difficulty of Evaluating Baselines: A Study on Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Numerical evaluations with comparisons to baselines play a central role when judging research in recommender systems. In this paper, we show that running baselines properly is difficult. We demonstrate this issue on two extensively studied datasets. First, we show that results for baselines that have been used in numerous publications over the past five years for the Movielens 10M benchmark are suboptimal. With a careful setup of a vanilla matrix factorization baseline, we are not only able to improve upon the reported results for this baseline but even outperform the reported results of any newly proposed method. Secondly, we recap the tremendous effort that was required by the community to obtain high quality results for simple methods on the Netflix Prize. Our results indicate that empirical findings in research papers are questionable unless they were obtained on standardized benchmarks where baselines have been tuned extensively by the research community.",
                        "Citation Paper Authors": "Authors:Steffen Rendle, Li Zhang, Yehuda Koren"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", where modu-\nlation is used to project class embeddings onto convolutional layers,\nand few-shot learning ",
                    "Citation Text": "Boris Oreshkin, Pau Rodr \u00b4\u0131guez L \u00b4opez, and Alexandre Lacoste,\n\u201cTadam: Task dependent adaptive metric for improved few-\nshot learning,\u201d in NeurIPS , 2018, pp. 721\u2013731.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10123",
                        "Citation Paper Title": "Title:TADAM: Task dependent adaptive metric for improved few-shot learning",
                        "Citation Paper Abstract": "Abstract:Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Boris N. Oreshkin, Pau Rodriguez, Alexandre Lacoste"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", which combines lin-\nguistic and visual information, image synthesis ",
                    "Citation Text": "Andrew Brock, Jeff Donahue, and Karen Simonyan, \u201cLarge\nscale gan training for high \ufb01delity natural image synthesis,\u201d\narXiv:1809.11096 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11096",
                        "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                        "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "intro-\nduces the concept of kernelized weight matrices, which reduces the\nnumber of free parameters at the cost of additional hyperparameters\nlike the choice of the kernel function.\nFinally, with the recent popularity of geometric deep learning\nmethods ",
                    "Citation Text": "Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam,\nand Pierre Vandergheynst, \u201cGeometric deep learning: going\nbeyond euclidean data,\u201d IEEE Signal Processing Magazine ,\nvol. 34, no. 4, pp. 18\u201342, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08097",
                        "Citation Paper Title": "Title:Geometric deep learning: going beyond Euclidean data",
                        "Citation Paper Abstract": "Abstract:Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",
                        "Citation Paper Authors": "Authors:Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.02526v3": {
            "Paper Title": "Vapur: A Search Engine to Find Related Protein-Compound Pairs in\n  COVID-19 Literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06197v1": {
            "Paper Title": "Context-Aware Drive-thru Recommendation Service at Fast Food Restaurants",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "have been adopted\nand yield better recommendation results. For exam-\nple, GRU4Rec ",
                    "Citation Text": "B. Hidasi, A. Karatzoglou, L. Baltrunas, and\nD. Tikk ,Session-based recommendations with recur-\nrent neural networks , CoRR, abs/1511.06939 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", and they di-\nrectly communicate with each other via the distributed\nkey-value store provided natively by MXNet. In this\nway, users are relieved from managing the complex\nsteps of distributed training, through the scikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort,\nV. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, et al. ,\nScikit-learn: Machine learning in python , the Journal\nof machine Learning research, 12 (2011), pp. 2825{\n2830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "hy-\nbrids attention into GRU to capture the behavior and\npurpose of users in a session. Instead of incorporat-\ning attention as a component into other models, Trans-\nformer ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-\nreit, L. Jones, A. N. Gomez,  L. Kaiser, and\nI. Polosukhin ,Attention is all you need , in Ad-\nvances in neural information processing systems, 2017,\npp. 5998{6008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "to incorporate\navailable context features into the recommendation pro-\ncess as explicit additional categories of data. In Wide &\nDeep Learning ",
                    "Citation Text": "X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S.\nChua ,Neural collaborative \fltering , in Proceedings of\nthe 26th international conference on world wide web,\n2017, pp. 173{182.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05031",
                        "Citation Paper Title": "Title:Neural Collaborative Filtering",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", which uses an\nitem-to-item similarity matrix to recommend the most\nsimilar items to the one that the user has clicked most\nrecently in the session. Another approach is mainly\nbased on Markov chains (e.g. Markov Decision Pro-\ncesses ",
                    "Citation Text": "G. Shani, D. Heckerman, and R. I. Brafman ,An\nmdp-based recommender system , Journal of Machine\nLearning Research, 6 (2005), pp. 1265{1295.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.0600",
                        "Citation Paper Title": "Title:An MDP-based Recommender System",
                        "Citation Paper Abstract": "Abstract:Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.",
                        "Citation Paper Authors": "Authors:Guy Shani, Ronen I. Brafman, David Heckerman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.05987v1": {
            "Paper Title": "SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05525v1": {
            "Paper Title": "Large Scale Product Graph Construction for Recommendation in E-commerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05317v1": {
            "Paper Title": "Weakly Supervised Medication Regimen Extraction from Medical\n  Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00761v2": {
            "Paper Title": "Massively Multilingual Document Alignment with Cross-lingual\n  Sentence-Mover's Distance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05037v1": {
            "Paper Title": "Cross-Stack Workload Characterization of Deep Recommendation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04880v1": {
            "Paper Title": "Designing for Recommending Intermediate States in A Scientific Workflow\n  Management System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11687v2": {
            "Paper Title": "MASK: A flexible framework to facilitate de-identification of clinical\n  texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04665v1": {
            "Paper Title": "Scaling Systematic Literature Reviews with Machine Learning Pipelines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04609v1": {
            "Paper Title": "Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04484v1": {
            "Paper Title": "Revisiting Alternative Experimental Settings for Evaluating Top-N Item\n  Recommendation Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06862v3": {
            "Paper Title": "Large-scale biometry with interpretable neural network regression on UK\n  Biobank body MRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14771v3": {
            "Paper Title": "A Hybrid Adaptive Educational eLearning Project based on Ontologies\n  Matching and Recommendation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04335v1": {
            "Paper Title": "NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative\n  COVID-19 Tweets using Ensembling and Adversarial Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04321v1": {
            "Paper Title": "Analyzing HPC Support Tickets: Experience and Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04260v1": {
            "Paper Title": "Fake Reviews Detection through Analysis of Linguistic Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00038v2": {
            "Paper Title": "AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab\n  Posts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09592v3": {
            "Paper Title": "Privacy-Preserving News Recommendation Model Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03710v1": {
            "Paper Title": "Topic Diffusion Discovery Based on Deep Non-negative Autoencoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03544v1": {
            "Paper Title": "A Self-supervised Approach for Semantic Indexing in the Context of\n  COVID-19 Pandemic",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "because the SSL-based model\nlearns some general auxiliary knowledge from the pre-text task that allows the model to \"understand\" the downstream\ntask better. ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "During the current pandemic so many related papers are being published at a much faster pace, ",
                    "Citation Text": "Andre Esteva, Anuprit Kale, Romain Paulus, Kazuma Hashimoto, Wenpeng Yin, Dragomir Radev, and Richard\nSocher. Co-search: Covid-19 information retrieval with semantic search, question answering, and abstractive\nsummarization. arXiv preprint arXiv:2006.09595 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09595",
                        "Citation Paper Title": "Title:CO-Search: COVID-19 Information Retrieval with Semantic Search, Question Answering, and Abstractive Summarization",
                        "Citation Paper Abstract": "Abstract:The COVID-19 global pandemic has resulted in international efforts to understand, track, and mitigate the disease, yielding a significant corpus of COVID-19 and SARS-CoV-2-related publications across scientific disciplines. As of May 2020, 128,000 coronavirus-related publications have been collected through the COVID-19 Open Research Dataset Challenge. Here we present CO-Search, a retriever-ranker semantic search engine designed to handle complex queries over the COVID-19 literature, potentially aiding overburdened health workers in finding scientific answers during a time of crisis. The retriever is built from a Siamese-BERT encoder that is linearly composed with a TF-IDF vectorizer, and reciprocal-rank fused with a BM25 vectorizer. The ranker is composed of a multi-hop question-answering module, that together with a multi-paragraph abstractive summarizer adjust retriever scores. To account for the domain-specific and relatively limited dataset, we generate a bipartite graph of document paragraphs and citations, creating 1.3 million (citation title, paragraph) tuples for training the encoder. We evaluate our system on the data of the TREC-COVID information retrieval challenge. CO-Search obtains top performance on the datasets of the first and second rounds, across several key metrics: normalized discounted cumulative gain, precision, mean average precision, and binary preference.",
                        "Citation Paper Authors": "Authors:Andre Esteva, Anuprit Kale, Romain Paulus, Kazuma Hashimoto, Wenpeng Yin, Dragomir Radev, Richard Socher"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.03343v1": {
            "Paper Title": "Slice-Aware Neural Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03159v1": {
            "Paper Title": "Where Are the Facts? Searching for Fact-checked Information to Alleviate\n  the Spread of Fake News",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03073v1": {
            "Paper Title": "Beyond [CLS] through Ranking by Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13313v3": {
            "Paper Title": "Modularized Transfomer-based Ranking Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08851v2": {
            "Paper Title": "Approximate Nearest Neighbour Search on Privacy-aware Encoding of User\n  Locations to Identify Susceptible Infections in Simulated Epidemics",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ", it is known that this random projection based trans-\nformation of Equation 1 is in fact distance preserving ",
                    "Citation Text": "X. Yi, C. Caramanis, and E. Price. Binary embedding: Fundamental limits and\nfast algorithm. In Proc. of ICML\u201915 , pages 2162\u20132170, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05746",
                        "Citation Paper Title": "Title:Binary Embedding: Fundamental Limits and Fast Algorithm",
                        "Citation Paper Abstract": "Abstract:Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary $N$ distinct points in $\\mathbb{S}^{p-1}$, our goal is to encode each point using $m$-dimensional binary strings such that we can reconstruct their geodesic distance up to $\\delta$ uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time $O\\big(mp\\big)$. We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires $m = \\Omega(\\frac{1}{\\delta^2}\\log{N})$ bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3) we also provide an analytic result about embedding a general set of points $K \\subseteq \\mathbb{S}^{p-1}$ with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.",
                        "Citation Paper Authors": "Authors:Xinyang Yi, Constantine Caramanis, Eric Price"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.01715v2": {
            "Paper Title": "Exploring Artist Gender Bias in Music Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ", but the metric has recently gained more traction in its\napplication to different domains. In Lin et al. ",
                    "Citation Text": "Kun Lin, Nasim Sonboli, Bamshad Mobasher, and Robin Burke. 2019. Crank up\nthe volume: Preference bias amplification in collaborative recommendation. In\nCEUR Workshop Proceedings , Vol. 2440. arXiv:1909.06362",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06362",
                        "Citation Paper Title": "Title:Crank up the volume: preference bias amplification in collaborative recommendation",
                        "Citation Paper Abstract": "Abstract:Recommender systems are personalized: we expect the results given to a particular user to reflect that user's preferences. Some researchers have studied the notion of calibration, how well recommendations match users' stated preferences, and bias disparity the extent to which mis-calibration affects different user groups. In this paper, we examine bias disparity over a range of different algorithms and for different item categories and demonstrate significant differences between model-based and memory-based algorithms.",
                        "Citation Paper Authors": "Authors:Kun Lin, Nasim Sonboli, Bamshad Mobasher, Robin Burke"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "find that from a user\u2019s perspective the groups who do not\nfavor popular items may receive worsened recommendations in\nterms of accuracy and calibration. Moreover, Ferraro et al. in ",
                    "Citation Text": "Andres Ferraro, Dmitry Bogdanov, Xavier Serra, and Jason Yoon. 2019. Artist and\nstyle exposure bias in collaborative filtering based music recommendations. In 1st\nWorkshop on Designing Human-Centric MIR Systems (wsHCMIR19), co-located at\n20th Conference of the International Society for Music Information Retrieval (ISMIR\n2019) . arXiv:1911.04827 http://arxiv.org/abs/1911.04827",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04827",
                        "Citation Paper Title": "Title:Artist and style exposure bias in collaborative filtering based music recommendations",
                        "Citation Paper Abstract": "Abstract:Algorithms have an increasing influence on the music that we consume and understanding their behavior is fundamental to make sure they give a fair exposure to all artists across different styles. In this on-going work we contribute to this research direction analyzing the impact of collaborative filtering recommendations from the perspective of artist and music style exposure given by the system. We first analyze the distribution of the recommendations considering the exposure of different styles or genres and compare it to the users' listening behavior. This comparison suggests that the system is reinforcing the popularity of the items. Then, we simulate the effect of the system in the long term with a feedback loop. From this simulation we can see how the system gives less opportunity to the majority of artists, concentrating the users on fewer items. The results of our analysis demonstrate the need for a better evaluation methodology for current music recommendation algorithms, not only limited to user-focused relevance metrics.",
                        "Citation Paper Authors": "Authors:Andres Ferraro, Dmitry Bogdanov, Xavier Serra, Jason Yoon"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.02667v1": {
            "Paper Title": "Incorporating Behavioral Hypotheses for Query Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07810v2": {
            "Paper Title": "CoDEx: A Comprehensive Knowledge Graph Completion Benchmark",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02458v1": {
            "Paper Title": "Identifying Spurious Correlations for Robust Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03774v2": {
            "Paper Title": "Survey for Trust-aware Recommender Systems: A Deep Learning Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02377v1": {
            "Paper Title": "Improving Neural Topic Models using Knowledge Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00164v2": {
            "Paper Title": "Named Entity Recognition Only from Word Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14201v2": {
            "Paper Title": "Leveraging Declarative Knowledge in Text and First-Order Logic for\n  Fine-Grained Propaganda Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00571v2": {
            "Paper Title": "Understanding tables with intermediate pre-training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01666v1": {
            "Paper Title": "Multi-Modal Retrieval using Graph Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "proposed an\narchitecture that added inductive capabilities, but lacked the graph\nstructure information during inference. GraphSAGE ",
                    "Citation Text": "William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation\nLearning on Large Graphs. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems (Long Beach, California, USA) (NIPS\u201917) .\nCurran Associates Inc., Red Hook, NY, USA, 1025\u20131035.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.01928v2": {
            "Paper Title": "Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01329v1": {
            "Paper Title": "Multi-Step Adversarial Perturbations on Recommender Systems Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01195v1": {
            "Paper Title": "Leveraging Semantic and Lexical Matching to Improve the Recall of\n  Document Retrieval Systems: A Hybrid Approach",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "which is a classical retrieval approach that is\nhighly effective and widely used by current retrieval systems. (For\nexample, BM25 is the main approach taken by systems in recent\nIR competitions ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M\nVoorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint\narXiv:2003.07820 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07820",
                        "Citation Paper Title": "Title:Overview of the TREC 2019 deep learning track",
                        "Citation Paper Abstract": "Abstract:The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the first track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs significantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M. Voorhees"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "as a\nmerger which was shown to be an effective approach for TREC-style\ndocuments in some previous works (e.g., ",
                    "Citation Text": "Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of BERT for\nad hoc document retrieval. arXiv:1903.10972",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10972",
                        "Citation Paper Title": "Title:Simple Applications of BERT for Ad Hoc Document Retrieval",
                        "Citation Paper Abstract": "Abstract:Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval. This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle. We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of.",
                        "Citation Paper Authors": "Authors:Wei Yang, Haotian Zhang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", BERT was used for weighting terms in the in-\nverted index of passages. In another work ",
                    "Citation Text": "Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\nfor efficient retrieval and ranking using deep neural networks. arXiv preprint\narXiv:1907.03693 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03693",
                        "Citation Paper Title": "Title:Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Classical information retrieval (IR) methods, such as query likelihood and BM25, score documents independently w.r.t. each query term, and then accumulate the scores. Assuming query term independence allows precomputing term-document scores using these models---which can be combined with specialized data structures, such as inverted index, for efficient retrieval. Deep neural IR models, in contrast, compare the whole query to the document and are, therefore, typically employed only for late stage re-ranking. We incorporate query term independence assumption into three state-of-the-art neural IR models: BERT, Duet, and CKNRM---and evaluate their performance on a passage ranking task. Surprisingly, we observe no significant loss in result quality for Duet and CKNRM---and a small degradation in the case of BERT. However, by operating on each query term independently, these otherwise computationally intensive models become amenable to offline precomputation---dramatically reducing the cost of query evaluations employing state-of-the-art neural ranking models. This strategy makes it practical to use deep models for retrieval from large collections---and not restrict their usage to late stage re-ranking.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz, Emine Yilmaz"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed a model that learns sparse vectors\nfor documents and queries which can be used for retrieval with an\ninverted index. In another work ",
                    "Citation Text": "Christophe Van Gysel, Maarten De Rijke, and Evangelos Kanoulas. 2018. Neural\nvector spaces for unsupervised information retrieval. ACM Transactions on\nInformation Systems (TOIS) 36, 4 (2018), 1\u201325.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.02702",
                        "Citation Paper Title": "Title:Neural Vector Spaces for Unsupervised Information Retrieval",
                        "Citation Paper Abstract": "Abstract:We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed.\nNVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.",
                        "Citation Paper Authors": "Authors:Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.00984v1": {
            "Paper Title": "An Empirical Study of DNNs Robustification Inefficacy in Protecting\n  Visual Recommenders",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "differs from previous works in the use of optimized pertur-\nbations, and their respective defenses, that lead to drastic perfor-\nmance reduction [ 19,36,37]. For example, He et al. ",
                    "Citation Text": "Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial\nPersonalized Ranking for Recommendation. In SIGIR 2018 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.03908",
                        "Citation Paper Title": "Title:Adversarial Personalized Ranking for Recommendation",
                        "Citation Paper Abstract": "Abstract:Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) --- the most widely used model in recommendation --- as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization.\nTo enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "by integrating high-level features\nextracted from a pre-trained CNN, and Wang et al. ",
                    "Citation Text": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian J. McAuley. [n.d.].\nVisually-Aware Fashion Recommendation and Design with Generative Image\nModels. In ICDM 2017 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.02231",
                        "Citation Paper Title": "Title:Visually-Aware Fashion Recommendation and Design with Generative Image Models",
                        "Citation Paper Abstract": "Abstract:Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using `off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning `fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pre-trained visual features. Furthermore, we show that our model can be used \\emph{generatively}, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.",
                        "Citation Paper Authors": "Authors:Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "creates new adversarial samples at training time, making the model\nmore robust to such perturbed inputs. Defensive Distillation ",
                    "Citation Text": "Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram\nSwami. 2016. Distillation as a Defense to Adversarial Perturbations Against DeepNeural Networks. In SP 2016 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04508",
                        "Citation Paper Title": "Title:Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "formalized the adver-\nsarial generation problem by solving a box-constrained L-BFGS.\nGoodfellow et al. proposed Fast Gradient Sign Method (FGSM) ",
                    "Citation Text": "Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and\nHarnessing Adversarial Examples. In ICLR 2015 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.00980v1": {
            "Paper Title": "MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on\n  a Massive Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00768v1": {
            "Paper Title": "SparTerm: Learning Term-based Sparse Representation for Fast Text\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "is a document expansion method which\nutilizes more powerful T5 ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2019. Exploring the Limits of\nTransfer Learning with a Unified Text-to-Text Transformer. ArXiv abs/1910.10683\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposed ORQA with bi-encoder architecture to retrieve candi-\ndate passages for question answering using FAISS ",
                    "Citation Text": "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity\nsearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08734",
                        "Citation Paper Title": "Title:Billion-scale similarity search with GPUs",
                        "Citation Paper Abstract": "Abstract:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\nWe propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
                        "Citation Paper Authors": "Authors:Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "show new possi-\nbilities for text retrieval. Based on dense representations, Lee ",
                    "Citation Text": "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent re-\ntrieval for weakly supervised open domain question answering. arXiv preprint\narXiv:1906.00300 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00300",
                        "Citation Paper Title": "Title:Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
                        "Citation Paper Authors": "Authors:Kenton Lee, Ming-Wei Chang, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.00722v1": {
            "Paper Title": "Evaluating a Generative Adversarial Framework for Information Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00665v1": {
            "Paper Title": "Event Detection in Twitter by Weighting Tweet's Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06390v2": {
            "Paper Title": "A Feature Analysis for Multimodal News Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00200v1": {
            "Paper Title": "RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07035v1": {
            "Paper Title": "MARS-Gym: A Gym framework to model, train, and evaluate Recommender\n  Systems for Marketplaces",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nInMARS-Gym , we consider the notion of fairness in three perspectives ",
                    "Citation Text": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond\ndisparate treatment & disparate impact: Learning classi\ufb01cation without disparate mistreatment. In Proceedings of\nthe 26th International Conference on World Wide Web , WWW \u201917, page 1171\u20131180, Republic and Canton of\nGeneva, CHE, 2017. International World Wide Web Conferences Steering Committee. ISBN 9781450349130.\ndoi: 10.1145/3038912.3052660. URL https://doi.org/10.1145/3038912.3052660 .\n14APREPRINT - OCTOBER 15, 2020",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08452",
                        "Citation Paper Title": "Title:Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment",
                        "Citation Paper Abstract": "Abstract:Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.",
                        "Citation Paper Authors": "Authors:Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.04426v2": {
            "Paper Title": "CuratorNet: Visually-aware Recommendation of Art Images",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "propose the system Vista, which\naddresses digital artwork recommendations based on pre-trained deep neural visual features, as well as temporal and\nsocial data. On the other hand, Messina et al. ",
                    "Citation Text": "Pablo Messina, Vicente Dominguez, Denis Parra, Christoph Trattner, and Alvaro Soto. 2018. Content-based artwork recommendation: integrating\npainting metadata with neural and manually-engineered visual features. User Modeling and User-Adapted Interaction (2018), 1\u201340.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05786",
                        "Citation Paper Title": "Title:Exploring Content-based Artwork Recommendation with Metadata and Visual Features",
                        "Citation Paper Abstract": "Abstract:Compared to other areas, artwork recommendation has received little attention, despite the continuous growth of the artwork market. Previous research has relied on ratings and metadata to make artwork recommendations, as well as visual features extracted with deep neural networks (DNN). However, these features have no direct interpretation to explicit visual features (e.g. brightness, texture) which might hinder explainability and user-acceptance. In this work, we study the impact of artwork metadata as well as visual features (DNN-based and attractiveness-based) for physical artwork recommendation, using images and transaction data from the UGallery online artwork store.\nOur results indicate that: (i) visual features perform better than manually curated data, (ii) DNN-based visual features perform better than attractiveness-based ones, and (iii) a hybrid approach improves the performance further. Our research can inform the development of new artwork recommenders relying on diverse content data.",
                        "Citation Paper Authors": "Authors:Pablo Messina, Vicente Dominguez, Denis Parra, Christoph Trattner, Alvaro Soto"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "deals with visually-aware digital art recommendation, building a model called Vista\nwhich combines ratings, temporal and social signals and visual features.\nAnother relevant work was the research by Lei et al. ",
                    "Citation Text": "Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, and Houqiang Li. 2016. Comparative Deep Learning of Hybrid Representations for Image\nRecommendations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 2545\u20132553.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01252",
                        "Citation Paper Title": "Title:Comparative Deep Learning of Hybrid Representations for Image Recommendations",
                        "Citation Paper Abstract": "Abstract:In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed \\emph{hybrid} and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions.",
                        "Citation Paper Authors": "Authors:Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "expands VBPR but they focus on\ngenerating images using Generative adversarial networks ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative\nadversarial nets. In Advances in neural information processing systems . 2672\u20132680.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "recommend to users by computing a simple K-NN based similarity score among users\u2019 purchased paintings and\nthe paintings in the dataset, a method that Kang et al. ",
                    "Citation Text": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian McAuley. 2017. Visually-aware fashion recommendation and design with generative\nimage models. In 2017 IEEE International Conference on Data Mining (ICDM) . IEEE, 207\u2013216.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.02231",
                        "Citation Paper Title": "Title:Visually-Aware Fashion Recommendation and Design with Generative Image Models",
                        "Citation Paper Abstract": "Abstract:Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using `off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning `fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pre-trained visual features. Furthermore, we show that our model can be used \\emph{generatively}, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.",
                        "Citation Paper Authors": "Authors:Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ", among others. Motivated by these results, McAuley et al. ",
                    "Citation Text": "Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In\nProceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 43\u201352.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.04757",
                        "Citation Paper Title": "Title:Image-based Recommendations on Styles and Substitutes",
                        "Citation Paper Abstract": "Abstract:Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.",
                        "Citation Paper Authors": "Authors:Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.13929v1": {
            "Paper Title": "Towards Intelligent Risk-based Customer Segmentation in Banking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13836v1": {
            "Paper Title": "SIR: Similar Image Retrieval for Product Search in E-Commerce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13167v2": {
            "Paper Title": "Video Face Recognition System: RetinaFace-mnet-faster and Secondary\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04690v4": {
            "Paper Title": "FLEN: Leveraging Field for Scalable CTR Prediction",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "consider the\nimportance of different field feature interactions. Field-weighted\nFactorization Machines (FwFM) ",
                    "Citation Text": "Pan, J.; Xu, J.; Ruiz, A. L.; Zhao, W.; Pan, S.; Sun, Y.; and Lu, Q. 2018. Field-\nweighted factorization machines for click-through rate prediction in display\nadvertising. In Proceedings of the 2018 World Wide Web Conference on World\nWide Web , 1349\u20131357. International World Wide Web Conferences Steering\nCommittee.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03514",
                        "Citation Paper Title": "Title:Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is a critical task in online display advertising. The data involved in CTR prediction are typically multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. One of the interesting characteristics of such data is that features from one field often interact differently with features from different other fields. Recently, Field-aware Factorization Machines (FFMs) have been among the best performing models for CTR prediction by explicitly modeling such difference. However, the number of parameters in FFMs is in the order of feature number times field number, which is unacceptable in the real-world production systems. In this paper, we propose Field-weighted Factorization Machines (FwFMs) to model the different feature interactions between different fields in a much more memory-efficient way. Our experimental evaluations show that FwFMs can achieve competitive prediction performance with only as few as 4% parameters of FFMs. When using the same number of parameters, FwFMs can bring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.",
                        "Citation Paper Authors": "Authors:Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, Quan Lu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "de-\nveloped hierarchical importance-aware factorization machine to\nmodel dynamic impacts of ads.\nField information has been acknowledged as crucial in CTR pre-\ndiction. A number of recent work has exploited field information.\nFor example, Field-aware Factorization Machines (FFM) ",
                    "Citation Text": "Juan, Y.; Lefortier, D.; and Chapelle, O. 2017. Field-aware factorization machines\nin a real-world online advertising system. In Proceedings of the 26th International\nConference on World Wide Web Companion , 680\u2013688. International World Wide\nWeb Conferences Steering Committee.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.04099",
                        "Citation Paper Title": "Title:Field-aware Factorization Machines in a Real-world Online Advertising System",
                        "Citation Paper Abstract": "Abstract:Predicting user response is one of the core machine learning tasks in computational advertising. Field-aware Factorization Machines (FFM) have recently been established as a state-of-the-art method for that problem and in particular won two Kaggle challenges. This paper presents some results from implementing this method in a production system that predicts click-through and conversion rates for display advertising and shows that this method it is not only effective to win challenges but is also valuable in a real-world prediction system. We also discuss some specific challenges and solutions to reduce the training time, namely the use of an innovative seeding algorithm and a distributed learning mechanism.",
                        "Citation Paper Authors": "Authors:Yuchin Juan, Damien Lefortier, Olivier Chapelle"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "improves over DeepFM and DCN by generating fea-\nture interactions in an explicit fashion and at the field-wise level.\nNFFM ",
                    "Citation Text": "Yang, Y.; Xu, B.; Shen, F.; and Zhao, J. 2019. Operation-aware neural networks\nfor user response prediction. arXiv preprint arXiv:1904.12579 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12579",
                        "Citation Paper Title": "Title:Operation-aware Neural Networks for User Response Prediction",
                        "Citation Paper Abstract": "Abstract:User response prediction makes a crucial contribution to the rapid development of online advertising system and recommendation system. The importance of learning feature interactions has been emphasized by many works. Many deep models are proposed to automatically learn high-order feature interactions. Since most features in advertising system and recommendation system are high-dimensional sparse features, deep models usually learn a low-dimensional distributed representation for each feature in the bottom layer. Besides traditional fully-connected architectures, some new operations, such as convolutional operations and product operations, are proposed to learn feature interactions better. In these models, the representation is shared among different operations. However, the best representation for different operations may be different. In this paper, we propose a new neural model named Operation-aware Neural Networks (ONN) which learns different representations for different operations. Our experimental results on two large-scale real-world ad click/conversion datasets demonstrate that ONN consistently outperforms the state-of-the-art models in both offline-training environment and online-training environment.",
                        "Citation Paper Authors": "Authors:Yi Yang, Baile Xu, Furao Shen, Jian Zhao"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "generalizes FM by stack-\ning neural network layers on top of a bi-interaction pooling layer.\nThe architecture of DeepFM ",
                    "Citation Text": "Guo, H.; Tang, R.; Ye, Y.; Li, Z.; and He, X. 2017. Deepfm: a factorization-machine\nbased neural network for ctr prediction. arXiv preprint arXiv:1703.04247 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04247",
                        "Citation Paper Title": "Title:DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \\& Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
                        "Citation Paper Authors": "Authors:Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ",\nwhich also has a shared raw feature input to both its \"wide\" (i.e. for\nbi-interaction) and \"deep\" (i.e. for high-order interaction) compo-\nnents. DCN ",
                    "Citation Text": "Wang, R.; Fu, B.; Fu, G.; and Wang, M. 2017. Deep & cross network for ad click\npredictions. In Proceedings of the ADKDD\u201917 , 12. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05123",
                        "Citation Paper Title": "Title:Deep & Cross Network for Ad Click Predictions",
                        "Citation Paper Abstract": "Abstract:Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.",
                        "Citation Paper Authors": "Authors:Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "leverages the strength\nof CNN to generate local patterns and recombines them to generate\nnew features. Then deep classifier is built upon the augmented\nfeature space. PIN ",
                    "Citation Text": "Qu, Y.; Fang, B.; Zhang, W.; Tang, R.; Niu, M.; Guo, H.; Yu, Y.; and He, X. 2018.\nProduct-based neural networks for user response prediction over multi-field\ncategorical data. ACM Transactions on Information Systems (TOIS) 37(1):5.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00311",
                        "Citation Paper Title": "Title:Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data",
                        "Citation Paper Abstract": "Abstract:User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this paper, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network (PNN) which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network In Network (PIN) which can generalize previous models. Extensive experiments on 4 industrial datasets and 1 contest dataset demonstrate that our models consistently outperform 8 baselines on both AUC and log loss. Besides, PIN makes great CTR improvement (relatively 34.67%) in online A/B test.",
                        "Citation Paper Authors": "Authors:Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, Xiuqiang He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.12969v1": {
            "Paper Title": "Simultaneous Relevance and Diversity: A New Recommendation Inference\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12765v1": {
            "Paper Title": "Inductively Representing Out-of-Knowledge-Graph Entities by Optimal\n  Estimation Under Translational Assumptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07042v1": {
            "Paper Title": "Explainable Recommendations via Attentive Multi-Persona Collaborative\n  Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14261v1": {
            "Paper Title": "Abusive Language Detection and Characterization of Twitter Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12496v1": {
            "Paper Title": "Modeling Dyadic Conversations for Personality Inference",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ".\nVariety of contextual information has also been incorpo-\nrated in RNN models for different speci\ufb01c tasks. Ghosh et\nal. ",
                    "Citation Text": "S. Ghosh, O. Vinyals, B. Strope, S. Roy, T. Dean, and L. Heck,\u201cContextual lstm (clstm) models for large scale nlp tasks,\u201d arXiv\npreprint arXiv:1602.06291 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.06291",
                        "Citation Paper Title": "Title:Contextual LSTM (CLSTM) models for Large scale NLP tasks",
                        "Citation Paper Abstract": "Abstract:Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems.",
                        "Citation Paper Authors": "Authors:Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, Larry Heck"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.12468v1": {
            "Paper Title": "Investigating Misinformation in Online Marketplaces: An Audit Study on\n  Amazon",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12414v1": {
            "Paper Title": "Towards a Natural Language Query Processing System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06987v1": {
            "Paper Title": "Learning Representations of Hierarchical Slates in Collaborative\n  Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12316v1": {
            "Paper Title": "ML-based Visualization Recommendation: Learning to Recommend\n  Visualizations from Data",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "). While some of those are ML-based, none\nrecommends entire visualizations, and thus does not solve the vi-\nsualization recommendation problem that lies at the heart of our\nwork. For example, VizML ",
                    "Citation Text": "Kevin Hu, Michiel A Bakker, Stephen Li, Tim Kraska, and C\u00e9sar Hidalgo. 2019.\nVizml: A machine learning approach to visualization recommendation. In CHI\n\u201919. 1\u201312.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04819",
                        "Citation Paper Title": "Title:VizML: A Machine Learning Approach to Visualization Recommendation",
                        "Citation Paper Abstract": "Abstract:Data visualization should be accessible for all analysts with data, not just the few with technical expertise. Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other ML-based systems.",
                        "Citation Paper Authors": "Authors:Kevin Z. Hu, Michiel A. Bakker, Stephen Li, Tim Kraska, C\u00e9sar A. Hidalgo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.13292v1": {
            "Paper Title": "RecoBERT: A Catalog Language Model for Text-Based Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13294v1": {
            "Paper Title": "Virtual Proximity Citation (VCP): A Supervised Deep Learning Method to\n  Relate Uncited Papers On Grounds of Citation Proximity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08245v3": {
            "Paper Title": "Reductive Clustering: An Efficient Linear-time Graph-based Divisive\n  Cluster Analysis Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12097v1": {
            "Paper Title": "Parsisanj: a semi-automatic component-based approach towards search\n  engine evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13299v1": {
            "Paper Title": "Learning to Match Jobs with Resumes from Sparse Interaction Data using\n  Multi-View Co-Teaching Network",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "proposed to encode the\njob and resume based on CNN. Qin et al. ",
                    "Citation Text": "Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, and\nHui Xiong. 2018. Enhancing Person-Job Fit for Talent Recruitment: An Ability-\naware Neural Network Approach. In SIGIR 2018 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08947",
                        "Citation Paper Title": "Title:Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural Network Approach",
                        "Citation Paper Abstract": "Abstract:The wide spread use of online recruitment services has led to information explosion in the job market. As a result, the recruiters have to seek the intelligent ways for Person Job Fit, which is the bridge for adapting the right job seekers to the right positions. Existing studies on Person Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement. To this end, in this paper, we propose a novel end to end Ability aware Person Job Fit Neural Network model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results. The key idea is to exploit the rich information available at abundant historical job application data. Specifically, we propose a word level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network. Along this line, four hierarchical ability aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement. Finally, extensive experiments on a large scale real world data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baselines.",
                        "Citation Paper Authors": "Authors:Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, Hui Xiong"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nOur paper is inspired from a recent work called co-teaching\nnetwork (CTN) ",
                    "Citation Text": "Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W.\nTsang, and Masashi Sugiyama. 2018. Co-teaching: Robust training of deep neural\nnetworks with extremely noisy labels. In NeurIPS 2018 . 8536\u20138546.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.06872",
                        "Citation Paper Title": "Title:Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels",
                        "Citation Paper Abstract": "Abstract:Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.",
                        "Citation Paper Authors": "Authors:Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, Masashi Sugiyama"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". However, traditional GCN mainly deals with homoge-\nneous links, which cannot effectively characterize different types\nof links. Inspired by the recent progress made in knowledge graph\ncompletion ",
                    "Citation Text": "Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,\nIvan Titov, and Max Welling. 2018. Modeling Relational Data with Graph\nConvolutional Networks. In ESWC 2018 . 593\u2013607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06103",
                        "Citation Paper Title": "Title:Modeling Relational Data with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
                        "Citation Paper Authors": "Authors:Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The co-teaching network adopts a learning to\nteach strategy for dealing with noise ",
                    "Citation Text": "Jiazhan Feng, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao, and Rui\nYan. 2019. Learning a Matching Model with Co-teaching for Multi-turn Response\nSelection in Retrieval-based Dialogue Systems. In ACL 2019 . 3805\u20133815.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04413",
                        "Citation Paper Title": "Title:Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models.",
                        "Citation Paper Authors": "Authors:Jiazhan Feng, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao, Rui Yan"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ".\nIn parallel to the curriculum learning, several studies leverage\ndifferent weighting mechanisms to lower the impact of the noisy\ninstances ",
                    "Citation Text": "Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jian-Huang Lai, and Tie-Yan\nLiu. 2018. Learning to Teach with Dynamic Loss Functions. In NeurIPS 2018 .\n6467\u20136478.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.12081",
                        "Citation Paper Title": "Title:Learning to Teach with Dynamic Loss Functions",
                        "Citation Paper Abstract": "Abstract:Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as \"learning to teach with dynamic loss functions\" (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models.",
                        "Citation Paper Authors": "Authors:Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jianhuang Lai, Tie-Yan Liu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ", alleviate\nthis problem by reordering the training samples. Easy instances\nwill be learned before the harder ones. Following this idea, many\nvariants have been proposed, such as the methods based on\ndeep reinforcement learning ",
                    "Citation Text": "Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. 2018. Learning to\nTeach. In ICLR 2018 . 350\u2013357.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03643",
                        "Citation Paper Title": "Title:Learning to Teach",
                        "Citation Paper Abstract": "Abstract:Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",
                        "Citation Paper Authors": "Authors:Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, Tie-Yan Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.11771v1": {
            "Paper Title": "Scalable Recommendation of Wikipedia Articles to Editors Using\n  Representation Learning",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "is a recent approach to learn such a\nrepresentation. However, its scalability is still limited ",
                    "Citation Text": "Dongyan Zhou, Songjie Niu, and Shimin Chen. 2018. Efficient Graph Computation\nfor Node2Vec. CoRR abs/1805.00280 (2018). http://dblp.uni-trier.de/db/journals/\ncorr/corr1805.html#abs-1805-00280",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.00280",
                        "Citation Paper Title": "Title:Efficient Graph Computation for Node2Vec",
                        "Citation Paper Abstract": "Abstract:Node2Vec is a state-of-the-art general-purpose feature learning method for network analysis. However, current solutions cannot run Node2Vec on large-scale graphs with billions of vertices and edges, which are common in real-world applications. The existing distributed Node2Vec on Spark incurs significant space and time overhead. It runs out of memory even for mid-sized graphs with millions of vertices. Moreover, it considers at most 30 edges for every vertex in generating random walks, causing poor result quality. In this paper, we propose Fast-Node2Vec, a family of efficient Node2Vec random walk algorithms on a Pregel-like graph computation framework. Fast-Node2Vec computes transition probabilities during random walks to reduce memory space consumption and computation overhead for large-scale graphs. The Pregel-like scheme avoids space and time overhead of Spark's read-only RDD structures and shuffle operations. Moreover, we propose a number of optimization techniques to further reduce the computation overhead for popular vertices with large degrees. Empirical evaluation show that Fast-Node2Vec is capable of computing Node2Vec on graphs with billions of vertices and edges on a mid-sized machine cluster. Compared to Spark-Node2Vec, Fast-Node2Vec achieves 7.7--122x speedups.",
                        "Citation Paper Authors": "Authors:Dongyan Zhou, Songjie Niu, Shimin Chen"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "is method for obtaining content-based\nrepresentations of paragraph or longer text in vector space. How-\never, one main advantage of our dataset is the availability of struc-\ntural knowledge ",
                    "Citation Text": "Cristian Consonni, David Laniado, and Alberto Montresor. 2019. WikiLinkGraphs:\nA complete, longitudinal and multi-language dataset of the Wikipedia link net-\nworks. arXiv preprint arXiv:1902.04298 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.04298",
                        "Citation Paper Title": "Title:WikiLinkGraphs: A Complete, Longitudinal and Multi-Language Dataset of the Wikipedia Link Networks",
                        "Citation Paper Abstract": "Abstract:Wikipedia articles contain multiple links connecting a subject to other pages of the encyclopedia. In Wikipedia parlance, these links are called internal links or wikilinks. We present a complete dataset of the network of internal Wikipedia links for the $9$ largest language editions. The dataset contains yearly snapshots of the network and spans $17$ years, from the creation of Wikipedia in 2001 to March 1st, 2018. While previous work has mostly focused on the complete hyperlink graph which includes also links automatically generated by templates, we parsed each revision of each article to track links appearing in the main text. In this way we obtained a cleaner network, discarding more than half of the links and representing all and only the links intentionally added by editors. We describe in detail how the Wikipedia dumps have been processed and the challenges we have encountered, including the need to handle special pages such as redirects, i.e., alternative article titles. We present descriptive statistics of several snapshots of this network. Finally, we propose several research opportunities that can be explored using this new dataset.",
                        "Citation Paper Authors": "Authors:Cristian Consonni, David Laniado, Alberto Montresor"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "There are several projects trying to solve the task of recommending\nitems to users at real-world scales of millions of users and mil-\nlions of items. For instance, Ying et al. for Pinterest ",
                    "Citation Text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.11352v1": {
            "Paper Title": "ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue\n  Systems (ClariQ)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12212v3": {
            "Paper Title": "ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot\n  Retrieval of Images from Textual Descriptions",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". Samples from VAE models tend to be blurry\n(i.e., with less information content) as compared to GANs, because\nof the probability mass diffusion over the data space ",
                    "Citation Text": "L. Theis, A. van den Oord, and M. Bethge. 2016. A note on the evaluation of\ngenerative models. In Proc. ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.01844",
                        "Citation Paper Title": "Title:A note on the evaluation of generative models",
                        "Citation Paper Abstract": "Abstract:Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
                        "Citation Paper Authors": "Authors:Lucas Theis, A\u00e4ron van den Oord, Matthias Bethge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.08125v1": {
            "Paper Title": "Testing the Quantitative Spacetime Hypothesis using Artificial Narrative\n  Comprehension (II) : Establishing the Geometry of Invariant Concepts, Themes,\n  and Namespaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03297v2": {
            "Paper Title": "Knowledge Enhanced Neural Fashion Trend Forecasting",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "studied clothing trends by\nstatistical analysis, but based on a large-scale social media image\ndataset. Al-Halah et al. ",
                    "Citation Text": "Ziad Al-Halah, Rainer Stiefelhagen, and Kristen Grauman. 2017. Fashion for-\nward: Forecasting visual style in fashion. In Proceedings of the IEEE International\nConference on Computer Vision . 388\u2013397.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06394",
                        "Citation Paper Title": "Title:Fashion Forward: Forecasting Visual Style in Fashion",
                        "Citation Paper Abstract": "Abstract:What is the future of fashion? Tackling this question from a data-driven vision perspective, we propose to forecast visual style trends before they occur. We introduce the first approach to predict the future popularity of styles discovered from fashion images in an unsupervised manner. Using these styles as a basis, we train a forecasting model to represent their trends over time. The resulting model can hypothesize new mixtures of styles that will become popular in the future, discover style dynamics (trendy vs. classic), and name the key visual attributes that will dominate tomorrow's fashion. We demonstrate our idea applied to three datasets encapsulating 80,000 fashion products sold across six years on Amazon. Results indicate that fashion forecasting benefits greatly from visual analysis, much more than textual or meta-data cues surrounding products.",
                        "Citation Paper Authors": "Authors:Ziad Al-Halah, Rainer Stiefelhagen, Kristen Grauman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.10989v1": {
            "Paper Title": "Towards a Flexible Embedding Learning Framework",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ", addressed other aspects ofgraph embedding learning, such as ef\ufb01ciency. Recently, Qiu\net al. ",
                    "Citation Text": "J. Qiu, Y . Dong, H. Ma, J. Li, K. Wang, and J. Tang, \u201cNetwork\nembedding as matrix factorization: Unifying deepwalk, line, pte, and\nnode2vec,\u201d in WSDM . ACM, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.02971",
                        "Citation Paper Title": "Title:Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                        "Citation Paper Abstract": "Abstract:Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                        "Citation Paper Authors": "Authors:Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "extended the original word2vec\napproach to a semi-supervised learning setting that results in\na more substantial predictive power for a particular task, and\nELMo ",
                    "Citation Text": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d arXiv\npreprint , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.05928v2": {
            "Paper Title": "Recommendation with Attribute-aware Product Networks: A Representation\n  Learning Model",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". Another common\napproach is to employ random walks on the user-item bipartite\ngraph ",
                    "Citation Text": "David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C Ma,\nZhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related pins at pinterest:\nThe evolution of a real-world recommender system. In Proceedings of the 26th\nInternational Conference on World Wide Web Companion . International World\nWide Web Conferences Steering Committee, 583\u2013592.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.07969",
                        "Citation Paper Title": "Title:Related Pins at Pinterest: The Evolution of a Real-World Recommender System",
                        "Citation Paper Abstract": "Abstract:Related Pins is the Web-scale recommender system that powers over 40% of user engagement on Pinterest. This paper is a longitudinal study of three years of its development, exploring the evolution of the system and its components from prototypes to present state. Each component was originally built with many constraints on engineering effort and computational resources, so we prioritized the simplest and highest-leverage solutions. We show how organic growth led to a complex system and how we managed this complexity. Many challenges arose while building this system, such as avoiding feedback loops, evaluating performance, activating content, and eliminating legacy heuristics. Finally, we offer suggestions for tackling these challenges when engineering Web-scale recommender systems.",
                        "Citation Paper Authors": "Authors:David C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C. Ma, Zhigang Zhong, Jenny Liu, Yushi Jing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.03805v2": {
            "Paper Title": "ISA: An Intelligent Shopping Assistant",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10791v1": {
            "Paper Title": "Using the Hammer Only on Nails: A Hybrid Method for Evidence Retrieval\n  for Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10778v1": {
            "Paper Title": "On Data Augmentation for Extreme Multi-label Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10619v1": {
            "Paper Title": "An Exponential Factorization Machine with Percentage Error Minimization\n  to Retail Sales Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08114v2": {
            "Paper Title": "A Deep Learning Approach to Geographical Candidate Selection through\n  Toponym Matching",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "use a noise detector in their entity linking system that\noperates at the token level (e.g. \u2018Bill Clinton (President)\u2019 matching\n\u2018Presidency of Bill Clinton\u2019), which learns true matchings from\nlists of positive and negative candidate pairs. Tam et al . ",
                    "Citation Text": "Derek Tam, Nicholas Monath, Ari Kobren, Aaron Traylor, Rajarshi Das, and\nAndrew McCallum. 2019. Optimal transport-based alignment of learned character\nrepresentations for string similarity. arXiv preprint arXiv:1907.10165 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10165",
                        "Citation Paper Title": "Title:Optimal Transport-based Alignment of Learned Character Representations for String Similarity",
                        "Citation Paper Abstract": "Abstract:String similarity models are vital for record linkage, entity resolution, and search. In this work, we present STANCE --a learned model for computing the similarity of two strings. Our approach encodes the  characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. We evaluate STANCE's ability to detect whether two strings can refer to the same entity--a task we term alias detection. We construct five new alias detection datasets (and make them publicly available). We show that STANCE or one of its variants outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. We also demonstrate STANCE's ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in B^3 F1 over the previous state-of-the-art approach.",
                        "Citation Paper Authors": "Authors:Derek Tam, Nicholas Monath, Ari Kobren, Aaron Traylor, Rajarshi Das, Andrew McCallum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.10295v1": {
            "Paper Title": "Beyond Triplet Loss: Person Re-identification with Fine-grained\n  Difference-aware Pairwise Loss",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "3. These\nmethods are typically much more dif\ufb01cult to train and are\ncomputationally expensive than the other methods, because\n3DSA ",
                    "Citation Text": "Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \u201cDensely semantically aligned\nperson reid,\u201d in Computer Vision and Pattern Recognition (CVPR) , 2019,\npp. 667\u2013676.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08967",
                        "Citation Paper Title": "Title:Densely Semantically Aligned Person Re-Identification",
                        "Citation Paper Abstract": "Abstract:We propose a densely semantically aligned person re-identification framework. It fundamentally addresses the body misalignment problem caused by pose/viewpoint variations, imperfect person detection, occlusion, etc. By leveraging the estimation of the dense semantics of a person image, we construct a set of densely semantically aligned part images (DSAP-images), where the same spatial positions have the same semantics across different images. We design a two-stream network that consists of a main full image stream (MF-Stream) and a densely semantically-aligned guiding stream (DSAG-Stream). The DSAG-Stream, with the DSAP-images as input, acts as a regulator to guide the MF-Stream to learn densely semantically aligned features from the original image. In the inference, the DSAG-Stream is discarded and only the MF-Stream is needed, which makes the inference system computationally efficient and robust. To the best of our knowledge, we are the first to make use of fine grained semantics to address the misalignment problems for re-ID. Our method achieves rank-1 accuracy of 78.9% (new protocol) on the CUHK03 dataset, 90.4% on the CUHK01 dataset, and 95.7% on the Market1501 dataset, outperforming state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "85.9 94.5 76.4 86.4 58.2 60.5 60.2 62.1\nBaseline2 (BCL) 84.0 92.3 74.5 83.6 60.5 63.3 64.9 66.4\nBaseline2 (FIDI) 86.8 94.5 77.5 88.1 69.1 72.1 73.2 75.0\nStripingAlignedReID ",
                    "Citation Text": "X. Zhang, H. Luo, and X. Fan, \u201cAlignedreid: Surpassing human-\nlevel performance in person re-identi\ufb01cation,\u201d arXiv preprint\narXiv:1711.08184 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08184",
                        "Citation Paper Title": "Title:AlignedReID: Surpassing Human-Level Performance in Person Re-Identification",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.",
                        "Citation Paper Authors": "Authors:Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ",\nbut it is limited to speci\ufb01c network structures. Circle loss ",
                    "Citation Text": "Y . Sun, C. Cheng, Y . Zhang, C. Zhang, L. Zheng, Z. Wang, and Y . Wei,\n\u201cCircle loss: A uni\ufb01ed perspective of pair similarity optimization,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2020, pp. 6398\u20136407.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10857",
                        "Citation Paper Title": "Title:Circle Loss: A Unified Perspective of Pair Similarity Optimization",
                        "Citation Paper Abstract": "Abstract:This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.",
                        "Citation Paper Authors": "Authors:Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, Yichen Wei"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". The attention maps are then obtained by apply-\ning the Grad-CAM visualization method ",
                    "Citation Text": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, \u201cGrad-cam: Visual explanations from deep networks via\ngradient-based localization,\u201d in International Conference on Computer\nVision (ICCV) , 2017, pp. 618\u2013626.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02391",
                        "Citation Paper Title": "Title:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
                        "Citation Paper Abstract": "Abstract:We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.10270v1": {
            "Paper Title": "Embedding-based Zero-shot Retrieval through Query Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10128v1": {
            "Paper Title": "Claraprint: a chord and melody based fingerprint for western classical\n  music cover detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10002v1": {
            "Paper Title": "DGTN: Dual-channel Graph Transition Network for Session-based\n  Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "frequently appear in recent SRS models and obtain new\nstate-of-the-art performance in terms of accuracy, especially\nRNN-based methods ",
                    "Citation Text": "B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, \u201cSession-based\nrecommendations with recurrent neural networks,\u201d in ICLR , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06939",
                        "Citation Paper Title": "Title:Session-based Recommendations with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.",
                        "Citation Paper Authors": "Authors:Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". These early\nmethods mainly generate representations of target nodes by\nusing the recurrent neural unit to aggregate information of\nneighbor nodes. Inspired by the success of Convolutional\nNeural Network (CNN) in the image classi\ufb01cation task, Bruna\net al. ",
                    "Citation Text": "J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun, \u201cSpectral networks and\nlocally connected networks on graphs,\u201d arXiv preprint arXiv:1312.6203 ,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6203",
                        "Citation Paper Title": "Title:Spectral Networks and Locally Connected Networks on Graphs",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
                        "Citation Paper Authors": "Authors:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "employ\nRNN with the Gated Recurrent Unit (GRU) into SRS and\noutperform traditional methods. Tan et al. ",
                    "Citation Text": "Y . K. Tan, X. Xu, and Y . Liu, \u201cImproved recurrent neural networks for\nsession-based recommendations,\u201d in DLRS , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.08117",
                        "Citation Paper Title": "Title:Improved Recurrent Neural Networks for Session-based Recommendations",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.",
                        "Citation Paper Authors": "Authors:Yong Kiam Tan, Xinxing Xu, Yong Liu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "propose a\nhybrid method taking account of the combination of Matrix\nFactorization and MC, namely FPMC.\nLike most other \ufb01elds ",
                    "Citation Text": "Z. Li, Z. Cui, S. Wu, X. Zhang, and L. Wang, \u201cFi-gnn: Modeling feature\ninteractions via graph neural networks for ctr prediction,\u201d in CIKM ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.05552",
                        "Citation Paper Title": "Title:Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction",
                        "Citation Paper Abstract": "Abstract:Click-through rate (CTR) prediction is an essential task in web applications such as online advertising and recommender systems, whose features are usually in multi-field form. The key of this task is to model feature interactions among different feature fields. Recently proposed deep learning based models follow a general paradigm: raw sparse input multi-filed features are first mapped into dense field embedding vectors, and then simply concatenated together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. However, the simple \\emph{unstructured combination} of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.\nIn this work, we propose to represent the multi-field features in a graph structure intuitively, where each node corresponds to a feature field and different fields can interact through edges. The task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph. To this end, we design a novel model Feature Interaction Graph Neural Networks (Fi-GNN). Taking advantage of the strong representative power of graphs, our proposed model can not only model sophisticated feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction. Experimental results on two real-world datasets show its superiority over the state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, Liang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.09671v1": {
            "Paper Title": "Towards application-specific query processing systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09588v1": {
            "Paper Title": "div2vec: Diversity-Emphasized Node Embedding",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ", that maximizes the co-occurrence probability among the words near the target word.Inspired byword2vec, Perozzi et al. ",
                    "Citation Text": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learn-ing of Social Representations. InProceedings of the 20th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining(New York, New York,USA)(KDD \u201914). Association for Computing Machinery, New York, NY, USA,701\u2013710. https://doi.org/10.1145/2623330.2623732",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "2.1 Random walk-based node embeddingsThe famousword2vecmethod transforms words into embeddingvectors such that similar words have similar embeddings. It usesa language model, called Skip-gram ",
                    "Citation Text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Je\uffffrey Dean. 2013. E\uffffcientEstimation of Word Representations in Vector Space. In1st International Con-ference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May2-4, 2013, Workshop Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).http://arxiv.org/abs/1301.3781",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2005.12668v3": {
            "Paper Title": "SciSight: Combining faceted navigation and research group detection for\n  COVID-19 exploratory scientific search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09290v1": {
            "Paper Title": "Can questions summarize a corpus? Using question generation for\n  characterizing COVID-19 research",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "to extract the most similar questions to the reference question. It uses contextual embeddings from\nBERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "that has been \ufb01ne-tuned on the\nMS MARCO passage ranking dataset ",
                    "Citation Text": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\nMcNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.\nMS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268v3 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.09268",
                        "Citation Paper Title": "Title:MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
                        "Citation Paper Abstract": "Abstract:We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.",
                        "Citation Paper Authors": "Authors:Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, Tong Wang"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". Some document representations also\nhave low explainability, making it hard to know why a pair of documents is considered similar.\n2.3 Question Generation\nExtractive question answering is a widely studied task in NLP, with the SQuAD dataset ",
                    "Citation Text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\ncomprehension of text. arXiv preprint arXiv:1606.05250 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Others combine\ntopic models with visualization tools like sankey charts to make generations more friendly to users ",
                    "Citation Text": "Karin Verspoor, Simon \u0160uster, Yulia Otmakhova, Shevon Mendis, Zenan Zhai, Biaoyan Fang, Jey Han Lau,\nTimothy Baldwin, Antonio Jimeno Yepes, and David Martinez. Covid-see: Scienti\ufb01c evidence explorer for\ncovid-19 related research. arXiv preprint arXiv:2008.07880 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07880",
                        "Citation Paper Title": "Title:COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research",
                        "Citation Paper Abstract": "Abstract:We present COVID-SEE, a system for medical literature discovery based on the concept of information exploration, which builds on several distinct text analysis and natural language processing methods to structure and organise information in publications, and augments search by providing a visual overview supporting exploration of a collection to identify key articles of interest. We developed this system over COVID-19 literature to help medical professionals and researchers explore the literature evidence, and improve findability of relevant information. COVID-SEE is available at this http URL.",
                        "Citation Paper Authors": "Authors:Karin Verspoor, Simon \u0160uster, Yulia Otmakhova, Shevon Mendis, Zenan Zhai, Biaoyan Fang, Jey Han Lau, Timothy Baldwin, Antonio Jimeno Yepes, David Martinez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.09259v1": {
            "Paper Title": "Bid Shading by Win-Rate Estimation and Surplus Maximization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01846v2": {
            "Paper Title": "Geosocial Location Classification: Associating Type to Places Based on\n  Geotagged Social-Media Posts",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "we averaged the embeddings\nof the words in each message. For comparison, we also implemented\nthis approach where each tweet is represented by the sentence em-\nbedding method of ",
                    "Citation Text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c Barrault, and Antoine\nBordes. 2017. Supervised Learning of Universal Sentence Representations from\nNatural Language Inference Data. In Proceedings of the 2017 Conference on Empir-\nical Methods in Natural Language Processing . Association for Computational Lin-\nguistics, Copenhagen, Denmark, 670\u2013680. h/t_tps://www.aclweb.org/anthology/\nD17-1070",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.03869v4": {
            "Paper Title": "Knowledge Guided Named Entity Recognition for BioMedical Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08621v1": {
            "Paper Title": "A Knowledge Graph based Approach for Mobile Application Recommendation",
            "Sentences": []
        }
    }
}