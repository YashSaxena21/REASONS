{
    "Natural Language Processing": {
        "http://arxiv.org/abs/1808.09401v2": {
            "Paper Title": "Temporal Information Extraction by Predicting Relative Time-lines",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.07722v3": {
            "Paper Title": "Stylized innovation: generating timelines by interrogating incrementally\n  available randomised dictionaries",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "Y . Liu, C. Mathis, M. D. Bajczyk, S. M. Marshall, L. Wilbraham,\nand L. Cronin,\nExploring and mapping chemical space with molecular assembly\ntrees,\nScience Advances 7, eabj2465 (2021). ",
                    "Citation Text": "V . Sood, M. Mathieu, A. Shreim, P. Grassberger, and\nM. Paczuski,\nInteracting branching process as a simple model of innovation,\nPhys. Rev. Lett. 105, 178701 (2010).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.5797",
                        "Citation Paper Title": "Title:The Interacting Branching Process as a Simple Model of Innovation",
                        "Citation Paper Abstract": "Abstract:We describe innovation in terms of a generalized branching process. Each new invention pairs with any existing one to produce a number of offspring, which is Poisson distributed with mean p. Existing inventions die with probability p/\\tau at each generation. In contrast to mean field results, no phase transition occurs; the chance for survival is finite for all p > 0. For \\tau = \\infty, surviving processes exhibit a bottleneck before exploding super-exponentially - a growth consistent with a law of accelerating returns. This behavior persists for finite \\tau. We analyze, in detail, the asymptotic behavior as p \\to 0.",
                        "Citation Paper Authors": "Authors:Vishal Sood, Myl\u00e9ne Mathieu, Amer Shreim, Peter Grassberger, Maya Paczuski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.03325v5": {
            "Paper Title": "Marshall-Olkin Power-Law Distributions in Length-Frequency of Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.03762v7": {
            "Paper Title": "Attention Is All You Need",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) ",
                    "Citation Text": "Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06114",
                        "Citation Paper Title": "Title:Multi-task Sequence to Sequence Learning",
                        "Citation Paper Abstract": "Abstract:Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "around each of\nthe two sub-layers, followed by layer normalization ",
                    "Citation Text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06450",
                        "Citation Paper Title": "Title:Layer Normalization",
                        "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                        "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.08316v4": {
            "Paper Title": "A Trio Neural Model for Dynamic Entity Relatedness Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.09667v3": {
            "Paper Title": "Video Captioning with Guidance of Multimodal Latent Topics",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "propose the tempo-\nral aention mechanism, and Pan et al. ",
                    "Citation Text": "Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting Zhuang. 2016. Hier-\narchical recurrent neural encoder for video representation with application to\ncaptioning. In CVPR . 1029\u20131038.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.03476",
                        "Citation Paper Title": "Title:Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning",
                        "Citation Paper Abstract": "Abstract:Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e., it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks. Notably, even using a single network with only RGB stream as input, HRNE beats all the recent systems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.",
                        "Citation Paper Authors": "Authors:Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "41.40 28.30 48.90\nYoutube\n2TextM&M TGM 48.76 34.36 80.45\nLSTM-YT ",
                    "Citation Text": "Subhashini Venugopalan, Huijuan Xu, Je Donahue, Marcus Rohrbach, Raymond\nMooney, and Kate Saenko. 2014. Translating Videos to Natural Language Using\nDeep Recurrent Neural Networks. Computer Science (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.4729",
                        "Citation Paper Title": "Title:Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.",
                        "Citation Paper Authors": "Authors:Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "utilize the textual mined topics tolearn interpretable features. Shen et al. ",
                    "Citation Text": "Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, and\nXiangyang Xue. 2017. Weakly Supervised Dense Video Captioning. In CVPR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01502",
                        "Citation Paper Title": "Title:Weakly Supervised Dense Video Captioning",
                        "Citation Paper Abstract": "Abstract:This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.",
                        "Citation Paper Authors": "Authors:Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "propose the spa-\ntial aention mechanism based on the basic encoder-decoder. You\net al. ",
                    "Citation Text": "anzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016.\nImage captioning with semantic aention. arXiv:1603.03925 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.03925",
                        "Citation Paper Title": "Title:Image Captioning with Semantic Attention",
                        "Citation Paper Abstract": "Abstract:Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
                        "Citation Paper Authors": "Authors:Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "(LSTM), is utilized to generate sequential words\nconditioned on image features ",
                    "Citation Text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show\nand tell: A neural image caption generator. In CVPR . 3156\u20133164.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4555",
                        "Citation Paper Title": "Title:Show and Tell: A Neural Image Caption Generator",
                        "Citation Paper Abstract": "Abstract:Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                        "Citation Paper Authors": "Authors:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.04816v3": {
            "Paper Title": "Jointly Identifying and Fixing Inconsistent Readings from Information\n  Extraction Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.08798v2": {
            "Paper Title": "Word Affect Intensities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09920v4": {
            "Paper Title": "Question Answering by Reasoning Across Documents with Graph\n  Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.00054v5": {
            "Paper Title": "Modeling Task Effects in Human Reading with Neural Network-based\n  Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.02134v8": {
            "Paper Title": "Adposition and Case Supersenses v2.6: Guidelines for English",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.08748v2": {
            "Paper Title": "LEPOR: An Augmented Machine Translation Evaluation Metric",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08438v3": {
            "Paper Title": "Paraphrases as Foreign Languages in Multilingual Neural Machine\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.02094v2": {
            "Paper Title": "Exploring Hyper-Parameter Optimization for Neural Machine Translation on\n  GPU Architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.06879v3": {
            "Paper Title": "Deep Keyphrase Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10424v4": {
            "Paper Title": "Measuring Societal Biases from Text Corpora with Smoothed First-Order\n  Co-occurrence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.01483v2": {
            "Paper Title": "JTAV: Jointly Learning Social Media Content Representation by Fusing\n  Textual, Acoustic, and Visual Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.03827v2": {
            "Paper Title": "Face2Text: Collecting an Annotated Image Description Corpus for the\n  Generation of Rich Face Descriptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.02772v2": {
            "Paper Title": "Effective Character-augmented Word Embedding for Machine Reading\n  Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07737v2": {
            "Paper Title": "SentiPers: A Sentiment Analysis Corpus for Persian",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.04148v2": {
            "Paper Title": "JeSemE: A Website for Exploring Diachronic Changes in Word Meaning and\n  Emotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04458v3": {
            "Paper Title": "Sparse Stochastic Zeroth-Order Optimization with an Application to\n  Bandit Structured Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.10467v5": {
            "Paper Title": "Generalized End-to-End Loss for Speaker Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.08493v6": {
            "Paper Title": "Contextual Salience for Fast and Accurate Sentence Vectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.10547v3": {
            "Paper Title": "Neural Network Architecture for Credibility Assessment of Textual Claims",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01399v2": {
            "Paper Title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge\n  Graph Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06280v4": {
            "Paper Title": "Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the\n  Embeddings of Words and Entities from Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02303v4": {
            "Paper Title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "(https://github.com/JafferWilson/Process-\nData-of-CNN-DailyMail).\n5.2.2 Newsroom Dataset. The Cornell Newsroom dataset (https://summari.es/) ",
                    "Citation Text": "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A Dataset of 1.3 Million Summaries with Diverse\nExtractive Strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , Vol. 1. 708\u2013719.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.11283",
                        "Citation Paper Title": "Title:Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
                        "Citation Paper Abstract": "Abstract:We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.",
                        "Citation Paper Authors": "Authors:Max Grusky, Mor Naaman, Yoav Artzi"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": ". It has also been demonstrated to be effective in\nimproving the performance of seq2seq models for the task of abstractive text summarization ",
                    "Citation Text": "Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04304",
                        "Citation Paper Title": "Title:A Deep Reinforced Model for Abstractive Summarization",
                        "Citation Paper Abstract": "Abstract:Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
                        "Citation Paper Authors": "Authors:Romain Paulus, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": ". In practice, the gradient\nwith the baseline is approximated with\n\u2207\u03b8L\u03b8\u2248\u2212(R(y\u03c0\n<T+1)\u2212b)T\u00d5\nt=1\u2207\u03b8logP\u03b8(y\u03c0\nt|y\u03c0\n<t,x) (39)\nBetter ways of sampling a sequence and different approaches to calculate the baseline can be found\nin [63, 114, 148, 152].\n3.2.2 MIXER ",
                    "Citation Text": "Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with\nrecurrent neural networks. arXiv preprint arXiv:1511.06732 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06732",
                        "Citation Paper Title": "Title:Sequence Level Training with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
                        "Citation Paper Authors": "Authors:Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10233v3": {
            "Paper Title": "An Investigation of Few-Shot Learning in Spoken Term Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11057v3": {
            "Paper Title": "NMT-based Cross-lingual Document Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08193v2": {
            "Paper Title": "Towards Automated Factchecking: Developing an Annotation Schema and\n  Benchmark for Consistent Automated Claim Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06297v2": {
            "Paper Title": "Adversarial Text Generation via Feature-Mover's Distance",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "uses GANs to tackle the task of unsupervised cipher cracking, utilizing the frame-\nwork of CycleGAN ",
                    "Citation Text": "J. Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10593",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                        "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". Moreover, our model even\noutperforms the controllable text generation method ",
                    "Citation Text": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of\ntext. In ICML , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00955",
                        "Citation Paper Title": "Title:Toward Controlled Generation of Text",
                        "Citation Paper Abstract": "Abstract:Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
                        "Citation Paper Authors": "Authors:Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10411v2": {
            "Paper Title": "Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.06353v4": {
            "Paper Title": "Transfer Learning for Improving Speech Emotion Classification Accuracy",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nApart from DNNs, researchers have also used interesting\ndeep architectures for transfer learning. In ",
                    "Citation Text": "J. Gideon, S. Khorram, Z. Aldeneh, D. Dimitriadis, and E . M.\nProvost, \u201cProgressive neural networks for transfer learni ng in\nemotion recognition,\u201d arXiv preprint arXiv:1706.03256 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03256",
                        "Citation Paper Title": "Title:Progressive Neural Networks for Transfer Learning in Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.",
                        "Citation Paper Authors": "Authors:John Gideon, Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitriadis, Emily Mower Provost"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.10044v2": {
            "Paper Title": "No One is Perfect: Analysing the Performance of Question Answering\n  Components over the DBpedia Knowledge Graph",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "is another recently released datasetthat contains 5,000 questions for DBpedia and has been used for\nevaluating question answering systems ",
                    "Citation Text": "Dennis Diefenbach, Andreas Both, Kamal Singh, and Pierre Maret. 2018. To-\nwards a Question Answering System over the Semantic Web. arXiv preprint\narXiv:1803.00832 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.00832",
                        "Citation Paper Title": "Title:Towards a Question Answering System over the Semantic Web",
                        "Citation Paper Abstract": "Abstract:Thanks to the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of Question Answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provided a conceptional solution for multilingual, KB-agnostic Question Answering over the Semantic Web. The provided first approximation validates this concept.",
                        "Citation Paper Authors": "Authors:Dennis Diefenbach, Andreas Both, Kamal Singh, Pierre Maret"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1805.03616v3": {
            "Paper Title": "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for\n  Abstractive Text Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01316v3": {
            "Paper Title": "Learning User Preferences and Understanding Calendar Contexts for Event\n  Scheduling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03712v3": {
            "Paper Title": "Unsupervised Keyphrase Extraction from Scientific Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06542v2": {
            "Paper Title": "Extract with Order for Coherent Multi-Document Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09364v4": {
            "Paper Title": "Learning pronunciation from a foreign language in speech synthesis\n  networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04640v2": {
            "Paper Title": "Jump to better conclusions: SCAN both left and right",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.04675v4": {
            "Paper Title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07311v3": {
            "Paper Title": "Early Detection of Social Media Hoaxes at Scale",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ", among others. As a state-of-the-art approach for semantic word\nrepresentation, here we make use of word embeddings ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing systems . 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.08666v2": {
            "Paper Title": "Using Aspect Extraction Approaches to Generate Review Summaries and User\n  Profiles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.07745v6": {
            "Paper Title": "Abstractive Text Classification Using Sequence-to-convolution Neural\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "and processed the summary with crop&pad . Same method was used to generate\nsummary example used in training Sequential Block.\n4.1 Datasets\nWe evaluated our model on three different datasets: AG\u2019s News, DBPedia, and Yahoo Answers ",
                    "Citation Text": "Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for\ntext classi\ufb01cation. CoRR , abs/1509.01626, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.01626",
                        "Citation Paper Title": "Title:Character-level Convolutional Networks for Text Classification",
                        "Citation Paper Abstract": "Abstract:This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
                        "Citation Paper Authors": "Authors:Xiang Zhang, Junbo Zhao, Yann LeCun"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ". Convolution Block can be replaced\nwith other text classi\ufb01cation models such as C-LSTM ",
                    "Citation Text": "Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Francis C. M. Lau. A C-LSTM neural network\nfor text classi\ufb01cation. CoRR , abs/1511.08630, 2015.\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.08630",
                        "Citation Paper Title": "Title:A C-LSTM Neural Network for Text Classification",
                        "Citation Paper Abstract": "Abstract:Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.",
                        "Citation Paper Authors": "Authors:Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C.M. Lau"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "who were the \ufb01rst to use Seq2seq RNNs and Attention model for\nabstractive text summarization. Our Tensor\ufb02ow ",
                    "Citation Text": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\nManjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\nscale machine learning on heterogeneous systems, 2015. Software available from tensor\ufb02ow.org.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "with gradient norm limited to 5. We\nset loss balancing weight \u0015to 1.0 when training AG\u2019s News and 2.0 for other datasets.\nWe initialized convolution layers using He Normal ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. CoRR , abs/1502.01852, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.01852",
                        "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                        "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "to improve the performance of our Sequential Block.\nOur approach to use Attentional Encoder-Decoder RNNs for abstractive summarization is closely\nrelated to Nallapati et al. ",
                    "Citation Text": "Ramesh Nallapati, Bing Xiang, and Bowen Zhou. Sequence-to-sequence rnns for text summa-\nrization. CoRR , abs/1602.06023, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.06023",
                        "Citation Paper Title": "Title:Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
                        "Citation Paper Abstract": "Abstract:In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",
                        "Citation Paper Authors": "Authors:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.11283v2": {
            "Paper Title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive\n  Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12354v7": {
            "Paper Title": "Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05241v4": {
            "Paper Title": "One Size Does Not Fit All: Generating and Evaluating Variable Number of\n  Keyphrases",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09000v4": {
            "Paper Title": "The division of labor in communication: Speakers help listeners account\n  for asymmetries in visual perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00056v2": {
            "Paper Title": "Entity Synonym Discovery via Multipiece Bilateral Context Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.04488v3": {
            "Paper Title": "Concept2vec: Metrics for Evaluating Quality of Embeddings for\n  Ontological Concepts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07282v3": {
            "Paper Title": "Modeling Online Discourse with Coupled Distributed Topics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10126v7": {
            "Paper Title": "Area Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00586v2": {
            "Paper Title": "Multilingual Embeddings Jointly Induced from Contexts and Concepts:\n  Simple, Strong and Scalable",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.06898v3": {
            "Paper Title": "Neural Automated Essay Scoring and Coherence Modeling for Adversarially\n  Crafted Input",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00945v2": {
            "Paper Title": "Image Chat: Engaging Grounded Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09617v4": {
            "Paper Title": "Exploiting Cross-Lingual Subword Similarities in Low-Resource Document\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08037v3": {
            "Paper Title": "Understanding Convolutional Neural Networks for Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03105v3": {
            "Paper Title": "Location Name Extraction from Targeted Text Streams using\n  Gazetteer-based Statistical Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09751v2": {
            "Paper Title": "A Practical Incremental Learning Framework For Sparse Entity Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00278v3": {
            "Paper Title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\n  Task-Oriented Dialogue Modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09092v3": {
            "Paper Title": "Disfluency Detection using Auto-Correlational Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07832v2": {
            "Paper Title": "LSTM-based Whisper Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.00696v3": {
            "Paper Title": "NLP-assisted software testing: A systematic mapping of the literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06858v2": {
            "Paper Title": "FRAGE: Frequency-Agnostic Word Representation",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "as the baseline model, which\nachieves state-of-the-art accuracy on multiple translation datasets. We use transformer_base and\ntransformer_big con\ufb01gurations following tensor2tensor ",
                    "Citation Text": "A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser,\nN. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural\nmachine translation. CoRR , abs/1803.07416, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07416",
                        "Citation Paper Title": "Title:Tensor2Tensor for Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, Jakob Uszkoreit"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "develops a novel way to split word into sub-word units\nwhich is widely used in neural machine translation. However, the low-frequency sub-word units are\nstill dif\ufb01cult to train: ",
                    "Citation Text": "M. Ott, M. Auli, D. Granger, and M. Ranzato. Analyzing uncertainty in neural machine\ntranslation. arXiv preprint arXiv:1803.00047 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.00047",
                        "Citation Paper Title": "Title:Analyzing Uncertainty in Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:Machine translation is a popular test bed for research in neural sequence-to-sequence models but despite much recent research, there is still a lack of understanding of these models. Practitioners report performance degradation with large beams, the under-estimation of rare words and a lack of diversity in the final translations. Our study relates some of these issues to the inherent uncertainty of the task, due to the existence of multiple valid translations for a single source sentence, and to the extrinsic uncertainty caused by noisy training data. We propose tools and metrics to assess how uncertainty in the data is captured by the model distribution and how it affects search strategies that generate translations. Our results show that search works remarkably well but that models tend to spread too much probability mass over the hypothesis space. Next, we propose tools to assess model calibration and show how to easily fix some shortcomings of current models. As part of this study, we release multiple human reference translations for two popular benchmarks.",
                        "Citation Paper Authors": "Authors:Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.11559v4": {
            "Paper Title": "Variational Self-attention Model for Sentence Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.06821v3": {
            "Paper Title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.07632v4": {
            "Paper Title": "Generating Thematic Chinese Poetry using Conditional Variational\n  Autoencoders with Hybrid Decoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00770v2": {
            "Paper Title": "A Survey on Natural Language Processing for Fake News Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09663v6": {
            "Paper Title": "Context Mover's Distance & Barycenters: Optimal Transport of Contexts\n  for Building Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05296v5": {
            "Paper Title": "Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07809v2": {
            "Paper Title": "Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03746v2": {
            "Paper Title": "A Structured Variational Autoencoder for Contextual Morphological\n  Inflection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03743v2": {
            "Paper Title": "Are All Languages Equally Hard to Language-Model?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03740v2": {
            "Paper Title": "Unsupervised Disambiguation of Syncretism in Inflected Lexicons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08205v4": {
            "Paper Title": "Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02549v6": {
            "Paper Title": "Language GANs Falling Short",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05710v5": {
            "Paper Title": "FPETS : Fully Parallel End-to-End Text-to-Speech System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08909v2": {
            "Paper Title": "Language Identification with Deep Bottleneck Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05886v2": {
            "Paper Title": "Meta-Embedding as Auxiliary Task Regularization",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "used this method to obtain\ngood performance on PoS tagging and Named Entity Recognition\n(NER) using a language model that predicts a target word given its\ncontext words. Similarly, Collobert et al. ",
                    "Citation Text": "Ronan Collobert, Jason Weston, L \u00b4eon Bottou, Michael Karlen, Koray\nKavukcuoglu, and Pavel Kuksa, \u2018Natural language processing (almost)\nfrom scratch\u2019, Journal of Machine Learning Research ,12(Aug), 2493\u2013\n2537, (2011).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.0398",
                        "Citation Paper Title": "Title:Natural Language Processing (almost) from Scratch",
                        "Citation Paper Abstract": "Abstract:We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
                        "Citation Paper Authors": "Authors:Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "have focused on \ufb01nding a linear transformation between count-\nbased and prediction-based embeddings, showing that linearly trans-\nformed count-based embeddings can be used for predictions in the\nlocalized neighborhoods in the target space. Kiela at al. ",
                    "Citation Text": "Douwe Kiela, Changhan Wang, and Kyunghyun Cho, \u2018Dynamic meta-\nembeddings for improved sentence representations\u2019, in Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Pro-\ncessing , pp. 1466\u20131477, Brussels, Belgium, (October-November 2018).\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07983",
                        "Citation Paper Title": "Title:Dynamic Meta-Embeddings for Improved Sentence Representations",
                        "Citation Paper Abstract": "Abstract:While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.",
                        "Citation Paper Authors": "Authors:Douwe Kiela, Changhan Wang, Kyunghyun Cho"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "use of a projection layer for meta-embedding (known\nas 1TON) optimized using an `2-based loss. Similarly, Bollegala et\nal. ",
                    "Citation Text": "Danushka Bollegala, Koheu Hayashi, and Ken-ichi Kawarabayashi,\n\u2018Think globally, embed locally \u2014 locally linear meta-embedding of\nwords\u2019, in Proc. of IJCAI-EACI , (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06671",
                        "Citation Paper Title": "Title:Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words",
                        "Citation Paper Abstract": "Abstract:Distributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks. However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete \\emph{meta-embeddings} of words. For this purpose, we propose an unsupervised locally linear meta-embedding learning method that takes pre-trained word embeddings as the input, and produces more accurate meta embeddings. Unlike previously proposed meta-embedding learning methods that learn a global projection over all words in a vocabulary, our proposed method is sensitive to the differences in local neighbourhoods of the individual source word embeddings. Moreover, we show that vector concatenation, a previously proposed highly competitive baseline approach for integrating word embeddings, can be derived as a special case of the proposed method. Experimental results on semantic similarity, word analogy, relation classification, and short-text classification tasks show that our meta-embeddings to significantly outperform prior methods in several benchmark datasets, establishing a new state of the art for meta-embeddings.",
                        "Citation Paper Authors": "Authors:Danushka Bollegala, Kohei Hayashi, Ken-ichi Kawarabayashi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.05288v4": {
            "Paper Title": "Dynamic Transfer Learning for Named Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.09157v3": {
            "Paper Title": "Cross-lingual, Character-Level Neural Morphological Tagging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.09360v3": {
            "Paper Title": "Learning of Colors from Color Names: Distribution and Point Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.00064v3": {
            "Paper Title": "Generalizing Procrustes Analysis for Better Bilingual Dictionary\n  Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04208v3": {
            "Paper Title": "Neural Semi-Markov Conditional Random Fields for Robust Character-Based\n  Part-of-Speech Tagging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.10854v3": {
            "Paper Title": "A Survey of the Usages of Deep Learning in Natural Language Processing",
            "Sentences": [
                {
                    "Sentence ID": 267,
                    "Sentence": ", and a number of other important topics surrounding\nneural machine translation. Finally, a new branch of machine\ntranslation has been opened up by groundbreaking research:\nmultilingual translation.\nA fairly recent study ",
                    "Citation Text": "M. Johnson, M. Schuster, Q. V . Le, M. Krikun, Y . Wu, Z. Chen,\nN. Thorat, F. Vi \u00b4egas, M. Wattenberg, G. Corrado etal., \u201cGoogle\u2019s\nmultilingual neural machine translation system: enabling zero-shot\ntranslation,\u201d arXiv preprint arXiv:1611.04558, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04558",
                        "Citation Paper Title": "Title:Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
                        "Citation Paper Abstract": "Abstract:We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.",
                        "Citation Paper Authors": "Authors:Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 264,
                    "Sentence": ", and the ability to translate to and from\nunderstudied languages, such as those that are polysynthetic ",
                    "Citation Text": "M. Mager, E. Mager, A. Medina-Urrea, I. Meza, and K. Kann, \u201cLost\nin translation: Analysis of information loss during machine transla-\ntion between polysynthetic and fusional languages,\u201d arXiv preprint\narXiv:1807.00286, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00286",
                        "Citation Paper Title": "Title:Lost in Translation: Analysis of Information Loss During Machine Translation Between Polysynthetic and Fusional Languages",
                        "Citation Paper Abstract": "Abstract:Machine translation from polysynthetic to fusional languages is a challenging task, which gets further complicated by the limited amount of parallel text available. Thus, translation performance is far from the state of the art for high-resource and more intensively studied language pairs. To shed light on the phenomena which hamper automatic translation to and from polysynthetic languages, we study translations from three low-resource, polysynthetic languages (Nahuatl, Wixarika and Yorem Nokki) into Spanish and vice versa. Doing so, we find that in a morpheme-to-morpheme alignment an important amount of information contained in polysynthetic morphemes has no Spanish counterpart, and its translation is often omitted. We further conduct a qualitative analysis and, thus, identify morpheme types that are commonly hard to align or ignored in the translation process.",
                        "Citation Paper Authors": "Authors:Manuel Mager, Elisabeth Mager, Alfonso Medina-Urrea, Ivan Meza, Katharina Kann"
                    }
                },
                {
                    "Sentence ID": 258,
                    "Sentence": "attained state-of-the-art results, performing unsupervised\nmachine translation using multiple languages in their language\nmodel pretraining.\nSeveral of the recent state-of-the-art models were examined\nby Chen et al. ",
                    "Citation Text": "M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster,\nL. Jones, N. Parmar, M. Schuster, Z. Chen etal., \u201cThe best of both\nworlds: Combining recent advances in neural machine translation,\u201d\narXiv preprint arXiv:1804.09849, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09849",
                        "Citation Paper Title": "Title:The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
                        "Citation Paper Authors": "Authors:Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, Macduff Hughes"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nIn a direction diverging from previous work, Vaswani et al. ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nNIPS, 2017, pp. 6000\u20136010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "and numerous\nnovel and effective advances to this model have since been\nmade ",
                    "Citation Text": "D. Bahdanau, K. Cho, and Y . Bengio, \u201cNeural machine translation by\njointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 246,
                    "Sentence": ". Encoder-decoder models have continuously\nde\ufb01ned the state of the art, being expanded to contain dozens\nof layers, with residual connections, attention mechanisms,\nand even residual attention mechanisms allowing the \ufb01nal\ndecoding layer to attend to the \ufb01rst encoding layer ",
                    "Citation Text": "Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey etal., \u201cGoogle\u2019s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,\u201d arXiv preprint arXiv:1609.08144, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08144",
                        "Citation Paper Title": "Title:Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
                        "Citation Paper Authors": "Authors:Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 239,
                    "Sentence": "achieved state of theart in QA experiments\non SQuAD 1.1 and SQuAD 2.0 datasets. Yang et al. ",
                    "Citation Text": "W. Yang, Y . Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li, and J. Lin,\n\u201cEnd-to-end open-domain question answering with bertserini,\u201d arXiv\npreprint arXiv:1902.01718, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01718",
                        "Citation Paper Title": "Title:End-to-End Open-Domain Question Answering with BERTserini",
                        "Citation Paper Abstract": "Abstract:We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",
                        "Citation Paper Authors": "Authors:Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "compared various attention models for abstractive sum-\nmarization. A state-of-the-art approach developed by Paulus\net al. ",
                    "Citation Text": "R. Paulus, C. Xiong, and R. Socher, \u201cA deep reinforced model for\nabstractive summarization,\u201d arXiv preprint arXiv:1705.04304, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04304",
                        "Citation Paper Title": "Title:A Deep Reinforced Model for Abstractive Summarization",
                        "Citation Paper Abstract": "Abstract:Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
                        "Citation Paper Authors": "Authors:Romain Paulus, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 230,
                    "Sentence": ". The\nuse of deep learning for image captioning has been surveyed\nvery recently ",
                    "Citation Text": "M. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, \u201cA comprehen-\nsive survey of deep learning for image captioning,\u201d ACM Computing\nSurveys (CSUR), vol. 51, no. 6, p. 118, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04020",
                        "Citation Paper Title": "Title:A Comprehensive Survey of Deep Learning for Image Captioning",
                        "Citation Paper Abstract": "Abstract:Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.",
                        "Citation Paper Authors": "Authors:Md. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, Hamid Laga"
                    }
                },
                {
                    "Sentence ID": 189,
                    "Sentence": "used an\nattention-based GRU model with a copy mechanism. This\nnetwork was novel in its use of a data structure known as\na coverage mechanism ",
                    "Citation Text": "Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, \u201cModeling coverage for neural\nmachine translation,\u201d arXiv preprint arXiv:1601.04811, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.04811",
                        "Citation Paper Title": "Title:Modeling Coverage for Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.",
                        "Citation Paper Authors": "Authors:Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "extracted query term representations\nfrom two pre-trained contextualized language models, ELMo ",
                    "Citation Text": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, \u201cDeep contextualized word representations,\u201d arXiv\npreprint arXiv:1802.05365, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 148,
                    "Sentence": "trained encoders\non four different language pairs: English and Arabic, English\nand Spanish, English and Chinese, and English and German.\nThe decoding classi\ufb01ers were trained on four distinct datasets:\nMulti-NLI ",
                    "Citation Text": "A. Williams, N. Nangia, and S. Bowman, \u201cA broad-coverage challenge\ncorpus for sentence understanding through inference,\u201d arXiv preprint\narXiv:1704.05426, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.05426",
                        "Citation Paper Title": "Title:A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
                        "Citation Paper Abstract": "Abstract:This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.",
                        "Citation Paper Authors": "Authors:Adina Williams, Nikita Nangia, Samuel R. Bowman"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": ".\nState-of-the-art results were achieved on the \ufb01rst and the third.\nTai et al. concocted a model using an RvNN with LSTM-\nlike nodes ",
                    "Citation Text": "K. S. Tai, R. Socher, and C. D. Manning, \u201cImproved semantic rep-\nresentations from tree-structured long short-term memory networks,\u201d\narXiv preprint arXiv:1503.00075, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.00075",
                        "Citation Paper Title": "Title:Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
                        "Citation Paper Abstract": "Abstract:Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
                        "Citation Paper Authors": "Authors:Kai Sheng Tai, Richard Socher, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.06385v4": {
            "Paper Title": "Bootstrapping Generators from Noisy Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.08272v4": {
            "Paper Title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08077v2": {
            "Paper Title": "Inducing and Embedding Senses with Scaled Gumbel Softmax",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09187v2": {
            "Paper Title": "Why Do Neural Response Generation Models Prefer Universal Replies?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00681v2": {
            "Paper Title": "On the Generation of Medical Question-Answer Pairs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00438v2": {
            "Paper Title": "Parameter-free Sentence Embedding via Orthogonal Basis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.02073v2": {
            "Paper Title": "Neural Personalized Response Generation as Domain Adaptation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "captured the advantages of the\nRNN encoder-decoder on response generation and the deep reinforcement learn-\ning on dialogue rewarding to generate context-aware dialogue responses.\nThe most similar work to this paper is ",
                    "Citation Text": "J. Li, M. Galley, C. Brockett, G. Spithourakis, J. Gao, B. Dolan, A persona-based\nneural conversation model, in: ACL, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06155",
                        "Citation Paper Title": "Title:A Persona-Based Neural Conversation Model",
                        "Citation Paper Abstract": "Abstract:We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "focused on resolving the\ngenerating of safe, commonplace, high frequency responses on the sequence to\nsequence neural network. Most recently, ",
                    "Citation Text": "J. Li, W. Monroe, A. Ritter, J. Dan, Deep reinforcement learning for dialogue\ngeneration.\nURL http://arxiv.org/abs/1606.01541",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01541",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning for Dialogue Generation",
                        "Citation Paper Abstract": "Abstract:Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "had drawn inspiration from the neural machine transla-\ntion [17, 18] and proposed an RNN encoder-decoder based approach to generate\ndialogue by considering the last one sentence and a larger range of context re-\nspectively. ",
                    "Citation Text": "I. V . Serban, A. Sordoni, Y . Bengio, A. Courville, J. Pineau, Building end-to-end\ndialogue systems using generative hierarchical neural network models, Computer\nScience.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04808",
                        "Citation Paper Title": "Title:Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
                        "Citation Paper Abstract": "Abstract:We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.",
                        "Citation Paper Authors": "Authors:Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposed a statistical dialogue generator based on a joint recurrent and con-\nvolutional neural network, which can directly learn from the data without any\nsemantic alignment or handcrafted rules. Further, ",
                    "Citation Text": "T. H. Wen, M. Gasic, N. Mrksic, P. H. Su, D. Vandyke, S. Young, Semantically\nconditioned lstm-based natural language generation for spoken dialogue systems,\nComputer Science.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01745",
                        "Citation Paper Title": "Title:Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.",
                        "Citation Paper Authors": "Authors:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "In the proposed approach, the response generation model is implemented by an\nRNN encoder-decoder framework, which is inspired by ",
                    "Citation Text": "D. Bahdanau, K. Cho, Y . Bengio, Neural machine translation by jointly learning\nto align and translate, Computer Science.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.02794v3": {
            "Paper Title": "Explicit Contextual Semantics for Text Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.08430v2": {
            "Paper Title": "A GRU-Gated Attention Model for Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.12164v3": {
            "Paper Title": "What the Vec? Towards Probabilistically Grounded Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ", and loosely re\ufb02ecting a KL divergence.\nEuclidean distance: kwi\u0000wjk2=k(logp(Ejwi)\np(Ejwj))Cyk2shows no obvious meaning.\nCosine similarity: Surprisingly,w>\niwj\nkwikkwjk, as often used effectively to predict word relatedness\nand/or similarity [ 33,5], has no immediate semantic interpretation. However, recent work ",
                    "Citation Text": "Carl Allen, Ivana Balazevic, and Timothy M Hospedales. On understanding knowledge graph\nrepresentation. arXiv preprint arXiv:1909.11611 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11611",
                        "Citation Paper Title": "Title:Interpreting Knowledge Graph Relation Representation from Word Embeddings",
                        "Citation Paper Abstract": "Abstract:Many models learn representations of knowledge graph data by exploiting its low-rank latent structure, encoding known relations between entities and enabling unknown facts to be inferred. To predict whether a relation holds between entities, embeddings are typically compared in the latent space following a relation-specific mapping. Whilst their predictive performance has steadily improved, how such models capture the underlying latent structure of semantic information remains unexplained. Building on recent theoretical understanding of word embeddings, we categorise knowledge graph relations into three types and for each derive explicit requirements of their representations. We show that empirical properties of relation representations and the relative performance of leading knowledge graph representation methods are justified by our analysis.",
                        "Citation Paper Authors": "Authors:Carl Allen, Ivana Bala\u017eevi\u0107, Timothy Hospedales"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.11459v3": {
            "Paper Title": "A neural joint model for Vietnamese word segmentation, POS tagging and\n  dependency parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12780v4": {
            "Paper Title": "Advancing PICO Element Detection in Biomedical Text via Deep Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11895v3": {
            "Paper Title": "Language Modeling for Code-Switching: Evaluation, Integration of\n  Monolingual Data, and Discriminative Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.11532v2": {
            "Paper Title": "TextWorld: A Learning Environment for Text-based Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01727v3": {
            "Paper Title": "AttentionXML: Label Tree-based Attention-Aware Deep Model for\n  High-Performance Extreme Multi-Label Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02279v2": {
            "Paper Title": "Cell-aware Stacked LSTMs for Modeling Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03986v2": {
            "Paper Title": "Multimodal Differential Network for Visual Question Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06538v6": {
            "Paper Title": "Cavity Filling: Pseudo-Feature Generation for Multi-Class Imbalanced\n  Data Problems in Deep Learning",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ", and an\nexample reweighing algorithm for deep learning was investigated ",
                    "Citation Text": "M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to Reweight Examples for Robust Deep\nLearning. ArXiv e-prints , March 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09050",
                        "Citation Paper Title": "Title:Learning to Reweight Examples for Robust Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",
                        "Citation Paper Authors": "Authors:Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". An approach to train the generative adversarial\nnetworks using imbalanced data and to generate the minor-class images was studied in ",
                    "Citation Text": "G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi. BAGAN: Data Augmentation\nwith Balancing GAN. ArXiv e-prints , March 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09655",
                        "Citation Paper Title": "Title:BAGAN: Data Augmentation with Balancing GAN",
                        "Citation Paper Abstract": "Abstract:Image classification datasets are often imbalanced, characteristic that negatively affects the accuracy of deep-learning classifiers. In this work we propose balancing GAN (BAGAN) as an augmentation tool to restore balance in imbalanced datasets. This is challenging because the few minority-class images may not be enough to train a GAN. We overcome this issue by including during the adversarial training all available images of majority and minority classes. The generative model learns useful features from majority classes and uses these to generate images for minority classes. We apply class conditioning in the latent space to drive the generation process towards a target class. The generator in the GAN is initialized with the encoder module of an autoencoder that enables us to learn an accurate class-conditioning in the latent space. We compare the proposed methodology with state-of-the-art GANs and demonstrate that BAGAN generates images of superior quality when trained with an imbalanced dataset.",
                        "Citation Paper Authors": "Authors:Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas, Cristiano Malossi"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". The neighbors progressive\ncompetitive algorithm, inspired by the k-nearest neighbor, has also been proposed ",
                    "Citation Text": "S. Saryazdi, B. Nikpour, and H. Nezamabadi-pour. NPC: Neighbors Progressive Competition\nAlgorithm for Classi\ufb01cation of Imbalanced Data Sets. ArXiv e-prints , November 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10934",
                        "Citation Paper Title": "Title:NPC: Neighbors Progressive Competition Algorithm for Classification of Imbalanced Data Sets",
                        "Citation Paper Abstract": "Abstract:Learning from many real-world datasets is limited by a problem called the class imbalance problem. A dataset is imbalanced when one class (the majority class) has significantly more samples than the other class (the minority class). Such datasets cause typical machine learning algorithms to perform poorly on the classification task. To overcome this issue, this paper proposes a new approach Neighbors Progressive Competition (NPC) for classification of imbalanced datasets. Whilst the proposed algorithm is inspired by weighted k-Nearest Neighbor (k-NN) algorithms, it has major differences from them. Unlike k- NN, NPC does not limit its decision criteria to a preset number of nearest neighbors. In contrast, NPC considers progressively more neighbors of the query sample in its decision making until the sum of grades for one class is much higher than the other classes. Furthermore, NPC uses a novel method for grading the training samples to compensate for the imbalance issue. The grades are calculated using both local and global information. In brief, the contribution of this paper is an entirely new classifier for handling the imbalance issue effectively without any manually-set parameters or any need for expert knowledge. Experimental results compare the proposed approach with five representative algorithms applied to fifteen imbalanced datasets and illustrate this algorithms effectiveness.",
                        "Citation Paper Authors": "Authors:Soroush Saryazdi, Bahareh Nikpour, Hossein Nezamabadi-pour"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", and\na cost function approach, i.e., class recti\ufb01cation loss, was studied ",
                    "Citation Text": "Q. Dong, S. Gong, and X. Zhu. Class Recti\ufb01cation Hard Mining for Imbalanced Deep Learning.\nArXiv e-prints , December 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.03162",
                        "Citation Paper Title": "Title:Class Rectification Hard Mining for Imbalanced Deep Learning",
                        "Citation Paper Abstract": "Abstract:Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.",
                        "Citation Paper Authors": "Authors:Qi Dong, Shaogang Gong, Xiatian Zhu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", and instance selections and geometric mean\naccuracy were studied ",
                    "Citation Text": "L. I. Kuncheva, \u00c1. Arnaiz-Gonz\u00e1lez, J.-F. D\u00edez-Pastor, and I. A. D. Gunn. Instance Selection\nImproves Geometric Mean Accuracy: A Study on Imbalanced Data Classi\ufb01cation. ArXiv\ne-prints , April 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07155",
                        "Citation Paper Title": "Title:Instance Selection Improves Geometric Mean Accuracy: A Study on Imbalanced Data Classification",
                        "Citation Paper Abstract": "Abstract:A natural way of handling imbalanced data is to attempt to equalise the class frequencies and train the classifier of choice on balanced data. For two-class imbalanced problems, the classification success is typically measured by the geometric mean (GM) of the true positive and true negative rates. Here we prove that GM can be improved upon by instance selection, and give the theoretical conditions for such an improvement. We demonstrate that GM is non-monotonic with respect to the number of retained instances, which discourages systematic instance selection. We also show that balancing the distribution frequencies is inferior to a direct maximisation of GM. To verify our theoretical findings, we carried out an experimental study of 12 instance selection methods for imbalanced data, using 66 standard benchmark data sets. The results reveal possible room for new instance selection methods for imbalanced data.",
                        "Citation Paper Authors": "Authors:Ludmila I. Kuncheva, \u00c1lvar Arnaiz-Gonz\u00e1lez, Jos\u00e9-Francisco D\u00edez-Pastor, Iain A. D. Gunn"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.09154v3": {
            "Paper Title": "A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.09751v3": {
            "Paper Title": "Skill2vec: Machine Learning Approach for Determining the Relevant Skills\n  from Job Description",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02889v3": {
            "Paper Title": "Scalable Micro-planned Generation of Discourse from Structured Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.00178v3": {
            "Paper Title": "Dynamic Sentence Sampling for Efficient Training of Neural Machine\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12735v2": {
            "Paper Title": "Spoken Language Understanding on the Edge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.12471v3": {
            "Paper Title": "Neural Network Acceptability Judgments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11878v2": {
            "Paper Title": "Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel\n  Textual Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.02847v2": {
            "Paper Title": "A Simple Method for Commonsense Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.08340v2": {
            "Paper Title": "Extracting Bilingual Persian Italian Lexicon from Comparable Corpora\n  Using Different Types of Seed Dictionaries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00552v2": {
            "Paper Title": "Multiple-Attribute Text Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.04231v3": {
            "Paper Title": "Syntax-Directed Attention for Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04206v3": {
            "Paper Title": "Temporal Pattern Attention for Multivariate Time Series Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08619v2": {
            "Paper Title": "Multi Task Deep Morphological Analyzer: Context Aware Joint\n  Morphological Tagging and Lemma Prediction",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "had formulated a factored NMT system by extending\nthe Bahdanau attention model ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "report an FST based approachto develop the first finite state analyzer for Urduusing\ntheLexccompiler for lexicon encodings. Humayoun et al. ",
                    "Citation Text": "Muhammad Humayoun, Harald Hammarstr\u00f6m, and Aarne Ranta. 2006. Urdu morphology, orthography and lexicon\nextraction . Chalmers tekniska h\u00f6gskola.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03071",
                        "Citation Paper Title": "Title:Urdu Morphology, Orthography and Lexicon Extraction",
                        "Citation Paper Abstract": "Abstract:Urdu is a challenging language because of, first, its Perso-Arabic script and second, its morphological system having inherent grammatical forms and vocabulary of Arabic, Persian and the native languages of South Asia. This paper describes an implementation of the Urdu language as a software API, and we deal with orthography, morphology and the extraction of the lexicon. The morphology is implemented in a toolkit called Functional Morphology (Forsberg & Ranta, 2004), which is based on the idea of dealing grammars as software libraries. Therefore this implementation could be reused in applications such as intelligent search of keywords, language training and infrastructure for syntax. We also present an implementation of a small part of Urdu syntax to demonstrate this reusability.",
                        "Citation Paper Authors": "Authors:Muhammad Humayoun, Harald Hammarstr\u00f6m, Aarne Ranta"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "to handle derivational forms by exploring derivational rules\nbuilt upon suffixes, and using these to find majority properties of derived words of a suffix. On\nthe other hand, Kumar et al. ",
                    "Citation Text": "Deepak Kumar, Manjeet Singh, and Seema Shukla. 2012. Fst based morphological analyzer for Hindi language. arXiv\npreprint arXiv:1207.5409 (2012).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.5409",
                        "Citation Paper Title": "Title:FST Based Morphological Analyzer for Hindi Language",
                        "Citation Paper Abstract": "Abstract:Hindi being a highly inflectional language, FST (Finite State Transducer) based approach is most efficient for developing a morphological analyzer for this language. The work presented in this paper uses the SFST (Stuttgart Finite State Transducer) tool for generating the FST. A lexicon of root words is created. Rules are then added for generating inflectional and derivational words from these root words. The Morph Analyzer developed was used in a Part Of Speech (POS) Tagger based on Stanford POS Tagger. The system was first trained using a manually tagged corpus and MAXENT (Maximum Entropy) approach of Stanford POS tagger was then used for tagging input sentences. The morphological analyzer gives approximately 97% correct results. POS tagger gives an accuracy of approximately 87% for the sentences that have the words known to the trained model file, and 80% accuracy for the sentences that have the words unknown to the trained model file.",
                        "Citation Paper Authors": "Authors:Deepak Kumar, Manjeet Singh, Seema Shukla"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.08782v3": {
            "Paper Title": "Collective Learning From Diverse Datasets for Entity Typing in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04961v5": {
            "Paper Title": "Putting the Horse Before the Cart:A Generator-Evaluator Framework for\n  Question Generation from Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12368v5": {
            "Paper Title": "A Pragmatic Guide to Geoparsing Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08989v2": {
            "Paper Title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02825v5": {
            "Paper Title": "Attending to Mathematical Language with Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08270v5": {
            "Paper Title": "Robust Text Classifier on Test-Time Budgets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05067v2": {
            "Paper Title": "Shall I Compare Thee to a Machine-Written Sonnet? An Approach to\n  Algorithmic Sonnet Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08267v3": {
            "Paper Title": "Neural Approaches to Conversational AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.10188v7": {
            "Paper Title": "Modeling Psychotherapy Dialogues with Kernelized Hashcode\n  Representations: A Nonparametric Information-Theoretic Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.08424v2": {
            "Paper Title": "Multimodal Word Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03726v3": {
            "Paper Title": "Learning to Represent Bilingual Dictionaries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06773v2": {
            "Paper Title": "Semi-Supervised Learning for Neural Keyphrase Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00780v3": {
            "Paper Title": "A Bayesian Approach for Sequence Tagging with Crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10437v3": {
            "Paper Title": "Variational Semi-supervised Aspect-term Sentiment Analysis via\n  Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12752v2": {
            "Paper Title": "Long Short-Term Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00207v5": {
            "Paper Title": "Towards Empathetic Open-domain Conversation Models: a New Benchmark and\n  Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.07976v3": {
            "Paper Title": "Neural-Davidsonian Semantic Proto-role Labeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10735v2": {
            "Paper Title": "CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.07931v6": {
            "Paper Title": "Unsupervised Neural Text Simplification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01486v3": {
            "Paper Title": "Clinical Concept Embeddings Learned from Massive Sources of Multimodal\n  Medical Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.10326v3": {
            "Paper Title": "Generalize Symbolic Knowledge With Neural Rule Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.04882v7": {
            "Paper Title": "Towards Understanding Linear Word Analogies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09725v2": {
            "Paper Title": "Interpretable Convolutional Filters with SincNet",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08053v2": {
            "Paper Title": "Learning Sentence Embeddings for Coherence Modelling and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10403v2": {
            "Paper Title": "Running on Fumes--Preventing Out-of-Gas Vulnerabilities in Ethereum\n  Smart Contracts using Static Resource Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.08991v2": {
            "Paper Title": "A Simple Theoretical Model of Importance for Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06834v3": {
            "Paper Title": "A Tutorial on Deep Latent Variable Models of Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03919v2": {
            "Paper Title": "Pretraining by Backtranslation for End-to-end ASR in Low-Resource\n  Settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12836v4": {
            "Paper Title": "Learning Cross-Lingual Sentence Representations via a Multi-task\n  Dual-Encoder Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08962v2": {
            "Paper Title": "WiRe57 : A Fine-Grained Benchmark for Open Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.02184v3": {
            "Paper Title": "Translating Terminological Expressions in Knowledge Bases with Neural\n  Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08065v2": {
            "Paper Title": "Learning Robust Heterogeneous Signal Features from Parallel Neural\n  Network for Audio Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08587v2": {
            "Paper Title": "Deep Dialog Act Recognition using Multiple Token, Segment, and Context\n  Information Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.08237v2": {
            "Paper Title": "Large-scale Hierarchical Alignment for Data-driven Text Rewriting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10860v5": {
            "Paper Title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level\n  Pretraining Beyond Language Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08816v2": {
            "Paper Title": "Learning cross-lingual phonological and orthagraphic adaptations: a case\n  study in improving neural machine translation between low-resource languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.09540v2": {
            "Paper Title": "NE-Table: A Neural key-value table for Named Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00491v3": {
            "Paper Title": "A Corpus for Reasoning About Natural Language Grounded in Photographs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04283v4": {
            "Paper Title": "Incorporating Syntactic and Semantic Information in Word Embeddings\n  using Graph Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04936v3": {
            "Paper Title": "On Accurate Evaluation of GANs for Language Generation",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "on the same data. One model is a mean pooled uni- and bigram\nembeddings followed by a feedforward network (UniSent). The other is a more computationally\nexpensive Transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA , pages 6000\u20136010, 2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", it is a widely accepted metric in the Computer Vision community. In this work we adapt this\nmetric for text by using InferSent text embedding model ",
                    "Citation Text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Supervised learning\nof universal sentence representations from natural language inference data. CoRR , abs/1705.02364, 2017.\nURL http://arxiv.org/abs/1705.02364 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.07080v2": {
            "Paper Title": "Bilingual Dictionary Induction for Bantu Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07092v2": {
            "Paper Title": "Unnamed Entity Recognition of Sense Mentions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07098v2": {
            "Paper Title": "Sense Perception Common Sense Relationships",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07807v2": {
            "Paper Title": "DTMT: A Novel Deep Transition Architecture for Neural Machine\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02701v4": {
            "Paper Title": "Trick Me If You Can: Human-in-the-loop Generation of Adversarial\n  Examples for Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00428v4": {
            "Paper Title": "Sanskrit Sandhi Splitting using seq2(seq)^2",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00241v2": {
            "Paper Title": "On the End-to-End Solution to Mandarin-English Code-switching Speech\n  Recognition",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "for English. BPE subword units have shown to be helpful for\nEnglish ",
                    "Citation Text": "A. Zeyer, K. Irie, R. Schl \u00a8uter, and H. Ney, \u201cImproved training\nof end-to-end attention models for speech recognition,\u201d arXiv\npreprint arXiv:1805.03294 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03294",
                        "Citation Paper Title": "Title:Improved training of end-to-end attention models for speech recognition",
                        "Citation Paper Abstract": "Abstract:Sequence-to-sequence attention-based models on subword units allow simple open-vocabulary end-to-end speech recognition. In this work, we show that such models can achieve competitive results on the Switchboard 300h and LibriSpeech 1000h tasks. In particular, we report the state-of-the-art word error rates (WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets of LibriSpeech. We introduce a new pretraining scheme by starting with a high time reduction factor and lowering it during training, which is crucial both for convergence and final performance. In some experiments, we also use an auxiliary CTC loss function to help the convergence. In addition, we train long short-term memory (LSTM) language models on subword units. By shallow fusion, we report up to 27% relative improvements in WER over the attention baseline without a language model.",
                        "Citation Paper Authors": "Authors:Albert Zeyer, Kazuki Irie, Ralf Schl\u00fcter, Hermann Ney"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". CTC performed\nwell on various corpora such as WSJ [12, 13] and SWB ",
                    "Citation Text": "A. Hannun, C. Case, J. Casper et al. , \u201cDeep speech: Scaling up\nend-to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.5567",
                        "Citation Paper Title": "Title:Deep Speech: Scaling up end-to-end speech recognition",
                        "Citation Paper Abstract": "Abstract:We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.",
                        "Citation Paper Authors": "Authors:Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00644v2": {
            "Paper Title": "Analyzing and learning the language for different types of harassment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01127v2": {
            "Paper Title": "Exploiting Explicit Paths for Multi-hop Reading Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06152v2": {
            "Paper Title": "Categorizing Comparative Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09471v2": {
            "Paper Title": "Joint Slot Filling and Intent Detection via Capsule Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09657v4": {
            "Paper Title": "Learning compositionally through attentive guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.00927v3": {
            "Paper Title": "Concept Transfer Learning for Adaptive Language Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.00794v2": {
            "Paper Title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.10731v3": {
            "Paper Title": "Sentiment Adaptive End-to-End Dialog Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10735v2": {
            "Paper Title": "A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09740v2": {
            "Paper Title": "Connecting the Dots Between MLE and RL for Sequence Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07514v2": {
            "Paper Title": "NSEEN: Neural Semantic Embedding for Entity Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.04116v3": {
            "Paper Title": "A Novel Document Generation Process for Topic Detection based on\n  Hierarchical Latent Tree Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.04783v2": {
            "Paper Title": "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and\n  Prince (1988) and the Past Tense Debate",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11752v5": {
            "Paper Title": "Multi-turn Dialogue Response Generation in an Adversarial Learning\n  Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09296v2": {
            "Paper Title": "Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for\n  Language Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.01719v2": {
            "Paper Title": "Phylogenetics of Indo-European Language families via an\n  Algebro-Geometric Analysis of their Syntactic Structures",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "in our analysis.\n2.Phylogenetic Algebraic Varieties and Invariants\nBefore we proceed to the analysis of the two sets of languages listed above, we recall brie\ry the\nnotation and the results we will be using from Phylogenetic Algebraic Geometry, see ",
                    "Citation Text": "E. Allman, J. Rhodes, Phylogenetic ideals and varieties for general Markov models , Adv. Appl. Math. Vol.40\n(2008) 127{148.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:math/0410604",
                        "Citation Paper Title": "Title:Phylogenetic ideals and varieties for the general Markov model",
                        "Citation Paper Abstract": "Abstract:  The general Markov model of the evolution of biological sequences along a tree leads to a parameterization of an algebraic variety. Understanding this variety and the polynomials, called phylogenetic invariants, which vanish on it, is a problem within the broader area of Algebraic Statistics.\nFor an arbitrary trivalent tree, we determine the full ideal of invariants for the 2-state model, establishing a conjecture of Pachter-Sturmfels. For the $\\kappa$-state model, we reduce the problem of determining a defining set of polynomials to that of determining a defining set for a 3-leaved tree.\nAlong the way, we prove several new cases of a conjecture of Garcia-Stillman-Sturmfels on certain statistical models on star trees, and reduce their conjecture to a family of subcases.",
                        "Citation Paper Authors": "Authors:Elizabeth S. Allman, John A. Rhodes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.08398v5": {
            "Paper Title": "STACL: Simultaneous Translation with Implicit Anticipation and\n  Controllable Latency using Prefix-to-Prefix Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08396v3": {
            "Paper Title": "The Privacy Policy Landscape After the GDPR",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01565v3": {
            "Paper Title": "Incorporating Chinese Radicals Into Neural Machine Translation: Deeper\n  Than Character Level",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07645v3": {
            "Paper Title": "Playing 20 Question Game with Policy-Based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.08159v2": {
            "Paper Title": "Multi-Perspective Relevance Matching with Hierarchical ConvNets for\n  Social Media Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03361v2": {
            "Paper Title": "An Unsupervised Approach for Aspect Category Detection Using Soft Cosine\n  Similarity Measure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08769v4": {
            "Paper Title": "What are the biases in my word embedding?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02223v5": {
            "Paper Title": "Character-Aware Decoder for Translation into Morphologically Rich\n  Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09353v2": {
            "Paper Title": "Learning to Discover, Ground and Use Words with Segmental Neural\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.10122v3": {
            "Paper Title": "Learning Neural Templates for Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.06939v2": {
            "Paper Title": "Event2Mind: Commonsense Inference on Events, Intents, and Reactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01747v3": {
            "Paper Title": "The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00196v2": {
            "Paper Title": "Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03511v3": {
            "Paper Title": "Effective Representation for Easy-First Dependency Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02364v2": {
            "Paper Title": "Effective Subword Segmentation for Text Comprehension",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "proposed to transfer the LSTM encoder\nfrom the neural machine translation (NMT) to the NLI task\nto contextualize word vectors. Pan et al. ",
                    "Citation Text": "Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting\nZhuang, Deng Cai, and Xiaofei He. Discourse marker\naugmented network with reinforcement learning for natu-\nral language inference. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics\n(ACL) , pages 989\u2013999, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.09692",
                        "Citation Paper Title": "Title:Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference",
                        "Citation Paper Abstract": "Abstract:Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as \"so\" or \"but\" to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.",
                        "Citation Paper Authors": "Authors:Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "introduced the byte pair encoding (BPE) compres-\nsion algorithm for open-vocabulary neural machine translation\nby encoding rare and unknown words as subword units. Zhang\net al. ",
                    "Citation Text": "Zhuosheng Zhang, Yafang Huang, and Hai Zhao.\nSubword-augmented embedding for cloze reading com-\nprehension. In Proceedings of the 27th International\nConference on Computational Linguistics (COLING) ,\npages 1802\u20131814, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.09103",
                        "Citation Paper Title": "Title:Subword-augmented Embedding for Cloze Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Representation learning is the foundation of machine reading comprehension. In state-of-the-art models, deep learning methods broadly use word and character level representations. However, character is not naturally the minimal linguistic unit. In addition, with a simple concatenation of character and word embedding, previous models actually give suboptimal solution. In this paper, we propose to use subword rather than character for word embedding enhancement. We also empirically explore different augmentation strategies on subword-augmented embedding to enhance the cloze-style reading comprehension model reader. In detail, we present a reader that uses subword-level representation to augment word embedding with a short list to handle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets.",
                        "Citation Paper Authors": "Authors:Zhuosheng Zhang, Yafang Huang, Hai Zhao"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "introduced a recurrent neural network language model\nwith LSTM units and a word-character gate to adaptively\n\ufb01nd the optimal mixture of the character-level and word-\nlevel inputs. Yang et al. ",
                    "Citation Text": "Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu,\nWilliam W. Cohen, and Ruslan Salakhutdinov. Words\nor characters? \ufb01ne-grained gating for reading compre-\nhension. In ICLR 2017 : 5th International Conference\non Learning Representations , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01724",
                        "Citation Paper Title": "Title:Words or Characters? Fine-grained Gating for Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.",
                        "Citation Paper Authors": "Authors:Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "concate-\nnated the character and word embedding to feed a two-layer\nHighway Network. Cai et al. ",
                    "Citation Text": "Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian\nWu, and Feiyue Huang. Fast and accurate neural word\nsegmentation for Chinese. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL) , pages 608\u2013615, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.07047",
                        "Citation Paper Title": "Title:Fast and Accurate Neural Word Segmentation for Chinese",
                        "Citation Paper Abstract": "Abstract:Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.",
                        "Citation Paper Authors": "Authors:Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, Feiyue Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.05867v2": {
            "Paper Title": "An Empirical Study on Crosslingual Transfer in Probabilistic Topic\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.12061v2": {
            "Paper Title": "Bilingual Character Representation for Efficiently Addressing\n  Out-of-Vocabulary Words in Code-Switching Named Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02166v2": {
            "Paper Title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised\n  Neural Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01110v3": {
            "Paper Title": "Text2Scene: Generating Compositional Scenes from Textual Descriptions",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "10.62 \u00060.19 0.486 0.278 0.166 0.106 0.130 0.360 0.216 0.057\nHDGAN ",
                    "Citation Text": "Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic\ntext-to-image synthesis with a hierarchically-nested adver-\nsarial network. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09178",
                        "Citation Paper Title": "Title:Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network",
                        "Citation Paper Abstract": "Abstract:This paper presents a novel method to deal with the challenging task of generating photographic images conditioned on semantic image descriptions. Our method introduces accompanying hierarchical-nested adversarial objectives inside the network hierarchies, which regularize mid-level representations and assist generator training to capture the complex image statistics. We present an extensile single-stream generator architecture to better adapt the jointed discriminators and push generated images up to high resolutions. We adopt a multi-purpose adversarial loss to encourage more effective image and text information usage in order to improve the semantic consistency and image fidelity simultaneously. Furthermore, we introduce a new visual-semantic similarity measure to evaluate the semantic consistency of generated images. With extensive experimental validation on three public datasets, our method significantly improves previous state of the arts on all datasets over different evaluation metrics.",
                        "Citation Paper Authors": "Authors:Zizhao Zhang, Yuanpu Xie, Lin Yang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "dataset; (II) predicting semantic object layouts\nof real images in the COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\nBourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll \u00b4ar, and C. Lawrence Zitnick. Microsoft\nCOCO: Common objects in context. European Conference\non Computer Vision (ECCV) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "which is commonly used in recent\ntext to image generation methods. However, as IS does not\nevaluate correspondence between images and captions, we\nalso employ an extrinsic evaluation using image captioning\nusing the Show-and-Tell caption generator ",
                    "Citation Text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and tell: A neural image caption gen-\nerator. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4555",
                        "Citation Paper Title": "Title:Show and Tell: A Neural Image Caption Generator",
                        "Citation Paper Abstract": "Abstract:Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                        "Citation Paper Authors": "Authors:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nFor synthetic image generation, we adopt the Inception\nScore (IS) metric ",
                    "Citation Text": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniquesfor training gans. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "6.7 \u00060.1 0.504 0.294 0.178 0.116 0.141 0.373 0.289 0.070\nStackGAN ",
                    "Citation Text": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei\nHuang, Xiaogang Wang, and Dimitris Metaxas. Stackgan:\nText to photo-realistic image synthesis with stacked genera-\ntive adversarial networks. In IEEE International Conference\non Computer Vision (ICCV) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10916",
                        "Citation Paper Title": "Title:StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.",
                        "Citation Paper Authors": "Authors:Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "performed pictorial generation from\nchat logs, while our work uses text which is considerably\nmore underspeci\ufb01ed. Gupta et al ",
                    "Citation Text": "Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem,\nand Aniruddha Kembhavi. Imagine this! scripts to composi-\ntions to videos. In European Conference on Computer Vision\n(ECCV) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.03608",
                        "Citation Paper Title": "Title:Imagine This! Scripts to Compositions to Videos",
                        "Citation Paper Abstract": "Abstract:Imagining a scene described in natural language with realistic layout and appearance of entities is the ultimate test of spatial, visual, and semantic world knowledge. Towards this goal, we present the Composition, Retrieval, and Fusion Network (CRAFT), a model capable of learning this knowledge from video-caption data and applying it while generating videos from novel captions. CRAFT explicitly predicts a temporal-layout of mentioned entities (characters and objects), retrieves spatio-temporal entity segments from a video database and fuses them to generate scene videos. Our contributions include sequential training of components of CRAFT while jointly modeling layout and appearances, and losses that encourage learning compositional representations for retrieval. We evaluate CRAFT on semantic fidelity to caption, composition consistency, and visual quality. CRAFT outperforms direct pixel generation approaches and generalizes well to unseen captions and to unseen video databases with no text annotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated video-caption dataset with over 25000 videos. For a glimpse of videos generated by CRAFT, see this https URL.",
                        "Citation Paper Authors": "Authors:Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.10986v5": {
            "Paper Title": "HCqa: Hybrid and Complex Question Answering on Textual Corpus and\n  Knowledge Graph",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "de\ufb01nes complex questions as composite questions requiring federating\ninformation from various heterogeneous sources. In general, researchers treat complex\nquestions in two ways: (i) decomposition approaches [18, 19] , (ii) segmenting ap-\nproaches [3, 25, 28\u201330, 42].\nBroccoli ",
                    "Citation Text": "Hannah Bast, Florian B\u00e4urle, Bj\u00f6rn Buchhold, and Elmar Haussmann. Broccoli:\nSemantic full-text search at your \ufb01ngertips. arXiv preprint arXiv:1207.2615 , 2012.\n16http://www.mpi-inf.mpg.de/departments/d5/software/clausie\n17https://qald.sebastianwalter.org/index.php?x=challenge&q=6\n31",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.2615",
                        "Citation Paper Title": "Title:Broccoli: Semantic Full-Text Search at your Fingertips",
                        "Citation Paper Abstract": "Abstract:We present Broccoli, a fast and easy-to-use search engine for what we call semantic full-text search. Semantic full-text search combines the capabilities of standard full-text search and ontology search. The search operates on four kinds of objects: ordinary words (e.g., edible), classes (e.g., plants), instances (e.g., Broccoli), and relations (e.g., occurs-with or native-to). Queries are trees, where nodes are arbitrary bags of these objects, and arcs are relations. The user interface guides the user in incrementally constructing such trees by instant (search-as-you-type) suggestions of words, classes, instances, or relations that lead to good hits. Both standard full-text search and pure ontology search are included as special cases. In this paper, we describe the query language of Broccoli, the main idea behind a new kind of index that enables fast processing of queries from that language as well as fast query suggestion, the natural language processing required, and the user interface. We evaluated query times and result quality on the full version of the English Wikipedia (40 GB XML dump) combined with the YAGO ontology (26 million facts). We have implemented a fully functional prototype based on our ideas and provide a web application to reproduce our quality experiments. Both are accessible via this http URL .",
                        "Citation Paper Authors": "Authors:Hannah Bast, Florian B\u00e4urle, Bj\u00f6rn Buchhold, Elmar Haussmann"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00720v2": {
            "Paper Title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math\n  Word Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.00063v3": {
            "Paper Title": "Adversarial Semantic Alignment for Improved Image Captions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04352v4": {
            "Paper Title": "Open Vocabulary Learning for Neural Chinese Pinyin IME",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.03552v3": {
            "Paper Title": "Multi-Source Cross-Lingual Model Transfer: Learning What to Share",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00099v2": {
            "Paper Title": "Generating Titles for Web Tables",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.05558v3": {
            "Paper Title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven\n  Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11737v2": {
            "Paper Title": "The meaning of \"most\" for visual question answering models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.04334v4": {
            "Paper Title": "DisSent: Sentence Representation Learning from Explicit Discourse\n  Relations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01063v2": {
            "Paper Title": "Augmenting Neural Response Generation with Context-Aware Topical\n  Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11546v2": {
            "Paper Title": "Like a Baby: Visually Situated Neural Language Acquisition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06729v2": {
            "Paper Title": "Robust Neural Machine Translation with Joint Textual and Phonetic\n  Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02805v2": {
            "Paper Title": "Faithful Multimodal Explanation for Visual Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00232v2": {
            "Paper Title": "Textbook Question Answering with Multi-modal Context Graph Understanding\n  and Self-supervised Open-set Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06309v3": {
            "Paper Title": "Commonsense for Generative Multi-Hop Question Answering Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.01641v2": {
            "Paper Title": "aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching\n  Model",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "introduced a recursive\nneural network (RNN) model that can reason over text that con-\ntains very few individual words by modeling textual composition-\nality. Yu et al. ",
                    "Citation Text": "L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep\nLearning for Answer Sentence Selection. In NIPS Deep\nLearning Workshop , Dec. 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.1632",
                        "Citation Paper Title": "Title:Deep Learning for Answer Sentence Selection",
                        "Citation Paper Abstract": "Abstract:Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.",
                        "Citation Paper Authors": "Authors:Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.09774v3": {
            "Paper Title": "Testing the Generalization Power of Neural Network Models Across NLI\n  Benchmarks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.10564v4": {
            "Paper Title": "Auto-Encoding Variational Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02731v3": {
            "Paper Title": "Exploiting Invertible Decoders for Unsupervised Sentence Representation\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11714v2": {
            "Paper Title": "Gender Bias in Neural Natural Language Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06619v3": {
            "Paper Title": "Diacritization of Maghrebi Arabic Sub-Dialects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04963v3": {
            "Paper Title": "Multiple Character Embeddings for Chinese Word Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03586v5": {
            "Paper Title": "Difficulty Controllable Generation of Reading Comprehension Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10324v2": {
            "Paper Title": "Iterative Document Representation Learning Towards Summarization with\n  Polishing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11683v2": {
            "Paper Title": "Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "VGG VG 30.03 49.10 39.98\nOurs BiLSTM+VGG VG 50.18 57.91 62.76\nOurs ELMo+VGG VG 48.76 60.08 60.01\nOurs ELMo+PNASNet VG 55.16 67.60 61.89\nCGVS ",
                    "Citation Text": "Vasili Ramanishka, Abir Das, Jianming Zhang, and Kate\nSaenko. Top-down visual saliency guided by captions. In\nIEEE International Conference on Computer Vision and Pat-\ntern Recognition , 2017. 2, 4, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.07360",
                        "Citation Paper Title": "Title:Top-down Visual Saliency Guided by Captions",
                        "Citation Paper Abstract": "Abstract:Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at this http URL.",
                        "Citation Paper Authors": "Authors:Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "along with 99,535 segmented image regions\nfrom the SAIAPR-12 dataset ",
                    "Citation Text": "Kan Chen, Rama Kovvuri, and Ram Nevatia. Query-guided\nregression network with context policy for phrase ground-\ning. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV) , 2017. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.01676",
                        "Citation Paper Title": "Title:Query-guided Regression Network with Context Policy for Phrase Grounding",
                        "Citation Paper Abstract": "Abstract:Given a textual description of an image, phrase grounding localizes objects in the image referred by query phrases in the description. State-of-the-art methods address the problem by ranking a set of proposals based on the relevance to each query, which are limited by the performance of independent proposal generation systems and ignore useful cues from context in the description. In this paper, we adopt a spatial regression method to break the performance limit, and introduce reinforcement learning techniques to further leverage semantic context information. We propose a novel Query-guided Regression network with Context policy (QRC Net) which jointly learns a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN). Experiments show QRC Net provides a significant improvement in accuracy on two popular datasets: Flickr30K Entities and Referit Game, with 14.25% and 17.14% increase over the state-of-the-arts respectively.",
                        "Citation Paper Authors": "Authors:Kan Chen, Rama Kovvuri, Ram Nevatia"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". We use 1k images from the test split of this\ndataset for evaluation.\nVisualGenome ",
                    "Citation Text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:Connecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vi-\nsion, 123(1):32\u201373, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.07332",
                        "Citation Paper Title": "Title:Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                        "Citation Paper Abstract": "Abstract:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "as a baseline for fair com-\nparison with other works [10, 47, 17], and the state of the\nart CNN, PNASNet-5 ",
                    "Citation Text": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia\nLi, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Mur-\nphy. Progressive neural architecture search. arXiv preprint\narXiv:1712.00559 , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00559",
                        "Citation Paper Title": "Title:Progressive Neural Architecture Search",
                        "Citation Paper Abstract": "Abstract:We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",
                        "Citation Paper Authors": "Authors:Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "to combine representations from dif-\nferent modalities. Other methods have used the Canoni-\ncal Correlation Analysis (CCA) [38, 39], which \ufb01nds linear\nprojections that maximize the correlation between projected\nvectors from the two views of heterogeneous data. ",
                    "Citation Text": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,\nTrevor Darrell, and Marcus Rohrbach. Multimodal com-\npact bilinear pooling for visual question answering and vi-\nsual grounding. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing , 2016. 1,\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01847",
                        "Citation Paper Title": "Title:Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
                        "Citation Paper Abstract": "Abstract:Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
                        "Citation Paper Authors": "Authors:Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00511v2": {
            "Paper Title": "Towards Coherent and Cohesive Long-form Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.00920v3": {
            "Paper Title": "DRCD: a Chinese Machine Reading Comprehension Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.07459v2": {
            "Paper Title": "Matching Article Pairs with Graphical Decomposition and Convolutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01498v2": {
            "Paper Title": "Skip-gram word embeddings in hyperbolic space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11365v2": {
            "Paper Title": "Unsupervised Multi-modal Neural Machine Translation",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ": It is a multi-modal zero-source UMT method trained using\nreinforcement learning.\n\u000fUNMT-text ",
                    "Citation Text": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ran-\nzato. Unsupervised machine translation using monolingual corpora only. In In-\nternational Conference on Learning Representations (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00043",
                        "Citation Paper Title": "Title:Unsupervised Machine Translation Using Monolingual Corpora Only",
                        "Citation Paper Abstract": "Abstract:Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.",
                        "Citation Paper Authors": "Authors:Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ",\nplease refer to the relevant readings for further details.\nWe would like to compare the proposed UMNMT model to the following UMT\nmodels.\n\u000fMUSE ",
                    "Citation Text": "Alexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer,\nand Herv \u00b4e J\u00b4egou. Word translation without parallel data. In International Con-\nference on Learning Representations (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.04087",
                        "Citation Paper Title": "Title:Word Translation Without Parallel Data",
                        "Citation Paper Abstract": "Abstract:State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "2016 test set of English $French (En$Fr)\nand English$German (En$De) language pairs. This dataset is a multilingual image\ncaption dataset with 29000 training samples of images and their annotations in English,\nGerman, French ",
                    "Citation Text": "Desmond Elliott, Stella Frank, Lo \u00a8\u0131c Barrault, Fethi Bougares, and Lucia Spe-\ncia. Findings of the second shared task on multimodal machine translation and\nmultilingual image description. In Proceedings of the Second Conference on Ma-\nchine Translation, Volume 2: Shared Task Papers , pages 215\u2013233, Copenhagen,\nDenmark, September 2017. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07177",
                        "Citation Paper Title": "Title:Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description",
                        "Citation Paper Abstract": "Abstract:We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive.",
                        "Citation Paper Authors": "Authors:Desmond Elliott, Stella Frank, Lo\u00efc Barrault, Fethi Bougares, Lucia Specia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.09648v2": {
            "Paper Title": "Adapting Visual Question Answering Models for Enhancing Multimodal\n  Community Q&A Platforms",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". A bilinear pooling-based method was\nfound to be effective by ",
                    "Citation Text": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and\nMarcus Rohrbach. 2016. Multimodal compact bilinear pooling for visual question\nanswering and visual grounding. arXiv preprint arXiv:1606.01847 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01847",
                        "Citation Paper Title": "Title:Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
                        "Citation Paper Abstract": "Abstract:Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
                        "Citation Paper Authors": "Authors:Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", where the text is\nseen as a query for retrieving attention weights for image regions.\nMethods involving attention over both image and text include ",
                    "Citation Text": "Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical\nquestion-image co-attention for visual question answering. In Advances In Neural\nInformation Processing Systems . 289\u2013297.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.00061",
                        "Citation Paper Title": "Title:Hierarchical Question-Image Co-Attention for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.",
                        "Citation Paper Authors": "Authors:Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". Attention based approaches learn a\nconvex combination of spatial image vectors as the contributor to\nthe final joint embedding. A simple and popular attention model\nfor VQA is the stacked attention network ",
                    "Citation Text": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked\nattention networks for image question answering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition . 21\u201329.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.02274",
                        "Citation Paper Title": "Title:Stacked Attention Networks for Image Question Answering",
                        "Citation Paper Abstract": "Abstract:This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
                        "Citation Paper Authors": "Authors:Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". A better method is concatenation of the element\nwise sum and product ",
                    "Citation Text": "Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. 2017. Du-\nalnet: Domain-invariant network for visual question answering. In Multimedia\nand Expo (ICME), 2017 IEEE International Conference on . IEEE, 829\u2013834.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06108",
                        "Citation Paper Title": "Title:DualNet: Domain-Invariant Network for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Visual question answering (VQA) task not only bridges the gap between images and language, but also requires that specific contents within the image are understood as indicated by linguistic context of the question, in order to generate the accurate answers. Thus, it is critical to build an efficient embedding of images and texts. We implement DualNet, which fully takes advantage of discriminative power of both image and textual features by separately performing two operations. Building an ensemble of DualNet further boosts the performance. Contrary to common belief, our method proved effective in both real images and abstract scenes, in spite of significantly different properties of respective domain. Our method was able to outperform previous state-of-the-art methods in real images category even without explicitly employing attention mechanism, and also outperformed our own state-of-the-art method in abstract scenes category, which recently won the first place in VQA Challenge 2016.",
                        "Citation Paper Authors": "Authors:Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "), it can also be casted into\na encoder-decoder framework that uses joint image-text represen-\ntations ",
                    "Citation Text": "Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying visual-\nsemantic embeddings with multimodal neural language models. arXiv preprint\narXiv:1411.2539 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.2539",
                        "Citation Paper Title": "Title:Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
                        "Citation Paper Abstract": "Abstract:Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
                        "Citation Paper Authors": "Authors:Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "). A recent shift to\nend-to-end deep learning approaches ( ",
                    "Citation Text": "Chen Zheng, Shuangfei Zhai, and Zhongfei Zhang. 2017. A Deep Learning\nApproach for Expert Identification in Question Answering Communities. arXiv\npreprint arXiv:1711.05350 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05350",
                        "Citation Paper Title": "Title:A Deep Learning Approach for Expert Identification in Question Answering Communities",
                        "Citation Paper Abstract": "Abstract:In this paper, we describe an effective convolutional neural network framework for identifying the expert in question answering community. This approach uses the convolutional neural network and combines user feature representations with question feature representations to compute scores that the user who gets the highest score is the expert on this question. Unlike prior work, this method does not measure expert based on measure answer content quality to identify the expert but only require question sentence and user embedding feature to identify the expert. Remarkably, Our model can be applied to different languages and different domains. The proposed framework is trained on two datasets, The first dataset is Stack Overflow and the second one is Zhihu. The Top-1 accuracy results of our experiments show that our framework outperforms the best baseline framework for expert identification.",
                        "Citation Paper Authors": "Authors:Chen Zheng, Shuangfei Zhai, Zhongfei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00405v4": {
            "Paper Title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.04805v2": {
            "Paper Title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01690v2": {
            "Paper Title": "Cycle-consistency training for end-to-end speech recognition",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "with a probability of 0.5 was applie d to all\nof the convolution and Prenet layers. Zoneout ",
                    "Citation Text": "David Krueger, Tegan Maharaj, J\u00b4 anos Kram\u00b4 ar, Mohamma d\nPezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal,\nYoshua Bengio, Aaron Courville, and Chris Pal, \u201cZoneout:\nRegularizing rnns by randomly preserving hidden activatio ns,\u201d\narXiv preprint arXiv:1606.01305 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01305",
                        "Citation Paper Title": "Title:Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
                        "Citation Paper Abstract": "Abstract:We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.",
                        "Citation Paper Authors": "Authors:David Krueger, Tegan Maharaj, J\u00e1nos Kram\u00e1r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, Chris Pal"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "was us ed\nto extract 80-dimensional log mel-\ufb01lter bank acoustic vect ors with\nthree-dimensional pitch features. The ASR encoder had an ei ght-\nlayered bidirectional long short-term memory with 320 cell s includ-\ning projection layers ",
                    "Citation Text": "Has \u00b8im Sak, Andrew Senior, and Franc \u00b8oise Beaufays, \u201cL ong\nshort-term memory recurrent neural network architectures for\nlarge scale acoustic modeling,\u201d in Proc. Interspeech , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1402.1128",
                        "Citation Paper Title": "Title:Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.",
                        "Citation Paper Authors": "Authors:Ha\u015fim Sak, Andrew Senior, Fran\u00e7oise Beaufays"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "introduced the TTE model but used the synthesized encoder st ate se-\nquences to train the ASR decoder from text data only. This is e quiv-\nalent to back-translation in MT ",
                    "Citation Text": "Rico Sennrich, Barry Haddow, and Alexandra Birch, \u201cImp rov-\ning neural machine translation models with monolingual dat a,\u201d\narXiv preprint arXiv:1511.06709 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06709",
                        "Citation Paper Title": "Title:Improving Neural Machine Translation Models with Monolingual Data",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.04276v2": {
            "Paper Title": "Retrieval-Enhanced Adversarial Training for Neural Response Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05739v4": {
            "Paper Title": "MeanSum: A Neural Model for Unsupervised Multi-document Abstractive\n  Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10238v2": {
            "Paper Title": "Learning Latent Beliefs and Performing Epistemic Reasoning for Efficient\n  and Meaningful Dialog Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.04044v7": {
            "Paper Title": "Kernelized Hashcode Representations for Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03449v3": {
            "Paper Title": "Explicit Utilization of General Knowledge in Machine Reading\n  Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07371v2": {
            "Paper Title": "Spelling Correction as a Foreign Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00513v2": {
            "Paper Title": "Auditing Data Provenance in Text-Generation Models",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "show that deep\nlearning models can achieve perfect accuracy even on randomly\nlabeled training data. Song et al. ",
                    "Citation Text": "C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that remember\ntoo much. In CCS, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07886",
                        "Citation Paper Title": "Title:Machine Learning Models that Remember Too Much",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data.\nWe consider a malicious ML provider who supplies model-training code to the data holder, does not observe the training, but then obtains white- or black-box access to the resulting model. In this setting, we design and implement practical algorithms, some of them very similar to standard ML techniques such as regularization and data augmentation, that \"memorize\" information about the training dataset in the model yet the model is as accurate and predictive as a conventionally trained model. We then explain how the adversary can extract memorized information from the model.\nWe evaluate our techniques on standard ML tasks for image classification (CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20 Newsgroups and IMDB). In all cases, we show how our algorithms create models that have high predictive power yet allow accurate extraction of subsets of their training data.",
                        "Citation Paper Authors": "Authors:Congzheng Song, Thomas Ristenpart, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "show that\nwell-generalized models can leak membership information, but the\nadversary must first identify a handful of vulnerable records in\nthe training dataset. Yeom et al. ",
                    "Citation Text": "S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning:\nAnalyzing the connection to overfitting. In CSF, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01604",
                        "Citation Paper Title": "Title:Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.\nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                        "Citation Paper Authors": "Authors:Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "use membership inference to evaluate the\ntradeoff between test accuracy and membership privacy in differ-\nentially private ML models. Hayes et al. ",
                    "Citation Text": "J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro. LOGAN: Membership\ninference attacks against generative models. In PETS , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07663",
                        "Citation Paper Title": "Title:LOGAN: Membership Inference Attacks Against Generative Models",
                        "Citation Paper Abstract": "Abstract:Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator's capacity to learn statistical differences in distributions.\nWe present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
                        "Citation Paper Authors": "Authors:Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00357v2": {
            "Paper Title": "Latent Variable Model for Multi-modal Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12739v2": {
            "Paper Title": "Neural separation of observed and unobserved distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.03782v3": {
            "Paper Title": "CoT: Cooperative Training for Generative Modeling of Discrete Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09492v5": {
            "Paper Title": "Learning to Attend On Essential Terms: An Enhanced Retriever-Reader\n  Model for Open-domain Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08733v2": {
            "Paper Title": "Neural Transductive Learning and Beyond: Morphological Generation in the\n  Minimal-Resource Setting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01479v5": {
            "Paper Title": "UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10652v3": {
            "Paper Title": "Show, Control and Tell: A Framework for Generating Controllable and\n  Grounded Captions",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "generates descriptions for speci\ufb01c image re-\ngions. Further, the Neural Baby Talk approach ",
                    "Citation Text": "J. Lu, J. Yang, D. Batra, and D. Parikh. Neural Baby Talk.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , 2018. 1, 2, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09845",
                        "Citation Paper Title": "Title:Neural Baby Talk",
                        "Citation Paper Abstract": "Abstract:We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". In particular, we employ the model\n\ufb01netuned on the Visual Genome dataset ",
                    "Citation Text": "R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,\nS. Chen, Y . Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-\nstein, and L. Fei-Fei. Visual Genome: Connecting Language\nand Vision Using Crowdsourced Dense Image Annotations.\nInternational Journal of Computer Vision , 123(1):32\u201373,\n2017. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.07332",
                        "Citation Paper Title": "Title:Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                        "Citation Paper Abstract": "Abstract:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.09536v6": {
            "Paper Title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01696v2": {
            "Paper Title": "TVQA: Localized, Compositional Video Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09672v3": {
            "Paper Title": "BanditSum: Extractive Summarization as a Contextual Bandit",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02134v2": {
            "Paper Title": "Transfer learning of language-independent end-to-end ASR with language\n  model fusion",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "algorithm wi th an\ninitial epsilon 1e\u22128. Epsilon was divided by a factor of 0.01 when\nthe teacher-forcing accuracy does not improve for the valid ation set\nat each epoch. Scheduled sampling ",
                    "Citation Text": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Sh azeer,\n\u201cScheduled sampling for sequence prediction with recurren t neural net-\nworks,\u201d in Proc. of NIPS , 2015, pp. 1171\u20131179.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.03099",
                        "Citation Paper Title": "Title:Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.",
                        "Citation Paper Authors": "Authors:Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.01064v4": {
            "Paper Title": "Improving Sentence Representations with Consensus Maximisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02971v2": {
            "Paper Title": "Improving Retrieval-Based Question Answering with Deep Inference Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.06422v3": {
            "Paper Title": "Evaluating neural network explanation methods using hybrid documents and\n  morphological agreement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.11359v4": {
            "Paper Title": "Automatic Generation of Chinese Short Product Titles for Mobile Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.11326v4": {
            "Paper Title": "Deep Cascade Multi-task Learning for Slot Filling in Online Shopping\n  Assistant",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.07889v2": {
            "Paper Title": "Improving Aspect Term Extraction with Bidirectional Dependency Tree\n  Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.06313v2": {
            "Paper Title": "Paying Attention to Multi-Word Expressions in Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06826v2": {
            "Paper Title": "Measuring Semantic Abstraction of Multilingual NMT with Paraphrase\n  Recognition and Generation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11181v2": {
            "Paper Title": "Neural Modular Control for Embodied Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "policy-gradient algorithm. Speci\ufb01cally, for the master policy, the gradient of the expected reward\nis written as:\nr\u0012Jp\u0019\u0012q\u0010Err\u0012logp\u0019\u0012pgi|sTiqqpQpsTi;giq\u0001c\u0012psTiqqs (4)\nwherec\u0012psTiqis the estimated value of sTiproduced by the critic for \u0019\u0012. To further reduce variance,\nwe follow ",
                    "Citation Text": "J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\ngeneralized advantage estimation. In ICLR , 2016.\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02438",
                        "Citation Paper Title": "Title:High-Dimensional Continuous Control Using Generalized Advantage Estimation",
                        "Citation Paper Abstract": "Abstract:Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.\nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
                        "Citation Paper Authors": "Authors:John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "Our work builds on and is related to prior work in hierarchical reinforcement and imitation learning,\ngrounded language learning, and embodied question-answering agents in simulated environments.\nHierarchical Reinforcement and Imitation Learning. Our formulation is closely related to Le et\nal. ",
                    "Citation Text": "H. M. Le, N. Jiang, A. Agarwal, M. Dud\u00edk, Y . Yue, and H. Daum\u00e9 III. Hierarchical imitation and\nreinforcement learning. In ICML , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.00590",
                        "Citation Paper Title": "Title:Hierarchical Imitation and Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma's Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.",
                        "Citation Paper Authors": "Authors:Hoang M. Le, Nan Jiang, Alekh Agarwal, Miroslav Dud\u00edk, Yisong Yue, Hal Daum\u00e9 III"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.02058v2": {
            "Paper Title": "The Marchex 2018 English Conversational Telephone Speech Recognition\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03169v3": {
            "Paper Title": "Short-Term Meaning Shift: A Distributional Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07290v2": {
            "Paper Title": "The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem\n  Solvers",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". A widely-adopted practice is to de\ufb01ne the\nprobability of each instance of derivation ybased on the feature\nrepresentation xfor a text problem and a parameter vector \u03b8, as\nin ",
                    "Citation Text": "S. Roy, S. Upadhyay, and D. Roth, \u201cEquation parsing : Map ping\nsentences to grounded equations,\u201d in EMNLP , 2016, pp. 1088\u20131097.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08824",
                        "Citation Paper Title": "Title:Equation Parsing: Mapping Sentences to Grounded Equations",
                        "Citation Paper Abstract": "Abstract:Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$ of the time, it also identifies the correct noun phrase $\\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.",
                        "Citation Paper Authors": "Authors:Subhro Roy, Shyam Upadhyay, Dan Roth"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "2018 78.5 73.3 75.5 52.96 72.68 30.06 60.25 -\nSeq2SeqET ",
                    "Citation Text": "L. Wang, Y . Wang, D. Cai, D. Zhang, and X. Liu, \u201cTranslati ng\nmath word problem to expression tree,\u201d in EMNLP . Association for\nComputational Linguistics, 2018, pp. 1064\u20131069.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.05632",
                        "Citation Paper Title": "Title:Translating a Math Word Problem to an Expression Tree",
                        "Citation Paper Abstract": "Abstract:Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains: a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of maximum likelihood estimation. In this paper, by considering the uniqueness of expression tree, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each model has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23K show that the ensemble model with equation normalization significantly outperforms the previous state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.09121v3": {
            "Paper Title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive\n  Meaning Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02226v3": {
            "Paper Title": "A Concept Specification and Abstraction-based Semantic Representation:\n  Addressing the Barriers to Rule-based Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09055v2": {
            "Paper Title": "DARTS: Differentiable Architecture Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05250v2": {
            "Paper Title": "Modality Attention for End-to-End Audio-visual Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "are discusse d in\nsubsection 2.4.\n2.1. End-to-end Model\nThe attention based Seq2seq architecture ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,\n\u201cNeural machine translation by jointly learning to align an d\ntranslate,\u201d arXiv preprint arXiv:1409.0473 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1707.08608v3": {
            "Paper Title": "Gradient-based Inference for Networks with Output Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12889v3": {
            "Paper Title": "Systematic Generalization: What Is Required and Can It Be Learned?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11535v2": {
            "Paper Title": "News Article Teaser Tweets and How to Generate Them",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05740v2": {
            "Paper Title": "Using Search Queries to Understand Health Information Needs in Africa",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00570v3": {
            "Paper Title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\n  Study on Dependency Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06683v3": {
            "Paper Title": "FlowQA: Grasping Flow in History for Conversational Machine\n  Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05634v2": {
            "Paper Title": "Adversarial Inference for Multi-Sentence Video Description",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "propose the Move Forward and Tell ap-\nproach, which localizes events and progressively decides\nwhen to generate the next sentence. This is related to the\ntask of dense captioning ",
                    "Citation Text": "Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-captioning events in videos. In\nProceedings of the IEEE International Conference on Com-\nputer Vision (ICCV) , pages 706\u2013715, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.00754",
                        "Citation Paper Title": "Title:Dense-Captioning Events in Videos",
                        "Citation Paper Abstract": "Abstract:Most natural videos contain numerous events. For example, in a video of a \"man playing a piano\", the video might also contain \"another man dancing\" or \"a crowd clapping\". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "propose a joint training ap-\nproach which incorporates multi-level adversarial discrim-\ninators, one for sentence level and another for coherent\ntopic transition at a paragraph level. ",
                    "Citation Text": "Xin Wang, Wenhu Chen, Yuan-Fang Wang, and\nWilliam Yang Wang. No metrics are perfect: Adver-\nsarial reward learning for visual storytelling. In Proceedings\nof the Annual Meeting of the Association for Computational\nLinguistics (ACL) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09160",
                        "Citation Paper Title": "Title:No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling",
                        "Citation Paper Abstract": "Abstract:Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",
                        "Citation Paper Authors": "Authors:Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "instead rely on\npolicy gradient, and their discriminator focuses on caption\nnaturalness and image relevance. Some works have applied\nadversarial learning to generate paragraph descriptions for\nimages/image sequences. ",
                    "Citation Text": "Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and\nEric P Xing. Recurrent topic-transition gan for visual para-\ngraph generation. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.07022",
                        "Citation Paper Title": "Title:Recurrent Topic-Transition GAN for Visual Paragraph Generation",
                        "Citation Paper Abstract": "Abstract:A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.",
                        "Citation Paper Authors": "Authors:Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "train a discriminator to distinguish natural human\ncaptions from fake generated captions, focusing on caption\ndiversity and image relevance. To sample captions they rely\non Gumbel-Softmax approximation ",
                    "Citation Text": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-\nrameterization with gumbel-softmax. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01144",
                        "Citation Paper Title": "Title:Categorical Reparameterization with Gumbel-Softmax",
                        "Citation Paper Abstract": "Abstract:Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",
                        "Citation Paper Authors": "Authors:Eric Jang, Shixiang Gu, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "by using a mixed loss\n(both cross-entropy and RL) and correcting CIDEr with an\nentailment penalty. ",
                    "Citation Text": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, and\nWilliam Yang Wang. Video captioning via hierarchical rein-\nforcement learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n4213\u20134222, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11135",
                        "Citation Paper Title": "Title:Video Captioning via Hierarchical Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.",
                        "Citation Paper Authors": "Authors:Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "propose a \ufb01ne-grained\nvideo captioning model for generating detailed sports nar-\nratives, and ",
                    "Citation Text": "Yilei Xiong, Bo Dai, and Dahua Lin. Move forward and\ntell: A progressive generator of video descriptions. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10018",
                        "Citation Paper Title": "Title:Move Forward and Tell: A Progressive Generator of Video Descriptions",
                        "Citation Paper Abstract": "Abstract:We present an efficient framework that can generate a coherent paragraph to describe a given video. Previous works on video captioning usually focus on video clips. They typically treat an entire video as a whole and generate the caption conditioned on a single embedding. On the contrary, we consider videos with rich temporal structures and aim to generate paragraph descriptions that can preserve the story flow while being coherent and concise. Towards this goal, we propose a new approach, which produces a descriptive paragraph by assembling temporally localized descriptions. Given a video, it selects a sequence of distinctive clips and generates sentences thereon in a coherent manner. Particularly, the selection of clips and the production of sentences are done jointly and progressively driven by a recurrent network -- what to describe next depends on what have been said before. Here, the recurrent network is learned via self-critical sequence training with both sentence-level and paragraph-level rewards. On the ActivityNet Captions dataset, our method demonstrated the capability of generating high-quality paragraph descriptions for videos. Compared to those by other methods, the descriptions produced by our method are often more relevant, more coherent, and more concise.",
                        "Citation Paper Authors": "Authors:Yilei Xiong, Bo Dai, Dahua Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.10290v2": {
            "Paper Title": "Acquiring Annotated Data with Cross-lingual Explicitation for Implicit\n  Discourse Relation Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07889v4": {
            "Paper Title": "Predicting the Argumenthood of English Prepositional Phrases",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08462v2": {
            "Paper Title": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05611v4": {
            "Paper Title": "Learning Graph Embeddings from WordNet-based Similarity Measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.00582v2": {
            "Paper Title": "Data-to-Text Generation with Content Selection and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09042v2": {
            "Paper Title": "Adversarial Decomposition of Text Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09528v2": {
            "Paper Title": "ComQA: A Community-sourced Dataset for Complex Factoid Question\n  Answering with Paraphrase Clusters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00266v2": {
            "Paper Title": "Learning to Describe Phrases with Local and Global Contexts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00383v2": {
            "Paper Title": "Addressing word-order Divergence in Multilingual Neural Machine\n  Translation for extremely Low Resource Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04660v2": {
            "Paper Title": "Primal Meaning Recommendation via On-line Encyclopedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00783v2": {
            "Paper Title": "Abstractive Summarization of Reddit Posts with Multi-level Memory\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08115v2": {
            "Paper Title": "Modeling Word Emotion in Historical Language: Quantity Beats Supposed\n  Stability in Seed Word Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.03594v2": {
            "Paper Title": "Online Learning for Effort Reduction in Interactive Neural Machine\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02358v2": {
            "Paper Title": "Transfer Learning via Unsupervised Task Discovery for Visual Question\n  Answering",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "and learn to point an index of word candi-\ndates ",
                    "Citation Text": "Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. In-\ncorporating Copying Mechanism in Image Captioning\nfor Learning Novel Objects. In CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05271",
                        "Citation Paper Title": "Title:Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects",
                        "Citation Paper Abstract": "Abstract:Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.",
                        "Citation Paper Authors": "Authors:Ting Yao, Yingwei Pan, Yehao Li, Tao Mei"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "decompose image cap-\ntioning task into visual classi\ufb01cation and language mod-\neling, and exploit unpaired visual and linguistic data as\nadditional resources to train visual classi\ufb01er and language\nmodel, respectively. Recent approaches incorporate pointer\nnetworks ",
                    "Citation Text": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\nPointer Networks. In NIPS , 2015. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.03134",
                        "Citation Paper Title": "Title:Pointer Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.",
                        "Citation Paper Authors": "Authors:Oriol Vinyals, Meire Fortunato, Navdeep Jaitly"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Sharing aligned image-word representa-\ntions between VQA models and image classi\ufb01ers has been\nproposed in ",
                    "Citation Text": "Tanmay Gupta, Kevin Shih, Saurabh Singh, and\nDerek Hoiem. Aligned Image-Word Representations\nImprove Inductive Transfer across Vision-Language\nTasks. In CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00260",
                        "Citation Paper Title": "Title:Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks",
                        "Citation Paper Abstract": "Abstract:An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.",
                        "Citation Paper Authors": "Authors:Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "to provide additional inputs to VQA models are\ninvestigated in [39, 40, 41]. Transfer between VQA datasets\nis studied in ",
                    "Citation Text": "Hexiang Hu, Wei-Lun Chao, and Fei Sha. Learning\nAnswer Embeddings for Visual Question Answering.\nInCVPR , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03724",
                        "Citation Paper Title": "Title:Learning Answer Embeddings for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.",
                        "Citation Paper Authors": "Authors:Hexiang Hu, Wei-Lun Chao, Fei Sha"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "are frequently used to initialize parameters\nof question encoders [12, 30, 35]. Exploiting information\nretrieval from knowledge base [6, 7] or external vision algo-\nrithms ",
                    "Citation Text": "Peng Wang, Qi Wu, Chunhua Shen, and Anton\nvan den Hengel. The VQA-Machine: Learning How\nto Use Existing Vision Algorithms to Answer New\nQuestions. In CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.05386",
                        "Citation Paper Title": "Title:The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions",
                        "Citation Paper Abstract": "Abstract:One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from {image,question,answer} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. We propose here instead a more general and scalable approach which exploits the fact that very good methods to achieve these operations already exist, and thus do not need to be trained. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.",
                        "Citation Paper Authors": "Authors:Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "is a widely accepted standard for\ndiverse VQA models [12, 43]. As an alternative, object de-\ntector ",
                    "Citation Text": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. Faster R-CNN: Towards Real-Time Object De-\ntection with Region Proposal Networks. In NIPS ,\n2015. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01497",
                        "Citation Paper Title": "Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "or by pro-\nviding different biases to train and test sets intentionally ",
                    "Citation Text": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and\nAniruddha Kembhavi. Don\u2019t Just Assume; Look and\nAnswer: Overcoming Priors for Visual Question An-\nswering. In CVPR , 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00377",
                        "Citation Paper Title": "Title:Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",
                        "Citation Paper Authors": "Authors:Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "The standard VQA evaluation assumes identically dis-\ntributed train and test set [5, 29, 45]. As this evaluation\nsetting turns out to be vulnerable to models exploiting bi-\nases in training set ",
                    "Citation Text": "Yash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. Making the V in VQA\nMatter: Elevating the Role of Image Understanding in\nVisual Question Answering. In CVPR , 2017. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00837",
                        "Citation Paper Title": "Title:Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.\nWe propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at this http URL as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).\nWe further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.\nFinally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",
                        "Citation Paper Authors": "Authors:Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.12956v2": {
            "Paper Title": "Combining Distant and Direct Supervision for Neural Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04319v3": {
            "Paper Title": "Playing by the Book: An Interactive Game Approach for Action Graph\n  Extraction from Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10092v2": {
            "Paper Title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning\n  for Vision-Language Navigation",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "for vision-language navigation\nin real 3D environments, which is built upon the Matter-\nport3D dataset ",
                    "Citation Text": "A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nie\u00dfner,\nM. Savva, S. Song, A. Zeng, and Y . Zhang. Matterport3d:\nLearning from rgb-d data in indoor environments. arXiv\npreprint arXiv:1709.06158 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.05726v2": {
            "Paper Title": "Answering Science Exam Questions Using Query Rewriting with Background\n  Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04155v4": {
            "Paper Title": "Vision-based Navigation with Language-based Assistance via Imitation\n  Learning with Indirect Intervention",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "similarly use\nreinforcement learning in simulated 3D environments forsuccessful execution of written instructions. On the vision-\nlanguage navigation task ",
                    "Citation Text": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S \u00a8underhauf, Ian Reid, Stephen Gould, and\nAnton van den Hengel. Vision-and-language navigation: In-\nterpreting visually-grounded navigation instructions in real\nenvironments. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , volume 2, 2018.\n1, 2, 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07280",
                        "Citation Paper Title": "Title:Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
                        "Citation Paper Abstract": "Abstract:A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.",
                        "Citation Paper Authors": "Authors:Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, Anton van den Hengel"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "propose a hierarchical solution for this\ntask where each level of the hierarchy is independently\nwarmed up with imitation learning and further improved\nwith reinforcement learning. ",
                    "Citation Text": "Karl Moritz Hermann, Felix Hill, Simon Green, Fumin\nWang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wo-\njciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin,\net al. Grounded language learning in a simulated 3d world.\narXiv preprint arXiv:1706.06551 , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06551",
                        "Citation Paper Title": "Title:Grounded Language Learning in a Simulated 3D World",
                        "Citation Paper Abstract": "Abstract:We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.",
                        "Citation Paper Authors": "Authors:Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, Phil Blunsom"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "present\nthe \u201cEmbodied Question Answering\u201d (EmbodiedQA) task\nwhere an agent explores and answers questions about the\nenvironment. ",
                    "Citation Text": "Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh,\nand Dhruv Batra. Neural modular control for embodied\nquestion answering. Proceedings of the Conference on Robot\nLearning , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11181",
                        "Citation Paper Title": "Title:Neural Modular Control for Embodied Question Answering",
                        "Citation Paper Abstract": "Abstract:We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.).\nWe use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies.\nOn the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.",
                        "Citation Paper Authors": "Authors:Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ",\nverbal constraints are used for safe robot navigation in com-\nplex real-world environments.\nSimulated environments. Simple simulators as Puddle\nWorld Navigation ",
                    "Citation Text": "Michael Janner, Karthik Narasimhan, and Regina Barzi-\nlay. Representation learning for grounded spatial reasoning.\nTransactions of the Association of Computational Linguis-\ntics, 6:49\u201361, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.03938",
                        "Citation Paper Title": "Title:Representation Learning for Grounded Spatial Reasoning",
                        "Citation Paper Abstract": "Abstract:The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.",
                        "Citation Paper Authors": "Authors:Michael Janner, Karthik Narasimhan, Regina Barzilay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.00271v2": {
            "Paper Title": "Learning Speaker Representations with Mutual Information",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "authors proposed a method called Contrastive Predicting\nCoding (CPC), that learns representations by predicting the fu-\nture in a latent space. It uses an autoregressive model optimized\nwith a probabilistic contrastive loss. In ",
                    "Citation Text": "R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal,\nA. Trischler, and Y . Bengio, \u201cLearning deep representations by\nmutual information estimation and maximization,\u201d arXiv e-prints ,\nvol. 1808.06670, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.06670",
                        "Citation Paper Title": "Title:Learning deep representations by mutual information estimation and maximization",
                        "Citation Paper Abstract": "Abstract:In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",
                        "Citation Paper Authors": "Authors:R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", a GAN that minimizes MI using positive and negative\nsamples has been proposed for Independent Component Anal-\nysis (ICA). A similar approach can be used to maximize MI.\nIn ",
                    "Citation Text": "A. van den Oord, Y . Li, and O. Vinyals, \u201cRepresentation learning\nwith contrastive predictive coding,\u201d CoRR , vol. abs/1807.03748,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.00070v2": {
            "Paper Title": "Effective Feature Representation for Clinical Text Concept Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11653v2": {
            "Paper Title": "LSTMs Exploit Linguistic Attributes of Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.08854v2": {
            "Paper Title": "pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence\n  Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01216v3": {
            "Paper Title": "Disentangling Language and Knowledge in Task-Oriented Dialogs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.13083v3": {
            "Paper Title": "GraphIE: A Graph-Based Framework for Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.03602v2": {
            "Paper Title": "The Lifted Matrix-Space Model for Semantic Composition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01694v2": {
            "Paper Title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07430v2": {
            "Paper Title": "Understanding and Measuring Psychological Stress using Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01116v2": {
            "Paper Title": "Bi-Directional Differentiable Input Reconstruction for Low-Resource\n  Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.07849v4": {
            "Paper Title": "Mutual Information Maximization for Simple and Accurate Part-Of-Speech\n  Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06194v2": {
            "Paper Title": "The Fast and the Flexible: training neural networks to learn to follow\n  instructions from small data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01697v2": {
            "Paper Title": "Learning to Explicitate Connectives with Seq2Seq Network for Implicit\n  Discourse Relation Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04210v2": {
            "Paper Title": "Densely Connected Attention Propagation for Reading Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.13327v2": {
            "Paper Title": "Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.07781v2": {
            "Paper Title": "Responsible team players wanted: an analysis of soft skill requirements\n  in job advertisements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04081v3": {
            "Paper Title": "Chat-crowd: A Dialog-based Platform for Visual Layout Composition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06963v3": {
            "Paper Title": "Multi-task Learning with Sample Re-weighting for Machine Reading\n  Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.00681v3": {
            "Paper Title": "Extracting an English-Persian Parallel Corpus from Comparable Corpora",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.03450v2": {
            "Paper Title": "Active Learning for New Domains in Natural Language Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07042v2": {
            "Paper Title": "CoQA: A Conversational Question Answering Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06570v3": {
            "Paper Title": "Detecting cognitive impairments by agreeing on interpretations of\n  linguistic features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.03557v2": {
            "Paper Title": "DeepTingle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04655v2": {
            "Paper Title": "Not Just Depressed: Bipolar Disorder Prediction on Reddit",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02980v2": {
            "Paper Title": "Personality facets recognition from text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10830v2": {
            "Paper Title": "From Recognition to Cognition: Visual Commonsense Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 86,
                    "Sentence": "typically answered with a short phrase. This\nline of work also includes \u2018pointing\u2019 questions [45, 93] and\ntemplated questions with open ended answers ",
                    "Citation Text": "Licheng Yu, Eunbyung Park, Alexander C. Berg, and\nTamara L. Berg. Visual Madlibs: Fill in the blank Im-\nage Generation and Question Answering. arXiv:1506.00278\n[cs], May 2015. arXiv: 1506.00278. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.00278",
                        "Citation Paper Title": "Title:Visual Madlibs: Fill in the blank Image Generation and Question Answering",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.",
                        "Citation Paper Authors": "Authors:Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "was one of the \ufb01rst large-scale datasets that framed visual\nunderstanding as a QA task, with questions about COCO\nimages ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740\u2013755.\nSpringer, 2014. 4, 8, 9, 10, 18",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1802.00209v3": {
            "Paper Title": "Dual Recurrent Attention Units for Visual Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.00912v4": {
            "Paper Title": "Tensorized Self-Attention: Efficiently Modeling Pairwise and Global\n  Dependencies Together",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07688v2": {
            "Paper Title": "Role of Intonation in Scoring Spoken English",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01628v2": {
            "Paper Title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03006v3": {
            "Paper Title": "Page Stream Segmentation with Convolutional Neural Nets Combining\n  Textual and Visual Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.00549v2": {
            "Paper Title": "Emergence of Communication in an Interactive World with Consistent\n  Speakers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12686v2": {
            "Paper Title": "Evaluating Text GANs as Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09844v2": {
            "Paper Title": "Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ".\n7in the image or database representation. An extension of this work is described in ",
                    "Citation Text": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman,\nFei-Fei Li, C. Lawrence Zitnick, and Ross B. Girshick. Inferring and execut-\ning programs for visual reasoning. arXiv:1705.03633v1 [cs.CV] , pages 1\u201313,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.03633",
                        "Citation Paper Title": "Title:Inferring and Executing Programs for Visual Reasoning",
                        "Citation Paper Abstract": "Abstract:Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.13441v2": {
            "Paper Title": "Improving Machine Reading Comprehension with General Reading Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03519v2": {
            "Paper Title": "Few-shot learning with attention-based sequence-to-sequence models",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ", leading to\nfurther improvement. Another concern pertains to the global atten-\ntion mechanism which is a bit too \ufb02exible for speech recognition\n(an essentially monotonic left-to-right process). Ways to encour-\nage monotonicity ",
                    "Citation Text": "Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and\nYoshua Bengio, \u201cEnd-to-end Continuous Speech Recogni-\ntion using Attention-based Recurrent NN: First Results,\u201d in\npresented at NIPS Workshop on Deep Learning and Repre-\nsentation Learning , Montr \u00b4eal, Canada, Dec. 2014, arXiv:\n1412.1602.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.1602",
                        "Citation Paper Title": "Title:End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results",
                        "Citation Paper Abstract": "Abstract:We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.",
                        "Citation Paper Authors": "Authors:Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.04616v3": {
            "Paper Title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with\n  Continuous Outputs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00235v3": {
            "Paper Title": "Learning to Caption Images through a Lifetime by Asking Questions",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". We call the reward from the caption scorer\ntheMix score, and denote it by r. We discuss challenges to\nusing a synthetic teacher in Sections 4.3 and 4.6.\n4. Experiments\nWe evaluate our approach on the challenging MSCOCO\ndataset ",
                    "Citation Text": "T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In European conference on computer\nvision , pages 740\u2013755. Springer, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". We baseline the re-\nward with the greedy decision reward (r\u0003)0(that is, what\nthe improved-caption would have been had DM sampled\ngreedily), following the self-critical policy gradient ",
                    "Citation Text": "S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel.\nSelf-critical sequence training for image captioning. arXiv\npreprint arXiv:1612.00563 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00563",
                        "Citation Paper Title": "Title:Self-critical Sequence Training for Image Captioning",
                        "Citation Paper Abstract": "Abstract:Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a \"baseline\" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",
                        "Citation Paper Authors": "Authors:Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, Vaibhava Goel"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "does\nnot retrain their classi\ufb01er. Our work differs from [33, 28] by\nproposing a way for the agent to learn in a lifetime setting.\nIn ",
                    "Citation Text": "J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. Weston.\nLearning through dialogue interactions by asking questions.\narXiv:1612.04936 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.04936",
                        "Citation Paper Title": "Title:Learning through Dialogue Interactions by Asking Questions",
                        "Citation Paper Abstract": "Abstract:A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such end-to-end learned interactive dialogue agents.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", a synthetic environment and questions are lim-\nited to programs rather than natural language. ",
                    "Citation Text": "J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh. Visual cu-\nriosity: Learning to ask questions to learn visual recognition.\narXiv preprint arXiv:1810.00912 , 2018.6. Supplementary Material\nThis supplementary contains details of the modules in\nour model, the training procedure, as well as additional\nqualitative examples. In Section 6.1 we discuss implemen-\ntation details of the captioner, question generator, decision\nmaker and VQA teacher. Furthermore, we describe the in-\nquisitive student and mute student in the lifetime setting. In\nSection 6.2 we discuss the challenges with asking N > 1\nquestions. In Section 6.3 we provide more detail on how\nhuman supervision is calculated for our experiments. In\nSection 6.4 we show an ablation study on the question gen-\nerator. Our study highlights the importance of each feature\nin the context used by the question generator. In Section 6.5\nwe show more qualitative examples and describe the failure\nmodes of our model.\n6.1. Implementation Details\n6.1.1 Lifetime Learning\nIn lifetime learning, data arrives in chunks. In the collection\nphase, the agent attempts to improve generated captions by\nquerying the teacher. In the update phase, the captioning\nmodule is updated on collected captions.\nIn our experiments, we vary the collection percentage\nH%and the size of the warmup chunk. Note: the size of\nthe warmup chunk is reported relative to the entire training\nsplit whereas % GT (reported in tables and \ufb01gures) is rel-\native to the total number of captions the baseline All GT is\ntrained on. For example 10% warmup refers to a dataset\nwith 11.3K/113K images and 57K/567K captions. We ex-\nplored the following settings.\n\u000fH%: 60, 70, 80, 90, 100%\n\u000fwarmup: 1, 3, 5, 10%\nIn the update phase, we train the captioner with ADAM",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00912",
                        "Citation Paper Title": "Title:Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition",
                        "Citation Paper Abstract": "Abstract:In an open-world setting, it is inevitable that an intelligent agent (e.g., a robot) will encounter visual objects, attributes or relationships it does not recognize. In this work, we develop an agent empowered with visual curiosity, i.e. the ability to ask questions to an Oracle (e.g., human) about the contents in images (e.g., What is the object on the left side of the red cube?) and build visual recognition model based on the answers received (e.g., Cylinder). In order to do this, the agent must (1) understand what it recognizes and what it does not, (2) formulate a valid, unambiguous and informative language query (a question) to ask the Oracle, (3) derive the parameters of visual classifiers from the Oracle response and (4) leverage the updated visual classifiers to ask more clarified questions. Specifically, we propose a novel framework and formulate the learning of visual curiosity as a reinforcement learning problem. In this framework, all components of our agent, visual recognition module (to see), question generation policy (to ask), answer digestion module (to understand) and graph memory module (to memorize), are learned entirely end-to-end to maximize the reward derived from the scene graph obtained by the agent as a consequence of the dialog with the Oracle. Importantly, the question generation policy is disentangled from the visual recognition system and specifics of the environment. Consequently, we demonstrate a sort of double generalization. Our question generation policy generalizes to new environments and a new pair of eyes, i.e., new visual system. Trained on a synthetic dataset, our results show that our agent learns new visual concepts significantly faster than several heuristic baselines, even when tested on synthetic environments with novel objects, as well as in a realistic environment.",
                        "Citation Paper Authors": "Authors:Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.10665v2": {
            "Paper Title": "Engaging Image Captioning Via Personality",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "A large body of work has focused on developing image\ncaptioning datasets and models that work on them. In this\npaper we also perform experiments on the COCO ",
                    "Citation Text": "X. Chen, H. Fang, T.-Y . Lin, R. Vedantam, S. Gupta,\nP. Doll \u00b4ar, and C. L. Zitnick. Microsoft coco captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.00325",
                        "Citation Paper Title": "Title:Microsoft COCO Captions: Data Collection and Evaluation Server",
                        "Citation Paper Abstract": "Abstract:In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.",
                        "Citation Paper Authors": "Authors:Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C. Lawrence Zitnick"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". In those tasks, simple word over-\nlap based automatic metrics are shown to perform weakly ",
                    "Citation Text": "C.-W. Liu, R. Lowe, I. V . Serban, M. Noseworthy,\nL. Charlin, and J. Pineau. How not to evaluate your\ndialogue system: An empirical study of unsupervised\nevaluation metrics for dialogue response generation.\narXiv preprint arXiv:1603.08023 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08023",
                        "Citation Paper Title": "Title:How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                        "Citation Paper Abstract": "Abstract:We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",
                        "Citation Paper Authors": "Authors:Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "which provides state-of-the-art performance\non ImagenNet image classi\ufb01cation, but has so far not been\napplied to captioning. For text encoding we use the lat-\nest advances in attention-based representations using Trans-\nformers ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural Infor-\nmation Processing Systems , pages 5998\u20136008, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Our work does not address\nthis issue. Another research direction is to attempt to pro-\nduce amusing captions either through wordplay (puns) ",
                    "Citation Text": "A. Chandrasekaran, D. Parikh, and M. Bansal. Punny\ncaptions: Witty wordplay in image descriptions. arXiv\npreprint arXiv:1704.08224 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.08224",
                        "Citation Paper Title": "Title:Punny Captions: Witty Wordplay in Image Descriptions",
                        "Citation Paper Abstract": "Abstract:Wit is a form of rich interaction that is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns, in image descriptions. We develop two approaches which involve retrieving witty descriptions for a given image from a large corpus of sentences, or generating them via an encoder-decoder neural network architecture. We compare our approach against meaningful baseline approaches via human studies and show substantial improvements. We find that when a human is subject to similar constraints as the model regarding word usage and style, people vote the image descriptions generated by our model to be slightly wittier than human-written witty descriptions. Unsurprisingly, humans are almost always wittier than the model when they are free to choose the vocabulary, style, etc.",
                        "Citation Paper Authors": "Authors:Arjun Chandrasekaran, Devi Parikh, Mohit Bansal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.01090v2": {
            "Paper Title": "Value-based Search in Execution Space for Mapping Instructions to\n  Programs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.00786v2": {
            "Paper Title": "Mapping Instructions to Actions in 3D Environments with Visual Goal\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00937v2": {
            "Paper Title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03408v2": {
            "Paper Title": "Beyond task success: A closer look at jointly learning to see, ask, and\n  GuessWhat",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.00807v5": {
            "Paper Title": "Learning Semantic Sentence Embeddings using Sequential Pair-wise\n  Discriminator",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00644v7": {
            "Paper Title": "Matching with Text Data: An Experimental Evaluation of Methods for\n  Matching Documents and of Measuring Match Quality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.05240v5": {
            "Paper Title": "Weakly-supervised Semantic Parsing with Abstract Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06278v2": {
            "Paper Title": "Survey of Computational Approaches to Lexical Semantic Change",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06682v2": {
            "Paper Title": "Trellis Networks for Sequence Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00613v3": {
            "Paper Title": "Shifting the Baseline: Single Modality Performance on Visual Navigation\n  & QA",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06267v2": {
            "Paper Title": "Neural Machine Translation of Text from Non-Native Speakers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.00372v2": {
            "Paper Title": "Mathematical foundations of matrix syntax",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06197v5": {
            "Paper Title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with\n  Reduced Overfitting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.04749v3": {
            "Paper Title": "Predicting Good Configurations for GitHub and Stack Overflow Topic\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06234v2": {
            "Paper Title": "Hierarchical Multitask Learning for CTC-based Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", and also some-\ntimes in natural language processing (NLP) ",
                    "Citation Text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.,\n\u201cDeep contextualized word representations,\u201d in Proc. Hu-\nman Language Technology/Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics\n(HLT/NAACL) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1806.11525v2": {
            "Paper Title": "Counting to Explore and Generalize in Text-based Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04122v3": {
            "Paper Title": "A Capsule Network-based Embedding Model for Knowledge Graph Completion\n  and Search Personalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02268v3": {
            "Paper Title": "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun\n  Translation in Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07617v2": {
            "Paper Title": "Towards Deep Conversational Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "; (2) A switching decoder inspired by Gulcehre et al. ",
                    "Citation Text": "Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing\nthe unknown words. In Proceedings of the 54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages 140\u2013149, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08148",
                        "Citation Paper Title": "Title:Pointing the Unknown Words",
                        "Citation Paper Abstract": "Abstract:The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context.~We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known.~We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.",
                        "Citation Paper Authors": "Authors:Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "involved collecting a movie recommendation\nthemed dialogue corpus with 24 dialogues, consisting of 2684 utterances and a mean of 112 utterances\nper dialogue. In contrast, our corpus has over 10k conversations and 160k utterances. See Serban\net al. ",
                    "Citation Text": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle Pineau. A survey\nof available corpora for building data-driven dialogue systems. arXiv:1512.05742 [cs.CL] ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.05742",
                        "Citation Paper Title": "Title:A Survey of Available Corpora for Building Data-Driven Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets, how they can be used to learn diverse dialogue strategies, and their other potential uses. We also examine methods for transfer learning between datasets and the use of external knowledge. Finally, we discuss appropriate choice of evaluation metrics for the learning objective.",
                        "Citation Paper Authors": "Authors:Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, Joelle Pineau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1702.06135v4": {
            "Paper Title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source\n  Sentences In Multiple Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.06744v4": {
            "Paper Title": "Learning to Organize Knowledge and Answer Questions with N-Gram Machines",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", which is a memory enhanced sequence-to-sequence model that translates questions\ninto programs in \u0015-calculus ",
                    "Citation Text": "P. Liang, M. I. Jordan, and D. Klein. Learning dependency-based compositional semantics. In\nAssociation for Computational Linguistics (ACL) , pages 590\u2013599, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1109.6841",
                        "Citation Paper Title": "Title:Learning Dependency-Based Compositional Semantics",
                        "Citation Paper Abstract": "Abstract:Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.\nOur goal is to learn a semantic parser from question-answer pairs instead, where the logical form is modeled as a latent variable. Motivated by this challenging learning problem, we develop a new semantic formalism, dependency-based compositional semantics (DCS), which has favorable linguistic, statistical, and computational properties. We define a log-linear distribution over DCS logical forms and estimate the parameters using a simple procedure that alternates between beam search and numerical optimization. On two standard semantic parsing benchmarks, our system outperforms all existing state-of-the-art systems, despite using no annotated logical forms.",
                        "Citation Paper Authors": "Authors:Percy Liang, Michael I. Jordan, Dan Klein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.03568v3": {
            "Paper Title": "Improving Question Answering by Commonsense-Based Pre-Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03604v2": {
            "Paper Title": "Federated Learning for Mobile Keyboard Prediction",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "recurrent neural network\ncalled the Coupled Input and Forget Gate (CIFG) ",
                    "Citation Text": "Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn \u00b4\u0131k,\nBas R. Steunebrink, and J \u00a8urgen Schmidhuber, \u201cLSTM:\nA search space odyssey,\u201d IEEE Trans. Neural Netw.\nLearning Syst. , vol. 28, no. 10, pp. 2222\u20132232, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.04069",
                        "Citation Paper Title": "Title:LSTM: A Search Space Odyssey",
                        "Citation Paper Abstract": "Abstract:Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ($\\approx 15$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.",
                        "Citation Paper Authors": "Authors:Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, J\u00fcrgen Schmidhuber"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.01088v2": {
            "Paper Title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate\n  Labeled-data Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12264v2": {
            "Paper Title": "Learning Comment Generation by Leveraging User-Generated Data",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nIn summarization, the pointer-generator networks with copy\nmechanism were proposed by ",
                    "Citation Text": "Abigail See, Peter J Liu, and Christopher D Manning,\n\u201cGet to the point: Summarization with pointer-generator\nnetworks,\u201d in Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , 2017, vol. 1, pp. 1073\u20131083.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "to copy words from the\nsource article to the generate a summary. Similarly to our\nwork, the mechanism triggers the model to take some impor-\ntant keywords in the article and use it to generate relevant\ncomments. ",
                    "Citation Text": "Andrea Madotto, Chien-Sheng Wu, and Pascale Fung,\n\u201cMem2seq: Effectively incorporating knowledge bases\ninto end-to-end task-oriented dialog systems,\u201d in Pro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) .\n2018, pp. 1468\u20131478, Association for Computational\nLinguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08217",
                        "Citation Paper Title": "Title:Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems",
                        "Citation Paper Abstract": "Abstract:End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",
                        "Citation Paper Authors": "Authors:Andrea Madotto, Chien-Sheng Wu, Pascale Fung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.10667v2": {
            "Paper Title": "Embedding Uncertain Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.03947v4": {
            "Paper Title": "textTOvec: Deep Contextualized Neural Autoregressive Topic Models of\n  Language with Distributed Compositional Prior",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07704v3": {
            "Paper Title": "A Regularized Framework for Sparse and Structured Neural Attention",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ".\nA different approach to incorporating structure into attention uses the posterior marginal probabilities\nfrom a conditional random \ufb01eld as attention weights ",
                    "Citation Text": "Y . Kim, C. Denton, L. Hoang, and A. M. Rush. Structured attention networks. In Proc. of ICLR ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.00887",
                        "Citation Paper Title": "Title:Structured Attention Networks",
                        "Citation Paper Abstract": "Abstract:Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
                        "Citation Paper Authors": "Authors:Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.07461v3": {
            "Paper Title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.07281v3": {
            "Paper Title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01241v2": {
            "Paper Title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.08471v3": {
            "Paper Title": "Locally Private Bayesian Inference for Count Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04284v2": {
            "Paper Title": "Improving End-to-end Speech Recognition with Pronunciation-assisted\n  Sub-word Modeling",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "improved upon sub-word methods\nin WFST-based speech recognition.\nApart from the application in ASR, the most recent tide of\nadopting sub-word representations is largely driven by neural\nmachine translation. ",
                    "Citation Text": "Rico Sennrich, Barry Haddow, and Alexandra Birch,\n\u201cNeural machine translation of rare words with subword\nunits,\u201d in Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2016,\nAugust 7-12, 2016, Berlin, Germany, Volume 1: Long\nPapers , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.07909",
                        "Citation Paper Title": "Title:Neural Machine Translation of Rare Words with Subword Units",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "to build a sub-word dictionary by greedily keep\nthe most frequent co-occurring character sequences. Concur-\nrently, ",
                    "Citation Text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing Liu,\nLukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku\nKudo, Hideto Kazawa, Keith Stevens, George Kurian,\nNishant Patil, Wei Wang, Cliff Young, Jason Smith,\nJason Riesa, Alex Rudnick, Oriol Vinyals, Greg Cor-\nrado, Macduff Hughes, and Jeffrey Dean, \u201cGoogle\u2019s\nneural machine translation system: Bridging the gap\nbetween human and machine translation,\u201d CoRR , vol.\nabs/1609.08144, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08144",
                        "Citation Paper Title": "Title:Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
                        "Citation Paper Authors": "Authors:Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.01969v2": {
            "Paper Title": "Summarizing Videos with Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05696v3": {
            "Paper Title": "Generating Multiple Diverse Responses for Short-Text Conversation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04556v6": {
            "Paper Title": "Unsupervised Controllable Text Formalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03865v2": {
            "Paper Title": "Multimodal Grounding for Sequence-to-Sequence Speech Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05701v3": {
            "Paper Title": "Plan-And-Write: Towards Better Automatic Storytelling",
            "Sentences": [
                {
                    "Sentence ID": 2016,
                    "Sentence": "presents a model that repro-\nduce different versions of a story from its symbolic repre-\nsentation. Pichotta and Mooney ",
                    "Citation Text": "Wang, Z.; He, W.; Wu, H.; Wu, H.; Li, W.; Wang, H.;\nand Chen, E. 2016. Chinese poetry generation with planning\nbased neural network. In COLING .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.09889",
                        "Citation Paper Title": "Title:Chinese Poetry Generation with Planning based Neural Network",
                        "Citation Paper Abstract": "Abstract:Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user's writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user's intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.",
                        "Citation Paper Authors": "Authors:Zhe Wang, Wei He, Hua Wu, Haiyang Wu, Wei Li, Haifeng Wang, Enhong Chen"
                    }
                },
                {
                    "Sentence ID": 2013,
                    "Sentence": "mined millions of personal stories from\nthe Web and identi\ufb01ed relevant existing stories in the cor-\npus. Li et al. ",
                    "Citation Text": "Rishes, E.; Lukin, S. M.; Elson, D. K.; and Walker,\nM. A. 2013. Generating different story tellings from seman-\ntic representations of narrative. In ICIDS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.08573",
                        "Citation Paper Title": "Title:Generating Different Story Tellings from Semantic Representations of Narrative",
                        "Citation Paper Abstract": "Abstract:In order to tell stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (NLG). However, there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events. In this paper we present an automatic method for converting from Scheherazade's story intention graph, a semantic representation, to the input required by the Personage NLG engine. Using 36 Aesop Fables distributed in DramaBank, a collection of story encodings, we train translation rules on one story and then test these rules by generating text for the remaining 35. The results are measured in terms of the string similarity metrics Levenshtein Distance and BLEU score. The results show that we can generate the 35 stories with correct content: the test set stories on average are close to the output of the Scheherazade realizer, which was customized to this semantic representation. We provide some examples of story variations generated by personage. In future work, we will experiment with measuring the quality of the same stories generated in different voices, and with techniques for making storytelling interactive.",
                        "Citation Paper Authors": "Authors:Elena Rishes, Stephanie M. Lukin, David K. Elson, Marilyn A. Walker"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.12343v2": {
            "Paper Title": "Content Selection in Deep Learning Models of Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04903v2": {
            "Paper Title": "Stream attention-based multi-array end-to-end speech recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05512v4": {
            "Paper Title": "Federated Learning for Keyword Spotting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07684v2": {
            "Paper Title": "Efficient keyword spotting using dilated convolutions and gating",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02122v2": {
            "Paper Title": "Robust and fine-grained prosody control of end-to-end speech synthesis",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "of Tacotron for the base encoder-\ndecoder architecture, but we used the original Tacotron ",
                    "Citation Text": "Yuxuan Wang, R.J. Skerry-Ryan, Daisy Stanton, Yonghui Wu,\nRon J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao,\nZhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgian-\nnakis, Rob Clark, and Rif A. Saurous, \u201cTacotron: Towards\nend-to-end speech synthesis,\u201d in Proc. Interspeech 2017 , 2017,\npp. 4006\u20134010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10135",
                        "Citation Paper Title": "Title:Tacotron: Towards End-to-End Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
                        "Citation Paper Authors": "Authors:Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". The output, denoted by p, is \ufb01xed-length prosody\nembedding. They enabled prosody transfers using the prosody em-\nbedding, but they could not gain control of prosody at a speci\ufb01c\npoint of time. Another problem was also reported ",
                    "Citation Text": "Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric\nBattenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A.\nSaurous, \u201cStyle tokens: Unsupervised style modeling, control\nand transfer in end-to-end speech synthesis,\u201d in Proceedings\nof the 35th International Conference on Machine Learning .\nPMLR, 2018, vol. 80, pp. 5180\u20135189.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09017",
                        "Citation Paper Title": "Title:Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",
                        "Citation Paper Authors": "Authors:Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, Ye Jia, Rif A. Saurous"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02802v2": {
            "Paper Title": "End-to-End Streaming Keyword Spotting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03255v2": {
            "Paper Title": "Multitask training with unlabeled data for end-to-end sign language\n  fingerspelling recognition",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "and machine translation\nFig. 1 . Structure of the proposed model (blue region: auto-\nencoder,\b: concatenation). The decoder component of the\nauto-encoder (the blue box on the right) is used only at train-\ning time. ",
                    "Citation Text": "D. Bahdanau, K. Cho, and Y . Bengio, \u201cNeural machine\ntranslation by jointly learning to align and translate,\u201d in\nICLR , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.11438v3": {
            "Paper Title": "American Sign Language fingerspelling recognition in the wild",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ".5For the detection network, we adapt the design\nof the Faster R-CNN object detector ",
                    "Citation Text": "S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN:\nTowards real-time object detection with region proposal\nnetworks,\u201d in NIPS , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01497",
                        "Citation Paper Title": "Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.03258v2": {
            "Paper Title": "Gaussian-Constrained training for speaker verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.09444v2": {
            "Paper Title": "Letter-Based Speech Recognition with Gated ConvNets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05920v2": {
            "Paper Title": "Speech and Speaker Recognition from Raw Waveform with SincNet",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". In\nthe spirit of reproducible research, we release the code of SincNet\non GitHub ",
                    "Citation Text": "M. Ravanelli, T. Parcollet, and Y . Bengio, \u201cThe PyTorch-Kaldi\nSpeech Recognition Toolkit,\u201d in arXiv:1811.07453 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.07453",
                        "Citation Paper Title": "Title:The PyTorch-Kaldi Speech Recognition Toolkit",
                        "Citation Paper Abstract": "Abstract:The availability of open-source software is playing a remarkable role in the popularization of speech recognition and deep learning. Kaldi, for instance, is nowadays an established framework used to develop state-of-the-art speech recognizers. PyTorch is used to build neural networks with the Python language and has recently spawn tremendous interest within the machine learning community thanks to its simplicity and flexibility.\nThe PyTorch-Kaldi project aims to bridge the gap between these popular toolkits, trying to inherit the efficiency of Kaldi and the flexibility of PyTorch. PyTorch-Kaldi is not only a simple interface between these software, but it embeds several useful features for developing modern speech recognizers. For instance, the code is specifically designed to naturally plug-in user-defined acoustic models. As an alternative, users can exploit several pre-implemented neural networks that can be customized using intuitive configuration files. PyTorch-Kaldi supports multiple feature and label streams as well as combinations of neural networks, enabling the use of complex neural architectures. The toolkit is publicly-released along with a rich documentation and is designed to properly work locally or on HPC clusters.\nExperiments, that are conducted on several datasets and tasks, show that PyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech recognizers.",
                        "Citation Paper Authors": "Authors:Mirco Ravanelli, Titouan Parcollet, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", where a set of parameterized Gaussian \ufb01lters are em-\nployed. This approach operates on the spectrogram domain, while\nSincNet directly considers the raw time domain waveform. Simi-\nlarly to our work, in ",
                    "Citation Text": "N. Zeghidour, N. Usunier, I. Kokkinos, T. Schatz, G. Synnaeve,\nand E. Dupoux, \u201cLearning \ufb01lterbanks from raw speech for\nphone recognition,\u201d in Proc. of ICASSP , 2018, pp. 5509\u20135513.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01161",
                        "Citation Paper Title": "Title:Learning Filterbanks from Raw Speech for Phone Recognition",
                        "Citation Paper Abstract": "Abstract:We train a bank of complex filters that operates on the raw waveform and is fed into a convolutional neural network for end-to-end phone recognition. These time-domain filterbanks (TD-filterbanks) are initialized as an approximation of mel-filterbanks, and then fine-tuned jointly with the remaining convolutional architecture. We perform phone recognition experiments on TIMIT and show that for several architectures, models trained on TD-filterbanks consistently outperform their counterparts trained on comparable mel-filterbanks. We get our best performance by learning all front-end steps, from pre-emphasis up to averaging. Finally, we observe that the filters at convergence have an asymmetric impulse response, and that some of them remain almost analytic.",
                        "Citation Paper Authors": "Authors:Neil Zeghidour, Nicolas Usunier, Iasonas Kokkinos, Thomas Schatz, Gabriel Synnaeve, Emmanuel Dupoux"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.06876v2": {
            "Paper Title": "Multi-task learning to improve natural language understanding",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nMulti-task learning for sequence-to-sequence models in Natural Language Pro-\ncessing is described in [8, 9, 10]. In ",
                    "Citation Text": "Luong, M.T., Le, Q.V ., Sutskever, I., Vinyals, O., Kaiser, L.: Multi-task se-\nquence to sequence learning. In: Proceedings of the Fourth International Con-\nference on Learning Representations (ICLR) (2016)Multi-task learning to improve natural language understanding 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06114",
                        "Citation Paper Title": "Title:Multi-task Sequence to Sequence Learning",
                        "Citation Paper Abstract": "Abstract:Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.06729v3": {
            "Paper Title": "You Shall Know the Most Frequent Sense by the Company it Keeps",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ", a well-known software package which learns vector\nembeddings from unlabelled monolingual data using a simple\nneural model. We denote the embedding of a word wasvw.\nWe adopt the assumption, supported by the work of ",
                    "Citation Text": "S. Arora, Y . Li, Y . Liang, T. Ma, and A. Risteski, \u201cLinear algebraic\nstructure of word senses, with applications to polysemy,\u201d arXiv preprint\narXiv:1601.03764 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.03764",
                        "Citation Paper Title": "Title:Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
                        "Citation Paper Abstract": "Abstract:Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 \"discourse atoms\" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.",
                        "Citation Paper Authors": "Authors:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1710.01025v3": {
            "Paper Title": "MMCR4NLP: Multilingual Multiway Corpora Repository for Natural Language\n  Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10973v3": {
            "Paper Title": "GLAC Net: GLocal Attention Cascading Networks for Multi-image Cued Story\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00324v3": {
            "Paper Title": "On the Winograd Schema: Situating Language Understanding in the\n  Data-Information-Knowledge Continuum",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04361v2": {
            "Paper Title": "RESIDE: Improving Distantly-Supervised Neural Relation Extraction using\n  Side Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.07427v2": {
            "Paper Title": "Multimodal Sentiment Analysis: Addressing Key Issues and Setting up the\n  Baselines",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05082v2": {
            "Paper Title": "A Unified Model for Opinion Target Extraction and Target Sentiment\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05599v2": {
            "Paper Title": "Improving Conditional Sequence Generative Adversarial Networks by\n  Stepwise Evaluation",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ". However,\nthe inconsistency of decoder inputs between training stage\nand inference stage leads to exposure bias ",
                    "Citation Text": "M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, \u201cSequence level\ntraining with recurrent neural networks,\u201d International Conference on\nLearning Representations , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06732",
                        "Citation Paper Title": "Title:Sequence Level Training with Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
                        "Citation Paper Authors": "Authors:Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". GANs improve machine translation models with\ndifferent network architectures including RNNSearch ",
                    "Citation Text": "L. Wu, Y . Xia, L. Zhao, F. Tian, T. Qin, J. Lai, and T.-Y . Liuu,\n\u201cAdversarial neural machine translation,\u201d in arXiv , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06933",
                        "Citation Paper Title": "Title:Adversarial Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English$\\rightarrow$French and German$\\rightarrow$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.",
                        "Citation Paper Authors": "Authors:Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.02135v3": {
            "Paper Title": "Expressive Speech Synthesis via Modeling Expressions with Variational\n  Autoencoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02050v2": {
            "Paper Title": "Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text\n  Translation",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "containing 16K tokens.\nASR model: Our ASR model follows the architecture of ",
                    "Citation Text": "C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Gonina, et al.,\n\u201cState-of-the-art speech recognition with sequence-to-sequence\nmodels,\u201d in Proc. ICASSP , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01769",
                        "Citation Paper Title": "Title:State-of-the-art Speech Recognition With Sequence-to-Sequence Models",
                        "Citation Paper Abstract": "Abstract:Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system.",
                        "Citation Paper Authors": "Authors:Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", create novel\nvoices by sampling from a continuous speaker embedding space ",
                    "Citation Text": "Y . Jia, Y . Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen,\nP. Nguyen, R. Pang, I. L. Moreno, and Y . Wu, \u201cTransfer learn-\ning from speaker veri\ufb01cation to multispeaker text-to-speech\nsynthesis,\u201d in Advances in NeurIPS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04558",
                        "Citation Paper Title": "Title:Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.",
                        "Citation Paper Authors": "Authors:Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ", since the network is trained to\npredict outputs from a pretrained MT model. In contrast, synthesizing\nspeech inputs using TTS is more similar to MT back-translation ",
                    "Citation Text": "R. Sennrich, B. Haddow, and A. Birch, \u201cImproving neural\nmachine translation models with monolingual data,\u201d in Proc.\nAssociation for Computational Linguistics (ACL) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06709",
                        "Citation Paper Title": "Title:Improving Neural Machine Translation Models with Monolingual Data",
                        "Citation Paper Abstract": "Abstract:Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.02047v6": {
            "Paper Title": "Classical linear logic, cobordisms and categorical semantics of\n  categorial grammars",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00228v3": {
            "Paper Title": "A sequential guiding network with attention for image captioning",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "were proposed as an extension of\nthe LSTM model by exploring different kind of semantic informa-\ntion that can be used as extra guiding input to the LSTM during\ndecoding steps. ",
                    "Citation Text": "Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei,\n\u201cBoosting image captioning with attributes,\u201d 2017 IEEE Inter-\nnational Conference on Computer Vision (ICCV) , pp. 4904\u2013\n4912, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01646",
                        "Citation Paper Title": "Title:Boosting Image Captioning with Attributes",
                        "Citation Paper Abstract": "Abstract:Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.",
                        "Citation Paper Authors": "Authors:Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "followed this direction by injecting more power-\nful high-level image attributes into the decoder. In their work, they\ninvestigate different architecture for injecting word occurrence pre-\ndiction attributes ",
                    "Citation Text": "Hao Fang, Saurabh Gupta, Forrest N. Iandola, Rupesh Kumar\nSrivastava, Li Deng, Piotr Doll \u00b4ar, Jianfeng Gao, Xiaodong He,\nMargaret Mitchell, John C. Platt, C. Lawrence Zitnick, and Ge-\noffrey Zweig, \u201cFrom captions to visual concepts and back,\u201d in\nCVPR , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4952",
                        "Citation Paper Title": "Title:From Captions to Visual Concepts and Back",
                        "Citation Paper Abstract": "Abstract:  This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",
                        "Citation Paper Authors": "Authors:Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll\u00e1r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.07066v2": {
            "Paper Title": "Detecting Incongruity Between News Headline and Body Text via a Deep\n  Hierarchical Encoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00146v3": {
            "Paper Title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05083v2": {
            "Paper Title": "Adversarial Learning of Semantic Relevance in Text to Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.00358v2": {
            "Paper Title": "A Systematic Classification of Knowledge, Reasoning, and Context within\n  the ARC Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08887v5": {
            "Paper Title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain\n  Semantic Parsing and Text-to-SQL Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06219v3": {
            "Paper Title": "GILE: A Generalized Input-Label Embedding for Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08895v3": {
            "Paper Title": "Neural Speech Synthesis with Transformer Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.07007v3": {
            "Paper Title": "QuaSE: Accurate Text Style Transfer under Quantifiable Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11749v3": {
            "Paper Title": "Unsupervised Text Style Transfer using Language Models as Discriminators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07882v2": {
            "Paper Title": "Guiding Policies with Language via Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02602v2": {
            "Paper Title": "Fast Neural Chinese Word Segmentation for Long Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07351v2": {
            "Paper Title": "Neural Multi-Task Learning for Citation Function and Provenance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.05300v4": {
            "Paper Title": "Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\n  Noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.08563v3": {
            "Paper Title": "Soft Label Memorization-Generalization for Natural Language Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05542v2": {
            "Paper Title": "Extractive Summary as Discrete Latent Variables",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06667v2": {
            "Paper Title": "Deep Transfer Reinforcement Learning for Text Summarization",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "or it could be the\nreward from a sequence coming from greedy selection\noverp\u0003\nt ",
                    "Citation Text": "R. Paulus, C. Xiong, and R. Socher. A deep reinforced\nmodel for abstractive summarization. arXiv preprint\narXiv:1705.04304 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04304",
                        "Citation Paper Title": "Title:A Deep Reinforced Model for Abstractive Summarization",
                        "Citation Paper Abstract": "Abstract:Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
                        "Citation Paper Authors": "Authors:Romain Paulus, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "was proposed for success-\nfully handling Out-of-Vocabulary (OOV) words. This\nmodel was later improved by using the coverage mecha-\nnism ",
                    "Citation Text": "A. See, P. J. Liu, and C. D. Manning. Get to the point:\nSummarization with pointer-generator networks. In\nACL, volume 1, pages 1073{1083, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "that for the\n\frst time used attention over a sequence-to-sequence\n(seq2seq) model for the problem of headline generation.\nTo further improve the performance of these models, the\npointer-generator model ",
                    "Citation Text": "R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre,\nand B. Xiang. Abstractive text summarization using\nsequence-to-sequence rnns and beyond. In SIGNLL ,pages 280{290, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.06023",
                        "Citation Paper Title": "Title:Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
                        "Citation Paper Abstract": "Abstract:In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",
                        "Citation Paper Authors": "Authors:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". These works range from\nfully extractive methods [4, 19, 35] to completely ab-\nstractive ones [28, 12, 8]. As one of the earliest works\non using neural networks for extractive summarization,\nNallapati et al. ",
                    "Citation Text": "R. Nallapati, F. Zhai, and B. Zhou. Summarunner:\nA recurrent neural network based sequence model for\nextractive summarization of documents. In AAAI ,\npages 3075{3081, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04230",
                        "Citation Paper Title": "Title:SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents",
                        "Citation Paper Abstract": "Abstract:We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.",
                        "Citation Paper Authors": "Authors:Ramesh Nallapati, Feifei Zhai, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\n2.4 Text Summarization. There is a vast amount\nof research work on the topic of text summarization us-\ning deep neural networks ",
                    "Citation Text": "T. Shi, Y. Keneshloo, N. Ramakrishnan, and C. K.\nReddy. Neural abstractive text summarization\nwith sequence-to-sequence models. arXiv preprint\narXiv:1812.02303 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02303",
                        "Citation Paper Title": "Title:Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
                        "Citation Paper Abstract": "Abstract:In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efficiency and parallelism for training a model. In this paper, we provide a comprehensive literature survey on different seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were first proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in NATS on the two recently released datasets, namely, Newsroom and Bytecup.",
                        "Citation Paper Authors": "Authors:Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". Usually, in these models, there is a\nteacher (larger model), a student (smaller model), and\nthe goal is to transfer knowledge from teacher to stu-\ndent. Recently, researchers have also used this idea to\ncreate models using meta-learning ",
                    "Citation Text": "A. Santoro, S. Bartunov, M. Botvinick, D. Wier-\nstra, and T. Lillicrap. Meta-learning with memory-\naugmented neural networks. In ICML , pages 1842{\n1850, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06065",
                        "Citation Paper Title": "Title:One-shot Learning with Memory-Augmented Neural Networks",
                        "Citation Paper Abstract": "Abstract:Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",
                        "Citation Paper Authors": "Authors:Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.00125v2": {
            "Paper Title": "Simple Fusion: Return of the Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12640v2": {
            "Paper Title": "Inferring Concept Prerequisite Relations from Online Educational\n  Resources",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08089v4": {
            "Paper Title": "Phonetic-and-Semantic Embedding of Spoken Words with Applications in\n  Spoken Content Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ".\n2.2. Stage 2 - Semantic Embedding over Phonetic Embed-\ndings Obtained in Stage 1\nAs shown in Figure 2, similar to the Word2Vec skip-gram\nmodel ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean, \u201cDistributed representations of\nwords and phrases and their compositionality,\u201d in Ad-\nvances in neural information processing systems , 2013,\npp. 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.08575v5": {
            "Paper Title": "Title-Guided Encoding for Keyphrase Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05337v2": {
            "Paper Title": "Hierarchical interpretations for neural network predictions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00647v2": {
            "Paper Title": "Real Time Monitoring of Social Media and Digital Press",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.08661v2": {
            "Paper Title": "COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02338v2": {
            "Paper Title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.01398v2": {
            "Paper Title": "Optimal Completion Distillation for Sequence Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05102v2": {
            "Paper Title": "Neural Relation Extraction Within and Across Sentence Boundaries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09906v2": {
            "Paper Title": "Diffusion Maps for Textual Network Embedding",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "uses information from structure,\ncontent and labels in a coupled neural network architecture, to learn the vertex representation. CENE ",
                    "Citation Text": "X. Sun, J. Guo, X. Ding, and T. Liu. A general framework for content-enhanced network\nrepresentation learning. arXiv preprint arXiv:1610.02906 , 2016.\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02906",
                        "Citation Paper Title": "Title:A General Framework for Content-enhanced Network Representation Learning",
                        "Citation Paper Abstract": "Abstract:This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.",
                        "Citation Paper Authors": "Authors:Xiaofei Sun, Jiang Guo, Xiao Ding, Ting Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", the random walk\nstrategy of DeepWalk is modi\ufb01ed for multi-scale representation learning. To exploit the distance\nbetween vertices, LINE ",
                    "Citation Text": "J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information\nnetwork embedding. In Proceedings of the 24th International Conference on World Wide Web .\nInternational World Wide Web Conferences Steering Committee, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.03578",
                        "Citation Paper Title": "Title:LINE: Large-scale Information Network Embedding",
                        "Citation Paper Abstract": "Abstract:This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.",
                        "Citation Paper Authors": "Authors:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "transform feature vectors of vertices into an af\ufb01nity graph, and then solve for the leading\neigenvectors as the embedding. Recent NE models focus on learning the vectorial representation of\nexisting networks. For example, DeepWalk ",
                    "Citation Text": "B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In\nProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and\ndata mining . ACM, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.06876v3": {
            "Paper Title": "Adversarial training for multi-context joint entity and relation\n  extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.07104v5": {
            "Paper Title": "Hierarchical Multi Task Learning With CTC",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02322v5": {
            "Paper Title": "Memory Augmented Policy Optimization for Program Synthesis and Semantic\n  Parsing",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "prioritizes replays\nbased on temporal-difference error for more ef\ufb01cient optimization. Hindsight experience replay ",
                    "Citation Text": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience\nreplay. NIPS , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01495",
                        "Citation Paper Title": "Title:Hindsight Experience Replay",
                        "Citation Paper Abstract": "Abstract:Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.\nWe demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",
                        "Citation Paper Authors": "Authors:Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "enables storage and usage of past experiences to\nimprove the sample ef\ufb01ciency of RL algorithms. Prioritized experience replay ",
                    "Citation Text": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\nICLR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05952",
                        "Citation Paper Title": "Title:Prioritized Experience Replay",
                        "Citation Paper Abstract": "Abstract:Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",
                        "Citation Paper Authors": "Authors:Tom Schaul, John Quan, Ioannis Antonoglou, David Silver"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10628v2": {
            "Paper Title": "Intent Detection and Slots Prompt in a Closed-Domain Chatbot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.04439v2": {
            "Paper Title": "Extractive Summarization using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04558v4": {
            "Paper Title": "Transfer Learning from Speaker Verification to Multispeaker\n  Text-To-Speech Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ".\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, V oiceLoop ",
                    "Citation Text": "Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. V oiceLoop: V oice \ufb01tting and\nsynthesis via a phonological loop. In Proc. International Conference on Learning Representa-\ntions (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06588",
                        "Citation Paper Title": "Title:VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
                        "Citation Paper Abstract": "Abstract:We present a new neural text to speech (TTS) method that is able to transform text to speech in voices that are sampled in the wild. Unlike other systems, our solution is able to deal with unconstrained voice samples and without requiring aligned phonemes or linguistic features. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. The speakers are similarly represented by a short vector that can also be fitted to new identities, even with only a few samples. Variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on several datasets demonstrate convincing capabilities, making TTS accessible to a wider range of applications. In order to promote reproducibility, we release our source code and models.",
                        "Citation Paper Authors": "Authors:Yaniv Taigman, Lior Wolf, Adam Polyak, Eliya Nachmani"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep V oice 3 ",
                    "Citation Text": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep V oice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07654",
                        "Citation Paper Title": "Title:Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
                        "Citation Paper Abstract": "Abstract:We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.",
                        "Citation Paper Authors": "Authors:Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s ",
                    "Citation Text": "Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis,\nRob Clark, and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis. In Proc.\nInterspeech , pages 4006\u20134010, August 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10135",
                        "Citation Paper Title": "Title:Tacotron: Towards End-to-End Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
                        "Citation Paper Authors": "Authors:Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of ICLR , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.05035v2": {
            "Paper Title": "Structured Neural Topic Models for Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06640v2": {
            "Paper Title": "Adversarial Text Generation Without Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00400v1": {
            "Paper Title": "Sentence-Level Sentiment Analysis of Financial News Using Distributed\n  Text Representations and Multi-Instance Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00399v1": {
            "Paper Title": "Unary and Binary Classification Approaches and their Implications for\n  Authorship Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11709v1": {
            "Paper Title": "Cross-language Citation Recommendation via Hierarchical Representation\n  Learning on Heterogeneous Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07853v3": {
            "Paper Title": "On Folding and Twisting (and whatknot): towards a characterization of\n  workspaces in syntax",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.05851v2": {
            "Paper Title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in\n  Knowledge Bases using Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09321v2": {
            "Paper Title": "Multiple topic identification in telephone conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11321v1": {
            "Paper Title": "Attention-Based Capsule Networks with Dynamic Routing for Relation\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11270v1": {
            "Paper Title": "Weakly-Supervised Hierarchical Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01122v1": {
            "Paper Title": "Machine Translation: A Literature Review",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "but is architecturally simpler than the latter.\nThe local attention can be viewed as a blend between soft and hard alignment approaches proposed\nby Xu et al. ",
                    "Citation Text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. CoRR , abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.\n03044 .\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.03044",
                        "Citation Paper Title": "Title:Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
                        "Citation Paper Abstract": "Abstract:Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
                        "Citation Paper Authors": "Authors:Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "proposed an end-to-end RNN architecture, they use it only to get additional\nphrase translation table to be eventually used in the SMT based system. Sutskever et al. ",
                    "Citation Text": "Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural net-\nworks. In Proceedings of the 27th International Conference on Neural Information Processing\nSystems - Volume 2 , NIPS\u201914, pages 3104\u20133112, Cambridge, MA, USA, 2014. MIT Press. URL\nhttp://dl.acm.org/citation.cfm?id=2969033.2969173 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.3215",
                        "Citation Paper Title": "Title:Sequence to Sequence Learning with Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
                        "Citation Paper Authors": "Authors:Ilya Sutskever, Oriol Vinyals, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "study the\neffectiveness of NMT systems in spoken language domains by using IWSLT 2015 dataset. They\nexplore two scenarios: NMT adaptation and NMT for low resource translation . For NMT adaptation\ntask, they take an existing state-of-the-art English-German system ",
                    "Citation Text": "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. CoRR , abs/1508.04025, 2015. URL http://arxiv.org/\nabs/1508.04025 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04025",
                        "Citation Paper Title": "Title:Effective Approaches to Attention-based Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Recently, continuous representations have been\nproposed for phrases and sentences and have been shown to carry task-dependent information to help\ndownstream language processing tasks (Grefenstette et al. ",
                    "Citation Text": "Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pul-\nman. Concrete sentence spaces for compositional distributional models of meaning. CoRR ,\nabs/1101.0309, 2011. URL http://arxiv.org/abs/1101.0309 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1101.0309",
                        "Citation Paper Title": "Title:Concrete Sentence Spaces for Compositional Distributional Models of Meaning",
                        "Citation Paper Abstract": "Abstract:Coecke, Sadrzadeh, and Clark (arXiv:1003.4394v1 [cs.CL]) developed a compositional model of meaning for distributional semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map, by constructing a corpus-based vector space for the type of sentence. Our construction method is based on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This enables us to compare meanings of sentences by simply taking the inner product of their vectors.",
                        "Citation Paper Authors": "Authors:Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, Stephen Pulman"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "showed that indeed the performance of a\nbasic encoder\u2013decoder deteriorates rapidly as the length of an input sentence increases. Bahdanau et\nal. ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.11158v1": {
            "Paper Title": "MEETING BOT: Reinforcement Learning for Dialogue Based Meeting\n  Scheduling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07325v3": {
            "Paper Title": "An Attention-Gated Convolutional Neural Network for Sentence\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10901v1": {
            "Paper Title": "Knowledge Representation Learning: A Quantitative Review",
            "Sentences": [
                {
                    "Sentence ID": 112,
                    "Sentence": "propose a\nbag-of-entity model which represents queries and articles with their entities. Moreover, ",
                    "Citation Text": "G.-H. Nguyen, L. Tamine, L. Soulier, N. Bricon-Souf, Toward a deep neural\napproach for knowledge-based ir, arXiv preprint arXiv:1606.07211.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07211",
                        "Citation Paper Title": "Title:Toward a Deep Neural Approach for Knowledge-Based IR",
                        "Citation Paper Abstract": "Abstract:This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases (KBs) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by recent works dealing with deep representation learn ing of texts. With this in mind, we argue that embedding KBs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations. In this paper, we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations.",
                        "Citation Paper Authors": "Authors:Gia-Hung Nguyen, Lynda Tamine, Laure Soulier, Nathalie Bricon-Souf"
                    }
                },
                {
                    "Sentence ID": 108,
                    "Sentence": "propose a neural generative question answering model which explores how to\nutilize the facts in KGs to answer simple factoid questions. Besides, KRL models is\nalso applied in ",
                    "Citation Text": "I. V . Serban, A. Garc \u00b4\u0131a-Dur \u00b4an, C. Gulcehre, S. Ahn, S. Chandar, A. Courville,\nY . Bengio, Generating factoid questions with recurrent neural networks: The\n30m factoid question-answer corpus, in: Proceedings of ACL, 2016, pp. 588\u2013\n598.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06807",
                        "Citation Paper Title": "Title:Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus",
                        "Citation Paper Abstract": "Abstract:Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.",
                        "Citation Paper Authors": "Authors:Iulian Vlad Serban, Alberto Garc\u00eda-Dur\u00e1n, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 105,
                    "Sentence": "proposes to combine TransE and existing distantly supervised RE models\nto extract novel facts, and obtains lots of improvements. Moreover, ",
                    "Citation Text": "X. Han, Z. Liu, M. Sun, Joint representation learning of text and knowledge for\nknowledge graph completion, arXiv preprint arXiv:1611.04125.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04125",
                        "Citation Paper Title": "Title:Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion",
                        "Citation Paper Abstract": "Abstract:Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.",
                        "Citation Paper Authors": "Authors:Xu Han, Zhiyuan Liu, Maosong Sun"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "propose to leverage the graphs structure\ninformation into KRL. Besides, relational path learning has also been used in relation\nextraction ",
                    "Citation Text": "W. Zeng, Y . Lin, Z. Liu, M. Sun, Incorporating relation paths in neural relation\nextraction, in: Proceedings of EMNLP, Association for Computational Linguis-\ntics, Copenhagen, Denmark, 2017, pp. 1769\u20131778.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.07479",
                        "Citation Paper Title": "Title:Incorporating Relation Paths in Neural Relation Extraction",
                        "Citation Paper Abstract": "Abstract:Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/PathNRE.",
                        "Citation Paper Authors": "Authors:Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, Maosong Sun"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "propose to utilize dynamic programming algorithm to make use\nof all relation paths ef\ufb01ciently. And ",
                    "Citation Text": "R. Das, A. Neelakantan, D. Belanger, A. McCallum, Chains of reasoning over\nentities, relations, and text using recurrent neural networks, in: Proceedings of\nEACL, Association for Computational Linguistics, Valencia, Spain, 2017, pp. 1000\n132\u2013141.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.01426",
                        "Citation Paper Title": "Title:Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art. The code and data are available at this https URL",
                        "Citation Paper Authors": "Authors:Rajarshi Das, Arvind Neelakantan, David Belanger, Andrew McCallum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10896v1": {
            "Paper Title": "Identifying Computer-Translated Paragraphs using Coherence Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10847v1": {
            "Paper Title": "The Clickbait Challenge 2017: Towards a Regression Model for Clickbait\n  Strength",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10757v1": {
            "Paper Title": "Advancing the State of the Art in Open Domain Dialog Systems through the\n  Alexa Prize",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10720v1": {
            "Paper Title": "QRFA: A Data-Driven Model of Information-Seeking Dialogues",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.07217v2": {
            "Paper Title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10604v1": {
            "Paper Title": "Cross-relation Cross-bag Attention for Distantly-supervised Relation\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10390v2": {
            "Paper Title": "Toward Extractive Summarization of Online Forum Discussions via\n  Hierarchical Attention Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07073v2": {
            "Paper Title": "BiographyNet: Extracting Relations Between People and Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.03625v2": {
            "Paper Title": "On Evaluating and Comparing Open Domain Dialog Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10315v1": {
            "Paper Title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction\n  Corpus",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ", the authors present an approach for automatic construction of an NER\ntraining corpus out of Wikipedia, which has been generated for nine languages\nbased on the textual and structural features present in the Wikipedia articles.\nSimilarly, in ",
                    "Citation Text": "Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, and Steven Skiena.\nPolyglot-NER: Massive Multilingual Named Entity Recognition. In Pro-\nceedings of the 2015 SIAM International Conference on Data Mining , pages\n586{594, 2015.\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.3791",
                        "Citation Paper Title": "Title:POLYGLOT-NER: Massive Multilingual Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance.\nOur method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise.\nOur evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation.",
                        "Citation Paper Authors": "Authors:Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10235v1": {
            "Paper Title": "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection\n  and Slot Filling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10234v1": {
            "Paper Title": "A New Concept of Deep Reinforcement Learning based Augmented General\n  Sequence Tagging System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10230v1": {
            "Paper Title": "Learning to Refine Source Representations for Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10119v1": {
            "Paper Title": "Sequence to Sequence Learning for Query Expansion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10479v1": {
            "Paper Title": "Multimodal deep learning for short-term stock volatility prediction",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ". Speci\fcally, we implement the Zeros & Imputation (ZI) method ",
                    "Citation Text": "Z. C. Lipton, D. Kale, R. Wetzel, Directly Modeling Missing Data in Se-\nquences with RNNs: Improved Classi\fcation of Clinical Time Series, in:\nProceedings of the 1st Machine Learning for Healthcare Conferenc, 2016,\npp. 253{270.\nURL http://proceedings.mlr.press/v56/Lipton16.html\n34",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.04130",
                        "Citation Paper Title": "Title:Modeling Missing Data in Clinical Time Series with RNNs",
                        "Citation Paper Abstract": "Abstract:We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves.",
                        "Citation Paper Authors": "Authors:Zachary C. Lipton, David C. Kale, Randall Wetzel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.11456v2": {
            "Paper Title": "GIRNet: Interleaved Multi-Task Recurrent State Sequence Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10037v1": {
            "Paper Title": "Building a Neural Semantic Parser from a Domain Ontology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.06232v3": {
            "Paper Title": "A Novel Framework for Robustness Analysis of Visual QA Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10942v2": {
            "Paper Title": "Understanding the Role of Two-Sided Argumentation in Online Consumer\n  Reviews: A Language-Based Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09798v1": {
            "Paper Title": "Pansori: ASR Corpus Generation from Open Online Video Contents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09664v1": {
            "Paper Title": "Non-Autoregressive Neural Machine Translation with Enhanced Decoder\n  Input",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09652v1": {
            "Paper Title": "A Cross-Architecture Instruction Embedding Model for Natural Language\n  Processing-Inspired Binary Code Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09650v1": {
            "Paper Title": "Improving Context-Aware Semantic Relationships in Sparse Mobile Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.06960v5": {
            "Paper Title": "Translating Neuralese",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09516v1": {
            "Paper Title": "Distant Supervision for Relation Extraction with Linear Attenuation\n  Simulation and Non-IID Relevance Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09359v1": {
            "Paper Title": "NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09355v1": {
            "Paper Title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in\n  Deep NLP Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09336v1": {
            "Paper Title": "An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "used only video frames as input, it can be easily extended\nto incorporate both audio and visual information as seen in ",
                    "Citation Text": "S. Petridis, T. Stafylakis, P. Ma, F. Cai, G. Tzimiropoulos, and M. Pantic. End-to-end audiovisual\nspeech recognition. arXiv preprint arXiv:1802.06424 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06424",
                        "Citation Paper Title": "Title:End-to-end Audiovisual Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Several end-to-end deep learning approaches have been recently presented which extract either audio or visual features from the input images or audio signals and perform speech recognition. However, research on end-to-end audiovisual models is very limited. In this work, we present an end-to-end audiovisual model based on residual networks and Bidirectional Gated Recurrent Units (BGRUs). To the best of our knowledge, this is the first audiovisual fusion model which simultaneously learns to extract features directly from the image pixels and audio waveforms and performs within-context word recognition on a large publicly available dataset (LRW). The model consists of two streams, one for each modality, which extract features directly from mouth regions and raw waveforms. The temporal dynamics in each stream/modality are modeled by a 2-layer BGRU and the fusion of multiple streams/modalities takes place via another 2-layer BGRU. A slight improvement in the classification rate over an end-to-end audio-only and MFCC-based model is reported in clean audio conditions and low levels of noise. In presence of high levels of noise, the end-to-end audiovisual model significantly outperforms both audio-only models.",
                        "Citation Paper Authors": "Authors:Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Feipeng Cai, Georgios Tzimiropoulos, Maja Pantic"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "Earlier solutions to speech recognition mostly used either classical signal processing techniques or\ndeep learning on only the video data or audio data to do the actual recognition. In the video space,\nLipNet ",
                    "Citation Text": "Y . M. Assael, B. Shillingford, S. Whiteson, and N. de Freitas. Lipnet: End-to-end sentence-level\nlipreading. arXiv preprint arXiv:1611.01599 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01599",
                        "Citation Paper Title": "Title:LipNet: End-to-End Sentence-level Lipreading",
                        "Citation Paper Abstract": "Abstract:Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).",
                        "Citation Paper Authors": "Authors:Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.08454v2": {
            "Paper Title": "Attention Based Natural Language Grounding by Navigating Virtual\n  Environment",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "we use entropy regularization for improved\nexploration. Further, in order to reduce the variance of the\npolicy gradient updates, we use the Generalized Advantage\nEstimator ",
                    "Citation Text": "J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel.\nHigh-dimensional continuous control using generalized ad-\nvantage estimation. arXiv preprint arXiv:1506.02438 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02438",
                        "Citation Paper Title": "Title:High-Dimensional Continuous Control Using Generalized Advantage Estimation",
                        "Citation Paper Abstract": "Abstract:Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.\nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
                        "Citation Paper Authors": "Authors:John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". These\nworks focus on learning optimal policy for different tasks\nusing only visual features, while our work involves the agent\nreceiving natural language instruction in addition to visual\nstate of the environment.\nThe authors in ",
                    "Citation Text": "H. Yu, H. Zhang, and W. Xu. Interactive grounded language\nacquisition and generalization in a 2d world. arXiv preprint\narXiv:1802.01433 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.01433",
                        "Citation Paper Title": "Title:Interactive Grounded Language Acquisition and Generalization in a 2D World",
                        "Citation Paper Abstract": "Abstract:We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher's language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences. In addition, we demonstrate human-interpretable intermediate outputs of the model in the appendix.",
                        "Citation Paper Authors": "Authors:Haonan Yu, Haichao Zhang, Wei Xu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "attempt to learn a navigation policy\nthat is optimal in a 2D maze-like setting by relying on a se-\nmantic parser. ",
                    "Citation Text": "H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and\nwalk: Neural mapping of navigational instructions to action\nsequences. In AAAI , pages 2772\u20132778, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.04089",
                        "Citation Paper Title": "Title:Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences",
                        "Citation Paper Abstract": "Abstract:We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence \"regions\" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.",
                        "Citation Paper Authors": "Authors:Hongyuan Mei, Mohit Bansal, Matthew R. Walter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.08425v1": {
            "Paper Title": "A Survey of Hierarchy Identification in Social Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08407v1": {
            "Paper Title": "Context, Attention and Audio Feature Explorations for Audio Visual\n  Scene-Aware Dialog",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08318v1": {
            "Paper Title": "Generating lyrics with variational autoencoder and multi-modal artist\n  embeddings",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "4on the original lyrics of the selected seven artists.\nThis is a commonly used approach to evaluate the style attribute of generated texts, e.g. in style\ntransfer ",
                    "Citation Text": "Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text by\ncross-alignment. In NIPS , pages 6833\u20136844, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.09655",
                        "Citation Paper Title": "Title:Style Transfer from Non-Parallel Text by Cross-Alignment",
                        "Citation Paper Abstract": "Abstract:This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.",
                        "Citation Paper Authors": "Authors:Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.08304v1": {
            "Paper Title": "Recommendation System based on Semantic Scholar Mining and Topic\n  modeling: A behavioral analysis of researchers from six conferences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08033v1": {
            "Paper Title": "Switch-LSTMs for Multi-Criteria Chinese Word Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07805v1": {
            "Paper Title": "Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression\n  Model for Online Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07754v1": {
            "Paper Title": "Streaming Voice Query Recognition using Causal Convolutional Recurrent\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.04394v5": {
            "Paper Title": "M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "by removing various sources of test\nleakage, making the dataset more challenging. The NELL995 dataset was released by ",
                    "Citation Text": "Wenhan Xiong, Thien Hoang, and William Yang Wang. DeepPath: A reinforcement learning\nmethod for knowledge graph reasoning. In Proc. EMNLP , pages 575\u2013584, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06690",
                        "Citation Paper Title": "Title:DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
                        "Citation Paper Abstract": "Abstract:We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",
                        "Citation Paper Authors": "Authors:Wenhan Xiong, Thien Hoang, William Yang Wang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "focused on learning vector represen-\ntations of entities and relations. Recent approaches have demonstrated limitations of these prior\napproaches: they suffer from cascading errors when dealing with compositional (multi-step) re-\nlationships ",
                    "Citation Text": "Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. In\nProc. EMNLP , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01094",
                        "Citation Paper Title": "Title:Traversing Knowledge Graphs in Vector Space",
                        "Citation Paper Abstract": "Abstract:Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Kelvin Guu, John Miller, Percy Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10549v1": {
            "Paper Title": "Automatic Summarization of Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07324v1": {
            "Paper Title": "Predicting user intent from search queries using both CNNs and RNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.05355v3": {
            "Paper Title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08773v3": {
            "Paper Title": "Learning Multilingual Word Embeddings in Latent Metric Space: A\n  Geometric Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07023v1": {
            "Paper Title": "From FiLM to Video: Multi-turn Question Answering with Multi-modal\n  Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06974v1": {
            "Paper Title": "Analogy Search Engine: Finding Analogies in Cross-Domain Research Papers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06722v1": {
            "Paper Title": "TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06705v1": {
            "Paper Title": "Conditional BERT Contextual Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06700v1": {
            "Paper Title": "Hateminers : Detecting Hate speech against Women",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06401v1": {
            "Paper Title": "What's to know? Uncertainty as a Guide to Asking Goal-oriented Questions",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", o\u0003is a target object that the\ndialogue refers to. In the Deal or No Deal ",
                    "Citation Text": "M. Lewis, D. Yarats, Y . N. Dauphin, D. Parikh, and D. Ba-\ntra. Deal or No Deal? End-to-End Learning for Negotiation\nDialogues. ArXiv e-prints , 2017. 2, 4, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05125",
                        "Citation Paper Title": "Title:Deal or No Deal? End-to-End Learning for Negotiation Dialogues",
                        "Citation Paper Abstract": "Abstract:Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (this https URL).",
                        "Citation Paper Authors": "Authors:Mike Lewis, Denis Yarats, Yann N. Dauphin, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "propose a goal-oriented dialogue\ntest-bed that requires a user chat with a bot to book a table\nat a restaurant. In visual goal-oriented dialogue De Vries et\nal. ",
                    "Citation Text": "H. de Vries, F. Strub, S. Chandar, O. Pietquin, H. Larochelle,\nand A. C. Courville. Guesswhat?! visual object discovery\nthrough multi-modal dialogue. In Conference on Computer\nVision and Pattern Recognition (CVPR) , 2017. 1, 2, 3, 4, 6,\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08481",
                        "Citation Paper Title": "Title:GuessWhat?! Visual object discovery through multi-modal dialogue",
                        "Citation Paper Abstract": "Abstract:We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.",
                        "Citation Paper Authors": "Authors:Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". A similar upper bound for Gaussian processes was\nproposed in ",
                    "Citation Text": "N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian\nprocess optimization in the bandit setting: no regret and ex-\nperimental design. In Proceedings of the 27th International\nConference on International Conference on Machine Learn-\ning, 2010. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0912.3995",
                        "Citation Paper Title": "Title:Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design",
                        "Citation Paper Abstract": "Abstract:Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.",
                        "Citation Paper Authors": "Authors:Niranjan Srinivas, Andreas Krause, Sham M. Kakade, Matthias Seeger"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "combine reinforcement learning and generative\nadversarial networks (GANs) to generate more human-like\nvisual dialogues. In ",
                    "Citation Text": "A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learn-\ning cooperative visual dialog agents with deep reinforcement\nlearning. arXiv preprint arXiv:1703.06585 , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06585",
                        "Citation Paper Title": "Title:Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.\nWe demonstrate two experimental results.\nFirst, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.\nSecond, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",
                        "Citation Paper Authors": "Authors:Abhishek Das, Satwik Kottur, Jos\u00e9 M. F. Moura, Stefan Lee, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "use reinforcement\nlearning to improve cooperative bot-bot dialogues, and Wu\net al. ",
                    "Citation Text": "Q. Wu, P. Wang, C. Shen, I. Reid, and A. van den Hen-\ngel. Are you talking to me? reasoned visual dialog\ngeneration through adversarial learning. arXiv preprint\narXiv:1711.07613 , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07613",
                        "Citation Paper Title": "Title:Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.",
                        "Citation Paper Authors": "Authors:Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton van den Hengel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.06199v1": {
            "Paper Title": "Inter-sentence Relation Extraction for Associating Biological Context\n  with Events in Biomedical Texts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06176v1": {
            "Paper Title": "Bootstrapping Conversational Agents With Weak Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06081v1": {
            "Paper Title": "A Neural Multi-Task Learning Framework to Jointly Model Medical Named\n  Entity Recognition and Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05199v2": {
            "Paper Title": "Recurrent Neural Networks with Pre-trained Language Model Embedding for\n  Slot Filling Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04377v2": {
            "Paper Title": "Deep Reader: Information extraction from Document images via relation\n  extraction and Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05936v1": {
            "Paper Title": "Detecting Reliable Novel Word Senses: A Network-Centric Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05774v1": {
            "Paper Title": "Don't Classify, Translate: Multi-Level E-Commerce Product Categorization\n  Via Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05718v1": {
            "Paper Title": "Multilayer Network Model of Movie Script",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.06497v2": {
            "Paper Title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10627v3": {
            "Paper Title": "Reliability and Learnability of Human Bandit Feedback for\n  Sequence-to-Sequence Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05407v1": {
            "Paper Title": "Abstractive Text Summarization by Incorporating Reader Comments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05366v1": {
            "Paper Title": "Dynamic Feature Generation Network for Answer Selection",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "match sentences with multi-\nple views and perspectives. Recurrent neural network model such\nas AP-BiLSTM ",
                    "Citation Text": "Cicero Nogueira Dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016.\nAttentive Pooling Networks. arXiv: Computation and Language (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.03609",
                        "Citation Paper Title": "Title:Attentive Pooling Networks",
                        "Citation Paper Abstract": "Abstract:In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.",
                        "Citation Paper Authors": "Authors:Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "applies multihop-sequential-LSTM to\nachieve step by step learning. Most recently, Compare-propagate\nmodel CAFE ",
                    "Citation Text": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A Compare-Propagate Ar-\nchitecture with Alignment Factorization for Natural Language Inference. arXiv:\nComputation and Language (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00102",
                        "Citation Paper Title": "Title:Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",
                        "Citation Paper Abstract": "Abstract:This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a $\\approx 3$ times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.",
                        "Citation Paper Authors": "Authors:Yi Tay, Luu Anh Tuan, Siu Cheung Hui"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "further develop gated attention and\nachieve success in answer selection. Inner-attention ",
                    "Citation Text": "Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning Natural\nLanguage Inference using Bidirectional LSTM model and Inner-Attention. arXiv:\nComputation and Language (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.09090",
                        "Citation Paper Title": "Title:Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention",
                        "Citation Paper Abstract": "Abstract:In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.",
                        "Citation Paper Authors": "Authors:Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1810.10927v2": {
            "Paper Title": "Bayesian Compression for Natural Language Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05001v1": {
            "Paper Title": "Temporal Analysis of Entity Relatedness and its Evolution using\n  Wikipedia and DBpedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05692v1": {
            "Paper Title": "Bayesian Sparsification of Gated Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04898v1": {
            "Paper Title": "SMT vs NMT: A Comparison over Hindi & Bengali Simple Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10400v1": {
            "Paper Title": "Detecting weak and strong Islamophobic hate speech on social media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10382v1": {
            "Paper Title": "The Global Anchor Method for Quantifying Linguistic Shifts and Domain\n  Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04662v1": {
            "Paper Title": "Unsupervised domain-agnostic identification of product names in social\n  media posts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04405v1": {
            "Paper Title": "Conditional Variational Autoencoder for Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07104v1": {
            "Paper Title": "Reading Industrial Inspection Sheets by Inferring Visual Relations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10408v1": {
            "Paper Title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ", and the word-embeddings were also trained using Euclidean Skip-gram\non the LDC data set.\nAll models were restarted once ",
                    "Citation Text": "Loshchilov, Ilya, Hutter, Frank (2017) SGDR: Stochastic Gradient Descent with Warm Restarts\naccessed on 11-12-2018 at https://arxiv.org/pdf/1608.03983.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.03983",
                        "Citation Paper Title": "Title:SGDR: Stochastic Gradient Descent with Warm Restarts",
                        "Citation Paper Abstract": "Abstract:Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at this https URL",
                        "Citation Paper Authors": "Authors:Ilya Loshchilov, Frank Hutter"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". The details\nare given in the appendix.\n4.2 Hyperbolic Transformer\nHyperboloid word embeddings are used downstream in a hyperbolic intent classi\fcation model. We\nuse the Transformer architecture proposed in ",
                    "Citation Text": "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan\nN., Kaiser, Lukasz, Polosukhin, illia (2017) Attention is All You Need , accessed on 2018-06-06\nathttps://arxiv.org/pdf/1706.03762.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.04224v1": {
            "Paper Title": "On the Dimensionality of Word Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04179v2": {
            "Paper Title": "Mapping Navigation Instructions to Continuous Control Actions with\n  Position-Visitation Prediction",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nOur approach is related to recent work on learning visuomotor control policies for grasping [25,\n26, 27], dexterous manipulation [28, 29, 30] and visual navigation ",
                    "Citation Text": "S. Bhatti, A. Desmaison, O. Miksik, N. Nardelli, N. Siddharth, and P. H. Torr. Playing doom with slam-\naugmented deep reinforcement learning. arXiv preprint arXiv:1612.00380 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00380",
                        "Citation Paper Title": "Title:Playing Doom with SLAM-Augmented Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:A number of recent approaches to policy learning in 2D game domains have been successful going directly from raw input images to actions. However when employed in complex 3D environments, they typically suffer from challenges related to partial observability, combinatorial exploration spaces, path planning, and a scarcity of rewarding scenarios. Inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions (object categories, localisation, etc.) to reason about the world, we build an agent-model that incorporates such abstractions into its policy-learning framework. We augment the raw image input to a Deep Q-Learning Network (DQN), by adding details of objects and structural elements encountered, along with the agent's localisation. The different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3D-scene reconstruction.We evaluate the efficacy of our approach in Doom, a 3D first-person combat game that exhibits a number of challenges discussed, and show that our augmented framework consistently learns better, more effective policies.",
                        "Citation Paper Authors": "Authors:Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, Philip H. S. Torr"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.04444v2": {
            "Paper Title": "Character-Level Language Modeling with Deeper Self-Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00253v3": {
            "Paper Title": "Hybrid Self-Attention Network for Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03519v1": {
            "Paper Title": "Deep-Net: Deep Neural Network for Cyber Security Use Cases",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03509v1": {
            "Paper Title": "Dialogue Generation: From Imitation Learning to Inverse Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03258v1": {
            "Paper Title": "Political Popularity Analysis in Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01245v2": {
            "Paper Title": "Transferable Natural Language Interface to Structured Queries aided by\n  Adversarial Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01910v2": {
            "Paper Title": "Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08748v2": {
            "Paper Title": "Persistent Hidden States and Nonlinear Transformation for Long\n  Short-Term Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00347v2": {
            "Paper Title": "How2: A Large-scale Dataset for Multimodal Language Understanding",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "and Multi30K which is an extension of Flickr30K into German ",
                    "Citation Text": "Desmond Elliott, Stella Frank, Khalil Sima\u2019an, and Lucia Specia. Multi30k: Multilingual\nenglish-german image descriptions. In Proceedings oftheWorkshop onVision andLanguage .\nACL, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.00459",
                        "Citation Paper Title": "Title:Multi30K: Multilingual English-German Image Descriptions",
                        "Citation Paper Abstract": "Abstract:We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) descriptions crowdsourced independently of the original English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks.",
                        "Citation Paper Authors": "Authors:Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02793v1": {
            "Paper Title": "Generation of Synthetic Electronic Medical Record Text",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "have achieved impressive performance in sev-\neral complex tasks, such as machine translation and dialogu e\ngeneration ",
                    "Citation Text": "Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jur afsky, D. (2017).\nAdversarial learning for neural dialogue generation. arXi v preprint\narXiv:1701.06547, unpublished.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.06547",
                        "Citation Paper Title": "Title:Adversarial Learning for Neural Dialogue Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.\nIn addition to adversarial training we describe a model for adversarial {\\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ":R=D\n1\u2212D, and we call it ODA (Optimal Discriminator\nActivation) for short. The second one is BRA(Bootstrapped\nRanking Activation), which is proposed in LeakGAN ",
                    "Citation Text": "Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y ., and Wang, J. (2 017). Long\ntext generation via adversarial training with leaked infor mation. arXiv\npreprint arXiv:1709.08624, unpublished.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.08624",
                        "Citation Paper Title": "Title:Long Text Generation via Adversarial Training with Leaked Information",
                        "Citation Paper Abstract": "Abstract:Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker.",
                        "Citation Paper Authors": "Authors:Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Yahi utiliz ed\na GAN framework to produce continuous time series data in\nEMRs, which can predict the effects of drug exposure ",
                    "Citation Text": "Yahi, A., Vanguri, R., Elhadad, N., and Tatonetti, N. P. (2017). Genera-\ntive Adversarial Networks for Electronic Health Records: A Framework\nfor Exploring and Evaluating Methods for Predicting Drug-I nduced Lab-\noratory Test Trajectories. arXiv preprint arXiv:1712.001 64, unpublished.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00164",
                        "Citation Paper Title": "Title:Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.",
                        "Citation Paper Authors": "Authors:Alexandre Yahi, Rami Vanguri, No\u00e9mie Elhadad, Nicholas P. Tatonetti"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02655v1": {
            "Paper Title": "Feature Analysis for Assessing the Quality of Wikipedia Articles through\n  Supervised Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01193v2": {
            "Paper Title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "assembled a dataset to test whether infer ence models actually capture composi-\ntionality beyond word level. They showed that InferSent sen tence embeddings ",
                    "Citation Text": "Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bo rdes, A. (2017). Supervised\nlearning of universal sentence representations from natur al language inference data. CoRR ,\nabs/1705.02364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", who introduce two datasets of natural lan guage explanations for the tasks of\nvisual question-answering and activity recognition. Anot her work in this direction is that of Ling\net al. ",
                    "Citation Text": "Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017) . Program induction by rationale\ngeneration: Learning to solve and explain algebraic word pr oblems. CoRR , abs/1705.04146.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04146",
                        "Citation Paper Title": "Title:Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems",
                        "Citation Paper Abstract": "Abstract:Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",
                        "Citation Paper Authors": "Authors:Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "in-\ntroduce a post-hoc extractive technique, LIME,\nthat explains the prediction of any classi\ufb01er via\na local linear approximation around the predic-\ntion. Alvarez-Melis and Jaakkola ",
                    "Citation Text": "Alvarez-Melis, D. and Jaakkola, T. S. (2017). A causal fr amework for explaining the predictions\nof black-box sequence-to-sequence models. CoRR , abs/1707.01943.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01943",
                        "Citation Paper Title": "Title:A causal framework for explaining the predictions of black-box sequence-to-sequence models",
                        "Citation Paper Abstract": "Abstract:We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an \"explanation\" consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.",
                        "Citation Paper Authors": "Authors:David Alvarez-Melis, Tommi S. Jaakkola"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02627v1": {
            "Paper Title": "Relevant Word Order Vectorization for Improved Natural Language\n  Processing in Electronic Healthcare Records",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02305v1": {
            "Paper Title": "Pathology Extraction from Chest X-Ray Radiology Reports: A Performance\n  Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02370v1": {
            "Paper Title": "Exploring the importance of context and embeddings in neural NER models\n  for task-oriented dialogue systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05268v2": {
            "Paper Title": "Supervised Machine Learning for Extractive Query Based Summarisation of\n  Biomedical Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02354v1": {
            "Paper Title": "Multi-Task Learning with Multi-View Attention for Answer Selection and\n  Knowledge Base Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00133v2": {
            "Paper Title": "Grounding Language for Transfer in Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.03793v4": {
            "Paper Title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate\n  Ranking",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ", lack this criteria, but\npresent promising results through expert analysis. Therefore,\nwe additionally developed a number of novel metrics for topic-\ndriven HG systems that quantify the plausibility of potential\nconnections. These metrics leverage word embeddings ",
                    "Citation Text": "T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of\nword representations in vector space,\u201d arXiv:1301.3781 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02205v1": {
            "Paper Title": "Are you tough enough? Framework for Robustness Validation of Machine\n  Comprehension Systems",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "show a keyword detection method in Q&A using Integrated Gradients, althoughthey do not venture on to create attacks using important words in Q&A task. ",
                    "Citation Text": "Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. InInternational Conference on Learning Representations, 2018.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.11342",
                        "Citation Paper Title": "Title:Generating Natural Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.",
                        "Citation Paper Authors": "Authors:Zhengli Zhao, Dheeru Dua, Sameer Singh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02142v1": {
            "Paper Title": "End-to-end contextual speech recognition using class language models and\n  a token passing decoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01889v1": {
            "Paper Title": "A Knowledge Graph Based Solution for Entity Discovery and Linking in\n  Open-Domain Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01887v1": {
            "Paper Title": "Approach for Semi-automatic Construction of Anti-infective Drug Ontology\n  Based on Entity Linking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01885v1": {
            "Paper Title": "Improving Medical Short Text Classification with Semantic Expansion\n  Using Word-Cluster Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04718v1": {
            "Paper Title": "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00381v2": {
            "Paper Title": "Towards Automatic Discovery of Cybercrime Supply Chains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01704v1": {
            "Paper Title": "Impact of Sentiment Detection to Recognize Toxic and Subversive Online\n  Comments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00677v2": {
            "Paper Title": "Clinical Document Classification Using Labeled and Unlabeled Data Across\n  Hospitals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01525v1": {
            "Paper Title": "A Face-to-Face Neural Conversation Model",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ", while diverse\ndialogues are preferred in practical scenarios. We address\nthis problem by using a sequence GAN ",
                    "Citation Text": "L. Yu, W. Zhang, J. Wang, and Y . Yu. Seqgan: Sequence\ngenerative adversarial nets with policy gradient. In AAAI ,\n2017. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.05473",
                        "Citation Paper Title": "Title:SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
                        "Citation Paper Abstract": "Abstract:As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.",
                        "Citation Paper Authors": "Authors:Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed a reconstruction algorithm that\ncaptures a person\u2019s physical appearance and persona behav-\nior. ",
                    "Citation Text": "J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and\nM. Niesner. Face2face: Real-time face capture and reen-\nactment of rgb videos. In CVPR , 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.14808",
                        "Citation Paper Title": "Title:Face2Face: Real-time Face Capture and Reenactment of RGB Videos",
                        "Citation Paper Abstract": "Abstract:We present Face2Face, a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.",
                        "Citation Paper Authors": "Authors:Justus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "by adding latent variables aiming to cap-\nture different topics of conversation, allowing the model to\nachieve a higher diversity in its response.\nAn interesting extension was proposed in ",
                    "Citation Text": "J. Li, M. Galley, C. Brockett, G. P. Spithourakis, J. Gao, and\nB. Dolan. A persona-based neural conversation model. In\narXiv:1603.06155 , 2016. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06155",
                        "Citation Paper Title": "Title:A Persona-Based Neural Conversation Model",
                        "Citation Paper Abstract": "Abstract:We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "pro-\nposed an improved objective function that encouraged di-\nversity in the generator. In ",
                    "Citation Text": "I. V . Serban, A. Sordoni, Y . Bengio, A. Courville, and\nJ. Pineau. Building end-to-end dialogue systems us-\ning generative hierarchical neural network models. In\narXiv:1507.04808 , 2015. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04808",
                        "Citation Paper Title": "Title:Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
                        "Citation Paper Abstract": "Abstract:We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.",
                        "Citation Paper Authors": "Authors:Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.01260v1": {
            "Paper Title": "Tartan: A retrieval-based socialbot powered by a dynamic finite-state\n  machine architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01250v1": {
            "Paper Title": "Quantification and Analysis of Scientific Language Variation Across\n  Research Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01207v1": {
            "Paper Title": "Practical Text Classification With Large Pre-Trained Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00899v1": {
            "Paper Title": "Toward Scalable Neural Dialogue State Tracking Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12793v2": {
            "Paper Title": "TIFTI: A Framework for Extracting Drug Intervals from Longitudinal\n  Clinic Notes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00855v1": {
            "Paper Title": "Towards Solving Text-based Games by Producing Adaptive Action Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00436v1": {
            "Paper Title": "Learning Representations of Social Media Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00382v1": {
            "Paper Title": "Improved and Robust Controversy Detection in General Web Pages Using\n  Semantic Approaches under Large Scale Conditions",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "makes use of attention to build document representations in\na hierarchical manner. It uses bi-directional Gated Recurrent Units\n(GRUs) ",
                    "Citation Text": "Cho K. & Bengio Y. Bahdanau, D. 2014. Neural machine translation by jointly\nlearning to align and translate.. In arXiv preprint arXiv:1409.0473 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". 2)explicit mod-\neling of controversy through platform-specific features, often in\nWikipedia or social-media settings. Features such as mutual reverts ",
                    "Citation Text": "Sumi R. Yasseri, T. 2012. Dynamics of Conflicts in Wikipedia. In PLOS ONE, 7(6),\n1\u201312.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1202.3643",
                        "Citation Paper Title": "Title:Dynamics of conflicts in Wikipedia",
                        "Citation Paper Abstract": "Abstract:In this work we study the dynamical features of editorial wars in Wikipedia (WP). Based on our previously established algorithm, we build up samples of controversial and peaceful articles and analyze the temporal characteristics of the activity in these samples. On short time scales, we show that there is a clear correspondence between conflict and burstiness of activity patterns, and that memory effects play an important role in controversies. On long time scales, we identify three distinct developmental patterns for the overall behavior of the articles. We are able to distinguish cases eventually leading to consensus from those cases where a compromise is far from achievable. Finally, we analyze discussion networks and conclude that edit wars are mainly fought by few editors only.",
                        "Citation Paper Authors": "Authors:Taha Yasseri, Robert Sumi, Andr\u00e1s Rung, Andr\u00e1s Kornai, J\u00e1nos Kert\u00e9sz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.11001v2": {
            "Paper Title": "Unsupervised Post-processing of Word Vectors via Conceptor Negation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00350v1": {
            "Paper Title": "A Study on Dialogue Reward Prediction for Open-Ended Conversational\n  Agents",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "used a similar approach in a restricted set of domains. ",
                    "Citation Text": "J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsk y. Adversarial learning for neural\ndialogue generation. In EMNLP , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.06547",
                        "Citation Paper Title": "Title:Adversarial Learning for Neural Dialogue Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.\nIn addition to adversarial training we describe a model for adversarial {\\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.10113v3": {
            "Paper Title": "Story Ending Generation with Incremental Encoding and Commonsense\n  Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.09931v4": {
            "Paper Title": "Integrating Local Context and Global Cohesiveness for Open Information\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.12730v2": {
            "Paper Title": "Audiovisual speaker conversion: jointly and simultaneously transforming\n  facial expression and acoustic characteristics",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "neural-network-based waveform model that generates\nwaveform sampling points one by one.\nThe WaveNet structure is the same as that of the one used in\nanother study ",
                    "Citation Text": "Hieu-Thi Luong, Xin Wang, Junichi Yamagishi, and Nobuyuki\nNishizawa, \u201cInvestigating accuracy of pitch-accent annota-\ntions in neural-network-based speech synthesis and denoising\neffects,\u201d in Proc. Interspeech , 2018, pp. 37\u201341.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.00665",
                        "Citation Paper Title": "Title:Investigating accuracy of pitch-accent annotations in neural network-based speech synthesis and denoising effects",
                        "Citation Paper Abstract": "Abstract:We investigated the impact of noisy linguistic features on the performance of a Japanese speech synthesis system based on neural network that uses WaveNet vocoder. We compared an ideal system that uses manually corrected linguistic features including phoneme and prosodic information in training and test sets against a few other systems that use corrupted linguistic features. Both subjective and objective results demonstrate that corrupted linguistic features, especially those in the test set, affected the ideal system's performance significantly in a statistical sense due to a mismatched condition between the training and test sets. Interestingly, while an utterance-level Turing test showed that listeners had a difficult time differentiating synthetic speech from natural speech, it further indicated that adding noise to the linguistic features in the training set can partially reduce the effect of the mismatch, regularize the model, and help the system perform better when linguistic features of the test set are noisy.",
                        "Citation Paper Authors": "Authors:Hieu-Thi Luong, Xin Wang, Junichi Yamagishi, Nobuyuki Nishizawa"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "is performed in\neach hidden layer after recti\ufb01ed linear unit (ReLU) ",
                    "Citation Text": "Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li, \u201cEmpirical\nevaluation of recti\ufb01ed activations in convolutional network,\u201d in\nICML Deep Learning Workshop , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00853",
                        "Citation Paper Title": "Title:Empirical Evaluation of Rectified Activations in Convolutional Network",
                        "Citation Paper Abstract": "Abstract:In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.",
                        "Citation Paper Authors": "Authors:Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "developed similar\nspeech enhancement methods that predict a mask from lip images\nand noisy acoustic features using a neural network to \ufb01lter out noise.\nA lip movement-to-speech synthesis system was developed by\nKumar et al. ",
                    "Citation Text": "Yaman Kumar, Mayank Aggarwal, Pratham Nawal, Shin\u2019ichi\nSatoh, Rajiv Ratn Shah, and Roger Zimmerman, \u201cHarness-\ning ai for speech reconstruction using multi-view silent video\nfeed,\u201d in 2018 ACM Multimedia Conference (MM \u201918) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00619",
                        "Citation Paper Title": "Title:Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed",
                        "Citation Paper Abstract": "Abstract:Speechreading or lipreading is the technique of understanding and getting phonetic features from a speaker's visual features such as movement of lips, face, teeth and tongue. It has a wide range of multimedia applications such as in surveillance, Internet telephony, and as an aid to a person with hearing impairments. However, most of the work in speechreading has been limited to text generation from silent videos. Recently, research has started venturing into generating (audio) speech from silent video sequences but there have been no developments thus far in dealing with divergent views and poses of a speaker. Thus although, we have multiple camera feeds for the speech of a user, but we have failed in using these multiple video feeds for dealing with the different poses. To this end, this paper presents the world's first ever multi-view speech reading and reconstruction system. This work encompasses the boundaries of multimedia research by putting forth a model which leverages silent video feeds from multiple cameras recording the same subject to generate intelligent speech for a speaker. Initial results confirm the usefulness of exploiting multiple camera views in building an efficient speech reading and reconstruction system. It further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Next, it lays out various innovative applications for the proposed system focusing on its potential prodigious impact in not just security arena but in many other multimedia analytics problems.",
                        "Citation Paper Authors": "Authors:Yaman Kumar, Mayank Aggarwal, Pratham Nawal, Shin'ichi Satoh, Rajiv Ratn Shah, Roger Zimmerman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.00195v1": {
            "Paper Title": "One for All: Neural Joint Modeling of Entities and Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00176v1": {
            "Paper Title": "A Deep Sequential Model for Discourse Parsing on Multi-Party Dialogues",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00161v1": {
            "Paper Title": "QADiver: Interactive Framework for Diagnosing QA Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01431v1": {
            "Paper Title": "Modeling natural language emergence with integral transform theory and\n  reinforcement learning",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nThe game between the speaker, parameterized as \u0012s, and listener, parameterized as \u0012l, is as follows:\n1. A sample image from each of the nunique classes from an image dataset is drawn and passed through a\npretrained VGG19 network ",
                    "Citation Text": "K. Simonyan, A. Zisserman (2014), Very Deep Convolutional Networks for Large-Scale Image Recognition. ArXiv e-prints:\n1409.1556",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.1556",
                        "Citation Paper Title": "Title:Very Deep Convolutional Networks for Large-Scale Image Recognition",
                        "Citation Paper Abstract": "Abstract:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.12900v1": {
            "Paper Title": "Detecting Offensive Content in Open-domain Conversations using Two Stage\n  Semi-supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12891v1": {
            "Paper Title": "Flexible and Scalable State Tracking Framework for Goal-Oriented\n  Dialogue Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12728v1": {
            "Paper Title": "Document Structure Measure for Hypernym discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01252v2": {
            "Paper Title": "Improving a Neural Semantic Parser by Counterfactual Learning from Human\n  Bandit Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12254v1": {
            "Paper Title": "The Effect of Heterogeneous Data for Alzheimer's Disease Detection from\n  Speech",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "used\nlinguistic and acoustic features to classify healthy and pathological speech transcripts with an accuracy\nof82%. Similarly, Karlekar et al. ",
                    "Citation Text": "Sweta Karlekar, Tong Niu, and Mohit Bansal. Detecting linguistic characteristics of Alzheimer\u2019s\ndementia by interpreting neural models. arXiv preprint arXiv:1804.06440 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.06440",
                        "Citation Paper Title": "Title:Detecting Linguistic Characteristics of Alzheimer's Dementia by Interpreting Neural Models",
                        "Citation Paper Abstract": "Abstract:Alzheimer's disease (AD) is an irreversible and progressive brain disease that can be stopped or slowed down with medical treatment. Language changes serve as a sign that a patient's cognitive functions have been impacted, potentially leading to early diagnosis. In this work, we use NLP techniques to classify and analyze the linguistic characteristics of AD patients using the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs, and their combination, to distinguish between language samples from AD and control patients. We achieve a new independent benchmark accuracy for the AD classification task. More importantly, we next interpret what these neural models have learned about the linguistic characteristics of AD patients, via analysis based on activation clustering and first-derivative saliency techniques. We then perform novel automatic pattern discovery inside activation clusters, and consolidate AD patients' distinctive grammar patterns. Additionally, we show that first derivative saliency can not only rediscover previous language patterns of AD patients, but also shed light on the limitations of neural models. Lastly, we also include analysis of gender-separated AD data.",
                        "Citation Paper Authors": "Authors:Sweta Karlekar, Tong Niu, Mohit Bansal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.12148v1": {
            "Paper Title": "Improving Robustness of Neural Dialog Systems in a Data-Efficient Way\n  with Turn Dropout",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04938v2": {
            "Paper Title": "LiveBot: Generating Live Video Comments Based on Visual and Textual\n  Contexts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11707v1": {
            "Paper Title": "Few-Shot Generalization Across Dialogue Tasks",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". The\nmain difference is that since the memory is truncated at the current time step, the interpolation gate\ncannot be applied to the attention probabilities. Therefore interpolation is applied to the Bahdanau\nscores ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.11005v2": {
            "Paper Title": "Application of Clinical Concept Embeddings for Heart Failure Prediction\n  in UK EHR data",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "Colin Walsh and George Hripcsak. The effects of data sources, cohort selection, and out-\ncome de\ufb01nition on a predictive model of risk of thirty-day hospital readmissions. Journal of\nbiomedical informatics , 52:418\u2013426, 2014. ",
                    "Citation Text": "Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Peter J Liu, Xiaobing\nLiu, Mimi Sun, Patrik Sundberg, Hector Yee, et al. Scalable and accurate deep learning for\nelectronic health records. arXiv preprint arXiv:1801.07860 , 2018.\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07860",
                        "Citation Paper Title": "Title:Scalable and accurate deep learning for electronic health records",
                        "Citation Paper Abstract": "Abstract:Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient's chart.",
                        "Citation Paper Authors": "Authors:Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Peter J. Liu, Xiaobing Liu, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Gavin E. Duggan, Gerardo Flores, Michaela Hardt, Jamie Irvine, Quoc Le, Kurt Litsch, Jake Marcus, Alexander Mossin, Justin Tansuwan, De Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou, Michael Pearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael Howell, Claire Cui, Greg Corrado, Jeff Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.02308v1": {
            "Paper Title": "On the Inductive Bias of Word-Character-Level Multi-Task Learning for\n  Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "found that in order to solve the problem of observing only\nfew labels per word, they needed to use a large dataset of 120;000hours to train a word-level model\ndirectly. Accordingly, Audhkhasi et al. ",
                    "Citation Text": "K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, and D. Nahamoo. Direct Acoustics-to-\nWord Models for English Conversational Speech Recognition. In Interspeech , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.07754",
                        "Citation Paper Title": "Title:Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.",
                        "Citation Paper Authors": "Authors:Kartik Audhkhasi, Bhuvana Ramabhadran, George Saon, Michael Picheny, David Nahamoo"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "used different \ufb01xed decompositions of words, sub-words and characters, it is also possible\nto optimize over alignments and decompositions jointly ",
                    "Citation Text": "H. Liu, Z. Zhu, X. Li, and S. Satheesh. Gram-CTC: Automatic Unit Selection and Target\nDecomposition for Sequence Labelling. In ICML , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00096",
                        "Citation Paper Title": "Title:Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling",
                        "Citation Paper Abstract": "Abstract:Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",
                        "Citation Paper Authors": "Authors:Hairong Liu, Zhenyao Zhu, Xiangang Li, Sanjeev Satheesh"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "Several works have explored using words instead of characters or phonemes as outputs of the end-to-\nend ASR model [ 3,28]. Soltau et al. ",
                    "Citation Text": "H. Soltau, H. Liao, and H. Sak. Neural Speech Recognizer: Acoustic-to-Word LSTM Model\nfor Large V ocabulary Speech Recognition. In Interspeech , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.09975",
                        "Citation Paper Title": "Title:Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
                        "Citation Paper Abstract": "Abstract:We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.",
                        "Citation Paper Authors": "Authors:Hagen Soltau, Hank Liao, Hasim Sak"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.11430v1": {
            "Paper Title": "Context-Aware Dialog Re-Ranking for Task-Oriented Dialog Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02309v1": {
            "Paper Title": "The MeSH-gram Neural Network Model: Extending Word Embedding Vectors\n  with MeSH Concepts for UMLS Semantic Similarity and Relatedness in the\n  Biomedical Domain",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", the authors computed the cosine of two Latent Semantic Indexing concept vectors based on Pointwise Mutual Information association measure matrix. Recent vector-based methods use neural networks in order to compute concept vectors. The word2vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean, Distributed Representations of Words and Phrases and their Com-positionality. In Advances in Neural Information Processing Systems (2013), 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.11161v1": {
            "Paper Title": "Cross-Lingual Approaches to Reference Resolution in Dialogue Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06718v2": {
            "Paper Title": "Automatic Severity Classification of Coronary Artery Disease via\n  Recurrent Capsule Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08669v3": {
            "Paper Title": "Fast and Accurate Recognition of Chinese Clinical Named Entities with\n  Residual Dilated Convolutions",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "backend, and each model\nis run in a single NVIDIA GeForce GTX 1080 Ti GPU.\nCharacter embeddings and feature embeddings are initialized\nvia word2vec ",
                    "Citation Text": "T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of\nword representations in vector space,\u201d arXiv preprint arXiv:1301.3781 ,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nThe \ufb01nal output of the convolutional layers is the sum of the\nseparate output of the two parts.\n1) Standard Convolution: The standard convolutions ",
                    "Citation Text": "N. Kalchbrenner, E. Grefenstette, and P. Blunsom, \u201cA convolutional neu-\nral network for modelling sentences,\u201d in Proceedings of the 52nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) , vol. 1, 2014, pp. 655\u2013665.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1404.2188",
                        "Citation Paper Title": "Title:A Convolutional Neural Network for Modelling Sentences",
                        "Citation Paper Abstract": "Abstract:The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.",
                        "Citation Paper Authors": "Authors:Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", especially the methods\nbased on Bidirectional RNN with CRF layer as the output\ninterface (Bi-RNN-CRF) ",
                    "Citation Text": "Z. Huang, W. Xu, and K. Yu, \u201cBidirectional LSTM-CRF models for\nsequence tagging,\u201d arXiv preprint arXiv:1508.01991 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01991",
                        "Citation Paper Title": "Title:Bidirectional LSTM-CRF Models for Sequence Tagging",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
                        "Citation Paper Authors": "Authors:Zhiheng Huang, Wei Xu, Kai Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.10776v1": {
            "Paper Title": "Joint Representation Learning of Cross-lingual Words and Entities via\n  Attentive Distant Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10773v1": {
            "Paper Title": "Verb Argument Structure Alternations in Word and Sentence Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08040v3": {
            "Paper Title": "Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic\n  Health Records",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12181v1": {
            "Paper Title": "What Should I Learn First: Introducing LectureBank for NLP Education and\n  Prerequisite Chain Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10686v1": {
            "Paper Title": "Beyond \"How may I help you?\": Assisting Customer Service Agents with\n  Proactive Responses",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10561v1": {
            "Paper Title": "CLEAR: A Dataset for Compositional Language and Elementary Acoustic\n  Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10550v1": {
            "Paper Title": "Challenges in the Automatic Analysis of Students' Diagnostic Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10475v1": {
            "Paper Title": "Sentence Encoding with Tree-constrained Relation Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10566v2": {
            "Paper Title": "Clinical Concept Extraction with Contextual Word Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10369v1": {
            "Paper Title": "ParsRec: A Novel Meta-Learning Approach to Recommending Bibliographic\n  Reference Parsers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04022v4": {
            "Paper Title": "Can LSTM Learn to Capture Agreement? The Case of Basque",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02286v2": {
            "Paper Title": "Dynamic Compositionality in Recursive Neural Networks with\n  Structure-aware Tag Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10211v1": {
            "Paper Title": "Multi-task Learning over Graph Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06031v2": {
            "Paper Title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09362v2": {
            "Paper Title": "Words Can Shift: Dynamically Adjusting Word Representations Using\n  Nonverbal Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07240v2": {
            "Paper Title": "Representation Mixing for TTS Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ", we do not\nuse multi-step prediction, convolutional layers in the mel-\ndecoding stage, or Grif\ufb01n-Lim inside the learning pathway.\nAudio processing closely resembles Tacotron 2 ",
                    "Citation Text": "J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Skerry-Ryan, et al., \u201cNatu-\nral tts synthesis by conditioning wavenet on mel spectrogram\npredictions,\u201d arXiv preprint arXiv:1712.05884 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05884",
                        "Citation Paper Title": "Title:Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
                        "Citation Paper Abstract": "Abstract:This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.",
                        "Citation Paper Authors": "Authors:Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "Representation mixing is closely related to the \u201dmixed-\ncharacter-and-phoneme\u201d setting described in Deep V oice\n3 ",
                    "Citation Text": "W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan,\nS. Narang, J. Raiman, and J. Miller, \u201cDeep voice 3: Scal-\ning text-to-speech with convolutional sequence learning,\u201d in\nInternational Conference on Learning Representations (ICLR\n2018) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07654",
                        "Citation Paper Title": "Title:Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
                        "Citation Paper Abstract": "Abstract:We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.",
                        "Citation Paper Authors": "Authors:Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.09786v1": {
            "Paper Title": "Recurrently Controlled Recurrent Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09755v1": {
            "Paper Title": "Estimation of Inter-Sentiment Correlations Employing Deep Neural Network\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02765v2": {
            "Paper Title": "Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video\n  Captioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09575v1": {
            "Paper Title": "A Hierarchical Neural Network for Sequence-to-Sequences Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09529v1": {
            "Paper Title": "Competency Questions and SPARQL-OWL Queries Dataset and Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07600v2": {
            "Paper Title": "A Trustworthy, Responsible and Interpretable System to Handle Chit Chat\n  in Conversational Bots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09386v1": {
            "Paper Title": "Explicit Interaction Model towards Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09368v1": {
            "Paper Title": "Fine Grained Classification of Personal Data Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09242v1": {
            "Paper Title": "AutoSense Model for Word Sense Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06546v2": {
            "Paper Title": "Poincar\u00e9 GloVe: Hyperbolic Word Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06497v2": {
            "Paper Title": "Goal-oriented Dialogue Policy Learning from Failures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10422v1": {
            "Paper Title": "Creating a contemporary corpus of similes in Serbian by using natural\n  language processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09021v1": {
            "Paper Title": "Bytes are All You Need: End-to-End Multilingual Speech Recognition and\n  Synthesis with Bytes",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "for speech processi ng. For\nASR, we adopt the Listen, Attend and Spell (LAS) ",
                    "Citation Text": "William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals ,\n\u201cListen, Attend and Spell: A Neural Network for Large V ocab-\nulary Conversational Speech Recognition ,\u201d in ICASSP , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.01211",
                        "Citation Paper Title": "Title:Listen, Attend and Spell",
                        "Citation Paper Abstract": "Abstract:We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.",
                        "Citation Paper Authors": "Authors:William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "layers , with\neach layer having 1,400hidden units. The decoder network consists\nof 2 unidirectional LSTM layers with 1,024hidden units. Additive\ncontent-based attention ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,\n\u201cNeural Machine Translation by Jointly Learning to Align an d\nTranslate,\u201d 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.08853v1": {
            "Paper Title": "Resource Mention Extraction for MOOC Discussion Forums",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "at-\ntempted a single-direction LSTM network to perform sequence tagging, and\nCollobert et al. ",
                    "Citation Text": "R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nP. Kuksa, Natural language processing (almost) from scratch, Journal\nof Machine Learning Research 12 (Aug) (2011) 2493{2537.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.0398",
                        "Citation Paper Title": "Title:Natural Language Processing (almost) from Scratch",
                        "Citation Paper Abstract": "Abstract:We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
                        "Citation Paper Authors": "Authors:Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.03707v2": {
            "Paper Title": "Answering Visual What-If Questions: From Actions to Predicted Scene\n  Descriptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08757v1": {
            "Paper Title": "The Best of Both Worlds: Lexical Resources To Improve Low-Resource\n  Part-of-Speech Tagging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08186v2": {
            "Paper Title": "Automatic Language Identification in Texts: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08600v1": {
            "Paper Title": "Contextualized Non-local Neural Networks for Sequence Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08100v2": {
            "Paper Title": "Another Diversity-Promoting Objective Function for Neural Dialogue\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08541v1": {
            "Paper Title": "Neural Machine Translation with Adequacy-Oriented Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11476v2": {
            "Paper Title": "Named Person Coreference in English News",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05724v2": {
            "Paper Title": "Improving Natural Language Inference Using External Knowledge in the\n  Science Questions Domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08162v1": {
            "Paper Title": "DeepZip: Lossless Data Compression using Recurrent Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", but faced\nstability and compatibility issues. We believe that the model size can be reduced\nsigni\fcantly without losing compression performance by using techniques similar to\nthose outlined in ",
                    "Citation Text": "Song Han, Huizi Mao, and William J Dally, \\Deep compression: Compressing deep\nneural networks with pruning, trained quantization and hu\u000bman coding,\" arXiv\npreprint arXiv:1510.00149 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.00149",
                        "Citation Paper Title": "Title:Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
                        "Citation Paper Abstract": "Abstract:Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.",
                        "Citation Paper Authors": "Authors:Song Han, Huizi Mao, William J. Dally"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.04164v2": {
            "Paper Title": "Understanding Actors and Evaluating Personae with Gaussian Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00240v2": {
            "Paper Title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08048v1": {
            "Paper Title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative\n  Relationships",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05788v2": {
            "Paper Title": "Mixture of Expert/Imitator Networks: Scalable Semi-supervised Learning\n  Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09764v2": {
            "Paper Title": "Deep Generative Models with Learnable Knowledge Constraints",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", other models [ 18,22,53,20], and\ndatasets for other related tasks (e.g., the human image generation method in Figure 1, and ",
                    "Citation Text": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In\nICML , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00955",
                        "Citation Paper Title": "Title:Toward Controlled Generation of Text",
                        "Citation Paper Abstract": "Abstract:Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
                        "Citation Paper Authors": "Authors:Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "which follows the objective:\nminq\u0019L(q\u0019) =KL(q\u0019(x)kp\u0019(x))\u0000\u000bEq\u0019[R(x)]; (6)\nwhere the KL divergence prevents the policy from changing too rapidly. Similar objectives have also\nbeen widely used in other workhorse algorithms such as trust-region policy optimization (TRPO) ",
                    "Citation Text": "J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In ICML ,\npages 1889\u20131897, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05477",
                        "Citation Paper Title": "Title:Trust Region Policy Optimization",
                        "Citation Paper Abstract": "Abstract:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                        "Citation Paper Authors": "Authors:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.08008v1": {
            "Paper Title": "End-to-End Retrieval in Continuous Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07631v1": {
            "Paper Title": "Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.08142v3": {
            "Paper Title": "Latent Multi-task Architecture Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10317v3": {
            "Paper Title": "Learning to Discriminate Noises for Incorporating External Information\n  in Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07497v1": {
            "Paper Title": "A Comparative Analysis of Content-based Geolocation in Blogs and Tweets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07485v1": {
            "Paper Title": "Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". One step advanced in our\nwork, we employ DCCAE ",
                    "Citation Text": "Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view\nrepresentation learning. In Proceedings of the 32Nd International Conference on\nInternational Conference on Machine Learning - Volume 37 , ICML\u201915, pages\n1083\u20131092. JMLR.org, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.01024",
                        "Citation Paper Title": "Title:On Deep Multi-View Representation Learning: Objectives and Optimization",
                        "Citation Paper Abstract": "Abstract:We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.",
                        "Citation Paper Authors": "Authors:Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "followed by \ufb01ne tuning.\n3We are happy to share this dataset to public after the paper gets published.\n14\u2022 PCNN: Progressive CNN ",
                    "Citation Text": "Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Robust image senti-\nment analysis using progressively trained and domain transferred deep networks.\nInProceedings of the Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence ,\nAAAI\u201915, pages 381\u2013388. AAAI Press, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.06041",
                        "Citation Paper Title": "Title:Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks",
                        "Citation Paper Abstract": "Abstract:Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.",
                        "Citation Paper Authors": "Authors:Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.02393v2": {
            "Paper Title": "Improving Neural Question Generation using Answer Separation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.05700v2": {
            "Paper Title": "Morphology-based Entity and Relational Entity Extraction Framework for\n  Arabic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07253v1": {
            "Paper Title": "Quantifying Uncertainties in Natural Language Processing Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07236v1": {
            "Paper Title": "Robust cross-domain disfluency detection with pattern match networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07234v1": {
            "Paper Title": "Improving Automatic Source Code Summarization via Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07223v1": {
            "Paper Title": "Deep Dive into Anonymity: A Large Scale Analysis of Quora Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07078v1": {
            "Paper Title": "An Affect-Rich Neural Conversational Model with Biased Attention and\n  Weighted Cross-Entropy Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09433v3": {
            "Paper Title": "\"Bilingual Expert\" Can Find Translation Errors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07039v1": {
            "Paper Title": "Combining Fact Extraction and Verification with Neural Semantic Matching\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07033v1": {
            "Paper Title": "Analyzing Compositionality-Sensitivity of NLI Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07032v1": {
            "Paper Title": "Mining Entity Synonyms with Efficient Neural Set Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08058v2": {
            "Paper Title": "Adversarial Training for Community Question Answer Selection Based on\n  Multi-scale Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06943v1": {
            "Paper Title": "Automatic Paper Summary Generation from Visual and Textual Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06303v2": {
            "Paper Title": "End-to-End Learning for Answering Structured Queries Directly over Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.07042v4": {
            "Paper Title": "Response Generation by Context-aware Prototype Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10999v1": {
            "Paper Title": "Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06596v1": {
            "Paper Title": "On Generality and Knowledge Transferability in Cross-Domain Duplicate\n  Question Detection for Heterogeneous Community Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06567v1": {
            "Paper Title": "Automatic Text Document Summarization using Semantic-based Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.01932v2": {
            "Paper Title": "A Knowledge-Grounded Neural Conversation Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01636v4": {
            "Paper Title": "Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". There\nexist label annotation mistakes by human annotators in the original\ndatasets and ",
                    "Citation Text": "Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, and Jiawei Han.\n2019. CrossWeigh: Training Named Entity Tagger from Imperfect Annotations.\narXiv preprint arXiv:1909.01441 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.01441",
                        "Citation Paper Title": "Title:CrossWeigh: Training Named Entity Tagger from Imperfect Annotations",
                        "Citation Paper Abstract": "Abstract:Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: this https URL.",
                        "Citation Paper Authors": "Authors:Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, Jiawei Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.14647v2": {
            "Paper Title": "Automatic Debate Evaluation with Argumentation Semantics and Natural\n  Language Argument Graph Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09726v2": {
            "Paper Title": "Improving Faithfulness of Abstractive Summarization by Controlling\n  Confounding Effect of Irrelevant Sentences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15629v3": {
            "Paper Title": "Language Control Diffusion: Efficiently Scaling through Space, Time, and\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01223v3": {
            "Paper Title": "Inducing Meaningful Units from Character Sequences with Dynamic Capacity\n  Slot Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00815v2": {
            "Paper Title": "Representation Learning for Weakly Supervised Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13288v2": {
            "Paper Title": "Discovering Salient Neurons in Deep NLP Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08604v2": {
            "Paper Title": "NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations\n  On-the-Fly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11326v4": {
            "Paper Title": "Towards Faithful Model Explanation in NLP: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10668v2": {
            "Paper Title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and\n  Semantic Parsing",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "sh owed that constrained generation from\nfew-shot prompted GPT-3 and \ufb01ne-tuned BART models outperfo rmed task-speci\ufb01c semantic pars-\ning architectures in low-resource settings. Scholak et al. ",
                    "Citation Text": "Torsten Scholak, Nathan Schucher, and Dzmitry Bahdana u. PICARD: Parsing incre-\nmentally for constrained auto-regressive decoding from la nguage models. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natural L anguage Processing ,\npages 9895\u20139901, Online and Punta Cana, Dominican Republic , November 2021. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.779. URL\nhttps://aclanthology.org/2021.emnlp-main.779 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05093",
                        "Citation Paper Title": "Title:PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
                        "Citation Paper Abstract": "Abstract:Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at this https URL), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
                        "Citation Paper Authors": "Authors:Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". The input to our model is the utterance concatenated with the string rep-\nresentation of the context (conversation context, databas e schema, etc.), and the output is the target\nparse. We evaluate three large language models: GPT-3 ",
                    "Citation Text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah , Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Aman da Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, R ewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Chr istopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Lang uage models are few-shot learn-\ners.Computing Research Repository , arXiv:2005.14165, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.00676v3": {
            "Paper Title": "Token-Modification Adversarial Attacks for Natural Language Processing:\n  A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.15462v4": {
            "Paper Title": "Improving Visual Grounding by Encouraging Consistent Gradient-based\n  Explanations",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". These methods take advantage of object detec-\ntors which can provide high quality locations. The recently\nproposed GLIP model ",
                    "Citation Text": "Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded\nlanguage-image pre-training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10965\u201310975, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.03857",
                        "Citation Paper Title": "Title:Grounded Language-Image Pre-training",
                        "Citation Paper Abstract": "Abstract:This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "at training time to guide attention maps in order\nto improve self-supervised representation learning. Simi-\nlarly Pillai et al ",
                    "Citation Text": "Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouli-\ngian, Dennis Fong, and Hamed Pirsiavash. Consistent ex-\nplanations by contrastive learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10213\u201310222, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00527",
                        "Citation Paper Title": "Title:Consistent Explanations by Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Post-hoc explanation methods, e.g., Grad-CAM, enable humans to inspect the spatial regions responsible for a particular network decision. However, it is shown that such explanations are not always consistent with human priors, such as consistency across image transformations. Given an interpretation algorithm, e.g., Grad-CAM, we introduce a novel training method to train the model to produce more consistent explanations. Since obtaining the ground truth for a desired model interpretation is not a well-defined task, we adopt ideas from contrastive self-supervised learning, and apply them to the interpretations of the model rather than its embeddings. We show that our method, Contrastive Grad-CAM Consistency (CGC), results in Grad-CAM interpretation heatmaps that are more consistent with human annotations while still achieving comparable classification accuracy. Moreover, our method acts as a regularizer and improves the accuracy on limited-data, fine-grained classification settings. In addition, because our method does not rely on annotations, it allows for the incorporation of unlabeled data into training, which enables better generalization of the model. Our code is available here: this https URL",
                        "Citation Paper Authors": "Authors:Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, Hamed Pirsiavash"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "explored a similar idea by using segmen-\ntation masks to guide attention maps to focus on signif-\nicant image regions for an attribute prediction task. Sel-\nvaraju et al ",
                    "Citation Text": "Ramprasaath R Selvaraju, Karan Desai, Justin Johnson, and\nNikhil Naik. Casting your model: Learning to localize im-\nproves self-supervised representations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 11058\u201311067, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04630",
                        "Citation Paper Title": "Title:CASTing Your Model: Learning to Localize Improves Self-Supervised Representations",
                        "Citation Paper Abstract": "Abstract:Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretraining. Despite their success these methods have been primarily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene images with many objects. Analyzing contrastive SSL methods shows that they have poor visual grounding and receive poor supervisory signal when trained on scene images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome these limitations. CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST significantly improves the features learned by SSL methods on scene images, and further experiments show that CAST-trained models are more robust to changes in backgrounds.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Karan Desai, Justin Johnson, Nikhil Naik"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "were proposed to\nprovide weighted feature maps for any networks with min-\nimal modifications to the model. Gradient-weighted Class\nActivation Mapping ( GradCAM ) ",
                    "Citation Text": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 618\u2013626,\n2017. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02391",
                        "Citation Paper Title": "Title:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
                        "Citation Paper Abstract": "Abstract:We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Vision-Language Representation Learning. Followed\nby the success of pretraining methods in NLP such as\nBERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09746v5": {
            "Paper Title": "Evaluating Human-Language Model Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10622v2": {
            "Paper Title": "mFACE: Multilingual Summarization with Factual Consistency Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02106v2": {
            "Paper Title": "How do media talk about the Covid-19 pandemic? Metaphorical thematic\n  clustering in Italian online newspapers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09611v2": {
            "Paper Title": "Optimizing Prompts for Text-to-Image Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11152v2": {
            "Paper Title": "Cross-lingual Lifelong Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08303v2": {
            "Paper Title": "Improving Radiology Summarization with Radiograph and Anatomy Prompts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10522v2": {
            "Paper Title": "Transformers Go for the LOLs: Generating (Humourous) Titles from\n  Scientific Abstracts End-to-End",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07434v2": {
            "Paper Title": "Exploring the Limits of Natural Language Inference Based Setup for\n  Few-Shot Intent Detection",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "shows that NLI can be used as a unified language processing\nmethod ",
                    "Citation Text": "W. Yin, N. F. Rajani, D. R. Radev, R. Socher, and C. Xiong,\n\u201cUniversal natural language processing with limited annotations:\nTry few-shot textual entailment as a start,\u201d in Proceedings of\nthe 2020 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 2020, Online, November 16-20, 2020 ,\nB. Webber, T. Cohn, Y . He, and Y . Liu, Eds. Association\nfor Computational Linguistics, 2020, pp. 8229\u20138239. [Online].\nAvailable: https://doi.org/10.18653/v1/2020.emnlp-main.660",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02584",
                        "Citation Paper Title": "Title:Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start",
                        "Citation Paper Abstract": "Abstract:A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations.\nUniversal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited. Code: this https URL",
                        "Citation Paper Authors": "Authors:Wenpeng Yin, Nazneen Fatema Rajani, Dragomir Radev, Richard Socher, Caiming Xiong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.07316v5": {
            "Paper Title": "MENLI: Robust Evaluation Metrics from Natural Language Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11464v3": {
            "Paper Title": "FactMix: Using a Few Labeled In-domain Examples to Generalize to\n  Cross-domain Named Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02535v3": {
            "Paper Title": "Analyzing Transformers in Embedding Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08143v4": {
            "Paper Title": "Can large language models reason about medical questions?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03087v3": {
            "Paper Title": "Iterative Vision-and-Language Navigation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "on IR2R-CE Train. The best performing\ncheckpoint on Val-Unseen is accepted as the final model. We\nuse the Progress Monitor ",
                    "Citation Text": "Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt\nKira, Richard Socher, and Caiming Xiong. Self-monitoring\nnavigation agent via auxiliary progress estimation. In Interna-tional Conference on Learning Representations (ICLR) , 2019.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.03035",
                        "Citation Paper Title": "Title:Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
                        "Citation Paper Abstract": "Abstract:The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "ported to IR2R-CE, then fine-\ntune the best-performing EnvDrop Val-Unseen checkpoint\nusing DAgger ",
                    "Citation Text": "St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction to no-\nregret online learning. In Artificial Intelligence and Statistics\n(AISTATS) , 2011. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1011.0686",
                        "Citation Paper Title": "Title:A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
                        "Citation Paper Abstract": "Abstract:Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.",
                        "Citation Paper Authors": "Authors:Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "used to train the\nCMA model. We initially train using teacher forcing on\nthe augmented EnvDrop ",
                    "Citation Text": "Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate\nunseen environments: Back translation with environmental\ndropout. North American Chapter of the Association for\nComputational Linguistics (NAACL) , 2019. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.04195",
                        "Citation Paper Title": "Title:Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout",
                        "Citation Paper Abstract": "Abstract:A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced 'unseen' triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective 'environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropped-out environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.",
                        "Citation Paper Authors": "Authors:Hao Tan, Licheng Yu, Mohit Bansal"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", we use the inverse pinhole\ncamera projection model to unproject depth measurements\nto 3D pointclouds. We also unproject egocentric semantics\u2014\nboth ground-truth ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3d: learning from RGB-D\ndata in indoor environments. In 3D Vision , 2017. Matter-\nPort3D dataset license available at: http://kaldir.vc.\nin.tum.de/matterport/MP_TOS.pdf . 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "where cells\nare either 0 (empty) or 1 (occupied), and semantic maps\ncontain one-hot vectors of thirteen common labels in R2R\nenvironments ",
                    "Citation Text": "Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan\nEssa, and Dhruv Batra. Semantic mapnet: Building allocen-\ntric semantic maps and representations from egocentric views.\nInAAAI Conference on Artificial Intelligence , 2021. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01191",
                        "Citation Paper Title": "Title:Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views",
                        "Citation Paper Abstract": "Abstract:We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map (\"what is where?\") from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length x width x feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations build by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour(\"Find chair\") or answering questions about the space (\"How many chairs did you see in the house?\"). Project page: this https URL.",
                        "Citation Paper Authors": "Authors:Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ",\nin response to language instructions requires contending\nwith the real, continuous world. Existing work has trans-\nferred policies for discrete VLN to the physical world by\nmanually curating a discrete representation of the world\nmap as a navigation graph ",
                    "Citation Text": "Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun\nMajumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-\nto-real transfer for vision-and-language navigation. In Con-\nference on Robot Learning (CoRL) , 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03807",
                        "Citation Paper Title": "Title:Sim-to-Real Transfer for Vision-and-Language Navigation",
                        "Citation Paper Abstract": "Abstract:We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).",
                        "Citation Paper Authors": "Authors:Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.01537v2": {
            "Paper Title": "A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ", we map speech segments to pre-\ntrained speech embeddings. Here, we use TRIpLET Loss network (TRILL),\nwhich has resulted in a good performance in non-semantic speech tasks includ-\ning AD classification on DementiaBank ",
                    "Citation Text": "Shor, J., Jansen, A., Maor, R., Lang, O., Tuval, O., de Chaumont Quitry,\nF., Tagliasacchi, M., Shavitt, I., Emanuel, D., Haviv, Y.A.: Towards\nlearning a universal non-semantic representation of speech. ArXiv\nabs/2002.12764 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.12764",
                        "Citation Paper Title": "Title:Towards Learning a Universal Non-Semantic Representation of Speech",
                        "Citation Paper Abstract": "Abstract:The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark, models, and evaluation code are publicly released.",
                        "Citation Paper Authors": "Authors:Joel Shor, Aren Jansen, Ronnie Maor, Oran Lang, Omry Tuval, Felix de Chaumont Quitry, Marco Tagliasacchi, Ira Shavitt, Dotan Emanuel, Yinnon Haviv"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.07861v2": {
            "Paper Title": "Text normalization for low-resource languages: the case of Ligurian",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07662v4": {
            "Paper Title": "NELLIE: A Neuro-Symbolic Inference Engine for Grounded, Compositional,\n  and Explainable Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02047v2": {
            "Paper Title": "Hyperbolic Relevance Matching for Neural Keyphrase Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09749v5": {
            "Paper Title": "Importance Estimation from Multiple Perspectives for Keyphrase\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.03327v7": {
            "Paper Title": "Latency Adjustable Transformer Encoder for Language Understanding",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": "dataset, featuring a million words from\n1989 Wall Street Journal material, and the 1BW ",
                    "Citation Text": "C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and\nT. Robinson, \u201cOne billion word benchmark for measuring progress in\nstatistical language modeling,\u201d 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.3005",
                        "Citation Paper Title": "Title:One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
                        "Citation Paper Abstract": "Abstract:We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.\nThe benchmark is available as a this http URL project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.",
                        "Citation Paper Authors": "Authors:Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ".\nTo assess the performance of the method in decoder models,\nwe conduct evaluations on several datasets. The WikiText-103 ",
                    "Citation Text": "S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel\nmixture models,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.07843",
                        "Citation Paper Title": "Title:Pointer Sentinel Mixture Models",
                        "Citation Paper Abstract": "Abstract:Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
                        "Citation Paper Authors": "Authors:Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01039v2": {
            "Paper Title": "SoftCorrect: Error Correction with Soft Detection for Automatic Speech\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07207v3": {
            "Paper Title": "Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across\n  Modalities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09897v2": {
            "Paper Title": "Inducing Character-level Structure in Subword-based Language Models with\n  Type-level Interchange Intervention Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08012v5": {
            "Paper Title": "Meta-Referential Games to Learn Compositional Learning Behaviours",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11790v2": {
            "Paper Title": "Addressing Token Uniformity in Transformers via Singular Value\n  Transformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03518v3": {
            "Paper Title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion\n  Cause Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09130v2": {
            "Paper Title": "SAMP: A Model Inference Toolkit of Post-Training Quantization for Text\n  Processing via Self-Adaptive Mixed-Precision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00369v2": {
            "Paper Title": "A General Search-based Framework for Generating Textual Counterfactual\n  Explanations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01528v4": {
            "Paper Title": "Effective and Imperceptible Adversarial Textual Attack via\n  Multi-objectivization",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": ": A sentence-level sentiment classification dataset of movie reviews; each instance is\nlabeled with positive or negative.\n(4)SNLI ",
                    "Citation Text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for\nlearning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP\u20192015 , pages 632\u2013642, Lisbon, Portugal, Sep 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.05326",
                        "Citation Paper Title": "Title:A large annotated corpus for learning natural language inference",
                        "Citation Paper Abstract": "Abstract:Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
                        "Citation Paper Authors": "Authors:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.07066v2": {
            "Paper Title": "Controllable Citation Sentence Generation with Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.00640v4": {
            "Paper Title": "AmbiFC: Fact-Checking Ambiguous Claims with Evidence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.17255v6": {
            "Paper Title": "A Cognitive Architecture for Machine Consciousness and Artificial\n  Superintelligence: Thought Is Structured by the Iterative Updating of Working\n  Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09512v3": {
            "Paper Title": "Rethinking Label Smoothing on Multi-hop Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09932v2": {
            "Paper Title": "Making Science Simple: Corpora for the Lay Summarisation of Scientific\n  Literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01959v3": {
            "Paper Title": "Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot\n  Document-Level Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.09705v3": {
            "Paper Title": "gBuilder: A Scalable Knowledge Graph Construction System for\n  Unstructured Corpus",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ".\n\u2022Batch is identical to the concept of batch learning. By batch-\ning the data, the dedicated computing hardware (like GPU,\nTPU) can maximize the acceleration of matrix calculations\nto increase the speed of deep learning model inference or\ntraining ",
                    "Citation Text": "S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,\nand E. Shelhamer, \u201ccudnn: Efficient primitives for deep learning,\u201d arXiv\npreprint arXiv:1410.0759 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.0759",
                        "Citation Paper Title": "Title:cuDNN: Efficient Primitives for Deep Learning",
                        "Citation Paper Abstract": "Abstract:We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.",
                        "Citation Paper Authors": "Authors:Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05225v2": {
            "Paper Title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", data\naugmentation [ 2,42] and learning with multiple teachers ",
                    "Citation Text": "Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian\nJiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, et al .2022. PROD: Progressive\nDistillation for Dense Retrieval. arXiv preprint arXiv:2209.13335 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.13335",
                        "Citation Paper Title": "Title:PROD: Progressive Distillation for Dense Retrieval",
                        "Citation Paper Abstract": "Abstract:Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.",
                        "Citation Paper Authors": "Authors:Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, Nan Duan"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "66M 34.4 35.1 97.6 71.7 44.7 68.5 45.5\nSPLADE v2 ",
                    "Citation Text": "Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St\u00e9phane Clinchant.\n2021. SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval.\nCoRR abs/2109.10086 (2021). arXiv:2109.10086 https://arxiv.org/abs/2109.10086",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.10086",
                        "Citation Paper Title": "Title:SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval",
                        "Citation Paper Abstract": "Abstract:In neural Information Retrieval (IR), ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning \\emph{sparse} representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. Introduced recently, the SPLADE model provides highly sparse representations and competitive results with respect to state-of-the-art dense and sparse approaches. In this paper, we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation. We also report results on the BEIR benchmark. Overall, SPLADE is considerably improved with more than $9$\\% gains on NDCG@10 on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.",
                        "Citation Paper Authors": "Authors:Thibault Formal, Carlos Lassance, Benjamin Piwowarski, St\u00e9phane Clinchant"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "proposes a curricu-\nlum learning-based optimization framework, for enhancing dense\nretrieval models through knowledge distillation. ",
                    "Citation Text": "Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi, Zhengjie Huang, Shikun\nFeng, Yu Sun, Hao Tian, Hua Wu, Shuaiqiang Wang, Dawei Yin, and Haifeng\nWang. 2022. ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self\nOn-the-fly Distillation for Dense Passage Retrieval. CoRR abs/2205.09153 (2022).\nhttps://doi.org/10.48550/arXiv.2205.09153 arXiv:2205.09153",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09153",
                        "Citation Paper Title": "Title:ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval",
                        "Citation Paper Abstract": "Abstract:Neural retrievers based on pre-trained language models (PLMs), such as dual-encoders, have achieved promising performance on the task of open-domain question answering (QA). Their effectiveness can further reach new state-of-the-arts by incorporating cross-architecture knowledge distillation. However, most of the existing studies just directly apply conventional distillation methods. They fail to consider the particular situation where the teacher and student have different structures. In this paper, we propose a novel distillation method that significantly advances cross-architecture distillation for dual-encoders. Our method 1) introduces a self on-the-fly distillation method that can effectively distill late interaction (i.e., ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation process to further improve the performance with a cross-encoder teacher. Extensive experiments are conducted to validate that our proposed solution outperforms strong baselines and establish a new state-of-the-art on open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi, Zhengjie Huang, Shikun Feng Yu Sun, Hao Tian, Hua Wu, Shuaiqiang Wang, Dawei Yin, Haifeng Wang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "proposes a dense retrieval approach that incorporates both query-\ncentric and passage-centric similarity relations. ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan\nHanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced\nTopic Aware Sampling. In Proceedings of the 44th International ACM SIGIR Con-\nference on Research and Development in Information Retrieval , Fernando Diaz,\nChirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.).\n113\u2013122. https://doi.org/10.1145/3404835.3462891",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06967",
                        "Citation Paper Title": "Title:Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling",
                        "Citation Paper Abstract": "Abstract:A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes for in-batch negative sampling. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours (as opposed to a common configuration of 8x V100s). We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "110M 39.5 - 98.6 - - - -\nColBERT v2 ",
                    "Citation Text": "Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei\nZaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\nInteraction. In Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies ,\nMarine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edz\n(Eds.). Association for Computational Linguistics, 3715\u20133734. https://doi.org/10.\n18653/v1/2022.naacl-main.272",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01488",
                        "Citation Paper Title": "Title:ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
                        "Citation Paper Abstract": "Abstract:Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.",
                        "Citation Paper Authors": "Authors:Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "110M 33.5 34.2 96.4 67.0 39.1 66.8 43.0\nRocketQA v2 ",
                    "Citation Text": "Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu,\nHaifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A Joint Training Method\nfor Dense Passage Retrieval and Passage Re-ranking. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing , Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). 2825\u20132835.\nhttps://doi.org/10.18653/v1/2021.emnlp-main.224",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.07367",
                        "Citation Paper Title": "Title:RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
                        "Citation Paper Abstract": "Abstract:In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage re-ranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, Ji-Rong Wen"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "proposes an\nefficient topic-aware query and balanced margin sampling tech-\nnique, for training dense retrieval models. ",
                    "Citation Text": "Hansi Zeng, Hamed Zamani, and Vishwa Vinay. 2022. Curriculum Learning for\nDense Retrieval Distillation. In SIGIR . 1979\u20131983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.13679",
                        "Citation Paper Title": "Title:Curriculum Learning for Dense Retrieval Distillation",
                        "Citation Paper Abstract": "Abstract:Recent work has shown that more effective dense retrieval models can be obtained by distilling ranking knowledge from an existing base re-ranking model. In this paper, we propose a generic curriculum learning based optimization framework called CL-DRD that controls the difficulty level of training data produced by the re-ranking (teacher) model. CL-DRD iteratively optimizes the dense retrieval (student) model by increasing the difficulty of the knowledge distillation data made available to it. In more detail, we initially provide the student model coarse-grained preference pairs between documents in the teacher's ranking and progressively move towards finer-grained pairwise document ordering requirements. In our experiments, we apply a simple implementation of the CL-DRD framework to enhance two state-of-the-art dense retrieval models. Experiments on three public passage retrieval datasets demonstrate the effectiveness of our proposed framework.",
                        "Citation Paper Authors": "Authors:Hansi Zeng, Hamed Zamani, Vishwa Vinay"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "and TREC 2020 DL Track ( TREC-Pas-20 and\nTREC-Doc-20 ) ",
                    "Citation Text": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021.\nOverview of the TREC 2020 deep learning track. CoRR abs/2102.07662 (2021).\narXiv:2102.07662 https://arxiv.org/abs/2102.07662",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07662",
                        "Citation Paper Title": "Title:Overview of the TREC 2020 deep learning track",
                        "Citation Paper Abstract": "Abstract:This is the second year of the TREC Deep Learning Track, with the goal of studying ad hoc ranking in the large training data regime. We again have a document retrieval task and a passage retrieval task, each with hundreds of thousands of human-labeled training queries. We evaluate using single-shot TREC-style evaluation, to give us a picture of which ranking methods work best when large data is available, with much more comprehensive relevance labeling on the small number of test queries. This year we have further evidence that rankers with BERT-style pretraining outperform other rankers in the large data regime.",
                        "Citation Paper Authors": "Authors:Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.04232v2": {
            "Paper Title": "Revealing Patient-Reported Experiences in Healthcare from Social Media\n  using the DAPMAV Framework",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": ", allowing temporal information to be extracted. Alternat ively,\nnarrative time may be taken from the depth of a story using wor d positions, as was the case in ",
                    "Citation Text": "Andrew J Reagan, Lewis Mitchell, Dilan Kiley, Christop her M Danforth, and Peter Sheridan Dodds. The emo-\ntional arcs of stories are dominated by six basic shapes. EPJ Data Science , 5(1):1\u201312, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07772",
                        "Citation Paper Title": "Title:The emotional arcs of stories are dominated by six basic shapes",
                        "Citation Paper Abstract": "Abstract:Advances in computing power, natural language processing, and digitization of text now make it possible to study a culture's evolution through its texts using a \"big data\" lens. Our ability to communicate relies in part upon a shared emotional experience, with stories often following distinct emotional trajectories and forming patterns that are meaningful to us. Here, by classifying the emotional arcs for a filtered subset of 1,327 stories from Project Gutenberg's fiction collection, we find a set of six core emotional arcs which form the essential building blocks of complex emotional trajectories. We strengthen our findings by separately applying Matrix decomposition, supervised learning, and unsupervised learning. For each of these six core emotional arcs, we examine the closest characteristic stories in publication today and find that particular emotional arcs enjoy greater success, as measured by downloads.",
                        "Citation Paper Authors": "Authors:Andrew J. Reagan, Lewis Mitchell, Dilan Kiley, Christopher M. Danforth, Peter Sheridan Dodds"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.05523v3": {
            "Paper Title": "Impact of Adversarial Training on Robustness and Generalizability of\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05751v3": {
            "Paper Title": "A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum\n  Machines",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "as our\nbaseline. Xtalk optimizes crosstalk at scheduling level. It uses SMT solver to determine where and if barriers\nshould be added to delay the execution. For hardware mapping, we use Sabre ",
                    "Citation Text": "Gushu Li, Yufei Ding, and Yuan Xie. Tackling the qubit mapping problem for nisq-era quantum devices. In Proceedings of\nthe Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pages\n1001\u20131014. ACM, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.02573",
                        "Citation Paper Title": "Title:Tackling the Qubit Mapping Problem for NISQ-Era Quantum Devices",
                        "Citation Paper Abstract": "Abstract:Due to little consideration in the hardware constraints, e.g., limited connections between physical qubits to enable two-qubit gates, most quantum algorithms cannot be directly executed on the Noisy Intermediate-Scale Quantum (NISQ) devices. Dynamically remapping logical qubits to physical qubits in the compiler is needed to enable the two-qubit gates in the algorithm, which introduces additional operations and inevitably reduces the fidelity of the algorithm. Previous solutions in finding such remapping suffer from high complexity, poor initial mapping quality, and limited flexibility and controllability.\nTo address these drawbacks mentioned above, this paper proposes a SWAP-based BidiREctional heuristic search algorithm SABRE, which is applicable to NISQ devices with arbitrary connections between qubits. By optimizing every search attempt,globally optimizing the initial mapping using a novel reverse traversal technique, introducing the decay effect to enable the trade-off between the depth and the number of gates of the entire algorithm, SABRE outperforms the best known algorithm with exponential speedup and comparable or better results on various benchmarks.",
                        "Citation Paper Authors": "Authors:Gushu Li, Yufei Ding, Yuan Xie"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". We show the qubit and gate number in Table. 1.\nThere are three types of benchmarks. The first type is the generic benchmark. The second type is for\n2-local VQA application. We use QAOA ",
                    "Citation Text": "Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization algorithm. arXiv preprint\narXiv:1411.4028 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4028",
                        "Citation Paper Title": "Title:A Quantum Approximate Optimization Algorithm",
                        "Citation Paper Abstract": "Abstract:We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.",
                        "Citation Paper Authors": "Authors:Edward Farhi, Jeffrey Goldstone, Sam Gutmann"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "in\n\u00a77 and conclude in \u00a78.\n2 BACKGROUND\n2.1 Circuit Error Model and Crosstalk\n1)Single-qubit gates and decoherence error : In superconducting transom systems, single-qubit gates are\nimplemented by driving the target qubit through microwave in the pulse level ",
                    "Citation Text": "Thomas Alexander, Naoki Kanazawa, Daniel J Egger, Lauren Capelluto, Christopher J Wood, Ali Javadi-Abhari, and David C\nMcKay. Qiskit pulse: programming quantum computers through the cloud with pulses. Quantum Science and Technology ,\n5(4):044006, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06755",
                        "Citation Paper Title": "Title:Qiskit Pulse: Programming Quantum Computers Through the Cloud with Pulses",
                        "Citation Paper Abstract": "Abstract:The quantum circuit model is an abstraction that hides the underlying physical implementation of gates and measurements on a quantum computer. For precise control of real quantum hardware, the ability to execute pulse and readout-level instructions is required. To that end, we introduce Qiskit Pulse, a pulse-level programming paradigm implemented as a module within Qiskit-Terra \\cite{Qiskit}. To demonstrate the capabilities of Qiskit Pulse, we calibrate both un-echoed and echoed variants of the cross-resonance entangling gate with a pair of qubits on an IBM Quantum system accessible through the cloud. We perform Hamiltonian characterization of both single and two-pulse variants of the cross-resonance entangling gate with varying amplitudes on a cloud-based IBM Quantum system. We then transform these calibrated sequences into a high-fidelity CNOT gate by applying pre and post local-rotations to the qubits, achieving average gate fidelities of $F=0.981$ and $F=0.979$ for the un-echoed and echoed respectively. This is comparable to the standard backend CNOT fidelity of $F_{CX}=0.984$. Furthermore, to illustrate how users can access their results at different levels of the readout chain, we build a custom discriminator to investigate qubit readout correlations. Qiskit Pulse allows users to explore advanced control schemes such as optimal control theory, dynamical decoupling, and error mitigation that are not available within the circuit model.",
                        "Citation Paper Authors": "Authors:Thomas Alexander, Naoki Kanazawa, Daniel J. Egger, Lauren Capelluto, Christopher J. Wood, Ali Javadi-Abhari, David McKay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.01818v2": {
            "Paper Title": "QAGCN: Answering Multi-Relation Questions via Single-Step Implicit\n  Reasoning over Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07084v3": {
            "Paper Title": "Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09744v3": {
            "Paper Title": "DSI++: Updating Transformer Memory with New Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08089v2": {
            "Paper Title": "Constrained Few-Shot Learning: Human-Like Low Sample Complexity Learning\n  and Non-Episodic Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14578v2": {
            "Paper Title": "MAUVE Scores for Generative Models: Theory and Practice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.09517v2": {
            "Paper Title": "Deep Learning for Hate Speech Detection: A Comparative Study",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": ", in Italian, English, Korean and German [16, 31, 36], or in English, Hindi, and German ",
                    "Citation Text": "Sayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid, and Vasudeva Varma. Leveraging multilingual\ntransformers for hate speech detection. arXiv preprint arXiv:2101.03207 , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.03207",
                        "Citation Paper Title": "Title:Leveraging Multilingual Transformers for Hate Speech Detection",
                        "Citation Paper Abstract": "Abstract:Detecting and classifying instances of hate in social media text has been a problem of interest in Natural Language Processing in the recent years. Our work leverages state of the art Transformer language models to identify hate speech in a multilingual setting. Capturing the intent of a post or a comment on social media involves careful evaluation of the language style, semantic content and additional pointers such as hashtags and emojis. In this paper, we look at the problem of identifying whether a Twitter post is hateful and offensive or not. We further discriminate the detected toxic content into one of the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN) and (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text encoder at the base, we are able to successfully identify and classify hate speech from multiple languages. On the provided testing corpora, we achieve Macro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi respectively while performing hate speech detection and of 60.70, 53.28 and 49.74 during fine-grained classification. In our experiments, we show the efficacy of Perspective API features for hate speech classification and the effects of exploiting a multilingual training scheme. A feature selection study is provided to illustrate impacts of specific features upon the architecture's classification head.",
                        "Citation Paper Authors": "Authors:Sayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid, Vasudeva Varma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.02160v2": {
            "Paper Title": "A Comprehensive Review of Visual-Textual Sentiment Analysis from Social\n  Media Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09710v2": {
            "Paper Title": "Continual Learning for Instruction Following from Realtime Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16934v2": {
            "Paper Title": "VideoDubber: Machine Translation with Speech-Aware Length Control for\n  Video Dubbing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09917v2": {
            "Paper Title": "Inverse Reinforcement Learning for Text Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09928v2": {
            "Paper Title": "Improving the Robustness of Summarization Models by Detecting and\n  Removing Input Noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14986v2": {
            "Paper Title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters\n  for Implicature Resolution by LLMs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08412v2": {
            "Paper Title": "Evaluating the Factual Consistency of Large Language Models Through News\n  Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08063v5": {
            "Paper Title": "Information Extraction in Low-Resource Scenarios: Survey and Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01722v3": {
            "Paper Title": "H_eval: A new hybrid evaluation metric for automatic speech recognition\n  tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00616v2": {
            "Paper Title": "Extensible Prompts for Language Models on Zero-shot Language Style\n  Customization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12727v3": {
            "Paper Title": "Generating More Pertinent Captions by Leveraging Semantics and Style on\n  Multi-Source Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10240v2": {
            "Paper Title": "Diffusion Glancing Transformer for Parallel Sequence to Sequence\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03002v2": {
            "Paper Title": "GraphPrompt: Graph-Based Prompt Templates for Biomedical Synonym\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.02804v2": {
            "Paper Title": "Just ClozE! A Novel Framework for Evaluating the Factual Consistency\n  Faster in Abstractive Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10013v2": {
            "Paper Title": "DocAsRef: An Empirical Study on Repurposing Reference-Based Summary\n  Quality Metrics Reference-Freely",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10498v4": {
            "Paper Title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models\n  on Planning and Reasoning about Change",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04568v2": {
            "Paper Title": "The Impact of Data Corruption on Named Entity Recognition for\n  Low-resourced Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00509v3": {
            "Paper Title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07278v2": {
            "Paper Title": "\u00daFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11603v3": {
            "Paper Title": "Representation Projection Invariance Mitigates Representation Collapse",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07496v3": {
            "Paper Title": "Unsupervised Opinion Summarization Using Approximate Geodesics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08358v3": {
            "Paper Title": "MEAL: Stable and Active Learning for Few-Shot Prompting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.15439v2": {
            "Paper Title": "StyleTTS: A Style-Based Generative Model for Natural and Diverse\n  Text-to-Speech Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.01311v3": {
            "Paper Title": "Inferring the Reader: Guiding Automated Story Generation with\n  Commonsense Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08436v2": {
            "Paper Title": "Don't Say What You Don't Know: Improving the Consistency of Abstractive\n  Summarization by Constraining Beam Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08522v2": {
            "Paper Title": "Classifying COVID-19 vaccine narratives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08955v5": {
            "Paper Title": "Making first order linear logic a generating grammar",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11483v4": {
            "Paper Title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13034v2": {
            "Paper Title": "Beyond Vectors: Subspace Representations for Set Operations of\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02712v2": {
            "Paper Title": "Improved Beam Search for Hallucination Mitigation in Abstractive\n  Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00768v3": {
            "Paper Title": "Simplifying and Understanding State Space Models with Diagonal Linear\n  RNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05544v3": {
            "Paper Title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive\n  Language Processing Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13854v3": {
            "Paper Title": "ComCLIP: Training-Free Compositional Image and Text Matching",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "We use standard CLIP, where image embeddings\nare generated by CLIP\u2019s vision encoder F; and text embed-\ndings are generated by CLIP\u2019s text encoder G. The cosine\nsimilarity between them is computed to do matching.\nSLIP ",
                    "Citation Text": "Norman Mu, Alexander Kirillov, David Wagner, and\nSaining Xie. Slip: Self-supervision meets language-\nimage pre-training. arXiv preprint arXiv:2112.12750 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.12750",
                        "Citation Paper Title": "Title:SLIP: Self-supervision meets Language-Image Pre-training",
                        "Citation Paper Abstract": "Abstract:Recent work has shown that self-supervised pre-training leads to improvements over supervised learning on challenging visual recognition tasks. CLIP, an exciting new approach to learning with language supervision, demonstrates promising performance on a wide variety of benchmarks. In this work, we explore whether self-supervised learning can aid in the use of language supervision for visual representation learning. We introduce SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training. After pre-training with Vision Transformers, we thoroughly evaluate representation quality and compare performance to both CLIP and self-supervised learning under three distinct settings: zero-shot transfer, linear classification, and end-to-end finetuning. Across ImageNet and a battery of additional datasets, we find that SLIP improves accuracy by a large margin. We validate our results further with experiments on different model sizes, training schedules, and pre-training datasets. Our findings show that SLIP enjoys the best of both worlds: better performance than self-supervision (+8.1% linear accuracy) and language supervision (+5.2% zero-shot accuracy).",
                        "Citation Paper Authors": "Authors:Norman Mu, Alexander Kirillov, David Wagner, Saining Xie"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "From 1000 images (each having 5-6 annota-\ntions), one annotation is selected randomly. Like Flickr30K,\nthe top 10 images from CLIP undergo ComCLIP processing,\nand subimages are created based on parsed elements.\nWinoground ",
                    "Citation Text": "Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. Winoground: Probing vision and language mod-\nels for visio-linguistic compositionality. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 5238\u20135248, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03162",
                        "Citation Paper Title": "Title:Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
                        "Citation Paper Abstract": "Abstract:We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at this https URL.",
                        "Citation Paper Authors": "Authors:Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", using the approximation of NGSM (Normal-\nized Weighted Geometric Mean) ",
                    "Citation Text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In\nInternational conference on machine learning , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.03044",
                        "Citation Paper Title": "Title:Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
                        "Citation Paper Abstract": "Abstract:Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
                        "Citation Paper Authors": "Authors:Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "performs hierarchical alignments in three different gran-\nularities, i.e., global-global, global-local, and local-local\nalignments for description-based person re-id. ",
                    "Citation Text": "Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-\ngrained video-text retrieval with hierarchical graph rea-\nsoning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages\n10638\u201310647, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00392",
                        "Citation Paper Title": "Title:Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning",
                        "Citation Paper Abstract": "Abstract:Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.",
                        "Citation Paper Authors": "Authors:Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "disentangles background, texture, shape,\netc., and uses object bounding boxes as supervision to syn-\nthesize images. ",
                    "Citation Text": "M Besserve, A Mehrjou, R Sun, and B Sch \u00a8olkopf.\nCounterfactuals uncover the modular structure of deepgenerative models. In Eighth International Conference\non Learning Representations (ICLR 2020) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03253",
                        "Citation Paper Title": "Title:Counterfactuals uncover the modular structure of deep generative models",
                        "Citation Paper Abstract": "Abstract:Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.",
                        "Citation Paper Authors": "Authors:Michel Besserve, Arash Mehrjou, R\u00e9my Sun, Bernhard Sch\u00f6lkopf"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "and bias towards spurious relations they have\nseen in the pretraining data, referred to as \u201cconfounders\u201d ",
                    "Citation Text": "Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang,\nZhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, and Fei\nWu. Devlbert: Learning deconfounded visio-linguistic\nrepresentations. In Proceedings of the 28th ACM Inter-\nnational Conference on Multimedia , pages 4373\u20134382,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06884",
                        "Citation Paper Title": "Title:DeVLBert: Learning Deconfounded Visio-Linguistic Representations",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose to investigate the problem of out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token (object or word) given another one can be high (due to the dataset biases) without robust (causal) relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability.",
                        "Citation Paper Authors": "Authors:Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "learns multiple networks that capture words, phrases, and\nsentence-level interactions with images and combines the\nscores of these networks to obtain a whole image-sentence\nscore. ",
                    "Citation Text": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi\nFeng, Kate Saenko, and Trevor Darrell. Natural lan-\nguage object retrieval. In Proceedings of the IEEE\n8conference on computer vision and pattern recognition ,\npages 4555\u20134564, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04164",
                        "Citation Paper Title": "Title:Natural Language Object Retrieval",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",
                        "Citation Paper Authors": "Authors:Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "incorporates\na soft form of attention into their recurrent model. ",
                    "Citation Text": "Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li.\nMultimodal convolutional neural networks for match-\ning image and sentence. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n2623\u20132631, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.06063",
                        "Citation Paper Title": "Title:Multimodal Convolutional Neural Networks for Matching Image and Sentence",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.",
                        "Citation Paper Authors": "Authors:Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13308v4": {
            "Paper Title": "SciRepEval: A Multi-Format Benchmark for Scientific Document\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03691v3": {
            "Paper Title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07505v2": {
            "Paper Title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00445v3": {
            "Paper Title": "Interpreting Embedding Spaces by Conceptualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09885v2": {
            "Paper Title": "An Attention-Based Model for Predicting Contextual Informativeness and\n  Curriculum Learning Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08094v2": {
            "Paper Title": "Joint processing of linguistic properties in brains and language models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.05794v3": {
            "Paper Title": "Designing Robust Transformers using Robust Kernel Density Estimation",
            "Sentences": [
                {
                    "Sentence ID": 76,
                    "Sentence": ", and compared the outcomes across various methodologies (Table 4). The baseline\nimplementation and datasets were adapted from ",
                    "Citation Text": "H. Wu, J. Wu, J. Xu, J. Wang, and M. Long. Flowformer: Linearizing transformers with\nconservation flows. In International Conference on Machine Learning , 2022. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.06258",
                        "Citation Paper Title": "Title:Flowformer: Linearizing Transformers with Conservation Flows",
                        "Citation Paper Abstract": "Abstract:Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: this https URL.",
                        "Citation Paper Authors": "Authors:Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "suggested fully attentional networks to enhance self-attention, achieving\nstate-of-the-art accuracy on corrupted images. Furthermore, ",
                    "Citation Text": "X. Mao, G. Qi, Y . Chen, X. Li, R. Duan, S. Ye, Y . He, and H. Xue. Towards robust vision\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 12042\u201312051, 2022. 2, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.07926",
                        "Citation Paper Title": "Title:Towards Robust Vision Transformer",
                        "Citation Paper Abstract": "Abstract:Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By using and combining robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. We further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results on ImageNet and six robustness benchmarks show the advanced robustness and generalization ability of RVT compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C and ImageNet-Sketch. The code will be available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, Hui Xue"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "delved into table understanding and\nsuggested a robust, structurally aware table-text encoding architecture to mitigate the effects of row\nand column order perturbations. ",
                    "Citation Text": "J. Liu, T. Singhal, L. T. Blessing, K. L. Wood, and K. H. Lim. Crisisbert: a robust transformer\nfor crisis classification and contextual crisis embedding. In Proceedings of the 32nd ACM\nConference on Hypertext and Social Media , pages 133\u2013141, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.06627",
                        "Citation Paper Title": "Title:CrisisBERT: a Robust Transformer for Crisis Classification and Contextual Crisis Embedding",
                        "Citation Paper Abstract": "Abstract:Classification of crisis events, such as natural disasters, terrorist attacks and pandemics, is a crucial task to create early signals and inform relevant parties for spontaneous actions to reduce overall damage. Despite crisis such as natural disasters can be predicted by professional institutions, certain events are first signaled by civilians, such as the recent COVID-19 pandemics. Social media platforms such as Twitter often exposes firsthand signals on such crises through high volume information exchange over half a billion tweets posted daily. Prior works proposed various crisis embeddings and classification using conventional Machine Learning and Neural Network models. However, none of the works perform crisis embedding and classification using state of the art attention-based deep neural networks models, such as Transformers and document-level contextual embeddings. This work proposes CrisisBERT, an end-to-end transformer-based model for two crisis classification tasks, namely crisis detection and crisis recognition, which shows promising results across accuracy and f1 scores. The proposed model also demonstrates superior robustness over benchmark, as it shows marginal performance compromise while extending from 6 to 36 events with only 51.4% additional data points. We also proposed Crisis2Vec, an attention-based, document-level contextual embedding architecture for crisis embedding, which achieve better performance than conventional crisis embedding methods such as Word2Vec and GloVe. To the best of our knowledge, our works are first to propose using transformer-based crisis classification and document-level contextual crisis embedding in the literature.",
                        "Citation Paper Authors": "Authors:Junhua Liu, Trisha Singhal, Lucienne T.M. Blessing, Kristin L. Wood, Kwan Hui Lim"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "induces a bi-directional information\nflow, which is limiting for position-sensitive datasets such as text or sequences. These methods also\nintroduce additional parameters. Beyond these vision-focused studies, robust transformers have also\nbeen explored in fields like text analysis and social media. ",
                    "Citation Text": "J. Yang, A. Gupta, S. Upadhyay, L. He, R. Goel, and S. Paul. Tableformer: Robust transformer\nmodeling for table-text encoding. arXiv preprint arXiv:2203.00274 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.00274",
                        "Citation Paper Title": "Title:TableFormer: Robust Transformer Modeling for Table-Text Encoding",
                        "Citation Paper Abstract": "Abstract:Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled the table structures or table-text alignments, hindering the table-text understanding ability. In this work, we propose a robust and structurally aware table-text encoding architecture TableFormer, where tabular structural biases are incorporated completely through learnable attention biases. TableFormer is (1) strictly invariant to row and column orders, and, (2) could understand tables better due to its tabular inductive biases. Our evaluations showed that TableFormer outperforms strong baselines in all settings on SQA, WTQ and TabFact table reasoning datasets, and achieves state-of-the-art performance on SQA, especially when facing answer-invariant row and column order perturbations (6% improvement over the best baseline), because previous SOTA models' performance drops by 4% - 6% when facing such perturbations while TableFormer is not affected.",
                        "Citation Paper Authors": "Authors:Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, Shachi Paul"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.09864v5": {
            "Paper Title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08289v2": {
            "Paper Title": "Decomposing Natural Logic Inferences in Neural NLI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05629v2": {
            "Paper Title": "Leveraging Large (Visual) Language Models for Robot 3D Scene\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.12040v3": {
            "Paper Title": "Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent\n  Semantic Communications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09462v2": {
            "Paper Title": "Latent Diffusion for Language Generation",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "to sequence-to-\nsequence tasks by concatenating the source and target sequence and only performing diffusion for the\ntarget sequence. Chen et al. ",
                    "Citation Text": "Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using\ndiffusion models with self-conditioning. arXiv preprint arXiv:2208.04202 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.04202",
                        "Citation Paper Title": "Title:Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning",
                        "Citation Paper Abstract": "Abstract:We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",
                        "Citation Paper Authors": "Authors:Ting Chen, Ruixiang Zhang, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "train a continuous diffusion\nmodel in the space of token embeddings that are learned jointly with the denoising objective and\ndecode generations with a rounding step. Strudel et al. ",
                    "Citation Text": "Robin Strudel, Corentin Tallec, Florent Altch\u00e9, Yilun Du, Yaroslav Ganin, Arthur Mensch,\nWill Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, and R\u00e9mi Leblond. Self-\nconditioned embedding diffusion for text generation, 2022. URL https://arxiv.org/abs/\n2211.04236 .\n14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.04236",
                        "Citation Paper Title": "Title:Self-conditioned Embedding Diffusion for Text Generation",
                        "Citation Paper Abstract": "Abstract:Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.",
                        "Citation Paper Authors": "Authors:Robin Strudel, Corentin Tallec, Florent Altch\u00e9, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, R\u00e9mi Leblond"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "introduced latent diffusion for image synthesis and demonstrated that they can be learned\nin the latent space of a pretrained autoencoder. Latent diffusion has since been successful in other\ndomains such as audio synthesis ",
                    "Citation Text": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. In International Conference on Learning Representations ,\n2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09761",
                        "Citation Paper Title": "Title:DiffWave: A Versatile Diffusion Model for Audio Synthesis",
                        "Citation Paper Abstract": "Abstract:In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.",
                        "Citation Paper Authors": "Authors:Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.03897v4": {
            "Paper Title": "Geodesic Multi-Modal Mixup for Robust Fine-Tuning",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": "adopt a two-stage learning scheme that combines linear probing and fine-tuning,\nand Goyal et al. ",
                    "Citation Text": "Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you\npretrain: Improved finetuning of zero-shot vision models. 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.00638",
                        "Citation Paper Title": "Title:Finetune like you pretrain: Improved finetuning of zero-shot vision models",
                        "Citation Paper Abstract": "Abstract:Finetuning image-text models such as CLIP achieves state-of-the-art accuracies on a variety of benchmarks. However, recent works like WiseFT (Wortsman et al., 2021) and LP-FT (Kumar et al., 2022) have shown that even subtle differences in the finetuning process can lead to surprisingly large differences in the final performance, both for in-distribution (ID) and out-of-distribution (OOD) data. In this work, we show that a natural and simple approach of mimicking contrastive pretraining consistently outperforms alternative finetuning approaches. Specifically, we cast downstream class labels as text prompts and continue optimizing the contrastive loss between image embeddings and class-descriptive prompt embeddings (contrastive finetuning).\nOur method consistently outperforms baselines across 7 distribution shifts, 6 transfer learning, and 3 few-shot learning benchmarks. On WILDS-iWILDCam, our proposed approach FLYP outperforms the top of the leaderboard by $2.3\\%$ ID and $2.7\\%$ OOD, giving the highest reported accuracy. Averaged across 7 OOD datasets (2 WILDS and 5 ImageNet associated shifts), FLYP gives gains of $4.2\\%$ OOD over standard finetuning and outperforms the current state of the art (LP-FT) by more than $1\\%$ both ID and OOD. Similarly, on 3 few-shot learning benchmarks, our approach gives gains up to $4.6\\%$ over standard finetuning and $4.4\\%$ over the state of the art. In total, these benchmarks establish contrastive finetuning as a simple, intuitive, and state-of-the-art approach for supervised finetuning of image-text models like CLIP. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, Aditi Raghunathan"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "interpolate the images with ratio \u03bb, and adopt contrastive loss with pseudo labels according\nto the mixing ratio \u03bb. However, there are few works on Mixup for multi-modal learning [ 52,53].\nSTEMM ",
                    "Citation Text": "Qingkai Fang, Rong Ye, Lei Li, Yang Feng, and Mingxuan Wang. Stemm: Self-learning with speech-text\nmanifold mixup for speech translation. arXiv preprint arXiv:2203.10426 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.10426",
                        "Citation Paper Title": "Title:STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation",
                        "Citation Paper Abstract": "Abstract:How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy. Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework. Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.",
                        "Citation Paper Authors": "Authors:Qingkai Fang, Rong Ye, Lei Li, Yang Feng, Mingxuan Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.00638v3": {
            "Paper Title": "Composable Text Controls in Latent Space with ODEs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09577v3": {
            "Paper Title": "CiteBench: A benchmark for Scientific Citation Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11081v2": {
            "Paper Title": "A Theory of Unsupervised Translation Motivated by Understanding Animal\n  Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07596v2": {
            "Paper Title": "Towards Abstractive Timeline Summarisation using Preference-based\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09146v3": {
            "Paper Title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between\n  the Retriever and the Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.00763v5": {
            "Paper Title": "Metaphorical User Simulators for Evaluating Task-oriented Dialogue\n  Systems",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": ", and popular human evaluation metrics include Satisfaction, Fluency, Coherence,\nTask success, Engagingness, etc. However, human evaluation is intrusive, time-intensive, and does\nnot scale ",
                    "Citation Text": "Eric Michael Smith, Orion Hsu, Rebecca Qian, Stephen Roller, Y-Lan Boureau, and Jason Weston. 2022. Human\nEvaluation of Conversations is an Open Problem: Comparing the Sensitivity of Various Methods for Evaluating\nDialogue Agents. ArXiv abs/2201.04723 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.04723",
                        "Citation Paper Title": "Title:Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents",
                        "Citation Paper Abstract": "Abstract:At the heart of improving conversational AI is the open problem of how to evaluate conversations. Issues with automatic metrics are well known (Liu et al., 2016, arXiv:1603.08023), with human evaluations still considered the gold standard. Unfortunately, how to perform human evaluations is also an open problem: differing data collection methods have varying levels of human agreement and statistical sensitivity, resulting in differing amounts of human annotation hours and labor costs. In this work we compare five different crowdworker-based human evaluation methods and find that different methods are best depending on the types of models compared, with no clear winner across the board. While this highlights the open problems in the area, our analysis leads to advice of when to use which one, and possible future directions.",
                        "Citation Paper Authors": "Authors:Eric Michael Smith, Orion Hsu, Rebecca Qian, Stephen Roller, Y-Lan Boureau, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ", a class of methods that assist a series of knowledge-intensive\nlanguage tasks (e.g., question answering, fact checking ",
                    "Citation Text": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\nJernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. 2021. KILT: A Benchmark for Knowledge Intensive\nLanguage Tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL \u201921) . 2523\u20132544.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.02252",
                        "Citation Paper Title": "Title:KILT: a Benchmark for Knowledge Intensive Language Tasks",
                        "Citation Paper Abstract": "Abstract:Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "introduce a reinforcement learning\napproach based on end-to-end modeling.\nExisting simulators mechanically inform the system of the slot, which limits the realism and the\nevaluation capability ",
                    "Citation Text": "Weiwei Sun, Shuo Zhang, Krisztian Balog, Zhaochun Ren, Pengjie Ren, Zhumin Chen, and Maarten de Rijke. 2021. Sim-\nulating User Satisfaction for the Evaluation of Task-oriented Dialogue Systems. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201921) . 2499\u20132506.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.03748",
                        "Citation Paper Title": "Title:Simulating User Satisfaction for the Evaluation of Task-oriented Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:Evaluation is crucial in the development process of task-oriented dialogue systems. As an evaluation method, user simulation allows us to tackle issues such as scalability and cost-efficiency, making it a viable choice for large-scale automatic evaluation. To help build a human-like user simulator that can measure the quality of a dialogue, we propose the following task: simulating user satisfaction for the evaluation of task-oriented dialogue systems. The purpose of the task is to increase the evaluation power of user simulations and to make the simulation more human-like. To overcome a lack of annotated data, we propose a user satisfaction annotation dataset, USS, that includes 6,800 dialogues sampled from multiple domains, spanning real-world e-commerce dialogues, task-oriented dialogues constructed through Wizard-of-Oz experiments, and movie recommendation dialogues. All user utterances in those dialogues, as well as the dialogues themselves, have been labeled based on a 5-level satisfaction scale. We also share three baseline methods for user satisfaction prediction and action prediction tasks. Experiments conducted on the USS dataset suggest that distributed representations outperform feature-based methods. A model based on hierarchical GRUs achieves the best performance in in-domain user satisfaction prediction, while a BERT-based model has better cross-domain generalization ability.",
                        "Citation Paper Authors": "Authors:Weiwei Sun, Shuo Zhang, Krisztian Balog, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "investigate the design of user simulators\nas an reinforcement learning environment. Tseng et al . ",
                    "Citation Text": "Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, and Bill Byrne. 2021. Transferable Dialogue Systems and User Simulators.\nArXiv abs/2107.11904 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.11904",
                        "Citation Paper Title": "Title:Transferable Dialogue Systems and User Simulators",
                        "Citation Paper Abstract": "Abstract:One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets.",
                        "Citation Paper Authors": "Authors:Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, Bill Byrne"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "propose a module-based framework for conversational information seeking.\n2.2 Evaluation of dialogue systems\nThe typical evaluation methods of dialogue systems include automatic evaluation and human\nevaluation ",
                    "Citation Text": "Jan Deriu, \u00c1lvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, and Mark Cieliebak.\n2020. Survey on Evaluation Methods for Dialogue Systems. Artificial Intelligence Review 54 (2020), 755\u2013810.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04071",
                        "Citation Paper Title": "Title:Survey on Evaluation Methods for Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:In this paper we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost and time intensive. Thus, much work has been put into finding methods, which allow to reduce the involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented dialogue systems, conversational dialogue systems, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then by presenting the evaluation methods regarding this class.",
                        "Citation Paper Authors": "Authors:Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, Mark Cieliebak"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "2.1 Task-oriented dialogue systems\nTask-oriented dialogue systems (TDSs) aim to assist users in completing tasks through conver-\nsations ",
                    "Citation Text": "Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Anuj Kumar Goyal, Peter Ku, Sanchit Agarwal,\nand Shuyang Gao. 2020. MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections\nand State Tracking Baselines. In Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC \u201920) .\n422\u2013428.\nACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: November 2023.Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems 1:27",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.01669",
                        "Citation Paper Title": "Title:MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines",
                        "Citation Paper Abstract": "Abstract:MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32% of state annotations across 40% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.",
                        "Citation Paper Authors": "Authors:Mihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, Dilek Hakkani-Tur"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ",\nACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: November 2023.1:4 Sun et al.\nand conversational question answering ",
                    "Citation Text": "Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A Conversational Question Answering Challenge.\nTransactions of the Association for Computational Linguistics 7 (2019), 249\u2013266.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.07042",
                        "Citation Paper Title": "Title:CoQA: A Conversational Question Answering Challenge",
                        "Citation Paper Abstract": "Abstract:Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at this http URL",
                        "Citation Paper Authors": "Authors:Siva Reddy, Danqi Chen, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". TDSs are developed either via module-based or end-to-end approaches, differing in\nwhether they manage the sub-steps of dialogue generation with multiple independent modules or\nnot. ",
                    "Citation Text": "Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen. 2021. A Survey on Conversational Recommender\nSystems. ACM Computing Surveys (CSUR) 54 (2021), 1\u201336.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00646",
                        "Citation Paper Title": "Title:A Survey on Conversational Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender systems are software applications that help users to find items of interest in situations of information overload. Current research often assumes a one-shot interaction paradigm, where the users' preferences are estimated based on past observed behavior and where the presentation of a ranked list of suggestions is the main, one-directional form of user interaction. Conversational recommender systems (CRS) take a different approach and support a richer set of interactions. These interactions can, for example, help to improve the preference elicitation process or allow the user to ask questions about the recommendations and to give feedback. The interest in CRS has significantly increased in the past few years. This development is mainly due to the significant progress in the area of natural language processing, the emergence of new voice-controlled home assistants, and the increased use of chatbot technology. With this paper, we provide a detailed survey of existing approaches to conversational recommendation. We categorize these approaches in various dimensions, e.g., in terms of the supported user intents or the knowledge they use in the background. Moreover, we discuss technological approaches, review how CRS are evaluated, and finally identify a number of gaps that deserve more research in the future.",
                        "Citation Paper Authors": "Authors:Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, Li Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09663v3": {
            "Paper Title": "Norm of Word Embedding Encodes Information Gain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06229v2": {
            "Paper Title": "Improving word mover's distance by leveraging self-attention matrix",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09171v2": {
            "Paper Title": "Rainproof: An Umbrella To Shield Text Generators From\n  Out-Of-Distribution Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09699v3": {
            "Paper Title": "SegAugment: Maximizing the Utility of Speech Translation Data with\n  Segmentation-based Augmentations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10564v2": {
            "Paper Title": "A Vision-free Baseline for Multimodal Grammar Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10764v3": {
            "Paper Title": "Learning List-Level Domain-Invariant Representations for Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10101v3": {
            "Paper Title": "BERT WEAVER: Using WEight AVERaging to enable lifelong learning for\n  transformer-based models in biomedical semantic search engines",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ", which can be a hindrance for the integration into\nrunning services.\n4.3 Implementation Details\nFor all conducted experiments, we build our code upon the Transformers library ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M.\nRush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations , pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". A well known example from the transformer domain is called AdapterFusion ",
                    "Citation Text": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion:\nNon-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume , pages 487\u2013503, Online, April 2021.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00247",
                        "Citation Paper Title": "Title:AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
                        "Citation Paper Abstract": "Abstract:Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, Iryna Gurevych"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Based on EWC, Liu et al. proposed an extension that makes use of a network\nreparameterization that basically rotates the parameter space ",
                    "Citation Text": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M. L\u00f3pez, and Andrew D. Bagdanov.\nRotate your networks: Better weight consolidation and less catastrophic forgetting. In 2018 24th International\nConference on Pattern Recognition (ICPR) , pages 2262\u20132268, 2018. ISSN: 1051-4651.\n12K\u00fchnel et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.02950",
                        "Citation Paper Title": "Title:Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting",
                        "Citation Paper Abstract": "Abstract:In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to other state-of-the-art in lifelong learning without forgetting.",
                        "Citation Paper Authors": "Authors:Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M. Lopez, Andrew D. Bagdanov"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". It is\na regularization-based technique that basically quantifies the importance of weights and thereby impedes important\nweights from being changed drastically. It has been successfully applied for an online personalization of speech\nrecognition systems, as an example ",
                    "Citation Text": "Khe Chai Sim, Fran\u00e7oise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas Kabel, Nikhil Khare, Tamar Lucassen,\nPetr Zadrazil, Harry Zhang, Leif Johnson, Giovanni Motta, and Lillian Zhou. Personalization of end-to-end speech\nrecognition on mobile devices for named entities. In 2019 IEEE Automatic Speech Recognition and Understanding\nWorkshop (ASRU) , pages 23\u201330, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.09251",
                        "Citation Paper Title": "Title:Personalization of End-to-end Speech Recognition On Mobile Devices For Named Entities",
                        "Citation Paper Abstract": "Abstract:We study the effectiveness of several techniques to personalize end-to-end speech models and improve the recognition of proper names relevant to the user. These techniques differ in the amounts of user effort required to provide supervision, and are evaluated on how they impact speech recognition performance. We propose using keyword-dependent precision and recall metrics to measure vocabulary acquisition performance. We evaluate the algorithms on a dataset that we designed to contain names of persons that are difficult to recognize. Therefore, the baseline recall rate for proper names in this dataset is very low: 2.4%. A data synthesis approach we developed brings it to 48.6%, with no need for speech input from the user. With speech input, if the user corrects only the names, the name recall rate improves to 64.4%. If the user corrects all the recognition errors, we achieve the best recall of 73.5%. To eliminate the need to upload user data and store personalized models on a server, we focus on performing the entire personalization workflow on a mobile device.",
                        "Citation Paper Authors": "Authors:Khe Chai Sim, Fran\u00e7oise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif Johnson, Giovanni Motta, Lillian Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.09593v2": {
            "Paper Title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation\n  Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07264v2": {
            "Paper Title": "Born for Auto-Tagging: Faster and better with new objective functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11087v3": {
            "Paper Title": "Conceptor-Aided Debiasing of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.12770v4": {
            "Paper Title": "Exploring the Value of Pre-trained Language Models for Clinical Named\n  Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.17122v2": {
            "Paper Title": "Mining Word Boundaries in Speech as Naturally Annotated Word\n  Segmentation Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.07051v3": {
            "Paper Title": "Language models show human-like content effects on reasoning tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16487v6": {
            "Paper Title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating\n  Seq2seq Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13005v2": {
            "Paper Title": "Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "proposed EL on emerging\nentities, but the dataset is English-only. In this work, we present\nthe first non-English zero-shot EL dataset on emerging entities.\nFew-Shot Entity Linking was frequently studied in recent\nyears. Provatorova et al . ",
                    "Citation Text": "Vera Provatorova, Samarth Bhargav, Svitlana Vakulenko, and Evangelos Kanoulas.\n2021. Robustness Evaluation of Entity Disambiguation Using Prior Probes: the\nCase of Entity Overshadowing. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 10501\u201310510.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10949",
                        "Citation Paper Title": "Title:Robustness Evaluation of Entity Disambiguation Using Prior Probes:the Case of Entity Overshadowing",
                        "Citation Paper Abstract": "Abstract:Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was previously shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between more and less common entities for all of the EL systems under evaluation, demonstrating the effects of prior probability bias and entity overshadowing.",
                        "Citation Paper Authors": "Authors:Vera Provatorova, Svitlana Vakulenko, Samarth Bhargav, Evangelos Kanoulas"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "suggested that high accuracy on pre-\nvious EL datasets can be obtained by merely learning the prior,\nand released ShadowLink test set whose \u201cShadow\u201d subset is sim-\nilar to our few-shot setting, but only available in English. Chen\net al. ",
                    "Citation Text": "Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, and Sameer Singh.\n2021. Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-\nBased NLP. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers) . Association for Computational\nLinguistics, Online, 4472\u20134485. https://doi.org/10.18653/v1/2021.acl-long.345",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.06830",
                        "Citation Paper Title": "Title:Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP",
                        "Citation Paper Abstract": "Abstract:Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.",
                        "Citation Paper Authors": "Authors:Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer Singh"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "For years, the primary focus of entity linking studies were con-\nstrained to English-only and fixed-KB [ 6,10,21,22]. Cross-lingual\nentity linking was introduced to link non-English mentions to Eng-\nlish KBs [ 17,24]. Recently, Botha et al . ",
                    "Citation Text": "Jan A. Botha, Zifei Shan, and Daniel Gillick. 2020. Entity Linking in 100 Languages.\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) . Association for Computational Linguistics, Online, 7833\u2013\n7845. https://doi.org/10.18653/v1/2020.emnlp-main.630",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.02690",
                        "Citation Paper Title": "Title:Entity Linking in 100 Languages",
                        "Citation Paper Abstract": "Abstract:We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset (http://goo.gle/mewsli-dataset) matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.",
                        "Citation Paper Authors": "Authors:Jan A. Botha, Zifei Shan, Daniel Gillick"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "(Section 5.2). Next, we set baselines on Hansel with our models and\nmGENRE (Section 5.3), showing the huge performance difference\nbetween TAC-KBP2015 and Hansel (discussed in Section 6.3).\n5.1 Experiment Details\nDE, TyDE and CA models are implemented with Tensorflow ",
                    "Citation Text": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,\nBenoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan\nYu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Ma-\nchine Learning. In 12th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 16) . USENIX Association, Savannah, GA, 265\u2013283. https:\n//www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09114v2": {
            "Paper Title": "CAPSTONE: Curriculum Sampling for Dense Retrieval with Document\n  Expansion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00269v5": {
            "Paper Title": "A Survey of Knowledge Enhanced Pre-trained Models",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". Clark et al. explore the functions of self-\nattention heads and report that they attend to words significantly in certain syntactic positions ",
                    "Citation Text": "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look at? An\nAnalysis of BERT\u2019s Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP . 276\u2013286.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04341",
                        "Citation Paper Title": "Title:What Does BERT Look At? An Analysis of BERT's Attention",
                        "Citation Paper Abstract": "Abstract:Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". Researchers have found\nthat token representations of PLMs can capture syntactic and semantic knowledge by probing\nclassifiers [ 32,36]. The quantitative analysis in question answering tasks demonstrates that PLMs\ncan encode structured commonsense knowledge ",
                    "Citation Text": "Leyang Cui, Sijie Cheng, Yu Wu, and Yue Zhang. 2021. On Commonsense Cues in BERT for Solving Commonsense\nTasks. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . 683\u2013693.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.03945",
                        "Citation Paper Title": "Title:On Commonsense Cues in BERT for Solving Commonsense Tasks",
                        "Citation Paper Abstract": "Abstract:BERT has been used for solving commonsense tasks such as CommonsenseQA. While prior research has found that BERT does contain commonsense information to some extent, there has been work showing that pre-trained models can rely on spurious associations (e.g., data bias) rather than key cues in solving sentiment classification and other problems. We quantitatively investigate the presence of structural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy.",
                        "Citation Paper Authors": "Authors:Leyang Cui, Sijie Cheng, Yu Wu, Yue Zhang"
                    }
                },
                {
                    "Sentence ID": 101,
                    "Sentence": ". Vashishth et al. believe that the combination of relations and nodes should be considered\ncomprehensively during the message transmission ",
                    "Citation Text": "Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. 2019. Composition-based Multi-Relational\nGraph Convolutional Networks. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.03082",
                        "Citation Paper Title": "Title:Composition-based Multi-Relational Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research.",
                        "Citation Paper Authors": "Authors:Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Partha Talukdar"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "and keeps the\ntranslational property of TransE. Following the same framework of SACN, Nathani et al. propose\nan attention-based feature embedding that captures both entity and relation features in the encoder ",
                    "Citation Text": "Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learning Attention-based Embeddings\nfor Relation Prediction in Knowledge Graphs. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . 4710\u20134723.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01195",
                        "Citation Paper Title": "Title:Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs",
                        "Citation Paper Abstract": "Abstract:The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention based feature embedding that captures both entity and relation features in any given entity's neighborhood. Additionally, we also encapsulate relation clusters and multihop relations in our model. Our empirical study offers insights into the efficacy of our attention based model and we show marked performance gains in comparison to state of the art methods on all datasets.",
                        "Citation Paper Authors": "Authors:Deepak Nathani, Jatin Chauhan, Charu Sharma, Manohar Kaul"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": "proposes\na new autoregressive method based on permutation language modeling to capture contextual\ninformation without introducing any new symbols.\nUnlike all these above pre-trained language models that aim at natural understanding or gen-\neration tasks, T5 ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of\nMachine Learning Research 21 (2020), 1\u201367.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ", BERT also employs the\nnext sentence prediction (NSP) task to learn the semantic connection between sentences, which\nobtains new start-of-art results on eleven NLP tasks and even becomes the basis of subsequent\nmodels. Based on BERT, RoBERTa ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "adopts a masked language modelling (MLM) objective\nwhere some of the tokens of a sequence are masked randomly, and the goal is to predict these\ntokens considering the corrupted sentence. Inspired by Skip-Thoughts ",
                    "Citation Text": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler.\n2015. Skip-thought vectors. In Proceedings of the 28th International Conference on Neural Information Processing\nSystems-Volume 2 . 3294\u20133302.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.06726",
                        "Citation Paper Title": "Title:Skip-Thought Vectors",
                        "Citation Paper Abstract": "Abstract:We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",
                        "Citation Paper Authors": "Authors:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "as a neural encoder, the ELMo extracts context-dependent representations\nfrom a bidirectional language model, which has shown to bring large improvement on a range of\nNLP tasks ",
                    "Citation Text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . 2227\u20132237.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.111:30 Yang et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "computes word-word cooccurrence statistics from a large corpus as a supervised\nsignal, and FastText ",
                    "Citation Text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword\ninformation. Transactions of the Association for Computational Linguistics 5 (2017), 135\u2013146.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.07550v3": {
            "Paper Title": "Evaluating and Inducing Personality in Pre-trained Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04074v3": {
            "Paper Title": "Are All Steps Equally Important? Benchmarking Essentiality Detection of\n  Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09944v2": {
            "Paper Title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". However,\nMelHuBERT outperforms all of them on the SUPERB bench-\nmark, despite only pre-trained on a third of LibriSpeech.4\nBEST-RQ ",
                    "Citation Text": "Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu,\nand Yonghui Wu, \u201cSelf-supervised learning with\nrandom-projection quantizer for speech recognition,\u201d\nICML , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.01855",
                        "Citation Paper Title": "Title:Self-supervised Learning with Random-projection Quantizer for Speech Recognition",
                        "Citation Paper Abstract": "Abstract:We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",
                        "Citation Paper Authors": "Authors:Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, Yonghui Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10015v3": {
            "Paper Title": "Benchmarking Spatial Relationships in Text-to-Image Generation",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ") which\nare used by generating a caption cfor the synthesized image\nx=g(t)and computing the captioning score with respect\nto the reference input text t. Note that purely visual metrics\n(FID and Inception Score ",
                    "Citation Text": "T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford,\nand X. Chen, \u201cImproved techniques for training gans,\u201d Advances\nin neural information processing systems , vol. 29, 2016. 1, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nThe difficulty of spatial understanding has been studied for\nvisual grounding ",
                    "Citation Text": "R. Liu, C. Liu, Y. Bai, and A. L. Yuille, \u201cClevr-ref+: Diagnosing\nvisual reasoning with referring expressions,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019 . Computer Vision\nFoundation / IEEE, 2019, pp. 4185\u20134194. [Online]. Available:\nhttp://openaccess.thecvf.com/content CVPR 2019/html/Liu\nCLEVR-Ref Diagnosing Visual Reasoning With Referring\nExpressions CVPR 2019 paper.html 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00850",
                        "Citation Paper Title": "Title:CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions",
                        "Citation Paper Abstract": "Abstract:Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.\nIn addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended.",
                        "Citation Paper Authors": "Authors:Runtao Liu, Chenxi Liu, Yutong Bai, Alan Yuille"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09674v2": {
            "Paper Title": "LR-Sum: Summarization for Less-Resourced Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.03018v2": {
            "Paper Title": "Aksharantar: Open Indic-language Transliteration datasets and models for\n  the Next Billion Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09462v2": {
            "Paper Title": "pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10526v3": {
            "Paper Title": "Open Domain Multi-document Summarization: A Comprehensive Study of Model\n  Brittleness under Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.10385v2": {
            "Paper Title": "GPT Understands, Too",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.02414v2": {
            "Paper Title": "GLM-130B: An Open Bilingual Pre-trained Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10755v2": {
            "Paper Title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09912v2": {
            "Paper Title": "Tokenization Consistency Matters for Generative Models on Extractive NLP\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09825v2": {
            "Paper Title": "What to Read in a Contract? Party-Specific Summarization of Legal\n  Obligations, Entitlements, and Prohibitions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10548v2": {
            "Paper Title": "T-Projection: High Quality Annotation Projection for Sequence Labeling\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05401v3": {
            "Paper Title": "MaXM: Towards Multilingual Visual Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10739v2": {
            "Paper Title": "JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and\n  Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "created a\njob transition graph using Random Walk-based vectors, and\nindicated the potential of job graph embedding. Zhang et al. ",
                    "Citation Text": "D. Zhang, J. Liu, H. Zhu, Y . Liu, L. Wang, P. Wang, and H. Xiong,\n\u201cJob2vec: Job title benchmarking with collective multi-view represen-\ntation learning,\u201d in Proceedings of the ACM International Conference\non Information and Knowledge Management (CIKM) , 2019, pp. 2763\u2013\n2771.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07429",
                        "Citation Paper Title": "Title:Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning",
                        "Citation Paper Abstract": "Abstract:Job Title Benchmarking (JTB) aims at matching job titles with similar expertise levels across various companies. JTB could provide precise guidance and considerable convenience for both talent recruitment and job seekers for position and salary calibration/prediction. Traditional JTB approaches mainly rely on manual market surveys, which is expensive and labor-intensive. Recently, the rapid development of Online Professional Graph has accumulated a large number of talent career records, which provides a promising trend for data-driven solutions. However, it is still a challenging task since (1) the job title and job transition (job-hopping) data is messy which contains a lot of subjective and non-standard naming conventions for the same position (e.g., Programmer, Software Development Engineer, SDE, Implementation Engineer), (2) there is a large amount of missing title/transition information, and (3) one talent only seeks limited numbers of jobs which brings the incompleteness and randomness modeling job transition patterns. To overcome these challenges, we aggregate all the records to construct a large-scale Job Title Benchmarking Graph (Job-Graph), where nodes denote job titles affiliated with specific companies and links denote the correlations between jobs. We reformulate the JTB as the task of link prediction over the Job-Graph that matched job titles should have links. Along this line, we propose a collective multi-view representation learning method (Job2Vec) by examining the Job-Graph jointly in (1) graph topology view, (2)semantic view, (3) job transition balance view, and (4) job transition duration view. We fuse the multi-view representations in the encode-decode paradigm to obtain a unified optimal representation for the task of link prediction. Finally, we conduct extensive experiments to validate the effectiveness of our proposed method.",
                        "Citation Paper Authors": "Authors:Denghui Zhang, Junming Liu, Hengshu Zhu, Yanchi Liu, Lichen Wang, Pengyang Wang, Hui Xiong"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "developed \u201cJob2Skills\u201d, a market-aware skill extraction\nsystem, which considers the salient level of a skill and extracts\nimportant skill entities from job postings and target members\nusing multi-resolution. Qin et al. ",
                    "Citation Text": "C. Qin, H. Zhu, T. Xu, C. Zhu, L. Jiang, E. Chen, and H. Xiong,\n\u201cEnhancing person-job fit for talent recruitment: An ability-aware neural\nnetwork approach,\u201d in Proceedings of the International ACM SIGIR\nConference on Research and Development in Information Retrieval\n(SIGIR) , 2018, pp. 25\u201334.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08947",
                        "Citation Paper Title": "Title:Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural Network Approach",
                        "Citation Paper Abstract": "Abstract:The wide spread use of online recruitment services has led to information explosion in the job market. As a result, the recruiters have to seek the intelligent ways for Person Job Fit, which is the bridge for adapting the right job seekers to the right positions. Existing studies on Person Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement. To this end, in this paper, we propose a novel end to end Ability aware Person Job Fit Neural Network model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results. The key idea is to exploit the rich information available at abundant historical job application data. Specifically, we propose a word level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network. Along this line, four hierarchical ability aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement. Finally, extensive experiments on a large scale real world data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baselines.",
                        "Citation Paper Authors": "Authors:Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, Hui Xiong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10465v3": {
            "Paper Title": "SODA: Million-scale Dialogue Distillation with Social Commonsense\n  Contextualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.11680v2": {
            "Paper Title": "Smooth Sailing: Improving Active Learning for Pre-trained Language\n  Models with Representation Smoothness Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00482v2": {
            "Paper Title": "IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn\n  Response Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.12422v3": {
            "Paper Title": "Non-Programmers Can Label Programs Indirectly via Active Examples: A\n  Case Study with Text-to-SQL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07440v2": {
            "Paper Title": "InterFair: Debiasing with Natural Language Feedback for Fair\n  Interpretable Predictions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05895v2": {
            "Paper Title": "Understanding ME? Multimodal Evaluation for Fine-grained Visual\n  Commonsense",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09935v2": {
            "Paper Title": "CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "that extract plans from LLMs\nusing prompting strategies assume access to extra informa-\ntion such as: 1) predefined skills with preconditions ",
                    "Citation Text": "M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes,\nB. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter,\nA. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth,\nN. Joshi, R. Julian, D. Kalashnikov, Y . Kuang, K.-H.\nLee, S. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor,\nJ. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Ser-\nmanet, N. Sievers, C. Tan, A. Toshev, V . Vanhoucke,\nF. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng,\n\u201cDo As I Can, Not As I Say: Grounding Language\nin Robotic Affordances,\u201d in Proceedings of The 6th\nConference on Robot Learning , ser. Proceedings of\nMachine Learning Research, K. Liu, D. Kulic, and\nJ. Ichnowski, Eds., vol. 205. PMLR, 14\u201318 Dec 2022,\npp. 287\u2013318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.01691",
                        "Citation Paper Title": "Title:Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                        "Citation Paper Abstract": "Abstract:Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", whereas PIandPEcan work with task and\nmotion planning approaches ",
                    "Citation Text": "C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Sil-\nver, L. P. Kaelbling, and T. Lozano-P\u00e9rez, \u201cIntegrated\ntask and motion planning,\u201d Annual Review of Control,\nRobotics, and Autonomous Systems , vol. 4, pp. 265\u2013\n293, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01083",
                        "Citation Paper Title": "Title:Integrated Task and Motion Planning",
                        "Citation Paper Abstract": "Abstract:The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.",
                        "Citation Paper Authors": "Authors:Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11093v3": {
            "Paper Title": "VER: Unifying Verbalizing Entities and Relations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.12588v4": {
            "Paper Title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15500v2": {
            "Paper Title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in\n  Explainable Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09873v2": {
            "Paper Title": "A Comparative Study on Textual Saliency of Styles from Eye Tracking,\n  Annotations, and Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14531v3": {
            "Paper Title": "Unifying Data Perspectivism and Personalization: An Application to\n  Social Norms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15631v3": {
            "Paper Title": "Exploring Effective Distillation of Self-Supervised Speech Models for\n  Automatic Speech Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.05261v3": {
            "Paper Title": "Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair\n  Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10901v3": {
            "Paper Title": "ALCAP: Alignment-Augmented Music Captioner",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02851v2": {
            "Paper Title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context\n  Tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08238v3": {
            "Paper Title": "Exploiting Contrastive Learning and Numerical Evidence for Confusing\n  Legal Judgment Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.00648v2": {
            "Paper Title": "PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme\n  price movement prediction of Bitcoin",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09702v3": {
            "Paper Title": "On Event Individuation for Document-Level Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05917v3": {
            "Paper Title": "Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08374v2": {
            "Paper Title": "A Computational Interface to Translate Strategic Intent from\n  Unstructured Language in a Low-Data Setting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16031v3": {
            "Paper Title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10341v2": {
            "Paper Title": "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data\n  Limitation With Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.05185v3": {
            "Paper Title": "Uniform Complexity for Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16773v5": {
            "Paper Title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog\n  with Reinforced Keywords Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06333v2": {
            "Paper Title": "Connecting degree and polarity: An artificial language learning study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.07025v2": {
            "Paper Title": "Learning to translate by learning to communicate",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13623v3": {
            "Paper Title": "Reinforcement Learning and Bandits for Speech and Language Processing:\n  Tutorial, Review and Outlook",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09561v5": {
            "Paper Title": "Large Language Models are Better Reasoners with Self-Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03251v3": {
            "Paper Title": "Towards Automatic Construction of Filipino WordNet: Word Sense Induction\n  and Synset Induction Using Sentence Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02147v3": {
            "Paper Title": "Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for\n  Text-to-Speech",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "based on random windows of different lengths, which\nhas been proved to improve the naturalness of word pronunciations ",
                    "Citation Text": "Zhenhui Ye, Zhou Zhao, Yi Ren, and Fei Wu. SyntaSpeech: Syntax-Aware Generative Adver-\nsarial Text-to-Speech. arXiv e-prints , page arXiv:2204.11792, April 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.11792",
                        "Citation Paper Title": "Title:SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech",
                        "Citation Paper Abstract": "Abstract:The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made fast and high-quality speech synthesis possible. However, current NAR-TTS models usually use phoneme sequence as input and thus cannot understand the tree-structured syntactic information of the input sequence, which hurts the prosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and light-weight NAR-TTS model, which integrates tree-structured syntactic information into the prosody modeling modules in PortaSpeech \\cite{ren2021portaspeech}. Specifically, 1) We build a syntactic graph based on the dependency tree of the input sentence, then process the text encoding with a syntactic graph encoder to extract the syntactic information. 2) We incorporate the extracted syntactic encoding with PortaSpeech to improve the prosody prediction. 3) We introduce a multi-length discriminator to replace the flow-based post-net in PortaSpeech, which simplifies the training pipeline and improves the inference speed, while keeping the naturalness of the generated audio. Experiments on three datasets not only show that the tree-structured syntactic information grants SyntaSpeech the ability to synthesize better audio with expressive prosody, but also demonstrate the generalization ability of SyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech. Ablation studies demonstrate the necessity of each component in SyntaSpeech. Source code and audio samples are available at this https URL",
                        "Citation Paper Authors": "Authors:Zhenhui Ye, Zhou Zhao, Yi Ren, Fei Wu"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". In order to comprehend the semantic meaning in the\ngiven sentence for polyphone disambiguation, previous methods [ 11,57,49,18,8] have adopted\nthe pre-trained language model ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10784v3": {
            "Paper Title": "Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical\n  Relation Extraction?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14082v3": {
            "Paper Title": "Bhasacitra: Visualising the dialect geography of South Asia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06348v4": {
            "Paper Title": "Can Brain Signals Reveal Inner Alignment with Human Languages?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.14276v3": {
            "Paper Title": "Example-based Hypernetworks for Out-of-Distribution Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09730v2": {
            "Paper Title": "Speaking Style Conversion in the Waveform Domain Using Discrete\n  Self-Supervised Units",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13877v4": {
            "Paper Title": "YATO: Yet Another deep learning based Text analysis Open toolkit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15237v2": {
            "Paper Title": "Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained\n  Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03350v3": {
            "Paper Title": "Measuring and Narrowing the Compositionality Gap in Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.17148v2": {
            "Paper Title": "ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data\n  Format",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05525v3": {
            "Paper Title": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04755v3": {
            "Paper Title": "From Cloze to Comprehension: Retrofitting Pre-trained Masked Language\n  Model to Pre-trained Machine Reader",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03029v4": {
            "Paper Title": "Efficiently Enhancing Zero-Shot Performance of Instruction Following\n  Model via Retrieval of Soft Prompt",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09968v2": {
            "Paper Title": "On Improving Summarization Factual Consistency from Natural Language\n  Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03536v2": {
            "Paper Title": "Knowledge Graph Embedding: A Survey from the Perspective of\n  Representation Spaces",
            "Sentences": [
                {
                    "Sentence ID": 158,
                    "Sentence": "proposes to learn embeddings in Euclidean space to\nmodel the transformation of entities, while RotatE ",
                    "Citation Text": "Sun, Z., Deng, Z.-H., Nie, J.-Y., and Tang, J. Rotate: Knowledge graph embedding by relational rotation in complex\nspace. arXiv preprint arXiv:1902.10197 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10197",
                        "Citation Paper Title": "Title:RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
                        "Citation Paper Abstract": "Abstract:We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
                        "Citation Paper Authors": "Authors:Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 162,
                    "Sentence": "extends complex space to quaternion where there\nare two planes of rotations. Inspired by RotatE, Tang et al. ",
                    "Citation Text": "Tang, Y., Huang, J., Wang, G., He, X., and Zhou, B. Orthogonal relation transforms with graph context modeling for\nknowledge graph embedding. In Annual Meeting of the Association for Computational Linguistics (2020), pp. 2713\u20132722.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04910",
                        "Citation Paper Title": "Title:Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding",
                        "Citation Paper Abstract": "Abstract:Translational distance-based knowledge graph embedding has shown progressive improvements on the link prediction task, from TransE to the latest state-of-the-art RotatE. However, N-1, 1-N and N-N predictions still remain challenging. In this work, we propose a novel translational distance-based approach for knowledge graph link prediction. The proposed method includes two-folds, first we extend the RotatE from 2D complex domain to high dimension space with orthogonal transforms to model relations for better modeling capacity. Second, the graph context is explicitly modeled via two directed context representations. These context representations are used as part of the distance scoring function to measure the plausibility of the triples during training and inference. The proposed approach effectively improves prediction accuracy on the difficult N-1, 1-N and N-N cases for knowledge graph link prediction task. The experimental results show that it achieves better performance on two benchmark data sets compared to the baseline RotatE, especially on data set (FB15k-237) with many high in-degree connection nodes.",
                        "Citation Paper Authors": "Authors:Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, Bowen Zhou"
                    }
                },
                {
                    "Sentence ID": 227,
                    "Sentence": ", each r\ud835\udc56is regarded as a\nscaling transformation between two moduli; Cross interaction operations are also applied\nby utilising Hadamard product in CrossE ",
                    "Citation Text": "Zhang, W., Paudel, B., Zhang, W., Bernstein, A., and Chen, H. Interaction embeddings for prediction and\nexplanation in knowledge graphs. In ACM International Conference on Web Search and Data Mining (2019), pp. 96\u2013104.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.04750",
                        "Citation Paper Title": "Title:Interaction Embeddings for Prediction and Explanation in Knowledge Graphs",
                        "Citation Paper Abstract": "Abstract:Knowledge graph embedding aims to learn distributed representations for entities and relations, and is proven to be effective in many applications. Crossover interactions --- bi-directional effects between entities and relations --- help select related information when predicting a new triple, but haven't been formally discussed before. In this paper, we propose CrossE, a novel knowledge graph embedding which explicitly simulates crossover interactions. It not only learns one general embedding for each entity and relation as most previous methods do, but also generates multiple triple specific embeddings for both of them, named interaction embeddings. We evaluate embeddings on typical link prediction tasks and find that CrossE achieves state-of-the-art results on complex and more challenging datasets. Furthermore, we evaluate embeddings from a new perspective --- giving explanations for predicted triples, which is important for real applications. In this work, an explanation for a triple is regarded as a reliable closed-path between the head and the tail entity. Compared to other baselines, we show experimentally that CrossE, benefiting from interaction embeddings, is more capable of generating reliable explanations to support its predictions.",
                        "Citation Paper Authors": "Authors:Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, Huajun Chen"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "is proposed to learn\ncontinuous-time representations of entities and relations dynamically by Neural ODE method ",
                    "Citation Text": "Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. In\nAdvances in Neural Information Processing Systems (2018), pp. 6572\u20136583.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.07366",
                        "Citation Paper Title": "Title:Neural Ordinary Differential Equations",
                        "Citation Paper Abstract": "Abstract:We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",
                        "Citation Paper Authors": "Authors:Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "successfully embeds KGs by leveraging\nthree fundamental Euclidean geometric operations. CompoundE3D ",
                    "Citation Text": "Ge, X., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. Knowledge graph embedding with 3d compound geometric\ntransformations. arXiv preprint arXiv:2304.00378 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.00378",
                        "Citation Paper Title": "Title:Knowledge Graph Embedding with 3D Compound Geometric Transformations",
                        "Citation Paper Abstract": "Abstract:The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.",
                        "Citation Paper Authors": "Authors:Xiou Ge, Yun-Cheng Wang, Bin Wang, C.-C. Jay Kuo"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "treats relations in knowledge graphs\nas a three-layer hierarchical relation structure, which can be effortlessly integrated into other KGE\nmodels to acquire abundant structural information. RSN ",
                    "Citation Text": "Guo, L., Sun, Z., and Hu, W. Learning to exploit long-term relational dependencies in knowledge graphs. In\nInternational Conference on Machine Learning (2019), PMLR, pp. 2505\u20132514.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04914",
                        "Citation Paper Title": "Title:Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs",
                        "Citation Paper Abstract": "Abstract:We study the problem of knowledge graph (KG) embedding. A widely-established assumption to this problem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on triple-level learning, which lack the capability of capturing long-term relational dependencies of entities. Moreover, triple-level learning is insufficient for the propagation of semantic information among entities, especially for the case of cross-KG embedding. In this paper, we propose recurrent skipping networks (RSNs), which employ a skipping mechanism to bridge the gaps between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently capture the long-term relational dependencies within and between KGs. We design an end-to-end framework to support RSNs on different tasks. Our experimental results showed that RSNs outperformed state-of-the-art embedding-based methods for entity alignment and achieved competitive performance for KG completion.",
                        "Citation Paper Authors": "Authors:Lingbing Guo, Zequn Sun, Wei Hu"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "model, interprets triples as text sequences and con-\nducts knowledge embedding by leveraging a pre-trained language model BERT ",
                    "Citation Text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: October 2023.Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces 35\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 170,
                    "Sentence": ":\ny\ud835\udc59+1\n\ud835\udc56=\ud835\udf0e\u0012\u2211\ufe01\n\ud835\udc5f\u2208R\u2211\ufe01\n\ud835\udc57\u2208N\ud835\udc5f\n\ud835\udc561\n\ud835\udc50\ud835\udc56,\ud835\udc5fW\ud835\udc59\n\ud835\udc5f\ud835\udc66\ud835\udc59\n\ud835\udc57+W\ud835\udc59\n0\ud835\udc66\ud835\udc59\n\ud835\udc56\u0013\n, (13)\nwhere N\ud835\udc5f\n\ud835\udc56denotes the set of neighbor indices of node \ud835\udc56under relation r\u2208Rand\ud835\udc50\ud835\udc56,\ud835\udc5fis a normalisation\nconstant. CompGCN ",
                    "Citation Text": "Vashishth, S., Sanyal, S., Nitin, V., and Talukdar, P. Composition-based multi-relational graph convolutional\nnetworks. arXiv preprint arXiv:1911.03082 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.03082",
                        "Citation Paper Title": "Title:Composition-based Multi-Relational Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research.",
                        "Citation Paper Authors": "Authors:Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Partha Talukdar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.12938v2": {
            "Paper Title": "Counterfactual Memorization in Neural Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.10977v2": {
            "Paper Title": "What should I Ask: A Knowledge-driven Approach for Follow-up Questions\n  Generation in Conversational Surveys",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03462v2": {
            "Paper Title": "NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual\n  Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.07580v2": {
            "Paper Title": "mGPT: Few-Shot Learners Go Multilingual",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.07994v2": {
            "Paper Title": "Knowledgeable Salient Span Mask for Enhancing Language Models as\n  Knowledge Base",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08614v8": {
            "Paper Title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "for an overview). U NIQORN\ncannot outperform powerful SoTA models in this retriever-\nreader space like P ATHRETRIEVER ",
                    "Citation Text": "Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., Xiong, C.,\n2020. Learning to retrieve reasoning paths over wikipedia graph\nfor question answering, in: ICLR.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.10470",
                        "Citation Paper Title": "Title:Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
                        "Citation Paper Abstract": "Abstract:Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.",
                        "Citation Paper Authors": "Authors:Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "7.1. QA over heterogeneous sources\nMethods for heterogeneous QA can be broadly grouped\nas adopting one of the three following means ",
                    "Citation Text": "Roy, R.S., Anand, A., 2022. Question Answering for the Curated\nWeb: Tasks and Methods in QA over Knowledge Bases and Text\nCollections. Springer.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11980",
                        "Citation Paper Title": "Title:Question Answering over Curated and Open Web Sources",
                        "Citation Paper Abstract": "Abstract:The last few years have seen an explosion of research on the topic of automated question answering (QA), spanning the communities of information retrieval, natural language processing, and artificial intelligence. This tutorial would cover the highlights of this really active period of growth for QA to give the audience a grasp over the families of algorithms that are currently being used. We partition research contributions by the underlying source from where answers are retrieved: curated knowledge graphs, unstructured text, or hybrid corpora. We choose this dimension of partitioning as it is the most discriminative when it comes to algorithm design. Other key dimensions are covered within each sub-topic: like the complexity of questions addressed, and degrees of explainability and interactivity introduced in the systems. We would conclude the tutorial with the most promising emerging trends in the expanse of QA, that would help new entrants into this field make the best decisions to take the community forward. Much has changed in the community since the last tutorial on QA in SIGIR 2016, and we believe that this timely overview will indeed benefit a large number of conference participants.",
                        "Citation Paper Authors": "Authors:Rishiraj Saha Roy, Avishek Anand"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.07220v2": {
            "Paper Title": "Understanding Translationese in Cross-Lingual Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01810v3": {
            "Paper Title": "Defending Against Backdoor Attacks in Natural Language Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07624v2": {
            "Paper Title": "Semantic Similarity Models for Depression Severity Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06869v4": {
            "Paper Title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for\n  Aligning Dialogue Agents with Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02605v3": {
            "Paper Title": "GraphFormers: GNN-nested Transformers for Representation Learning on\n  Textual Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11023v3": {
            "Paper Title": "AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable\n  Usage Representations",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "aim at variable misuse\nidentification with Graph Relational Embedding Attention Trans-\nformers. Allamanis et al. ",
                    "Citation Text": "Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021. Self-\nSupervised Bug Detection and Repair. Advances in Neural Information Processing\nSystems 34 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.12787",
                        "Citation Paper Title": "Title:Self-Supervised Bug Detection and Repair",
                        "Citation Paper Abstract": "Abstract:Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.",
                        "Citation Paper Authors": "Authors:Miltiadis Allamanis, Henry Jackson-Flux, Marc Brockschmidt"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "designed a set of deep neural models to adapt given snippet\nto surrounding code. Vasic et al. ",
                    "Citation Text": "Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.\n2019. Neural program repair by jointly learning to localize and repair. arXiv\npreprint arXiv:1904.01720 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01720",
                        "Citation Paper Title": "Title:Neural Program Repair by Jointly Learning to Localize and Repair",
                        "Citation Paper Abstract": "Abstract:Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.",
                        "Citation Paper Authors": "Authors:Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, Rishabh Singh"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "train a neural networks with fea-\ntures extracted from abstract syntax tree paths to suggest variable\nand method names. David et al. ",
                    "Citation Text": "Yaniv David, Uri Alon, and Eran Yahav. 2020. Neural reverse engineering of\nstripped binaries using augmented control flow graphs. Proceedings of the ACM\non Programming Languages 4, OOPSLA (2020), 1\u201328.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.09122",
                        "Citation Paper Title": "Title:Neural Reverse Engineering of Stripped Binaries using Augmented Control Flow Graphs",
                        "Citation Paper Abstract": "Abstract:We address the problem of reverse engineering of stripped executables, which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and the diverse assembly code patterns arising from compiler optimizations.\nWe present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with neural models. The main idea is to use static analysis to obtain augmented representations of call sites; encode the structure of these call sites using the control-flow graph (CFG) and finally, generate a target name while attending to these call sites. We use our representation to drive graph-based, LSTM-based and Transformer-based architectures.\nOur evaluation shows that our models produce predictions that are difficult and time consuming for humans, while improving on existing methods by 28% and by 100% over state-of-the-art neural textual models that do not use any static analysis. Code and data for this evaluation are available at this https URL .",
                        "Citation Paper Authors": "Authors:Yaniv David, Uri Alon, Eran Yahav"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "presents a recurrent neu-\nral networks based technique to infer natural identifier names for\nminified names. Alon et al. ",
                    "Citation Text": "Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A general path-\nbased representation for predicting program properties. ACM SIGPLAN Notices\n53, 4 (2018), 404\u2013419.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09544",
                        "Citation Paper Title": "Title:A General Path-Based Representation for Predicting Program Properties",
                        "Citation Paper Abstract": "Abstract:Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming and increase programmer productivity. A major challenge when learning from programs is $\\textit{how to represent programs in a way that facilitates effective learning}$.\nWe present a $\\textit{general path-based representation}$ for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.\nWe show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages.\nWe evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C\\#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.",
                        "Citation Paper Authors": "Authors:Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "present a approach\nfor predicting program properties via a probabilistic model learnt\nfrom massive codebases. Bavishi et al. ",
                    "Citation Text": "Rohan Bavishi, Michael Pradel, and Koushik Sen. 2018. Context2Name: A deep\nlearning-based approach to infer natural variable names from usage contexts.\narXiv preprint arXiv:1809.05193 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.05193",
                        "Citation Paper Title": "Title:Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts",
                        "Citation Paper Abstract": "Abstract:Most of the JavaScript code deployed in the wild has been minified, a process in which identifier names are replaced with short, arbitrary and meaningless names. Minified code occupies less space, but also makes the code extremely difficult to manually inspect and understand. This paper presents Context2Name, a deep learningbased technique that partially reverses the effect of minification by predicting natural identifier names for minified names. The core idea is to predict from the usage context of a variable a name that captures the meaning of the variable. The approach combines a lightweight, token-based static analysis with an auto-encoder neural network that summarizes usage contexts and a recurrent neural network that predict natural names for a given usage context. We evaluate Context2Name with a large corpus of real-world JavaScript code and show that it successfully predicts 47.5% of all minified identifiers while taking only 2.9 milliseconds on average to predict a name. A comparison with the state-of-the-art tools JSNice and JSNaughty shows that our approach performs comparably in terms of accuracy while improving in terms of efficiency. Moreover, Context2Name complements the state-of-the-art by predicting 5.3% additional identifiers that are missed by both existing tools.",
                        "Citation Paper Authors": "Authors:Rohan Bavishi, Michael Pradel, Koushik Sen"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed a Python method text-to-texttransfer transformer (PyMT5) for method generation and code sum-\nmarization. Tufano et al. ",
                    "Citation Text": "Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan. 2020.\nGenerating accurate assert statements for unit test cases using pretrained trans-\nformers. arXiv preprint arXiv:2009.05634 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.05634",
                        "Citation Paper Title": "Title:Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers",
                        "Citation Paper Abstract": "Abstract:Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task. In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests. The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62% of the cases in the first attempt. The results show 80% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.",
                        "Citation Paper Authors": "Authors:Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Sundaresan"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "introduced GPT-C, a generative\ntransformer model trained on source code for line-level code com-\npletion. Clement et al. ",
                    "Citation Text": "Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and\nNeel Sundaresan. 2020. PyMT5: multi-mode translation of natural language and\nPython code with transformers. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . 9052\u20139065.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.03150",
                        "Citation Paper Title": "Title:PyMT5: multi-mode translation of natural language and Python code with transformers",
                        "Citation Paper Abstract": "Abstract:Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",
                        "Citation Paper Authors": "Authors:Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "extended this work\nby proposing an GraphCodeBERT to predict edges in a dataflow\ngraph. Svyatkovskiy et al. ",
                    "Citation Text": "Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020.\nIntellicode compose: Code generation using transformer. In Proceedings of the 28th\nACM Joint Meeting on European Software Engineering Conference and Symposium\non the Foundations of Software Engineering . 1433\u20131443.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.08025",
                        "Citation Paper Title": "Title:IntelliCode Compose: Code Generation Using Transformer",
                        "Citation Paper Abstract": "Abstract:In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments.\nIn this paper, we introduce IntelliCode Compose $-$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, $C\\#$, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook.\nOur best model yields an average edit similarity of $86.7\\%$ and a perplexity of 1.82 for Python programming language.",
                        "Citation Paper Authors": "Authors:Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "proposed CodeBERT, a RoBERTa-based MLM model pre-\ntrained on code, for natural language code search and code doc-\numentation generation tasks. Guo et al. ",
                    "Citation Text": "Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long\nZhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al .2020. GraphCodeBERT:\nPre-training code representations with data flow. arXiv preprint arXiv:2009.08366\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08366",
                        "Citation Paper Title": "Title:GraphCodeBERT: Pre-training Code Representations with Data Flow",
                        "Citation Paper Abstract": "Abstract:Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
                        "Citation Paper Authors": "Authors:Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Our work relates to the literature on transformers for code, code\ndeobfuscation, and program repair.\nTransformers for Code. Self-supervised pretraining objectives\nhave shown to be highly effective for code intelligence tasks. Feng et\nal. ",
                    "Citation Text": "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-\nTrained Model for Programming and Natural Languages. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing: Findings .\n1536\u20131547.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08155",
                        "Citation Paper Title": "Title:CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
                        "Citation Paper Abstract": "Abstract:We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.",
                        "Citation Paper Authors": "Authors:Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14883v3": {
            "Paper Title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel\n  Training",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ", which the users can use\ndirectly. This does not require the users to have domain expertise so\nthat they do not have to manually design their parallelism strategy\nlike GShard ",
                    "Citation Text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,\nYanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard:\nScaling Giant Models with Conditional Computation and Automatic Sharding.\nInInternational Conference on Learning Representations . https://openreview.net/\nforum?id=qrwe7XHTmYb",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16668",
                        "Citation Paper Title": "Title:GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                        "Citation Paper Abstract": "Abstract:Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
                        "Citation Paper Authors": "Authors:Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "proposed 1D tensor parallelism which splits the linear layer in the\nrow or column dimensions for the Transformer architecture ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.05895v4": {
            "Paper Title": "Latent Diffusion Energy-Based Model for Interpretable Text Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06082v2": {
            "Paper Title": "One Sense per Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07136v3": {
            "Paper Title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and\n  Stronger",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00193v3": {
            "Paper Title": "FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06282v4": {
            "Paper Title": "DialoGen: Generalized Long-Range Context Representation for Dialogue\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07467v2": {
            "Paper Title": "Query Rewriting for Effective Misinformation Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09110v2": {
            "Paper Title": "Holistic Evaluation of Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06774v3": {
            "Paper Title": "Large-Scale Bidirectional Training for Zero-Shot Image Captioning",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "unifies modalities (vision,\nlanguage, multimodal) and tasks for pretraining.\n2.3. Zero-Shot Image Captioning\nTewel et al . ",
                    "Citation Text": "Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf.\nZerocap: Zero-shot image-to-text generation for visual-\nsemantic arithmetic. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 17918\u201317928, June 2022. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14447",
                        "Citation Paper Title": "Title:ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic",
                        "Citation Paper Abstract": "Abstract:Recent text-to-image matching models apply contrastive learning to large corpora of uncurated pairs of images and sentences. While such models can provide a powerful score for matching and subsequent zero-shot tasks, they are not capable of generating caption given an image. In this work, we repurpose such models to generate a descriptive text given an image at inference time, without any further training or tuning steps. This is done by combining the visual-semantic model with a large language model, benefiting from the knowledge in both web-scale models. The resulting captions are much less restrictive than those obtained by supervised captioning methods. Moreover, as a zero-shot learning method, it is extremely flexible and we demonstrate its ability to perform image arithmetic in which the inputs can be either images or text, and the output is a sentence. This enables novel high-level vision capabilities such as comparing two images or solving visual analogy tests. Our code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Yoad Tewel, Yoav Shalev, Idan Schwartz, Lior Wolf"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", current state-of-the-art in zero-\nshot image captioning, on MS-COCO Captions ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll \u00b4ar. Microsoft\ncoco: Common objects in context. In Proceedings of the\nEuropean Conference on Computer Vision , 2014. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "with 1280 dimensional states and 10 masked self-attention\nheads. We use 64 BPE-encoded ",
                    "Citation Text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. In\nProceedings of the Annual Meeting of the Association for\nComputational Linguistics , 2016. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.07909",
                        "Citation Paper Title": "Title:Neural Machine Translation of Rare Words with Subword Units",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Unlike AugV AE, we use Parametric Rec-\ntified Linear Unit (PReLU) ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectifiers: Surpassing human-level perfor-\nmance on imagenet classification. In Proceedings of Inter-\nnational Conference on Computer Vision , 2015. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.01852",
                        "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                        "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "shows data and parameter-efficient results compared to uni-\ndirectional models. OFA ",
                    "Citation Text": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang. OFA: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In Proceedings of the International Conference\non Machine Learning , 2022. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.03052",
                        "Citation Paper Title": "Title:OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
                        "Citation Paper Abstract": "Abstract:In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision & language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05735v4": {
            "Paper Title": "Learning ASR pathways: A sparse multilingual ASR model",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "with \u03b1= 0.5to re-balance the\ntraining data.\n4.2. Implementation details\nFor our dense multilingual model, we use a streaming RNN-T model\nwith 30 Emformer layers with 512 input dims, 2048 feed-forward\ndims, GeLU non-linearity, and no memory banks ",
                    "Citation Text": "Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng\nYeh, Julian Chan, et al., \u201cEmformer: Efficient memory trans-\nformer based acoustic model for low latency streaming speech\nrecognition,\u201d in Proc. ICASSP , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10759",
                        "Citation Paper Title": "Title:Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition",
                        "Citation Paper Abstract": "Abstract:This paper proposes an efficient memory transformer Emformer for low latency streaming speech recognition. In Emformer, the long-range history context is distilled into an augmented memory bank to reduce self-attention's computation complexity. A cache mechanism saves the computation for the key and value in self-attention for the left context. Emformer applies a parallelized block processing in training to support low latency models. We carry out experiments on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets WER $2.50\\%$ on test-clean and $5.62\\%$ on test-other. Comparing with a strong baseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds training speedup and $18\\%$ relative real-time factor (RTF) reduction in decoding with relative WER reduction $17\\%$ on test-clean and $9\\%$ on test-other. For a low latency scenario with an average latency of 80 ms, Emformer achieves WER $3.01\\%$ on test-clean and $7.09\\%$ on test-other. Comparing with the LSTM baseline with the same latency and model size, Emformer gets relative WER reduction $9\\%$ and $16\\%$ on test-clean and test-other, respectively.",
                        "Citation Paper Authors": "Authors:Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, Mike Seltzer"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", LTH was used to identify language-specific\nsub-networks inside the pre-trained multilingual XLSR model ",
                    "Citation Text": "Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdel-\nrahman Mohamed, and Michael Auli, \u201cUnsupervised cross-\nlingual representation learning for speech recognition,\u201d arXiv\npreprint arXiv:2006.13979 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13979",
                        "Citation Paper Title": "Title:Unsupervised Cross-lingual Representation Learning for Speech Recognition",
                        "Citation Paper Abstract": "Abstract:This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Multilingual ASR. Among recent works on multilingual ASR,\nlanguage-specific modeling is usually adopted to achieve univer-\nsally decent performance on all languages. This includes language-\naware encoding ",
                    "Citation Text": "Jinchuan Tian, Jianwei Yu, Chunlei Zhang, Chao Weng,\nYuexian Zou, and Dong Yu, \u201cLae: Language-aware en-\ncoder for monolingual and multilingual asr,\u201d arXiv preprint\narXiv:2206.02093 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.02093",
                        "Citation Paper Title": "Title:LAE: Language-Aware Encoder for Monolingual and Multilingual ASR",
                        "Citation Paper Abstract": "Abstract:Despite the rapid progress in automatic speech recognition (ASR) research, recognizing multilingual speech using a unified ASR system remains highly challenging. Previous works on multilingual speech recognition mainly focus on two directions: recognizing multiple monolingual speech or recognizing code-switched speech that uses different languages interchangeably within a single utterance. However, a pragmatic multilingual recognizer is expected to be compatible with both directions. In this work, a novel language-aware encoder (LAE) architecture is proposed to handle both situations by disentangling language-specific information and generating frame-level language-aware representations during encoding. In the LAE, the primary encoding is implemented by the shared block while the language-specific blocks are used to extract specific representations for each language. To learn language-specific information discriminatively, a language-aware training method is proposed to optimize the language-specific blocks in LAE. Experiments conducted on Mandarin-English code-switched speech suggest that the proposed LAE is capable of discriminating different languages in frame-level and shows superior performance on both monolingual and multilingual ASR tasks. With either a real-recorded or simulated code-switched dataset, the proposed LAE achieves statistically significant improvements on both CTC and neural transducer systems. Code is released",
                        "Citation Paper Authors": "Authors:Jinchuan Tian, Jianwei Yu, Chunlei Zhang, Chao Weng, Yuexian Zou, Dong Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01299v2": {
            "Paper Title": "Late Audio-Visual Fusion for In-The-Wild Speaker Diarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.15593v2": {
            "Paper Title": "GPT-Neo for commonsense reasoning -- a theoretical and practical lens",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "examine the proficiency of language models for transfer learn-\ning capabilities on downstream tasks using intermediate pre-training\nand adapter-based methods. Similarly, ",
                    "Citation Text": "Arora, Simran, et al. \"Ask me anything: A simple strategy for prompting language\nmodels.\" arXiv preprint arXiv:2210.02441 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02441",
                        "Citation Paper Title": "Title:Ask Me Anything: A simple strategy for prompting language models",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly \"perfect prompt\" for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: this https URL",
                        "Citation Paper Authors": "Authors:Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher R\u00e9"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "conduct a comprehen-\nsive examination on four commonsense benchmark tasks. Similarly, ",
                    "Citation Text": "Li, Xiang Lorraine, et al. \"A systematic investigation of commonsense knowledge\nin large language models.\" Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.00607",
                        "Citation Paper Title": "Title:A Systematic Investigation of Commonsense Knowledge in Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -- a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.",
                        "Citation Paper Authors": "Authors:Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d'Autume, Phil Blunsom, Aida Nematzadeh"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "Recent work on evaluation of LLMs for commonsense natural lan-\nguage inference has garnered significant attention for larger models\nunder zero-shot and few-shot settings. ",
                    "Citation Text": "Zellers, Rowan, et al. \"Swag: A large-scale adversarial dataset for grounded com-\nmonsense inference.\" arXiv preprint arXiv:1808.05326 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05326",
                        "Citation Paper Title": "Title:SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
                        "Citation Paper Abstract": "Abstract:Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.\nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
                        "Citation Paper Authors": "Authors:Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.12758v2": {
            "Paper Title": "Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned\n  Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ") and be helped by other agents (i.e. teachable agents ",
                    "Citation Text": "Olivier Sigaud, Hugo Caselles-Dupr\u00e9, C\u00e9dric Colas, Ahmed Akakzia, Pierre-Yves Oudeyer, and\nMohamed Chetouani. Towards teachable autonomous agents. arXiv preprint arXiv:2105.11977 ,\n2021.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.11977",
                        "Citation Paper Title": "Title:Towards Teachable Autotelic Agents",
                        "Citation Paper Abstract": "Abstract:Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of Artificial Intelligence, these extremes respectively map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autotelic agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world non-expert users to orient the learning trajectories of agents towards their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This paper presents a roadmap towards the design of teachable autonomous agents. Building on developmental psychology and education sciences, we start by identifying key features enabling assisted discovery processes in child-tutor interactions. This leads to the production of a checklist of features that future TAA will need to demonstrate. The checklist allows us to precisely pinpoint the various limitations of current reinforcement learning agents and to identify the promising first steps towards TAA. It also shows the way forward by highlighting key research directions towards the design or autonomous agents that can be taught by ordinary people via natural pedagogy.",
                        "Citation Paper Authors": "Authors:Olivier Sigaud, Ahmed Akakzia, Hugo Caselles-Dupr\u00e9, C\u00e9dric Colas, Pierre-Yves Oudeyer, Mohamed Chetouani"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Our work is rooted in the Goal-Conditioned Reinforcement Learning (GCRL) literature ",
                    "Citation Text": "C\u00e9dric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents with\nintrinsically motivated goal-conditioned reinforcement learning: a short survey. Journal of\nArtificial Intelligence Research , 74:1159\u20131199, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09830",
                        "Citation Paper Title": "Title:Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey",
                        "Citation Paper Abstract": "Abstract:Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by $autotelic$ $agents$: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: $developmental$ $reinforcement$ $learning$. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the $intrinsically$ $motivated$ $acquisition$ $of$ $open$-$ended$ $repertoires$ $of$ $skills$. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.",
                        "Citation Paper Authors": "Authors:C\u00e9dric Colas, Tristan Karch, Olivier Sigaud, Pierre-Yves Oudeyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.06829v2": {
            "Paper Title": "Permutation invariant matrix statistics and computational language tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06972v2": {
            "Paper Title": "Disentangling Prosody Representations with Unsupervised Speech\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "adopts a properly tuned autoen-\ncoder as the information bottleneck to force the model to\ndisentangle linguistic content and speaker identity with self-\nreconstruction. SpeechFlow ",
                    "Citation Text": "K. Qian, Y . Zhang, S. Chang, M. Hasegawa-Johnson, and\nD. Cox, \u201cUnsupervised speech decomposition via triple infor-\nmation bottleneck,\u201d in International Conference on Machine\nLearning , 2020, pp. 7836\u20137846.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11284",
                        "Citation Paper Title": "Title:Unsupervised Speech Decomposition via Triple Information Bottleneck",
                        "Citation Paper Abstract": "Abstract:Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain. In this paper, we propose SpeechSplit, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechSplit is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, Mark Hasegawa-Johnson"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nInspired by the success of self-supervised pretraining in\nASR tasks, researchers directly utilize pretrained speech rep-\nresentations for SER, such as attempting different fine-tuning\nstrategies ",
                    "Citation Text": "Y . Wang, A. Boumadane, and A. Heba, \u201cA fine-tuned\nWav2Vec 2.0/HuBERT benchmark for speech emotion recogni-\ntion, speaker verification and spoken language understanding,\u201d\narXiv preprint arXiv:2111.02735 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.02735",
                        "Citation Paper Title": "Title:A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
                        "Citation Paper Abstract": "Abstract:Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
                        "Citation Paper Authors": "Authors:Yingzhi Wang, Abdelmoumene Boumadane, Abdelwahab Heba"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "propose Emovox to control fine-grained emotional\nintensity by integrating intensity and emotion classification\ninto EVC training. Inspired by the mechanism of speech\nproduction, Luo et al. ",
                    "Citation Text": "Z. Luo, S. Lin, R. Liu, J. Baba, Y . Yoshikawa, and H. Ishiguro,\n\u201cDecoupling speaker-independent emotions for voice conver-\nsion via source-filter networks,\u201d IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , vol. 31, pp. 11\u201324,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.01164",
                        "Citation Paper Title": "Title:Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks",
                        "Citation Paper Abstract": "Abstract:Emotional voice conversion (VC) aims to convert a neutral voice to an emotional (e.g. happy) one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as speaker, content, etc.) is the key to achieving remarkable performance. Some recent attempts about speech representation decoupling on the neutral speech can not work well on the emotional speech, due to the more complex acoustic properties involved in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion features from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, and one decoder. Note that all encoder modules adopt a designed information bottlenecks auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel two-stage training strategy based on the 2D Valence-Arousal (VA) space was proposed. Experimental results show that the proposed SFEVC along with a two-stage training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.",
                        "Citation Paper Authors": "Authors:Zhaojie Luo, Shoufeng Lin, Rui Liu, Jun Baba, Yuichiro Yoshikawa, Ishiguro Hiroshi"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "are adopted to\nlearn continual and discretized prosody representations from\na reference audio respectively.\nThe basic idea of information bottleneck approaches is to\ncontrol the information flow by carefully designing appropriate\nbottlenecks. AutoVC ",
                    "Citation Text": "K. Qian, Y . Zhang, S. Chang, X. Yang, and M. Hasegawa-\nJohnson, \u201cAutoVC: Zero-shot voice style transfer with only\nautoencoder loss,\u201d in International Conference on Machine\nLearning , 2019, pp. 5210\u20135219.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.05879",
                        "Citation Paper Title": "Title:AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss",
                        "Citation Paper Abstract": "Abstract:Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.",
                        "Citation Paper Authors": "Authors:Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "in-\ntroduce \u201cglobal style tokens\u201d to automatically discover expres-\nsive styles. In addition, Variational Autoencoder (V AE) ",
                    "Citation Text": "Y .-J. Zhang, S. Pan, L. He, and Z.-H. Ling, \u201cLearning la-\ntent representations for style control and transfer in end-to-\nend speech synthesis,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing . IEEE, 2019, pp.\n6945\u20136949.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.04342",
                        "Citation Paper Title": "Title:Learning latent representations for style control and transfer in end-to-end speech synthesis",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.",
                        "Citation Paper Authors": "Authors:Ya-Jie Zhang, Shifeng Pan, Lei He, Zhen-Hua Ling"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "TTS system to\ncapture meaningful variations of prosody and successfully per-\nform speaking style transfer. Subsequently, Wang et al. ",
                    "Citation Text": "Y . Wang, D. Stanton, Y . Zhang, R.-S. Ryan, E. Battenberg,\nJ. Shor, Y . Xiao, Y . Jia, F. Ren, and R. A. Saurous, \u201cStyle\ntokens: Unsupervised style modeling, control and transfer in\nend-to-end speech synthesis,\u201d in International Conference on\nMachine Learning , 2018, pp. 5180\u20135189.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09017",
                        "Citation Paper Title": "Title:Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",
                        "Citation Paper Authors": "Authors:Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, Ye Jia, Rif A. Saurous"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.07425v3": {
            "Paper Title": "Robust and Explainable Identification of Logical Fallacies in Natural\n  Language Arguments",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". On theother hand, neuro-symbolic methods that, e.g., pose reason-\ning as a soft logic problem ",
                    "Citation Text": "Clark, P., Tafjord, O., Richardson, K., 2020b. Transformers as soft\nreasoners over language. URL: https://arxiv.org/abs/2002.05867,\ndoi:10.48550/ARXIV.2002.05867 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05867",
                        "Citation Paper Title": "Title:Transformers as Soft Reasoners over Language",
                        "Citation Paper Abstract": "Abstract:Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.",
                        "Citation Paper Authors": "Authors:Peter Clark, Oyvind Tafjord, Kyle Richardson"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": ". We see two\nparallel streams of AI methods that should be explored in\ndepth for logical fallacies. On the one hand, a promising\nnew stream relies on neural language models through meth-\nods like chain-of-thought reasoning ",
                    "Citation Text": "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia,\nF., Chi, E., Le, Q., Zhou, D., 2022. Chain of thought prompting\nelicits reasoning in large language models. URL: https://arxiv.org/\nabs/2201.11903, doi: 10.48550/ARXIV.2201.11903 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "can be done by lexicaliz-\ning knowledge into task-targetted evidence paths and com-\nbining them with the task input [76, 83]. The idea in K-\nBERT ",
                    "Citation Text": "Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., Wang, P.,\n2019a. K-bert: Enabling language representation with knowledgegraph. URL: https://arxiv.org/abs/1909.07606, doi: 10.48550/\nARXIV.1909.07606 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.07606",
                        "Citation Paper Title": "Title:K-BERT: Enabling Language Representation with Knowledge Graph",
                        "Citation Paper Abstract": "Abstract:Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by equipped with a KG without pre-training by-self because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.",
                        "Citation Paper Authors": "Authors:Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, Ping Wang"
                    }
                },
                {
                    "Sentence ID": 101,
                    "Sentence": "), IBR models could have not behaved as well as other\nmethods discussed, inline with ",
                    "Citation Text": "Ribeiro, M.T., Singh, S., Guestrin, C., 2016. \"why should i\ntrust you?\": Explaining the predictions of any classifier. URL:\nhttps://arxiv.org/abs/1602.04938, doi: 10.48550/ARXIV.1602.\n04938 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", we\nreplace the BART encoder model in Prototex with a self-\nsupervised language model, Electra ",
                    "Citation Text": "Clark, K., Luong, M.T., Le, Q.V ., Manning, C.D., 2020a. ELEC-\nTRA: Pre-training text encoders as discriminators rather than gener-\nators, in: ICLR. URL: https://openreview.net/pdf?id=r1xMH1BtvB.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10555",
                        "Citation Paper Title": "Title:ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
                        "Citation Paper Abstract": "Abstract:Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": "method. The architecture of\nPrototex is shown in Figure 3. Prototex is based on the Pro-\ntotype Classification Network proposed in ",
                    "Citation Text": "Li, O., Liu, H., Chen, C., Rudin, C., 2017. Deep learning for\ncase-based reasoning through prototypes: A neural network that\nexplains its predictions. URL: https://arxiv.org/abs/1710.04806,\ndoi:10.48550/ARXIV.1710.04806 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.04806",
                        "Citation Paper Title": "Title:Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as \"black box\" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.",
                        "Citation Paper Authors": "Authors:Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "and they are intuitively important to detect\ncertain logical fallacies, such as Appeal to Emotion . To cap-\nture the usage of empathetic and emotional terminology, we\nuse a RoBERTa model ",
                    "Citation Text": "Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,\nLewis, M., Zettlemoyer, L., Stoyanov, V ., 2019b. Roberta: A ro-\nbustly optimized bert pretraining approach. URL: https://arxiv.org/\nabs/1907.11692, doi: 10.48550/ARXIV.1907.11692 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 99,
                    "Sentence": ", a Transformer-based\nretriever that is optimized for capturing overall sentence sim-\nilarity using a contrastive loss function. We also include\nsentence encoders that are reportedly able to manipulate a\nwide range of concepts, by using Sentence-BERT ",
                    "Citation Text": "Reimers, N., Gurevych, I., 2019. Sentence-bert: Sentence embed-\ndings using siamese bert-networks, in: Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing,\nAssociation for Computational Linguistics. URL: http://arxiv.org/\nabs/1908.10084.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.10084",
                        "Citation Paper Title": "Title:Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                        "Citation Paper Abstract": "Abstract:BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\nIn this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                        "Citation Paper Authors": "Authors:Nils Reimers, Iryna Gurevych"
                    }
                },
                {
                    "Sentence ID": 128,
                    "Sentence": ", not as a bar-\nrier, but as a way of training more robust models referred to\nas Curriculum learning (\u00a74.2). Finally, we devise data aug-mentation strategies to address data sparsity and improve the\nstability of our models ",
                    "Citation Text": "Zheng, S., Song, Y ., Leung, T., Goodfellow, I., 2016. Improving\nthe robustness of deep neural networks via stability training. URL:\nhttps://arxiv.org/abs/1604.04326, doi: 10.48550/ARXIV.1604.\n04326 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.04326",
                        "Citation Paper Title": "Title:Improving the Robustness of Deep Neural Networks via Stability Training",
                        "Citation Paper Abstract": "Abstract:In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.",
                        "Citation Paper Authors": "Authors:Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "apply case-based reasoning to predict the outcome\nof legal cases. Ford et al. ",
                    "Citation Text": "Ford, C., Kenny, E.M., Keane, M.T., 2020. Play mnist for me! user\nstudies on the effects of post-hoc, example-based explanations &;\nerror rates on debugging a deep learning, black-box classifier URL:\nhttps://arxiv.org/abs/2009.06349, doi: 10.48550/ARXIV.2009.\n06349 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.06349",
                        "Citation Paper Title": "Title:Play MNIST For Me! User Studies on the Effects of Post-Hoc, Example-Based Explanations & Error Rates on Debugging a Deep Learning, Black-Box Classifier",
                        "Citation Paper Abstract": "Abstract:This paper reports two experiments (N=349) on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.",
                        "Citation Paper Authors": "Authors:Courtney Ford, Eoin M. Kenny, Mark T. Keane"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.12983v2": {
            "Paper Title": "Explainable and High-Performance Hate and Offensive Speech Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13778v2": {
            "Paper Title": "CC-Riddle: A Question Answering Dataset of Chinese Character Riddles",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "present\nthe first large dataset for answering riddle-style commonsense questions, aiming to test the abilities\nnecessary for advanced natural language understanding(NLU). Efrat et al. ",
                    "Citation Text": "A. Efrat, U. Shaham, D. Kilman, and O. Levy. Cryptonite: A cryptic crossword benchmark for\nextreme ambiguity in language. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing , pages 4186\u20134192, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01242",
                        "Citation Paper Title": "Title:Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language",
                        "Citation Paper Abstract": "Abstract:Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%).",
                        "Citation Paper Authors": "Authors:Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "To the best of our knowledge, there has been no publicly available large-scale Chinese character\nriddle dataset, while datasets for common riddles have been developed before. Lin et al. ",
                    "Citation Text": "B. Y . Lin, Z. Wu, Y . Yang, D.-H. Lee, and X. Ren. Riddlesense: Reasoning about riddle\nquestions featuring linguistic creativity and commonsense knowledge. In Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021 , pages 1504\u20131515, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.00376",
                        "Citation Paper Title": "Title:RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge",
                        "Citation Paper Abstract": "Abstract:Question: I have five fingers but I am not alive. What am I? Answer: a glove. Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there are currently no dedicated datasets aiming to test these abilities. Herein, we present RiddleSense, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddle-style commonsense questions. We systematically evaluate a wide range of models over the challenge, and point out that there is a large gap between the best-supervised model and human performance -- suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems.",
                        "Citation Paper Authors": "Authors:Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, Xiang Ren"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.04176v3": {
            "Paper Title": "Crime Hot-Spot Modeling via Topic Modeling and Relative Density\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13397v4": {
            "Paper Title": "Development of Hybrid ASR Systems for Low Resource Medical Domain\n  Conversational Telephone Speech",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08390v2": {
            "Paper Title": "Lessons learned from the evaluation of Spanish Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00318v2": {
            "Paper Title": "Smoothing Entailment Graphs with Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10708v2": {
            "Paper Title": "Zero-shot Triplet Extraction by Template Infilling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08214v2": {
            "Paper Title": "Procedures as Programs: Hierarchical Control of Situated Agents through\n  Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07803v3": {
            "Paper Title": "Attacking Open-domain Question Answering by Injecting Misinformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14704v5": {
            "Paper Title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.04392v4": {
            "Paper Title": "Contrastive Demonstration Tuning for Pre-trained Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08738v3": {
            "Paper Title": "Learning Decoupled Retrieval Representation for Nearest Neighbour Neural\n  Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00038v2": {
            "Paper Title": "Differentially Private Optimization on Large Model at Small Cost",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.12714v3": {
            "Paper Title": "Generative Knowledge Graph Construction: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.10560v2": {
            "Paper Title": "Empowering Fake-News Mitigation: Insights from Sharers' Social Media\n  Post-Histories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10678v3": {
            "Paper Title": "Towards Realistic Low-resource Relation Extraction: A Benchmark with\n  Empirical Baseline Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09597v8": {
            "Paper Title": "Reasoning with Language Model Prompting: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04351v3": {
            "Paper Title": "Sentiment Analysis and Effect of COVID-19 Pandemic using College\n  SubReddit Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11717v2": {
            "Paper Title": "Temporal Analysis on Topics Using Word2Vec",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11501v2": {
            "Paper Title": "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks\n  for Visual Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "Model# Image-captionParameters Structured knowledgeTest Acc.(%)\nin pretraining Q \u2192A QA \u2192R Q \u2192AR\nViLBERT ",
                    "Citation Text": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In NeurIPS , 2019. 1, 2, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.02265",
                        "Citation Paper Title": "Title:ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
                        "Citation Paper Abstract": "Abstract:We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.",
                        "Citation Paper Authors": "Authors:Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01378v2": {
            "Paper Title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.02982v3": {
            "Paper Title": "Event and Entity Extraction from Generated Video Captions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04634v3": {
            "Paper Title": "Open-world Story Generation with Structured Knowledge Enhancement: A\n  Comprehensive Survey",
            "Sentences": [
                {
                    "Sentence ID": 177,
                    "Sentence": "focus on the scenario that users constantly provide\ncue phrases to drive the generation process. Other works\nsuch as Dramatron ",
                    "Citation Text": "P. Mirowski, K. W. Mathewson, J. Pittman, R. Evans, Co-\nwriting screenplays and theatre scripts with language models:\nAn evaluation by industry professionals (2022). doi:10.48550/\narXiv.2209.14958 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14958",
                        "Citation Paper Title": "Title:Co-Writing Screenplays and Theatre Scripts with Language Models: An Evaluation by Industry Professionals",
                        "Citation Paper Abstract": "Abstract:Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron's usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report critical reflections both from our interviewees and from independent reviewers who watched stagings of the works to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations -- including plagiarism and bias -- and participatory models for the design and deployment of such tools.",
                        "Citation Paper Authors": "Authors:Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans"
                    }
                },
                {
                    "Sentence ID": 176,
                    "Sentence": "propose an interactive system that allows flexible\nhuman involvement in story planning, editing, and revising\n15without regeneration of the whole story. Brahman et al. ",
                    "Citation Text": "F. Brahman, A. Petrusca, S. Chaturvedi, Cue me in: Content-\ninducing approaches to interactive story generation, in: ACL-\nIJCAI, 2020, pp. 588\u2013597.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09935",
                        "Citation Paper Title": "Title:Cue Me In: Content-Inducing Approaches to Interactive Story Generation",
                        "Citation Paper Abstract": "Abstract:Automatically generating stories is a challenging problem that requires producing causally related and logical sequences of events about a topic. Previous approaches in this domain have focused largely on one-shot generation, where a language model outputs a complete story based on limited initial input from a user. Here, we instead focus on the task of interactive story generation, where the user provides the model mid-level sentence abstractions in the form of cue phrases during the generation process. This provides an interface for human users to guide the story generation. We present two content-inducing approaches to effectively incorporate this additional information. Experimental results from both automatic and human evaluations show that these methods produce more topically coherent and personalized stories compared to baseline methods.",
                        "Citation Paper Authors": "Authors:Faeze Brahman, Alexandru Petrusca, Snigdha Chaturvedi"
                    }
                },
                {
                    "Sentence ID": 174,
                    "Sentence": ".\nNon-chronological narrative techniques like flashbacks and\nflashforwards can stir readers\u2019 curiosity. Targeting this, Han\net al. ",
                    "Citation Text": "R. Han, H. Chen, Y. Tian, N. Peng, Go back in time: Generat-\ning flashbacks in stories with event temporal prompts, in: Pro-\nceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Lan-\nguage Technologies, Association for Computational Linguistics,\n2022, pp. 1450\u20131470. doi:10.18653/v1/2022.naacl-main.104 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01898",
                        "Citation Paper Title": "Title:Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts",
                        "Citation Paper Abstract": "Abstract:Stories or narratives are comprised of a sequence of events. To compose interesting stories, professional writers often leverage a creative writing technique called flashback that inserts past events into current storylines as we commonly observe in novels and plays. However, it is challenging for machines to generate flashback as it requires a solid understanding of event temporal order (e.g. \"feeling hungry\" before \"eat,\" not vice versa), and the creativity to arrange storylines so that earlier events do not always appear first in narrative order. Two major issues in existing systems that exacerbate the challenges: 1) temporal bias in pertaining and story datasets that leads to monotonic event temporal orders; 2) lack of explicit guidance that helps machines decide where to insert flashbacks. We propose to address these issues using structured storylines to encode events and their pair-wise temporal relations (before, after and vague) as temporal prompts that guide how stories should unfold temporally. We leverage a Plan-and-Write framework enhanced by reinforcement learning to generate storylines and stories end-to-end. Evaluation results show that the proposed method can generate more interesting stories with flashbacks while maintaining textual diversity, fluency, and temporal coherence.",
                        "Citation Paper Authors": "Authors:Rujun Han, Hong Chen, Yufei Tian, Nanyun Peng"
                    }
                },
                {
                    "Sentence ID": 172,
                    "Sentence": "that includes a narrative generation\nsection and an image generation section. At around the\nsame time, Hong et al. proposed a character-based visual\nstorytelling model driven by coherence ",
                    "Citation Text": "X. Hong, A. Sayeed, K. Mehra, V. Demberg, B. Schiele, Visual\nwriting prompts: Character-grounded story generation with\ncurated image sequences (2023). doi:10.48550/arXiv.2301.\n08571 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.08571",
                        "Citation Paper Title": "Title:Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences",
                        "Citation Paper Abstract": "Abstract:Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent and have more narrativity compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and have more narrativity than stories generated with the current state-of-the-art model.",
                        "Citation Paper Authors": "Authors:Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele"
                    }
                },
                {
                    "Sentence ID": 162,
                    "Sentence": "aim to evaluate stories more compre-\nhensively. Nonetheless, more reliable evaluation metrics are\nneeded. One inspiration can be imitating humans\u2019 mental\nprocesses [160, 161].Guan et al. propose a benchmark OpenMEVA ",
                    "Citation Text": "J. Guan, Z. Zhang, Z. Feng, Z. Liu, W. Ding, X. Mao, C. Fan,\nM. Huang, Openmeva: A benchmark for evaluating open-ended\nstory generation metrics, in: ACL-IJCNLP, 2021, pp. 6394\u2013\n6407. doi:10.18653/v1/2021.acl-long.500 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.08920",
                        "Citation Paper Title": "Title:OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics",
                        "Citation Paper Abstract": "Abstract:Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research.",
                        "Citation Paper Authors": "Authors:Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang"
                    }
                },
                {
                    "Sentence ID": 158,
                    "Sentence": ". Relying solely on one evaluation metric\nmay underestimate interesting and good-quality stories.\nThus, recruiting human judges to rate stories is indispens-\nable for fair evaluation, despite the cost. Recently, some\nhybrid-metric systems like ADEM ",
                    "Citation Text": "R. Lowe, M. Noseworthy, I. V. Serban, N. Angelard-Gontier,\nY. Bengio, J. Pineau, Towards an automatic Turing test: Learn-\ning to evaluate dialogue responses, in: ACL, 2017, pp. 1116\u2013\n1126. doi:10.18653/v1/P17-1103 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07149",
                        "Citation Paper Title": "Title:Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses",
                        "Citation Paper Abstract": "Abstract:Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.",
                        "Citation Paper Authors": "Authors:Ryan Lowe, Michael Noseworthy, Iulian V. Serban, Nicolas Angelard-Gontier, Yoshua Bengio, Joelle Pineau"
                    }
                },
                {
                    "Sentence ID": 152,
                    "Sentence": "transforms a\n14plot outline into a coherent story by tracking the dynamic\nplot states. It loosely integrates key plots into the output\nnarrative. Pascua et al. ",
                    "Citation Text": "D. Pascual, B. Egressy, C. Meister, R. Cotterell, R. Wattenhofer,\nA plug-and-play method for controlled text generation, in:\nEMNLP Findings, 2021, pp. 3346\u20133361. doi:10.18653/v1/\n2021.findings-emnlp.334 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.09707",
                        "Citation Paper Title": "Title:A Plug-and-Play Method for Controlled Text Generation",
                        "Citation Paper Abstract": "Abstract:Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.",
                        "Citation Paper Authors": "Authors:Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "combine a PLM with a sim-\nple attribute classifier consisting of a user-specified bag\nof words. This classifier alters the latent representations\nof input words and pushes the PLM\u2019s hidden activation\nto guide the generation. PlotMachine ",
                    "Citation Text": "H. Rashkin, A. Celikyilmaz, Y. Choi, J. Gao, PlotMachines:\nOutline-conditioned generation with dynamic plot state track-\ning, in: EMNLP, 2020, pp. 4274\u20134295. doi:10.18653/v1/2020.\nemnlp-main.349 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14967",
                        "Citation Paper Title": "Title:PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking",
                        "Citation Paper Abstract": "Abstract:We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
                        "Citation Paper Authors": "Authors:Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". Such models has been applied\nto neural machine translation and significantly speed up the\n3Structured Knowledge-\nenhanced Story GenerationStructured Knowledge\nas TextFinetuning PLMsKEG ",
                    "Citation Text": "J. Guan, F. Huang, Z. Zhao, X. Zhu, M. Huang, A knowledge-\nenhanced pretraining model for commonsense story generation,\nTACL 8 (2020) 93\u2013108. doi:10.1162/tacl_a_00302 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.05139",
                        "Citation Paper Title": "Title:A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation",
                        "Citation Paper Abstract": "Abstract:Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.",
                        "Citation Paper Authors": "Authors:Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang"
                    }
                },
                {
                    "Sentence ID": 142,
                    "Sentence": "NN-based \u2191 An unreferenced metric for evaluating open-world story generation.\nMAUVE ",
                    "Citation Text": "K. Pillutla, S. Swayamdipta, R. Zellers, J. Thickstun, S. Welleck,\nY. Choi, Z. Harchaoui, Mauve: Measuring the gap between\nneural text and human text using divergence frontiers, Advances\nin Neural Information Processing Systems 34 (2021) 4816\u20134828.\ndoi:10.48550/arXiv.2102.01454 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.01454",
                        "Citation Paper Title": "Title:MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers",
                        "Citation Paper Abstract": "Abstract:As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.",
                        "Citation Paper Authors": "Authors:Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaid Harchaoui"
                    }
                },
                {
                    "Sentence ID": 141,
                    "Sentence": "NN-based \u2191 Evaluate semantic similarity between two pieces of texts which are encoded by Bert.\nUNION ",
                    "Citation Text": "J. Guan, M. Huang, UNION: An Unreferenced Metric for\nEvaluating Open-ended Story Generation, in: EMNLP, 2020,\npp. 9157\u20139166. doi:10.18653/v1/2020.emnlp-main.736 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07602",
                        "Citation Paper Title": "Title:UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation",
                        "Citation Paper Abstract": "Abstract:Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable unreferenced metric for evaluating open-ended story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.",
                        "Citation Paper Authors": "Authors:Jian Guan, Minlie Huang"
                    }
                },
                {
                    "Sentence ID": 127,
                    "Sentence": "Crowd-sourcing,\nATOMIC ,\nConceptNet1,331,113 638 ,127\n(43,958)23 inferential (X gets X\u2019s car repaired ,happens\nafter,X drives an old car ),\n(money ,has property ,earned by\nworking )\nGLUCOSE ",
                    "Citation Text": "N. Mostafazadeh, A. Kalyanpur, L. Moon, D. Buchanan,\nL. Berkowitz, O. Biran, J. Chu-Carroll, GLUCOSE: Gener-\naLized and COntextualized story explanations, in: EMNLP,\n2020, pp. 4569\u20134586. doi:10.18653/v1/2020.emnlp-main.370 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07758",
                        "Citation Paper Title": "Title:GLUCOSE: GeneraLized and COntextualized Story Explanations",
                        "Citation Paper Abstract": "Abstract:When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of ~670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.",
                        "Citation Paper Authors": "Authors:Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, Jennifer Chu-Carroll"
                    }
                },
                {
                    "Sentence ID": 126,
                    "Sentence": "Stories, Books,\nGoogle Ngrams,\nWiktionary Idioms877,108 309 ,515\n(24,313)9 inferential (PersonX adopts a dog ,xEffect ,\nPersonX then smiles ),\n(PersonX smiles at PersonY ,\noReact ,PersonY feel happy )\nAtomic-2020 ",
                    "Citation Text": "J. D. Hwang, C. Bhagavatula, R. L. Bras, J. Da, K. Sakaguchi,\nA. Bosselut, Y. Choi, Comet-atomic 2020: On symbolic and\nneural commonsense knowledge graphs, in: AAAI, Vol. 35,\n2021, pp. 6384\u20136392. doi:10.48550/arXiv.2010.05953 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.05953",
                        "Citation Paper Title": "Title:COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs",
                        "Citation Paper Abstract": "Abstract:Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.\nIn this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.\nWith this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains ~12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.",
                        "Citation Paper Authors": "Authors:Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 125,
                    "Sentence": "is created from\nvarious online study guides. It contains literary pieces and\ntheir summaries paired with descriptions of characters that\nappear in them. STORAL ",
                    "Citation Text": "J. Guan, Z. Liu, M. Huang, A corpus for understanding and\ngenerating moral stories, in: NAACL, 2022, pp. 5069\u20135087.doi:10.18653/v1/2022.naacl-main.374 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.09438",
                        "Citation Paper Title": "Title:A Corpus for Understanding and Generating Moral Stories",
                        "Citation Paper Abstract": "Abstract:Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.",
                        "Citation Paper Authors": "Authors:Jian Guan, Ziqi Liu, Minlie Huang"
                    }
                },
                {
                    "Sentence ID": 123,
                    "Sentence": "contains diary-like short\nstories of recalled and imagined events. These stories are\nalso written by human writers. TVRecap dataset ",
                    "Citation Text": "M. Chen, K. Gimpel, Tvrecap: A dataset for generating stories\nwith character descriptions (2021). doi:10.48550/arxiv.2109.\n08833 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.08833",
                        "Citation Paper Title": "Title:TVStoryGen: A Dataset for Generating Stories with Character Descriptions",
                        "Citation Paper Abstract": "Abstract:We introduce TVStoryGen, a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVStoryGen contains stories that are authored by professional screen-writers and that feature complex interactions among multiple characters. Generating stories in TVStoryGen requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, we propose to train reverse models on our dataset for evaluating the faithfulness of generated stories. We create TVStoryGen from fan-contributed websites, which allows us to collect 26k episode recaps with 1868.7 tokens on average. Empirically, we take a hierarchical story generation approach and find that the neural model that uses oracle content selectors for character descriptions demonstrates the best performance on automatic metrics, showing the potential of our dataset to inspire future research on story generation with constraints. Qualitative analysis shows that the best-performing model sometimes generates content that is unfaithful to the short summaries, suggesting promising directions for future work.",
                        "Citation Paper Authors": "Authors:Mingda Chen, Kevin Gimpel"
                    }
                },
                {
                    "Sentence ID": 120,
                    "Sentence": "collects coherent\nand fluent passages of human-written online stories. These\nstories are prompted by given topics or cues. Fairy Tales\nandMystery datasets ",
                    "Citation Text": "P. Ammanabrolu, W. Cheung, D. Tu, W. Broniec, M. Riedl,\nBringing stories alive: Generating interactive fiction worlds,\nin: AIIDE, Vol. 16, 2020, pp. 3\u20139. doi:10.48550/arXiv.2001.\n10161 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.10161",
                        "Citation Paper Title": "Title:Bringing Stories Alive: Generating Interactive Fiction Worlds",
                        "Citation Paper Abstract": "Abstract:World building forms the foundation of any task that requires narrative intelligence. In this work, we focus on procedurally generating interactive fiction worlds---text-based worlds that players \"see\" and \"talk to\" using natural language. Generating these worlds requires referencing everyday and thematic commonsense priors in addition to being semantically consistent, interesting, and coherent throughout. Using existing story plots as inspiration, we present a method that first extracts a partial knowledge graph encoding basic information regarding world structure such as locations and objects. This knowledge graph is then automatically completed utilizing thematic knowledge and used to guide a neural language generation model that fleshes out the rest of the world. We perform human participant-based evaluations, testing our neural model's ability to extract and fill-in a knowledge graph and to generate language conditioned on it against rule-based and human-made baselines. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, Mark O. Riedl"
                    }
                },
                {
                    "Sentence ID": 118,
                    "Sentence": "is the first dataset for the sequential vision-to-language\ntask. It contains sequences of images where each image is\ndescribed with an event and sequential images constitute a\ncoherent story. ROCStories dataset ",
                    "Citation Text": "N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra,\nL. Vanderwende, P. Kohli, J. Allen, A corpus and cloze eval-\nuation for deeper understanding of commonsense stories, in:\nNAACL, 2016, pp. 839\u2013849. doi:10.18653/v1/N16-1098 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01696",
                        "Citation Paper Title": "Title:A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories",
                        "Citation Paper Abstract": "Abstract:Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",
                        "Citation Paper Authors": "Authors:Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "to\nfind all character coreference. With the extracted events\nand character coreference, the plot lines can be built upon.\nMartin et al. ",
                    "Citation Text": "L. Martin, P. Ammanabrolu, X. Wang, W. Hancock, S. Singh,\nB. Harrison, M. Riedl, Event representations for automated\nstory generation with deep neural nets, in: AAAI, Vol. 32, 2018.\ndoi:10.48550/arXiv.1706.01331 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01331",
                        "Citation Paper Title": "Title:Event Representations for Automated Story Generation with Deep Neural Nets",
                        "Citation Paper Abstract": "Abstract:Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.",
                        "Citation Paper Authors": "Authors:Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, Mark O. Riedl"
                    }
                },
                {
                    "Sentence ID": 101,
                    "Sentence": "to extract relational triples as events and use\na pre-trained neural coreference resolution model ",
                    "Citation Text": "K. Clark, C. D. Manning, Deep reinforcement learning for\nmention-ranking coreference models, in: EMNLP, 2016, pp.\n2256\u20132262. doi:10.18653/v1/D16-1245 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08667",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning for Mention-Ranking Coreference Models",
                        "Citation Paper Abstract": "Abstract:Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": ". A knowl-\nedge retriever then uses the extracted keywords and queries\nConceptNet to retrieve related knowledge sentences. After\nobtaining these sentences, it further utilizes BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding, in: NAACL, 2019, pp. 4171\u20134186. doi:10.18653/v1/\nN19-1423 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "Wikipedia\nFigure 2: A methodological taxonomy of methods on structured knowledge-enhanced story generation. Yellow boxes denote proposed\n(sub)categories. Green boxes denote existing works under their respective subcategories. Light blue boxes are the structured knowledge that\neach method utilizes.\ngeneration process ",
                    "Citation Text": "Y. Xiao, L. Wu, J. Guo, J. Li, M. Zhang, T. Qin, T.-y. Liu, A\nsurvey on non-autoregressive generation for neural machine\ntranslation and beyond (2022). doi:10.48550/arXiv.2204.\n09269 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.09269",
                        "Citation Paper Title": "Title:A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
                        "Citation Paper Abstract": "Abstract:Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications. The web page of this survey is at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, Tie-yan Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.00301v3": {
            "Paper Title": "Learning to Select from Multiple Options",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.08501v4": {
            "Paper Title": "Think Before You Speak: Explicitly Generating Implicit Commonsense\n  Knowledge for Response Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.17546v3": {
            "Paper Title": "Preventing Verbatim Memorization in Language Models Gives a False Sense\n  of Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.04553v2": {
            "Paper Title": "Can Deep Neural Networks Predict Data Correlations from Column Names?",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ". They form the basis for this study as well. Other\napplications of language models in the context of databases include\ndata discovery and integration [ 21,30] as well as data preparation\ntasks ",
                    "Citation Text": "Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam\nMadden, and Mourad Ouzzani. 2021. Rpt: Relational pre-trained transformer is\nalmost all you need towards democratizing data preparation. PVLDB 14, 8 (2021),\n1254\u20131261. https://doi.org/10.14778/3457390.3457391 arXiv:2012.02469",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02469",
                        "Citation Paper Title": "Title:RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation",
                        "Citation Paper Abstract": "Abstract:Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising auto-encoder for tuple-to-X models (X could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation.",
                        "Citation Paper Authors": "Authors:Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, Mourad Ouzzani"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ", have recently led to significant advances on a multitude\nof NLP tasks ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In EMNLP . 38\u201345. https://doi.org/10.18653/v1/2020.emnlp-demos.6\narXiv:arXiv:1910.03771v5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04476v2": {
            "Paper Title": "Discover, Explanation, Improvement: An Automatic Slice Detection\n  Framework for Natural Language Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09172v2": {
            "Paper Title": "Deep Emotion Recognition in Textual Conversations: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 97,
                    "Sentence": "is used.\nFinally a linear unit predicts the emotion distributions. This work achieves an F1-score of 66.96% in the IEMOCAP\ndataset.\n4.9.7 DAG-ERC ",
                    "Citation Text": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan. Directed acyclic graph network for conversational\nemotion recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages\n1551\u20131560, Online, August 2021. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.12907",
                        "Citation Paper Title": "Title:Directed Acyclic Graph Network for Conversational Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC.",
                        "Citation Paper Authors": "Authors:Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "* Memory Network, GRU, Attention 62.43\nGraph Neural Network (GNN)\nGhosal et al., 2019 ",
                    "Citation Text": "Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, and Alexander Gelbukh. DialogueGCN:\nA graph convolutional neural network for emotion recognition in conversation. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) , pages 154\u2013164, Hong Kong, China, November 2019.\nAssociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.11540",
                        "Citation Paper Title": "Title:DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.",
                        "Citation Paper Authors": "Authors:Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, Alexander Gelbukh"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Transformer, LSTM, Hierarchical, Attention 77.09\nfine-tuned pre-trained language models. Another viable way to perform context modelling is to feed several appended\nutterances to the pre-trained language model ",
                    "Citation Text": "Patr\u00edcia Pereira, Helena Moniz, Isabel Dias, and Joao Paulo Carvalho. Context-dependent embedding utterance\nrepresentations for emotion recognition in conversations. In Proceedings of the 13th Workshop on Computational\nApproaches to Subjectivity, Sentiment, & Social Media Analysis , pages 228\u2013236. Association for Computational\nLinguistics, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08216",
                        "Citation Paper Title": "Title:Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations",
                        "Citation Paper Abstract": "Abstract:Emotion Recognition in Conversations (ERC) has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pre-trained transformer language models. In our approach, we feed the conversational context appended to the utterance to be classified as input to the RoBERTa encoder, to which we append a simple classification module, thus discarding the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context. We also investigate how the number of introduced conversational turns influences our model performance. The effectiveness of our approach is validated on the open-domain DailyDialog dataset and on the task-oriented EmoWOZ dataset.",
                        "Citation Paper Authors": "Authors:Patr\u00edcia Pereira, Helena Moniz, Isabel Dias, Joao Paulo Carvalho"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "Transformer 59.20\n18Deep Emotion Recognition in Textual Conversations: A Survey\nTable 8: Summary of publications on Emotion Recognition in Conversations, reporting the methods used along with\ntheir performance.\nAuthor, Year Methods Dataset, Metric\nTransformer Friends, w-F1\nHuang et al., 2019 ",
                    "Citation Text": "Yen-Hao Huang, Ssu-Rui Lee, Mau-Yun Ma, Yi-Hsin Chen, Ya-Wen Yu, and Yi-Shin Chen. Emotionx-idea:\nEmotion bert\u2013an affectional model for conversation. arXiv preprint arXiv:1908.06264 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.06264",
                        "Citation Paper Title": "Title:EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation",
                        "Citation Paper Abstract": "Abstract:In this paper, we investigate the emotion recognition ability of the pre-training language model, namely BERT. By the nature of the framework of BERT, a two-sentence structure, we adapt BERT to continues dialogue emotion prediction tasks, which rely heavily on the sentence-level context-aware understanding. The experiments show that by mapping the continues dialogue into a causal utterance pair, which is constructed by the utterance and the reply utterance, models can better capture the emotions of the reply utterance. The present method has achieved 0.815 and 0.885 micro F1 score in the testing dataset of Friends and EmotionPush, respectively.",
                        "Citation Paper Authors": "Authors:Yen-Hao Huang, Ssu-Rui Lee, Mau-Yun Ma, Yi-Hsin Chen, Ya-Wen Yu, Yi-Shin Chen"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "Transformer, Contrastive Learning 69.74 67.25 40.94\nLi et al., 2022 ",
                    "Citation Text": "Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu. EmoCaps: Emotion capsule based model for conver-\nsational emotion recognition. In Findings of the Association for Computational Linguistics: ACL 2022 , pages\n1610\u20131618, Dublin, Ireland, May 2022. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13504",
                        "Citation Paper Title": "Title:EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Zaijing Li, Fengxiao Tang, Ming Zhao, Yusen Zhu"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": "Transformer, GNN 68.93 64.17 40.05 64.18\nMau et al., 2021 ",
                    "Citation Text": "Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao, and Xuan Li. DialogueTRM: Exploring multi-modal\nemotional dynamics in a conversation. In Findings of the Association for Computational Linguistics: EMNLP\n2021 , pages 2694\u20132704, Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07637",
                        "Citation Paper Title": "Title:DialogueTRM: Exploring the Intra- and Inter-Modal Emotional Behaviors in the Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion Recognition in Conversations (ERC) is essential for building empathetic human-machine systems. Existing studies on ERC primarily focus on summarizing the context information in a conversation, however, ignoring the differentiated emotional behaviors within and across different modalities. Designing appropriate strategies that fit the differentiated multi-modal emotional behaviors can produce more accurate emotional predictions. Thus, we propose the DialogueTransformer to explore the differentiated emotional behaviors from the intra- and inter-modal perspectives. For intra-modal, we construct a novel Hierarchical Transformer that can easily switch between sequential and feed-forward structures according to the differentiated context preference within each modality. For inter-modal, we constitute a novel Multi-Grained Interactive Fusion that applies both neuron- and vector-grained feature interactions to learn the differentiated contributions across all modalities. Experimental results show that DialogueTRM outperforms the state-of-the-art by a significant margin on three benchmark datasets.",
                        "Citation Paper Authors": "Authors:Yuzhao Mao, Qi Sun, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li, Jianping Shen"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "Transformer, GNN 68.73 66.18 46.11 59.76\nLiang et al., 2022 ",
                    "Citation Text": "Chen Liang, Jing Xu, Yangkun Lin, Chong Yang, and Yongliang Wang. S+PAGE: A speaker and position-aware\ngraph neural network model for emotion recognition in conversation. In Proceedings of the 2nd Conference\nof the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers) , pages 148\u2013157, Online only, November\n2022. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.12389",
                        "Citation Paper Title": "Title:S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) has attracted much attention in recent years for its necessity in widespread applications. Existing ERC methods mostly model the self and inter-speaker context separately, posing a major issue for lacking enough interaction between them. In this paper, we propose a novel Speaker and Position-Aware Graph neural network model for ERC (S+PAGE), which contains three stages to combine the benefits of both Transformer and relational graph convolution network (R-GCN) for better contextual modeling. Firstly, a two-stream conversational Transformer is presented to extract the coarse self and inter-speaker contextual features for each utterance. Then, a speaker and position-aware conversation graph is constructed, and we propose an enhanced R-GCN model, called PAG, to refine the coarse features guided by a relative positional encoding. Finally, both of the features from the former two stages are input into a conditional random field layer to model the emotion transfer.",
                        "Citation Paper Authors": "Authors:Chen Liang, Chong Yang, Jing Xu, Juyang Huang, Yongliang Wang, Yang Dong"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "Graph Neural Network, Bi-LSTM, Attention 66.61 58.45\nHu et al., 2022 ",
                    "Citation Text": "Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, and Yang Mo. Mm-dfn: Multimodal dynamic fusion\nnetwork for emotion recognition in conversations. In ICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pages 7037\u20137041. IEEE, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02385",
                        "Citation Paper Title": "Title:MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations",
                        "Citation Paper Abstract": "Abstract:Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal contextual features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of MM-DFN.",
                        "Citation Paper Authors": "Authors:Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, Yang Mo"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "Graph Neural Network, Attention 65.22 60.91 34.42 54.31\nHu et al., 2021 ",
                    "Citation Text": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin. MMGCN: Multimodal fusion via deep graph convolution\nnetwork for emotion recognition in conversation. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 5666\u20135675, Online, August 2021. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06779",
                        "Citation Paper Title": "Title:MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users' emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.",
                        "Citation Paper Authors": "Authors:Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.08975v2": {
            "Paper Title": "Detecting Text Formality: A Study of Text Classification Approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09537v2": {
            "Paper Title": "Less is More: A Lightweight and Robust Neural Architecture for Discourse\n  Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05798v2": {
            "Paper Title": "BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08913v2": {
            "Paper Title": "Claim Optimization in Computational Argumentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.12191v4": {
            "Paper Title": "Kernelized Concept Erasure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.01824v2": {
            "Paper Title": "A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine\n  Reading Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10313v2": {
            "Paper Title": "Beyond Triplet: Leveraging the Most Data for Multimodal Machine\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10802v3": {
            "Paper Title": "Combing for Credentials: Active Pattern Extraction from Smart Reply",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nWhile much of the privacy focus has been on language\nmodels trained with auto-regressive objective, ",
                    "Citation Text": "Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and By-\nron C Wallace. Does bert pretrained on clinical notes reveal sensitive\ndata? arXiv:2104.07762 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07762",
                        "Citation Paper Title": "Title:Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?",
                        "Citation Paper Abstract": "Abstract:Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \"attacks\" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at this https URL",
                        "Citation Paper Authors": "Authors:Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C. Wallace"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nOn the other hand, large language models have been\nshown to leak information in different forms (e.g. training\ndata extraction ",
                    "Citation Text": "Nicholas Carlini, Florian Tram `er, Eric Wallace, Matthew Jagielski,\nAriel Herbert-V oss, Katherine Lee, Adam Roberts, Tom Brown, Dawn\nSong, \u00b4Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting\ntraining data from large language models. In 30th USENIX Security\nSymposium , pages 2633\u20132650, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.07805",
                        "Citation Paper Title": "Title:Extracting Training Data from Large Language Models",
                        "Citation Paper Abstract": "Abstract:It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "is orders of magnitudes higher than\nours for their attacks to be effective (0.3% vs our 0.005%).\nComparison with ",
                    "Citation Text": "Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas\nWutschitz, and Santiago Zanella-B \u00b4eguelin. Analyzing leak-\nage of personally identifiable information in language models.\narXiv:2302.00539 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00539",
                        "Citation Paper Title": "Title:Analyzing Leakage of Personally Identifiable Information in Language Models",
                        "Citation Paper Abstract": "Abstract:Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at this https URL.",
                        "Citation Paper Authors": "Authors:Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B\u00e9guelin"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "have transformed the field of natural language\nprocessing. These pre-trained models can be fine-tuned on\na wide range of downstream tasks to provide impressive\nperformance and unprecedented abilities so far ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\nNAACL-HLT \u201919, pages 4171\u20134186, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "investigates the trade-off between memorization\nand model size, data sample repetition and the context on\nwhich the extraction is aimed. Relevantly, recent work has\nshown that deduplicating training data mitigates privacy\nrisks ",
                    "Citation Text": "Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicat-\ning training data mitigates privacy risks in language models.\narXiv:2202.06539 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.06539",
                        "Citation Paper Title": "Title:Deduplicating Training Data Mitigates Privacy Risks in Language Models",
                        "Citation Paper Abstract": "Abstract:Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.",
                        "Citation Paper Authors": "Authors:Nikhil Kandpal, Eric Wallace, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "demonstrates that Model API access to both pre-trained and\nfine-tuned versions of a language model can be exploited\nby an adversary to extract sensitive sequences from the\ntypically more sensitive fine-tuning dataset. ",
                    "Citation Text": "Huseyin A Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones,\nVictor R \u00a8uhle, James Withers, and Robert Sim. Training data leakage\nanalysis in language models. arXiv:2101.05405 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05405",
                        "Citation Paper Title": "Title:Training Data Leakage Analysis in Language Models",
                        "Citation Paper Abstract": "Abstract:Recent advances in neural network based language models lead to successful deployments of such models, improving user experience in various applications. It has been demonstrated that strong performance of language models comes along with the ability to memorize rare training samples, which poses serious privacy threats in case the model is trained on confidential user content. In this work, we introduce a methodology that investigates identifying the user content in the training data that could be leaked under a strong and realistic threat model. We propose two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data. Our metrics further enable comparing different models trained on the same data in terms of privacy. We demonstrate our approach through extensive numerical studies on both RNN and Transformer based models. We further illustrate how the proposed metrics can be utilized to investigate the efficacy of mitigations like differentially private training or API hardening.",
                        "Citation Paper Authors": "Authors:Huseyin A. Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor R\u00fchle, James Withers, Robert Sim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.08142v3": {
            "Paper Title": "Semantic Representations of Mathematical Expressions in a Continuous\n  Vector Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.12636v3": {
            "Paper Title": "A Zipf's Law-based Text Generation Approach for Addressing Imbalance in\n  Entity Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13861v2": {
            "Paper Title": "A New Multifractal-based Deep Learning Model for Text Mining",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": "Li, X., & Ning, H. (2020, September). Deep Pyramid Convolutional Neural Network Integrated with Self -attention \nMechanism and Highway Network for Text Classification. In Journa l of Physics: Conference Series (Vol. 1642, No. 1, \np. 012008). IOP Publishing. ",
                    "Citation Text": "Qin, X., Zhou, Y., Guo, Y., Wu, D., Tian, Z., Jiang, N., ... & Wang, W. (2021, October). Mask is all you need: \nRethinking mask r -cnn for dense and arbitrary -shaped scene text de tection. In Proceedings of the 29th ACM \nInternational Conference on Multimedia (pp. 414 -423).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.03426",
                        "Citation Paper Title": "Title:Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection",
                        "Citation Paper Abstract": "Abstract:Due to the large success in object detection and instance segmentation, Mask R-CNN attracts great attention and is widely adopted as a strong baseline for arbitrary-shaped scene text detection and spotting. However, two issues remain to be settled. The first is dense text case, which is easy to be neglected but quite practical. There may exist multiple instances in one proposal, which makes it difficult for the mask head to distinguish different instances and degrades the performance. In this work, we argue that the performance degradation results from the learning confusion issue in the mask head. We propose to use an MLP decoder instead of the \"deconv-conv\" decoder in the mask head, which alleviates the issue and promotes robustness significantly. And we propose instance-aware mask learning in which the mask head learns to predict the shape of the whole instance rather than classify each pixel to text or non-text. With instance-aware mask learning, the mask branch can learn separated and compact masks. The second is that due to large variations in scale and aspect ratio, RPN needs complicated anchor settings, making it hard to maintain and transfer across different datasets. To settle this issue, we propose an adaptive label assignment in which all instances especially those with extreme aspect ratios are guaranteed to be associated with enough anchors. Equipped with these components, the proposed method named MAYOR achieves state-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500, ICDAR2015, CTW1500, and Total-Text.",
                        "Citation Paper Authors": "Authors:Xugong Qin, Yu Zhou, Youhui Guo, Dayan Wu, Zhihong Tian, Ning Jiang, Hongbin Wang, Weiping Wang"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ". HAZOP \nreports are a repository of rich and insightful information about hazard events,  and classifying them  is conducive to the \noverall planning and decision -making for industry development ",
                    "Citation Text": "Wang, Z., Wang, B., Ren, M., & Gao, D. (2023). A new hazard event classificati on model via deep learning and \nmultifractal. Computers in Industry, 147, 103875.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.05263",
                        "Citation Paper Title": "Title:A new hazard event classification model via deep learning and multifractal",
                        "Citation Paper Abstract": "Abstract:Hazard and operability analysis (HAZOP) is the paradigm of industrial safety that can reveal the hazards of process from its node deviations, consequences, causes, measures and suggestions, and such hazards can be considered as hazard events (HaE). The classification research on HaE has much irreplaceable pragmatic values. In this paper, we present a novel deep learning model termed DLF through multifractal to explore HaE classification where the motivation is that HaE can be naturally regarded as a kind of time series. Specifically, first HaE is vectorized to get HaE time series by employing BERT. Then, a new multifractal analysis method termed HmF-DFA is proposed to win HaE fractal series by analyzing HaE time series. Finally, a new hierarchical gating neural network (HGNN) is designed to process HaE fractal series to accomplish the classification of HaE from three aspects: severity, possibility and risk. We take HAZOP reports of 18 processes as cases, and launch the experiments on this basis. Results demonstrate that compared with other classifiers, DLF classifier performs better under metrics of precision, recall and F1-score, especially for the severity aspect. Also, HmF-DFA and HGNN effectively promote HaE classification. Our HaE classification system can serve application incentives to experts, engineers, employees, and other enterprises. We hope our research can contribute added support to the daily practice in industrial safety.",
                        "Citation Paper Authors": "Authors:Zhenhua Wang, Bin Wang, Ming Ren, Dong Gao"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "Wang, J., & Shao, W. (2021). Multifractal analysis with detrending weighted average algorithm of historical volatility. \nFractals, 29(05), 2150193. ",
                    "Citation Text": "Dubey, S. R., Singh, S. K., & Chaudhuri, B. B. (2022). Activation functions  in deep learning: A comprehensive survey \nand benchmark. Neurocomputing.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.14545",
                        "Citation Paper Title": "Title:Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
                        "Citation Paper Abstract": "Abstract:Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Shiv Ram Dubey, Satish Kumar Singh, Bidyut Baran Chaudhuri"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "Hazrina, S., Sharef, N. M., Ibrahim, H., Murad, M. A. A., & Noah, S. A. M. (2017). Review on the advancements of \ndisambiguation in seman tic question answering system. Information Processing & Management, 53(1), 52 -69. ",
                    "Citation Text": "Agrawal, S., Roy, D., & Mitra, M. (2021). Tag embedding based personalized point of interest recommendation system. \nInformation Processing & Management, 58(6), 102690.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06389",
                        "Citation Paper Title": "Title:Tag Embedding Based Personalized Point Of Interest Recommendation System",
                        "Citation Paper Abstract": "Abstract:Personalized Point of Interest recommendation is very helpful for satisfying users' needs at new places. In this article, we propose a tag embedding based method for Personalized Recommendation of Point Of Interest. We model the relationship between tags corresponding to Point Of Interest. The model provides representative embedding corresponds to a tag in a way that related tags will be closer. We model Point of Interest-based on tag embedding and also model the users (user profile) based on the Point Of Interest rated by them. finally, we rank the user's candidate Point Of Interest based on cosine similarity between user's embedding and Point of Interest's embedding. Further, we find the parameters required to model user by discrete optimizing over different measures (like ndcg@5, MRR, ...). We also analyze the result while considering the same parameters for all users and individual parameters for each user. Along with it we also analyze the effect on the result while changing the dataset to model the relationship between tags. Our method also minimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase 2 dataset and have significant improvement over all the measures on the state of the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%, which shows the effectiveness of the method.",
                        "Citation Paper Authors": "Authors:Suraj Agrawal, Dwaipayan Roy, Mandar Mitra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.04053v3": {
            "Paper Title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of\n  Text-to-Image Generation Models",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "ob-\nject detector. We initialize DETR parameters from the of-\nficial checkpoint with ResNet101 ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In CVPR ,\n2016. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "evaluate\nthe diversity and inclusiveness of images containing peo-\nple of specific occupations with respect to gender and race.\n[81, 86, 85, 9, 7] investigate biases in image-text retrieval\nmodels. Bansal et al. ",
                    "Citation Text": "Hritik Bansal, Da Yin, and Masoud Monajatipoor. How\nwell can Text-to-Image Generative Models understand Eth-\nical Natural Language Interventions? In EMNLP , 2022. 3,\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.15230",
                        "Citation Paper Title": "Title:How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?",
                        "Citation Paper Abstract": "Abstract:Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., 'a photo of a lawyer'). Following Zhao et al. (2021), we study the effect on the diversity of the generated images when adding ethical intervention that supports equitable judgment (e.g., 'if all individuals can be a lawyer irrespective of their gender') in the input prompts. To this end, we introduce an Ethical NaTural Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset to evaluate the change in image generations conditional on ethical interventions across three social axes -- gender, skin color, and culture. Through ENTIGEN framework, we find that the generations from minDALL.E, DALL.E-mini and Stable Diffusion cover diverse social groups while preserving the image quality. Preliminary studies indicate that a large change in the model predictions is triggered by certain phrases such as 'irrespective of gender' in the context of gender bias in the ethical interventions. We release code and annotated data at this https URL.",
                        "Citation Paper Authors": "Authors:Hritik Bansal, Da Yin, Masoud Monajatipoor, Kai-Wei Chang"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": ". To measure\nimage-text alignment, metrics based on retrieval, caption-\ning, and object detection models have been proposed. R-\nprecision ",
                    "Citation Text": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe\nGan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-\nGrained Text to Image Generation with Attentional Genera-\ntive Adversarial Networks. In CVPR , 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10485",
                        "Citation Paper Title": "Title:AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",
                        "Citation Paper Authors": "Authors:Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "that\nmostly contains single-object images. Therefore, they are\nnot suitable for more complex datasets ",
                    "Citation Text": "Stanislav Frolov, Tobias Hinz, Federico Raue, J \u00a8orn Hees, and\nAndreas Dengel. Adversarial Text-to-Image Synthesis: A\nReview. Neural Networks , 144:187\u2013209, jan 2021. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.09983",
                        "Citation Paper Title": "Title:Adversarial Text-to-Image Synthesis: A Review",
                        "Citation Paper Abstract": "Abstract:With the advent of generative adversarial networks, synthesizing images from textual descriptions has recently become an active research area. It is a flexible and intuitive way for conditional image generation with significant progress in the last years regarding visual realism, diversity, and semantic alignment. However, the field still faces several challenges that require further research efforts such as enabling the generation of high-resolution images with multiple objects, and developing suitable and reliable evaluation metrics that correlate with human judgement. In this review, we contextualize the state of the art of adversarial text-to-image synthesis models, their development since their inception five years ago, and propose a taxonomy based on the level of supervision. We critically examine current strategies to evaluate text-to-image synthesis models, highlight shortcomings, and identify new areas of research, ranging from the development of better datasets and evaluation metrics to possible improvements in architectural design and model training. This review complements previous surveys on generative adversarial networks with a focus on text-to-image synthesis which we believe will help researchers to further advance the field.",
                        "Citation Paper Authors": "Authors:Stanislav Frolov, Tobias Hinz, Federico Raue, J\u00f6rn Hees, Andreas Dengel"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "Text-to-Image Generation Models. [54, 64] pioneered\ndeep learning-based text-to-image generation. ",
                    "Citation Text": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-\ngeswaran, Bernt Schiele, and Honglak Lee. Generative ad-\nversarial text to image synthesis. In ICML , 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.05396",
                        "Citation Paper Title": "Title:Generative Adversarial Text to Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
                        "Citation Paper Authors": "Authors:Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.02423v2": {
            "Paper Title": "CLSE: Corpus of Linguistically Significant Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05994v4": {
            "Paper Title": "A Survey of Knowledge Enhanced Pre-trained Language Models",
            "Sentences": [
                {
                    "Sentence ID": 93,
                    "Sentence": ", and the other is retrieve,\nrerank and rewrite method which focuses on re-ranking\nretrieved items for generation ",
                    "Citation Text": "B. Kim, J. Ahn, and G. Kim, \u201cSequential latent knowledge selec-\ntion for knowledge-grounded dialogue,\u201d in ICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.07510",
                        "Citation Paper Title": "Title:Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue",
                        "Citation Paper Abstract": "Abstract:Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).",
                        "Citation Paper Authors": "Authors:Byeongchang Kim, Jaewoo Ahn, Gunhee Kim"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": ".\nIncorporating this type of knowledge into PLMs can facil-\nitate the demonstration of reasoning path via its good in-\nterpretability. For example, RuleBERT ",
                    "Citation Text": "M. Saeed, N. Ahmadi, P . Nakov, and P . Papotti, \u201cRulebert: Teach-\ning soft rules to pre-trained language models,\u201d in EMNLP , 2021,\npp. 1460\u20131476.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.13006",
                        "Citation Paper Title": "Title:RuleBert: Teaching Soft Rules to Pre-trained Language Models",
                        "Citation Paper Abstract": "Abstract:While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",
                        "Citation Paper Authors": "Authors:Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "adopts contrastive learning\nand prompt learning to integrate entity knowledge.\nThe second is to change the attention mechanism of the\nmodel. For example, LUKE ",
                    "Citation Text": "I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto,\n\u201cLUKE: deep contextualized entity representations with entity-\naware self-attention,\u201d in EMNLP , 2020, pp. 6442\u20136454.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01057",
                        "Citation Paper Title": "Title:LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
                        "Citation Paper Abstract": "Abstract:Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "introduces an entity-aware\nself-attention mechanism to capture the entity information\nin calculating attention scores.\nThe third is to change the model structure ",
                    "Citation Text": "Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, \u201cERNIE:\nenhanced language representation with informative entities,\u201d in\nACL , 2019, pp. 1441\u20131451.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07129",
                        "Citation Paper Title": "Title:ERNIE: Enhanced Language Representation with Informative Entities",
                        "Citation Paper Abstract": "Abstract:Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from this https URL.",
                        "Citation Paper Authors": "Authors:Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "conduct the pre-training process on large-scale scientific\ndomain corpora and achieve promising results on down-\nstream academic NLP tasks. S2ORC-BERT ",
                    "Citation Text": "K. Lo, L. L. Wang, M. Neumann, R. Kinney, and D. S. Weld,\n\u201cS2ORC: the semantic scholar open research corpus,\u201d in ACL ,\n2020, pp. 4969\u20134983.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02782",
                        "Citation Paper Title": "Title:S2ORC: The Semantic Scholar Open Research Corpus",
                        "Citation Paper Abstract": "Abstract:We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",
                        "Citation Paper Authors": "Authors:Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S. Weld"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "incorporates large-scale out-of-domain commonsense\ndescriptions to enhance the representation of input text.\nKformer ",
                    "Citation Text": "Y. Yao, S. Huang, L. Dong, F. Wei, H. Chen, and N. Zhang,\n\u201cKformer: Knowledge injection in transformer feed-forward lay-\ners,\u201d in NLPCC , vol. 13551, 2022, pp. 131\u2013143.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.05742",
                        "Citation Paper Title": "Title:Kformer: Knowledge Injection in Transformer Feed-Forward Layers",
                        "Citation Paper Abstract": "Abstract:Recent days have witnessed a diverse set of knowledge injection models for pre-trained language models (PTMs); however, most previous studies neglect the PTMs' own ability with quantities of implicit knowledge stored in parameters. A recent study has observed knowledge neurons in the Feed Forward Network (FFN), which are responsible for expressing factual knowledge. In this work, we propose a simple model, Kformer, which takes advantage of the knowledge stored in PTMs and external knowledge via knowledge injection in Transformer FFN layers. Empirically results on two knowledge-intensive tasks, commonsense reasoning (i.e., SocialIQA) and medical question answering (i.e., MedQA-USMLE), demonstrate that Kformer can yield better performance than other knowledge injection technologies such as concatenation or attention-based injection. We think the proposed simple model and empirical findings may be helpful for the community to develop more powerful knowledge injection methods. Code available in this https URL.",
                        "Citation Paper Authors": "Authors:Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, Ningyu Zhang"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "fuse knowledge in the fine-\ntuning stage of PLMs for improving task performance.\n3.2 Incorporating Text Knowledge into PLMs\nText knowledge is usually retrieved from general-domain\ntext collection (such as WikiText ",
                    "Citation Text": "U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis, \u201cGeneralization through memorization: Nearest neigh-\nbor language models,\u201d in ICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00172",
                        "Citation Paper Title": "Title:Generalization through Memorization: Nearest Neighbor Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                        "Citation Paper Authors": "Authors:Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "selects\nthe nearest K neighbors from training samples as knowl-\nedge incorporated into PLM, and its earlier idea comes\nfrom cache-LM ",
                    "Citation Text": "E. Grave, A. Joulin, and N. Usunier, \u201cImproving neural language\nmodels with a continuous cache,\u201d in ICLR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.04426",
                        "Citation Paper Title": "Title:Improving Neural Language Models with a Continuous Cache",
                        "Citation Paper Abstract": "Abstract:We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
                        "Citation Paper Authors": "Authors:Edouard Grave, Armand Joulin, Nicolas Usunier"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", language models can\nbe trained on a large amount of raw textual data to ob-\ntain general-purpose representations. Then, the pre-trained\nmodels will be applied to various downstream tasks by fine-\ntuning them through task-specific objective functions ",
                    "Citation Text": "P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,\n\u201cPre-train, prompt, and predict: A systematic survey of prompt-\ning methods in natural language processing,\u201d arXiv preprint\narXiv:2107.13586 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.13586",
                        "Citation Paper Title": "Title:Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website this http URL including constantly-updated survey, and paperlist.",
                        "Citation Paper Authors": "Authors:Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "unifies three language modeling\nobjectives, which can be adapted to NLU and NLG tasks\nsimultaneously.\nAs the Transformer architecture with multi-head self-\nattention mechanism is put forward ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\ninNIPS , 2017, pp. 5998\u20136008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10003v2": {
            "Paper Title": "(QA)$^2$: Question Answering with Questionable Assumptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.06313v4": {
            "Paper Title": "A Deep Convolutional Neural Networks Based Multi-Task Ensemble Model for\n  Aspect and Polarity Classification in Persian Reviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09153v2": {
            "Paper Title": "An Empirical Investigation of the Role of Pre-training in Lifelong\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06767v3": {
            "Paper Title": "Parameter-Efficient Finetuning for Robust Continual Multilingual\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02552v2": {
            "Paper Title": "Explaining Machine Learning Models in Natural Conversations: Towards a\n  Conversational XAI Agent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07562v3": {
            "Paper Title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for\n  Multilingual Tweet Representations at Twitter",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "inject entities and their relations\nfrom knowledge graphs to augment the pre-training corpus. OAG-\nBERT ",
                    "Citation Text": "Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, P. Zhang, Hongxia Yang,\nYuxiao Dong, and Jie Tang. 2022. OAG-BERT: Towards a Unified Backbone\nLanguage Model for Academic Knowledge Services. Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.02410",
                        "Citation Paper Title": "Title:OAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services",
                        "Citation Paper Abstract": "Abstract:Academic knowledge services have substantially facilitated the development of the science enterprise by providing a plenitude of efficient research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand scientific contents, hindering deployments into real products. To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. Its zero-shot capability furthers the path to mitigate the need of expensive annotations. OAG-BERT has been deployed for real-world applications, such as the reviewer recommendation function for National Nature Science Foundation of China (NSFC) -- one of the largest funding agencies in China -- and paper tagging in AMiner. All codes and pre-trained models are available via the CogDL toolkit.",
                        "Citation Paper Authors": "Authors:Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia Yang, Yuxiao Dong, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Another avenue\nof improvement has been improving the training objectives used\nto train PLMs. A broad spectrum of pre-training objectives have\nbeen explored with different levels of success, Notable examples\ninclude masked language modeling ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nInProceedings of the 2019 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill\nBurstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-\ntional Linguistics, 4171\u20134186. https://doi.org/10.18653/v1/n19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.06264v3": {
            "Paper Title": "A Simplified Variant of G\u00f6del's Ontological Argument",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08806v2": {
            "Paper Title": "Grammar-Based Grounded Lexicon Learning",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", is a meaningful direction. Finally,\nfuture work may consider integrating G2L2 with program-synthesis algorithms ",
                    "Citation Text": "Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt,\nArmando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable\nknowledge with wake-sleep bayesian program learning. arXiv:2006.08381 , 2020. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.08381",
                        "Citation Paper Title": "Title:DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning",
                        "Citation Paper Abstract": "Abstract:Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.",
                        "Citation Paper Authors": "Authors:Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Joshua B. Tenenbaum"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". Second, meta-learning models that can leverage learned words to bootstrap\nthe learning of novel words, such as syntactic bootstrapping ",
                    "Citation Text": "Jon Gauthier, Roger Levy, and Joshua B Tenenbaum. Word learning and the acquisition of syntactic\u2013\nsemantic overhypotheses. In CogSci , 2018. 2, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04988",
                        "Citation Paper Title": "Title:Word learning and the acquisition of syntactic--semantic overhypotheses",
                        "Citation Paper Abstract": "Abstract:Children learning their first language face multiple problems of induction: how to learn the meanings of words, and how to build meaningful phrases from those words according to syntactic rules. We consider how children might solve these problems efficiently by solving them jointly, via a computational model that learns the syntax and semantics of multi-word utterances in a grounded reference game. We select a well-studied empirical case in which children are aware of patterns linking the syntactic and semantic properties of words --- that the properties picked out by base nouns tend to be related to shape, while prenominal adjectives tend to refer to other properties such as color. We show that children applying such inductive biases are accurately reflecting the statistics of child-directed speech, and that inducing similar biases in our computational model captures children's behavior in a classic adjective learning experiment. Our model incorporating such biases also demonstrates a clear data efficiency in learning, relative to a baseline model that learns without forming syntax-sensitive overhypotheses of word meaning. Thus solving a more complex joint inference problem may make the full problem of language acquisition easier, not harder.",
                        "Citation Paper Authors": "Authors:Jon Gauthier, Roger Levy, Joshua B. Tenenbaum"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", or only focus on inducing syntactic structures such\nas phrase-structure grammar ",
                    "Citation Text": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu. Visually grounded neural syntax acquisition.\nInACL, 2019. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02890",
                        "Citation Paper Title": "Title:Visually Grounded Neural Syntax Acquisition",
                        "Citation Paper Abstract": "Abstract:We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.",
                        "Citation Paper Authors": "Authors:Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.05337v5": {
            "Paper Title": "A Survey of Controllable Text Generation using Transformer-based\n  Pre-trained Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08471v5": {
            "Paper Title": "Improving Semantic Matching through Dependency-Enhanced Pre-trained\n  Model with Adaptive Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08986v2": {
            "Paper Title": "Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be\n  Imitated?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01368v3": {
            "Paper Title": "PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.11342v5": {
            "Paper Title": "Dive into Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05534v3": {
            "Paper Title": "PreSTU: Pre-Training for Scene-Text Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09778v4": {
            "Paper Title": "I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Supervision",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". Pre-trained\nlanguage models have been shown to learn skills that can\ntransfer to new modalities ",
                    "Citation Text": "Kevin Lu, Aditya Grover, P. Abbeel, and Igor Mordatch.\nPretrained transformers as universal computation engines.\nArXiv , abs/2103.05247, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.05247",
                        "Citation Paper Title": "Title:Pretrained Transformers as Universal Computation Engines",
                        "Citation Paper Abstract": "Abstract:We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",
                        "Citation Paper Authors": "Authors:Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ", or using a pre-\ntrained model to generate text describing an image to pass\ninto the language model ",
                    "Citation Text": "Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-\nmanski, Adrian Wong, Stefan Welker, Federico Tombari,\nAveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny\nLee, Vincent Vanhoucke, and Pete Florence. Socratic mod-els: Composing zero-shot multimodal reasoning with lan-\nguage. arXiv , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.00598",
                        "Citation Paper Title": "Title:Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                        "Citation Paper Abstract": "Abstract:Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
                        "Citation Paper Authors": "Authors:Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "Using Contrastive Models. Many vision and lan-\nguage contrastive models have been constructed, including\nCLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.01175v2": {
            "Paper Title": "A Part-of-Speech Tagger for Yiddish",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09699v4": {
            "Paper Title": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "with in-context learning. Then we conduct ablation experi-\nments on the contribution of each component, showing that\nPROMPT CAPis giving consistent gains over generic cap-\ntions. In addition, experiments on WebQA ",
                    "Citation Text": "Yingshan Chang, Guihong Cao, Mridu Narang, Jianfeng Gao,\nHisami Suzuki, and Yonatan Bisk. Webqa: Multihop and\nmultimodal qa. 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , Jun 2022. 2, 5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.00590",
                        "Citation Paper Title": "Title:WebQA: Multihop and Multimodal QA",
                        "Citation Paper Abstract": "Abstract:Scaling Visual Question Answering (VQA) to the open-domain and multi-hop nature of web searches, requires fundamental advances in visual representation learning, knowledge aggregation, and language generation. In this work, we introduce WebQA, a challenging new benchmark that proves difficult for large-scale state-of-the-art models which lack language groundable visual representations for novel objects and the ability to reason, yet trivial for humans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose sources to aggregate, and 3) Produce a fluent language response. This is the behavior we should be expecting from IoT devices and digital assistants. Existing work prefers to assume that a model can either reason about knowledge in images or in text. WebQA includes a secondary text-only QA task to ensure improved visual performance does not come at the cost of language understanding. Our challenge for the community is to create unified multimodal reasoning models that answer questions regardless of the source modality, moving us closer to digital assistants that not only query language knowledge, but also the richer visual online world.",
                        "Citation Paper Authors": "Authors:Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, Yonatan Bisk"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "to evaluate\nPROMPT CAP\u2019s generalization ability on images and tasks\nfrom different domains. WebQA images are crawled from\nthe web and are from domains different from the COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740\u2013755.\nSpringer, 2014. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Caption + Tags + Feature GPT-3 (175B) + Wikidata 54.4\nREVIVE (Single) ",
                    "Citation Text": "Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chen-\nguang Zhu, and Lu Yuan. Revive: Regional visual representa-\ntion matters in knowledge-based visual question answering.\nArXiv , abs/2206.01201, 2022. 1, 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.01201",
                        "Citation Paper Title": "Title:REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:This paper revisits visual representation in knowledge-based visual question answering (VQA) and demonstrates that using regional information in a better way can significantly improve the performance. While visual representation is extensively studied in traditional VQA, it is under-explored in knowledge-based VQA even though these two tasks share the common spirit, i.e., rely on visual input to answer the question. Specifically, we observe that in most state-of-the-art knowledge-based VQA methods: 1) visual features are extracted either from the whole image or in a sliding window manner for retrieving knowledge, and the important relationship within/among object regions is neglected; 2) visual features are not well utilized in the final answering model, which is counter-intuitive to some extent. Based on these observations, we propose a new knowledge-based VQA method REVIVE, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. The key motivation is that object regions and inherent relationship are important for knowledge-based VQA. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous state-of-the-art method by a large margin (+3.6%). We also conduct detailed analysis and show the necessity of regional information in different framework components for knowledge-based VQA. Code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, Lu Yuan"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "Caption + Feature GPT-3 (175B) + Wikidata 58.0\nIn-Context Learning & Zero-Shot\nBLIP-2 VIT-G FlanT5 XXL ",
                    "Citation Text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. ArXiv ,\nabs/2301.12597, 2023. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12597",
                        "Citation Paper Title": "Title:BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Citation Paper Abstract": "Abstract:The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.15445v3": {
            "Paper Title": "Efficient Utilization of Large Pre-Trained Models for Low Resource ASR",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "Unsupervised approaches have gained popularity since\nthey have shown a potential of high performance with only\nlittle annotated data ",
                    "Citation Text": "A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J.\nEdin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L.\nMaal\u00f8e, et al., \u201cSelf-supervised speech representation\nlearning: A review,\u201d IEEE Journal of Selected Topics in\nSignal Processing , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.10643",
                        "Citation Paper Title": "Title:Self-Supervised Speech Representation Learning: A Review",
                        "Citation Paper Abstract": "Abstract:Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.",
                        "Citation Paper Authors": "Authors:Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D. Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maal\u00f8e, Tara N. Sainath, Shinji Watanabe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.02743v3": {
            "Paper Title": "Integrating Knowledge Graph embedding and pretrained Language Models in\n  Hypercomplex Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08637v3": {
            "Paper Title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09095v2": {
            "Paper Title": "Rethinking the Role of Scale for In-Context Learning: An\n  Interpretability-based Case Study at 66 Billion Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16198v4": {
            "Paper Title": "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": "that was recently revisited in the task of semantic\nsegmentation ",
                    "Citation Text": "Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Re-\ntrieve and co-segment for zero-shot transfer. arXiv preprint\narXiv:2206.07045 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.07045",
                        "Citation Paper Title": "Title:ReCo: Retrieve and Co-segment for Zero-shot Transfer",
                        "Citation Paper Abstract": "Abstract:Semantic segmentation has a broad range of applications, but its real-world impact has been significantly limited by the prohibitive annotation costs necessary to enable deployment. Segmentation methods that forgo supervision can side-step these costs, but exhibit the inconvenient requirement to provide labelled examples from the target distribution to assign concept names to predictions. An alternative line of work in language-image pre-training has recently demonstrated the potential to produce models that can both assign names across large vocabularies of concepts and enable zero-shot transfer for classification, but do not demonstrate commensurate segmentation abilities. In this work, we strive to achieve a synthesis of these two approaches that combines their strengths. We leverage the retrieval abilities of one such language-image pre-trained model, CLIP, to dynamically curate training sets from unlabelled images for arbitrary collections of concept names, and leverage the robust correspondences offered by modern image representations to co-segment entities among the resulting collections. The synthetic segment collections are then employed to construct a segmentation model (without requiring pixel labels) whose knowledge of concepts is inherited from the scalable pre-training process of CLIP. We demonstrate that our approach, termed Retrieve and Co-segment (ReCo) performs favourably to unsupervised segmentation approaches while inheriting the convenience of nameable predictions and zero-shot transfer. We also demonstrate ReCo's ability to generate specialist segmenters for extremely rare objects.",
                        "Citation Paper Authors": "Authors:Gyungin Shin, Weidi Xie, Samuel Albanie"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "to first generate\ncustomised textual prompts for each class by prompting\nGPT-3 ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems , 33:1877\u20131901, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.08063v5": {
            "Paper Title": "NECE: Narrative Event Chain Extraction Toolkit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07920v2": {
            "Paper Title": "PInKS: Preconditioned Commonsense Inference with Minimal Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.08712v3": {
            "Paper Title": "PaCo: Preconditions Attributed to Commonsense Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07340v5": {
            "Paper Title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in\n  Mathematics Education",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "The state-of-the-art language model BERT (Bidirectional Encoder\nRepresentations From Transformer) ",
                    "Citation Text": "Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nThe Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , Vol. 1. 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "25M review sentiment\nIMDB [8, 27] 14.6M review sentiment\nLinguisticsVUA-20 ",
                    "Citation Text": "Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee, Dong-\nwon Lee, and Jongwuk Lee. 2021. MelBERT : Metaphor Detection via Contextu-\nalized Late Interaction using Metaphorical Identification Theories. In Proceedings\nof the Annual Conference of the North American Chapter of the Association for\nComputational Linguistics .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13615",
                        "Citation Paper Title": "Title:MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories",
                        "Citation Paper Abstract": "Abstract:Automated metaphor detection is a challenging task to identify metaphorical expressions of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to distinguish between the contextual and literal meaning of words. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.",
                        "Citation Paper Authors": "Authors:Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee, Dongwon Lee, Jongwuk Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.00463v2": {
            "Paper Title": "Mismatching-Aware Unsupervised Translation Quality Estimation For\n  Low-Resource Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01066v3": {
            "Paper Title": "What Can Transformers Learn In-Context? A Case Study of Simple Function\n  Classes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06817v2": {
            "Paper Title": "RT-1: Robotics Transformer for Real-World Control at Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16878v3": {
            "Paper Title": "Transformers are Short Text Classifiers: A Study of Inductive Short Text\n  Classifiers on Benchmarks and Real-world Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07306v6": {
            "Paper Title": "Constraining Linear-chain CRFs to Regular Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00732v3": {
            "Paper Title": "Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". There also has been a growing interest in\nusing MKGs for various applications, such as question answering,\nrecommendation, and few-shot learning. In ",
                    "Citation Text": "Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and Qi Wug. 2022. MuKEA:\nMultimodal Knowledge Extraction and Accumulation for Knowledge-based Vi-\nsual Question Answering. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09138",
                        "Citation Paper Title": "Title:MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Knowledge-based visual question answering requires the ability of associating external knowledge for open-ended cross-modal scene understanding. One limitation of existing solutions is that they capture relevant knowledge from text-only knowledge bases, which merely contain facts expressed by first-order predicates or language descriptions while lacking complex but indispensable multimodal knowledge for visual understanding. How to construct vision-relevant and explainable multimodal knowledge for the VQA scenario has been less studied. In this paper, we propose MuKEA to represent multimodal knowledge by an explicit triplet to correlate visual objects and fact answers with implicit relations. To bridge the heterogeneous gap, we propose three objective losses to learn the triplet representations from complementary views: embedding structure, topological relation and semantic space. By adopting a pre-training and fine-tuning learning strategy, both basic and domain-specific multimodal knowledge are progressively accumulated for answer prediction. We outperform the state-of-the-art by 3.35% and 6.08% respectively on two challenging knowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the complementary benefits of the multimodal knowledge with existing knowledge bases and the advantages of our end-to-end framework over the existing pipeline methods. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, Qi Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11671v2": {
            "Paper Title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10302v3": {
            "Paper Title": "One Shot Learning as Instruction Data Prospector for Large Language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "demonstrated\nremarkably strong performance by strategically selecting a thousand high-quality data points for\nlearning. InstructMining ",
                    "Citation Text": "Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selection for\nlarge language models. arXiv preprint arXiv:2307.06290 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.06290",
                        "Citation Paper Title": "Title:Instruction Mining: When Data Mining Meets Large Language Model Finetuning",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) are initially pretrained for broad capabilities and then finetuned with instruction-following datasets to improve their performance in interacting with humans. Despite advances in finetuning, a standardized guideline for selecting high-quality datasets to optimize this process remains elusive. In this paper, we first propose InstructMining, an innovative method designed for automatically selecting premium instruction-following data for finetuning LLMs. Specifically, InstructMining utilizes natural language indicators as a measure of data quality, applying them to evaluate unseen datasets. During experimentation, we discover that double descent phenomenon exists in large language model finetuning. Based on this observation, we further leverage BlendSearch to help find the best subset among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show that InstructMining-7B achieves state-of-the-art performance on two of the most popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",
                        "Citation Paper Authors": "Authors:Yihan Cao, Yanbin Kang, Chi Wang, Lichao Sun"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "progressively modified the original instruction in a step-by-step manner, allowing\nfor precise control over the difficulty and complexity of the generated instructions. Tree-Instruct ",
                    "Citation Text": "Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin L Zhang.\nA preliminary study of the intrinsic relationship between complexity and alignment. arXiv preprint\narXiv:2308.05696 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.05696",
                        "Citation Paper Title": "Title:A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment",
                        "Citation Paper Abstract": "Abstract:Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \\textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new instruction data based on the modified tree. By adjusting the number of added nodes, we can control the difficulty level in the modified instruction data. Our preliminary experiments reveal the following insights: (1) Increasing complexity consistently leads to sustained performance improvements. For instance, using 1,000 instruction data and 10 nodes resulted in a substantial 24\\% increase in win rate. (2) Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3) Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.",
                        "Citation Paper Authors": "Authors:Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, Nevin L. Zhang"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "selected a handful of instances randomly from the initial task pool and used them as demonstrations to\ninstruct a language model in generating new instructions along with corresponding input-output pairs.\nEvol-Instruct ",
                    "Citation Text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint\narXiv:2304.12244 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.12244",
                        "Citation Paper Title": "Title:WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                        "Citation Paper Abstract": "Abstract:Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",
                        "Citation Paper Authors": "Authors:Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "took the initiative to delve into open-domain\ninstruction tuning using the open-source LLM LLaMA ",
                    "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "introduced the novel\nconcept of instruction tuning , aiming to improve zero shot task performance by transforming NLP\ntasks into natural language instructions during model training. Furthermore, InstructGPT ",
                    "Citation Text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. In NeurIPS , pages 27730\u201327744, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02155",
                        "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                        "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "pioneered the initial effort of training various natural\nlanguage processing (NLP) tasks in a unified text-to-text format. FLAN ",
                    "Citation Text": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01652",
                        "Citation Paper Title": "Title:Finetuned Language Models Are Zero-Shot Learners",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                        "Citation Paper Authors": "Authors:Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "Instruction Tuning Recent works have introduced a series of techniques that aim at refining\nlarge language models (LLMs), showcasing their ability to generalize effectively to instructions not\nencountered before. For instance, T5 ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research , 21(1):5485\u20135551, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.05300v3": {
            "Paper Title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.08372v2": {
            "Paper Title": "Hierarchical Aligned Multimodal Learning for NER on Tweet Posts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06374v2": {
            "Paper Title": "UstanceBR: a multimodal language resource for stance prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.04589v3": {
            "Paper Title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14033v2": {
            "Paper Title": "T-Eval: Evaluating the Tool Utilization Capability Step by Step",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17432v2": {
            "Paper Title": "Video Understanding with Large Language Models: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 190,
                    "Sentence": ". The SumMe dataset features shorter, user-generated videos covering various activities\nlike holidays and sports. Its focus on diverse and unstructured content, accompanied by human-created\nsummary annotations, makes it ideal for algorithms dealing with varied video types.\nVideoXum ",
                    "Citation Text": "Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Jiebo Luo. Videoxum:\nCross-modal visual and textural summarization of videos. arXiv preprint arXiv:2303.12060 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.12060",
                        "Citation Paper Title": "Title:VideoXum: Cross-modal Visual and Textural Summarization of Videos",
                        "Citation Paper Abstract": "Abstract:Video summarization aims to distill the most important information from a source video to produce either an abridged clip or a textual narrative. Traditionally, different methods have been proposed depending on whether the output is a video or text, thus ignoring the correlation between the two semantically related tasks of visual summarization and textual summarization. We propose a new joint video and text summarization task. The goal is to generate both a shortened video clip along with the corresponding textual summary from a long video, collectively referred to as a cross-modal summary. The generated shortened video clip and text narratives should be semantically well aligned. To this end, we first build a large-scale human-annotated dataset -- VideoXum (X refers to different modalities). The dataset is reannotated based on ActivityNet. After we filter out the videos that do not meet the length requirements, 14,001 long videos remain in our new dataset. Each video in our reannotated dataset has human-annotated video summaries and the corresponding narrative summaries. We then design a novel end-to-end model -- VTSUM-BILP to address the challenges of our proposed task. Moreover, we propose a new metric called VT-CLIPScore to help evaluate the semantic consistency of cross-modality summary. The proposed model achieves promising performance on this new task and establishes a benchmark for future research.",
                        "Citation Paper Authors": "Authors:Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, Jiebo Luo"
                    }
                },
                {
                    "Sentence ID": 187,
                    "Sentence": ". A dataset of 2,000 cooking videos from YouTube, annotated with step-by-step\ninstructions, specifically designed for procedural understanding in the cooking domain.\nMovieNet ",
                    "Citation Text": "Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset for movie\nunderstanding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August\n23\u201328, 2020, Proceedings, Part IV 16 , pages 709\u2013727. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.10937",
                        "Citation Paper Title": "Title:MovieNet: A Holistic Dataset for Movie Understanding",
                        "Citation Paper Abstract": "Abstract:Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet -- a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at this https URL.",
                        "Citation Paper Authors": "Authors:Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 186,
                    "Sentence": ". Over 100 million uncurated instructional videos, uniquely corrected for narration-\nvideo mismatch using MIL-NCE, providing a robust dataset for various video understanding tasks.\nMovie Audio Descriptions (MAD) ",
                    "Citation Text": "Mattia Soldan, Alejandro Pardo, Juan Le\u00f3n Alc\u00e1zar, Fabian Caba, Chen Zhao, Silvio Giancola, and\nBernard Ghanem. Mad: A scalable dataset for language grounding in videos from movie audio descrip-\ntions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n5026\u20135035, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.00431",
                        "Citation Paper Title": "Title:MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions",
                        "Citation Paper Abstract": "Abstract:The recent and increasing interest in video-language research has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing the fitness of these datasets for the video-language grounding task. Recent works have begun to discover significant limitations in these datasets, suggesting that state-of-the-art techniques commonly overfit to hidden dataset biases. In this work, we present MAD (Movie Audio Descriptions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies. MAD contains over 384,000 natural language sentences grounded in over 1,200 hours of videos and exhibits a significant reduction in the currently diagnosed biases for video-language grounding datasets. MAD's collection strategy enables a novel and more challenging version of video-language grounding, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released MAD's data and baselines code at this https URL.",
                        "Citation Paper Authors": "Authors:Mattia Soldan, Alejandro Pardo, Juan Le\u00f3n Alc\u00e1zar, Fabian Caba Heilbron, Chen Zhao, Silvio Giancola, Bernard Ghanem"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ",\nto gain a huge improvement on the UCF-101 and HMDB51 datasets. Subsequently, people began\nemploying the Kinetics-400 (K-400) ",
                    "Citation Text": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06950",
                        "Citation Paper Title": "Title:The Kinetics Human Action Video Dataset",
                        "Citation Paper Abstract": "Abstract:We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
                        "Citation Paper Authors": "Authors:Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 166,
                    "Sentence": ". The LLMV A-GEBC model, designed for Generic Event Boundary Captioning\n(GEBC) ",
                    "Citation Text": "Yuxuan Wang, Difei Gao, Licheng Yu, Weixian Lei, Matt Feiszli, and Mike Zheng Shou. Geb+: A\nbenchmark for generic event boundary captioning, grounding and retrieval. In European Conference on\nComputer Vision , pages 709\u2013725. Springer, 2022.\n33",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.00486",
                        "Citation Paper Title": "Title:GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval",
                        "Citation Paper Abstract": "Abstract:Cognitive science has shown that humans perceive videos in terms of events separated by the state changes of dominant subjects. State changes trigger new events and are one of the most useful among the large amount of redundant information perceived. However, previous research focuses on the overall understanding of segments without evaluating the fine-grained status changes inside. In this paper, we introduce a new dataset called Kinetic-GEB+. The dataset consists of over 170k boundaries associated with captions describing status changes in the generic events in 12K videos. Upon this new dataset, we propose three tasks supporting the development of a more fine-grained, robust, and human-like understanding of videos through status changes. We evaluate many representative baselines in our dataset, where we also design a new TPD (Temporal-based Pairwise Difference) Modeling method for visual difference and achieve significant performance improvements. Besides, the results show there are still formidable challenges for current methods in the utilization of different granularities, representation of visual difference, and the accurate localization of status changes. Further analysis shows that our dataset can drive developing more powerful methods to understand status changes and thus improve video level comprehension. The dataset is available at this https URL",
                        "Citation Paper Authors": "Authors:Yuxuan Wang, Difei Gao, Licheng Yu, Stan Weixian Lei, Matt Feiszli, Mike Zheng Shou"
                    }
                },
                {
                    "Sentence ID": 184,
                    "Sentence": ". This dataset centers on everyday household activities. It stands out due to its\nrealistic settings and the complexity of overlapping activities in its videos.\nYouTube8M ",
                    "Citation Text": "Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadara-\njan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.08675",
                        "Citation Paper Title": "Title:YouTube-8M: A Large-Scale Video Classification Benchmark",
                        "Citation Paper Abstract": "Abstract:Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets.\nIn this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download.\nWe trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.",
                        "Citation Paper Authors": "Authors:Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": ". Speech, on the other hand, is handled by a\nspecialized speech encoder, typically a pre-trained speech recognition model like Whisper ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. In International Conference on Machine Learning ,\npages 28492\u201328518. PMLR, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04356",
                        "Citation Paper Title": "Title:Robust Speech Recognition via Large-Scale Weak Supervision",
                        "Citation Paper Abstract": "Abstract:We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 119,
                    "Sentence": "2048 CLIP ViT-G/14, EV A-CLIP /times-circle /times-circle Q-former+Long/short-Term Memory - GPT-3.5/Claude - 07/2023\nFA VOR ",
                    "Citation Text": "Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao\nZhang. Fine-grained audio-visual joint representations for multimodal large language models, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.05863",
                        "Citation Paper Title": "Title:Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models",
                        "Citation Paper Abstract": "Abstract:Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at this https URL, and the training code and model checkpoints will be released soon.",
                        "Citation Paper Authors": "Authors:Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang"
                    }
                },
                {
                    "Sentence ID": 170,
                    "Sentence": ") to process primary and supplementary visual\nfeatures. The model generates video query tokens enhanced with boundary embeddings and positional\nencodings. For caption generation, it utilizes an LLM, specifically OPT ",
                    "Citation Text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv:2205.01068 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01068",
                        "Citation Paper Title": "Title:OPT: Open Pre-trained Transformer Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                        "Citation Paper Authors": "Authors:Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 167,
                    "Sentence": ", uniquely combines advanced feature extractors with an LLM for precise video\ncaptioning. It employs CLIP-ViTG ",
                    "Citation Text": "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12104\u2013\n12113, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.04560",
                        "Citation Paper Title": "Title:Scaling Vision Transformers",
                        "Citation Paper Abstract": "Abstract:Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
                        "Citation Paper Authors": "Authors:Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer"
                    }
                },
                {
                    "Sentence ID": 124,
                    "Sentence": ". VLog (Video as a Long Document) utilizes a collection of pretrained models to record and\ninterpret visual and audio information from videos. This suite includes BLIP2 for image captioning,\nGRIT ",
                    "Citation Text": "Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.\nGrit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.00280",
                        "Citation Paper Title": "Title:GRiT: A Generative Region-to-text Transformer for Object Understanding",
                        "Citation Paper Abstract": "Abstract:This paper presents a Generative RegIon-to-Text transformer, GRiT, for object understanding. The spirit of GRiT is to formulate object understanding as <region, text> pairs, where region locates objects and text describes objects. For example, the text in object detection denotes class names while that in dense captioning refers to descriptive sentences. Specifically, GRiT consists of a visual encoder to extract image features, a foreground object extractor to localize objects, and a text decoder to generate open-set object descriptions. With the same model architecture, GRiT can understand objects via not only simple nouns, but also rich descriptive sentences including object attributes or actions. Experimentally, we apply GRiT to object detection and dense captioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for object detection and 15.5 mAP on Visual Genome for dense captioning. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang"
                    }
                },
                {
                    "Sentence ID": 117,
                    "Sentence": "Varying 7 Task-Specific Video Encoders /times-circle /times-circle Linear Layer - GPT-2, T5, OPT, LLaMA 1.5B/6.5B/6.7B/7B 05/2023\nVideo-LLaMA ",
                    "Citation Text": "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding. arXiv preprint arXiv:2306.02858 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.02858",
                        "Citation Paper Title": "Title:Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
                        "Citation Paper Abstract": "Abstract:We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
                        "Citation Paper Authors": "Authors:Hang Zhang, Xin Li, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 137,
                    "Sentence": ", MSR-\nVTT, MUSIC-A VQA, ActivityNet-QA, MSR-VTT-QA, TGIF-FrameQA, DiDeMo, Flickr ",
                    "Citation Text": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\nmodels. In Proceedings of the IEEE international conference on computer vision , pages 2641\u20132649,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04870",
                        "Citation Paper Title": "Title:Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
                        "Citation Paper Abstract": "Abstract:The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.",
                        "Citation Paper Authors": "Authors:Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik"
                    }
                },
                {
                    "Sentence ID": 126,
                    "Sentence": "to generate detailed descriptions of these clips. These\ndescriptions are integrated into a comprehensive script using GPT-4, facilitating a deep understanding\nof the video. For evaluation, MM-VID was tested on various datasets, including Ego4D ",
                    "Citation Text": "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar,\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,\npages 18995\u201319012, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.07058",
                        "Citation Paper Title": "Title:Ego4D: Around the World in 3,000 Hours of Egocentric Video",
                        "Citation Paper Abstract": "Abstract:We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "100 CLIP ViT-L/14@224 /times-circle/check-circle Transformer Encoder 64 TPU v4 T5 0.2B 02/2023\nVideo ChatCaptioner ",
                    "Citation Text": "Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, and Mohamed Elhoseiny. Video chatcaptioner:\nTowards the enriched spatiotemporal descriptions. arXiv preprint arXiv:2304.04227 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.04227",
                        "Citation Paper Title": "Title:Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions",
                        "Citation Paper Abstract": "Abstract:Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing more visual details about the videos. The code is publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": "focuses on surveying video captioning and video action recognition tasks, respectively. Other\nvideo understanding tasks such as the video question answering and grounding are not considered.\nMoreover, ",
                    "Citation Text": "Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. A\nsurvey on video diffusion models. arXiv preprint arXiv:2310.10647 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.10647",
                        "Citation Paper Title": "Title:A Survey on Video Diffusion Models",
                        "Citation Paper Abstract": "Abstract:The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "surveys multimodal foundation\nmodels for general vision-language tasks, which includes both image and video applications. ",
                    "Citation Text": "Moloud Abdar, Meenakshi Kollati, Swaraja Kuraparthi, Farhad Pourpanah, Daniel McDuff, Mohammad\nGhavamzadeh, Shuicheng Yan, Abduallah Mohamed, Abbas Khosravi, Erik Cambria, et al. A review of\ndeep learning for video captioning. arXiv preprint arXiv:2304.11431 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.11431",
                        "Citation Paper Title": "Title:A Review of Deep Learning for Video Captioning",
                        "Citation Paper Abstract": "Abstract:Video captioning (VC) is a fast-moving, cross-disciplinary area of research that bridges work in the fields of computer vision, natural language processing (NLP), linguistics, and human-computer interaction. In essence, VC involves understanding a video and describing it with language. Captioning is used in a host of applications from creating more accessible interfaces (e.g., low-vision navigation) to video question answering (V-QA), video retrieval and content generation. This survey covers deep learning-based VC, including but, not limited to, attention-based architectures, graph networks, reinforcement learning, adversarial networks, dense video captioning (DVC), and more. We discuss the datasets and evaluation metrics used in the field, and limitations, applications, challenges, and future directions for VC.",
                        "Citation Paper Authors": "Authors:Moloud Abdar, Meenakshi Kollati, Swaraja Kuraparthi, Farhad Pourpanah, Daniel McDuff, Mohammad Ghavamzadeh, Shuicheng Yan, Abduallah Mohamed, Abbas Khosravi, Erik Cambria, Fatih Porikli"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": ". The emergence of large language models pre-trained on extensive datasets has\nintroduced a novel in-context learning capability ",
                    "Citation Text": "Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, and Jianhua Yao. Dnagpt: A\ngeneralized pretrained tool for multiple dna sequence analysis tasks. bioRxiv , pages 2023\u201307, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.05628",
                        "Citation Paper Title": "Title:DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks",
                        "Citation Paper Abstract": "Abstract:Pre-trained large language models demonstrate potential in extracting information from DNA sequences, yet adapting to a variety of tasks and data modalities remains a challenge. To address this, we propose DNAGPT, a generalized DNA pre-training model trained on over 200 billion base pairs from all mammals. By enhancing the classic GPT model with a binary classification task (DNA sequence order), a numerical regression task (guanine-cytosine content prediction), and a comprehensive token language, DNAGPT can handle versatile DNA analysis tasks while processing both sequence and numerical data. Our evaluation of genomic signal and region recognition, mRNA abundance regression, and artificial genomes generation tasks demonstrates DNAGPT's superior performance compared to existing models designed for specific downstream tasks, benefiting from pre-training using the newly designed model structure.",
                        "Citation Paper Authors": "Authors:Daoan Zhang, Weitong Zhang, Yu Zhao, Jianguo Zhang, Bing He, Chenchen Qin, Jianhua Yao"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ").\nLarge Language Models for Video Understanding . Recently, large language models (LLMs) have\nadvanced rapidly ",
                    "Citation Text": "Hanjia Lyu, Jinfa Huang, Daoan Zhang, Yongsheng Yu, Xinyi Mou, Jinsheng Pan, Zhengyuan Yang,\nZhongyu Wei, and Jiebo Luo. Gpt-4v (ision) as a social media analysis engine. arXiv preprint\narXiv:2311.07547 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.07547",
                        "Citation Paper Title": "Title:GPT-4V(ision) as A Social Media Analysis Engine",
                        "Citation Paper Abstract": "Abstract:Recent research has offered insights into the extraordinary capabilities of Large Multimodal Models (LMMs) in various general vision and language tasks. There is growing interest in how LMMs perform in more specialized domains. Social media content, inherently multimodal, blends text, images, videos, and sometimes audio. Understanding social multimedia content remains a challenging problem for contemporary machine learning frameworks. In this paper, we explore GPT-4V(ision)'s capabilities for social multimedia analysis. We select five representative tasks, including sentiment analysis, hate speech detection, fake news identification, demographic inference, and political ideology detection, to evaluate GPT-4V. Our investigation begins with a preliminary quantitative analysis for each task using existing benchmark datasets, followed by a careful review of the results and a selection of qualitative samples that illustrate GPT-4V's potential in understanding multimodal social media content. GPT-4V demonstrates remarkable efficacy in these tasks, showcasing strengths such as joint understanding of image-text pairs, contextual and cultural awareness, and extensive commonsense knowledge. Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context of evolving celebrity and politician knowledge, reflecting the known hallucination problem. The insights gleaned from our findings underscore a promising future for LMMs in enhancing our comprehension of social media content and its users through the analysis of multimodal information.",
                        "Citation Paper Authors": "Authors:Hanjia Lyu, Jinfa Huang, Daoan Zhang, Yongsheng Yu, Xinyi Mou, Jinsheng Pan, Zhengyuan Yang, Zhongyu Wei, Jiebo Luo"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "for video understanding allows them to generalize across diverse tasks with minimal\nadditional labeling, overcoming the early deep learning models\u2019 need for extensive task-specific\ndata. VideoBERT ",
                    "Citation Text": "Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. In Proceedings of the IEEE/CVF international conference\non computer vision , pages 7464\u20137473, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01766",
                        "Citation Paper Title": "Title:VideoBERT: A Joint Model for Video and Language Representation Learning",
                        "Citation Paper Abstract": "Abstract:Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",
                        "Citation Paper Authors": "Authors:Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "tend to attain\nhigh efficiency. The introduction of Vision Transformers (ViT) ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nTo improve the efficiency, the 3D network has been decomposed into 2D and 1D networks in various\nstudies (e.g., S3D ",
                    "Citation Text": "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal\nfeature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European\nConference on Computer Vision (ECCV) , September 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.04851",
                        "Citation Paper Title": "Title:Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
                        "Citation Paper Abstract": "Abstract:Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).",
                        "Citation Paper Authors": "Authors:Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "were also adapted from 2D to 3D, resulting in the emergence of R3D ",
                    "Citation Text": "Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Learning spatio-temporal features with 3d residual\nnetworks for action recognition. In Proceedings of the IEEE International Conference on Computer\nVision (ICCV) Workshops , Oct 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07632",
                        "Citation Paper Title": "Title:Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatio-temporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of 3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "utilizes the initialization and the architecture of 2D CNN, Inception ",
                    "Citation Text": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 1\u20139, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "combined both CNN and IDT\n3to capture the motion information to improve the performance, which verified the capability of deep\nneural networks for video understanding. To handle long-form video understanding, Long Short-Term\nMemory (LSTM) was adopted ",
                    "Citation Text": "Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and\nGeorge Toderici. Beyond short snippets: Deep networks for video classification. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 4694\u20134702, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.08909",
                        "Citation Paper Title": "Title:Beyond Short Snippets: Deep Networks for Video Classification",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).",
                        "Citation Paper Authors": "Authors:Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "was the earliest method\nintroducing a deep neural network, specifically a Convolutional Neural Network (CNN), for video un-\nderstanding. However, the performance was not superior to the best handcrafted-feature method due\nto the inadequate use of motion information. Two-stream networks ",
                    "Citation Text": "Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion\nfor video action recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 1933\u20131941, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.06573",
                        "Citation Paper Title": "Title:Convolutional Two-Stream Network Fusion for Video Action Recognition",
                        "Citation Paper Abstract": "Abstract:Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14504v2": {
            "Paper Title": "Theory of Hallucinations based on Equivariance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.06911v2": {
            "Paper Title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with\n  Graph, Image, and Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.06355v2": {
            "Paper Title": "VideoChat: Chat-Centric Video Understanding",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ". MiniGPT-4 is a multimodal large language model, fine-tuned on multimodal\ntasks, and exhibits respectable zero-shot image comprehension in dialogues ",
                    "Citation Text": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.10592",
                        "Citation Paper Title": "Title:MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
                        "Citation Paper Abstract": "Abstract:The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "to refine their descriptions for improved\nclarity. Moreover, we integrate the Whisper ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04356",
                        "Citation Paper Title": "Title:Robust Speech Recognition via Large-Scale Weak Supervision",
                        "Citation Paper Abstract": "Abstract:We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ", and more. While the majority of these models\u2019 outputs are comparatively\nindependent, we utilize the pretrained T5 language model ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research , 21(1):5485\u20135551, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "pioneered this approach by capitalizing on\nboth vision and language models using web-scale image-text interwoven data, unveiling exceptional\nzero-shot image-text abilities in a conversational format for the first time. The study in ",
                    "Citation Text": "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.14045",
                        "Citation Paper Title": "Title:Language Is Not All You Need: Aligning Perception with Language Models",
                        "Citation Paper Abstract": "Abstract:A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
                        "Citation Paper Authors": "Authors:Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "shows notable zero-shot performance across numerous benchmarks. InstructGPT mod-\nels ",
                    "Citation Text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems , 35:27730\u201327744, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02155",
                        "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                        "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "consolidates the tasks through masked language modeling. Although these approaches\nyield impressive results in multimodal benchmarks, their training relies on limited video-text data,\nwhich leads to difficulties in video-only tasks such as action recognition. On the other hand, MERLOT\nReserve ",
                    "Citation Text": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 16375\u201316387, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02639",
                        "Citation Paper Title": "Title:MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound",
                        "Citation Paper Abstract": "Abstract:As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce MERLOT Reserve, a model that represents videos jointly over time -- through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos.\nEmpirical results show that MERLOT Reserve learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining -- even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark.\nWe analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.",
                        "Citation Paper Authors": "Authors:Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", video-text contrastive learning [ 49,46], masked video\nmodeling [ 41,44,46] and video-text masked modeling ",
                    "Citation Text": "Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-\nto-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12681",
                        "Citation Paper Title": "Title:VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling",
                        "Citation Paper Abstract": "Abstract:A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. Recent studies try to mitigate this disconnection via end-to-end training. To make it computationally feasible, prior works tend to \"imagify\" video inputs, i.e., a handful of sparsely sampled frames are fed into a 2D CNN, followed by a simple mean-pooling or concatenation to obtain the overall video representations. Although achieving promising results, such simple approaches may lose temporal information that is essential for performing downstream VidL tasks. In this work, we present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. Further, unlike previous studies that found pre-training tasks on video inputs (e.g., masked frame modeling) not very effective, we design a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. Specifically, the original video frame patches are \"tokenized\" into discrete visual tokens, and the goal is to recover the original visual tokens based on the masked patches. Comprehensive analysis demonstrates the effectiveness of both explicit temporal modeling via video transformer and MVM. As a result, VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks.",
                        "Citation Paper Authors": "Authors:Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10997v3": {
            "Paper Title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.19923v3": {
            "Paper Title": "Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06281v2": {
            "Paper Title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.06452v2": {
            "Paper Title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.04444v3": {
            "Paper Title": "What's the Magic Word? A Control Theory of LLM Prompting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.17408v3": {
            "Paper Title": "LMBot: Distilling Graph Knowledge into Language Model for Graph-less\n  Deployment in Twitter Bot Detection",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "propose a relational KD frame-\nwork, Linkless Link Prediction (LLP) that distills relational knowl-\nedge which is centered around each node to the student MLP. Zhao\net al. ",
                    "Citation Text": "Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and\nJian Tang. 2022. Learning on large-scale text-attributed graphs via variational\ninference. arXiv preprint arXiv:2210.14709 (2022).LMBot : Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection WSDM \u201924, March 4\u20138, 2024, Merida, Mexico.\nA ALGORITHM FOR TRAINING LMBOT\nAlgorithm 1 LMBot Optimization Algotithm\nInput: Twitter bot detection dataset \ud835\udc46\nOutput: Optimized LM parameters \u0398LM, Optimized GNN/MLP\nparameters \u0398GNN/MLP\n1:initialize \u0398LM,\u0398GNN/MLP\n2:foreach user\ud835\udc56\u2208\ud835\udc46do\n3: preprocess user textual sequence \ud835\udc95\ud835\udc56\n4: obtain user embedding \ud835\udc9b\ud835\udc56\u2190Equation (1)\n5:\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 LM finetuning\u2190Equation (3)\n6:\u0398LM\u2190BackPropagate ( \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 LM finetuning )\n7:while \u0398LM,\u0398GNN/MLP have not converged do\n8:\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 GNN/MLP\u2190Equation (9)\n9: \u0398GNN/MLP\u2190BackPropagate ( \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 GNN/MLP )\n10: Update soft label \ud835\udc9a\ud835\udc56\u2190Equation (10)\n11:\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 LM\u2190Equation (11)\n12: \u0398LM\u2190BackPropagate ( \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 LM)\n13: Update user embedding \ud835\udc9b\ud835\udc56\u2190Equation (1)\n14: end while\n15:end for\n16:return \u0398LM,\u0398GNN/MLP\nB DATASET STATISTICS\nTable 3: Statistics of the 4 selected datasets\nDataset Cresci-2015 Cresci-2017 Midterm-2018 TwiBot-20\n# User 5,301 14,368 50,538 229,580\n# Human 1,950 3,474 8,092 5,237\n# Bot 3,351 10,894 42,446 6,589\n# Tweet 2,827,757 6,637,615 0 33,488,192\nGraph \" % % \"\nC BASELINE DETAILS\n\u2022SGBot",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.14709",
                        "Citation Paper Title": "Title:Learning on Large-scale Text-attributed Graphs via Variational Inference",
                        "Citation Paper Abstract": "Abstract:This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",
                        "Citation Paper Authors": "Authors:Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "propose graph-less neural networks\n(GLNNs) by distilling GNN knowledge into MLPs, which have no\ngraph dependency and infer faster, to address GNN scalability while\nmaintaining accuracy. Guo et al . ",
                    "Citation Text": "Zhichun Guo, William Shiao, Shichang Zhang, Yozen Liu, Nitesh Chawla, Neil\nShah, and Tong Zhao. 2022. Linkless Link Prediction via Relational Distillation.\narXiv preprint arXiv:2210.05801 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.05801",
                        "Citation Paper Title": "Title:Linkless Link Prediction via Relational Distillation",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distribution-based matching strategies that complement each other. Extensive experiments demonstrate that LLP boosts the link prediction performance of MLPs with significant margins, and even outperforms the teacher GNNs on 7 out of 8 benchmarks. LLP also achieves a 70.68x speedup in link prediction inference compared to GNNs on the large-scale OGB dataset.",
                        "Citation Paper Authors": "Authors:Zhichun Guo, William Shiao, Shichang Zhang, Yozen Liu, Nitesh V. Chawla, Neil Shah, Tong Zhao"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "propose a two-stage framework that performs distil-\nlation during pretraining and task training, enabling TinyBERT to\ncapture BERT\u2019s general and task knowledge. Sanh et al . ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "use multiple user\ninformation types in community-aware mixture-of-experts layers\nto improve the detection of deceptive bots and adapt to different\nTwitter communities. Tan et al . ",
                    "Citation Text": "Zhaoxuan Tan, Shangbin Feng, Melanie Sclar, Herun Wan, Minnan Luo, Yejin\nChoi, and Yulia Tsvetkov. 2023. BotPercent: Estimating Twitter bot populations\nfrom groups to crowds. arXiv preprint arXiv:2302.00381 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00381",
                        "Citation Paper Title": "Title:BotPercent: Estimating Bot Populations in Twitter Communities",
                        "Citation Paper Abstract": "Abstract:Twitter bot detection is vital in combating misinformation and safeguarding the integrity of social media discourse. While malicious bots are becoming more and more sophisticated and personalized, standard bot detection approaches are still agnostic to social environments (henceforth, communities) the bots operate at. In this work, we introduce community-specific bot detection, estimating the percentage of bots given the context of a community. Our method -- BotPercent -- is an amalgamation of Twitter bot detection datasets and feature-, text-, and graph-based models, adjusted to a particular community on Twitter. We introduce an approach that performs confidence calibration across bot detection models, which addresses generalization issues in existing community-agnostic models targeting individual bots and leads to more accurate community-level bot estimations. Experiments demonstrate that BotPercent achieves state-of-the-art performance in community-level Twitter bot detection across both balanced and imbalanced class distribution settings, %outperforming existing approaches and presenting a less biased estimator of Twitter bot populations within the communities we analyze. We then analyze bot rates in several Twitter groups, including users who engage with partisan news media, political communities in different countries, and more. Our results reveal that the presence of Twitter bots is not homogeneous, but exhibiting a spatial-temporal distribution with considerable heterogeneity that should be taken into account for content moderation and social media policy making. The implementation of BotPercent is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhaoxuan Tan, Shangbin Feng, Melanie Sclar, Herun Wan, Minnan Luo, Yejin Choi, Yulia Tsvetkov"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "integrate text and graph modality with a text-graph interaction\nmodule, with additional functionality to detect advanced bots with\na semantic consistency model. Liu et al . ",
                    "Citation Text": "Yuhan Liu, Zhaoxuan Tan, Heng Wang, Shangbin Feng, Qinghua Zheng, and\nMinnan Luo. 2023. BotMoE: Twitter Bot Detection with Community-Aware\nMixtures of Modal-Specific Experts. arXiv preprint arXiv:2304.06280 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.06280",
                        "Citation Paper Title": "Title:BotMoE: Twitter Bot Detection with Community-Aware Mixtures of Modal-Specific Experts",
                        "Citation Paper Abstract": "Abstract:Twitter bot detection has become a crucial task in efforts to combat online misinformation, mitigate election interference, and curb malicious propaganda. However, advanced Twitter bots often attempt to mimic the characteristics of genuine users through feature manipulation and disguise themselves to fit in diverse user communities, posing challenges for existing Twitter bot detection models. To this end, we propose BotMoE, a Twitter bot detection framework that jointly utilizes multiple user information modalities (metadata, textual content, network structure) to improve the detection of deceptive bots. Furthermore, BotMoE incorporates a community-aware Mixture-of-Experts (MoE) layer to improve domain generalization and adapt to different Twitter communities. Specifically, BotMoE constructs modal-specific encoders for metadata features, textual content, and graphical structure, which jointly model Twitter users from three modal-specific perspectives. We then employ a community-aware MoE layer to automatically assign users to different communities and leverage the corresponding expert networks. Finally, user representations from metadata, text, and graph perspectives are fused with an expert fusion layer, combining all three modalities while measuring the consistency of user information. Extensive experiments demonstrate that BotMoE significantly advances the state-of-the-art on three Twitter bot detection benchmarks. Studies also confirm that BotMoE captures advanced and evasive bots, alleviates the reliance on training data, and better generalizes to new and previously unseen user communities.",
                        "Citation Paper Authors": "Authors:Yuhan Liu, Zhaoxuan Tan, Heng Wang, Shangbin Feng, Qinghua Zheng, Minnan Luo"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "design features\nfrom profile, content, graph, neighbor, timing, and automation and\nclassifies with random forest. HOFA ",
                    "Citation Text": "Sen Ye, Zhaoxuan Tan, Zhenyu Lei, Ruijie He, Hongrui Wang, Qinghua Zheng,\nand Minnan Luo. 2023. HOFA: Twitter Bot Detection with Homophily-Oriented\nAugmentation and Frequency Adaptive Attention. arXiv preprint arXiv:2306.12870\n(2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.12870",
                        "Citation Paper Title": "Title:HOFA: Twitter Bot Detection with Homophily-Oriented Augmentation and Frequency Adaptive Attention",
                        "Citation Paper Abstract": "Abstract:Twitter bot detection has become an increasingly important and challenging task to combat online misinformation, facilitate social content moderation, and safeguard the integrity of social platforms. Though existing graph-based Twitter bot detection methods achieved state-of-the-art performance, they are all based on the homophily assumption, which assumes users with the same label are more likely to be connected, making it easy for Twitter bots to disguise themselves by following a large number of genuine users. To address this issue, we proposed HOFA, a novel graph-based Twitter bot detection framework that combats the heterophilous disguise challenge with a homophily-oriented graph augmentation module (Homo-Aug) and a frequency adaptive attention module (FaAt). Specifically, the Homo-Aug extracts user representations and computes a k-NN graph using an MLP and improves Twitter's homophily by injecting the k-NN graph. For the FaAt, we propose an attention mechanism that adaptively serves as a low-pass filter along a homophilic edge and a high-pass filter along a heterophilic edge, preventing user features from being over-smoothed by their neighborhood. We also introduce a weight guidance loss to guide the frequency adaptive attention module. Our experiments demonstrate that HOFA achieves state-of-the-art performance on three widely-acknowledged Twitter bot detection benchmarks, which significantly outperforms vanilla graph-based bot detection techniques and strong heterophilic baselines. Furthermore, extensive studies confirm the effectiveness of our Homo-Aug and FaAt module, and HOFA's ability to demystify the heterophilous disguise challenge.",
                        "Citation Paper Authors": "Authors:Sen Ye, Zhaoxuan Tan, Zhenyu Lei, Ruijie He, Hongrui Wang, Qinghua Zheng, Minnan Luo"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "extract 6 numerical features for each user, and the extracted\nfeatures are fed into GCN layers for classification. Yang et al . ",
                    "Citation Text": "Yingguang Yang, Renyu Yang, Yangyang Li, Kai Cui, Zhiqin Yang, Yue Wang,\nJie Xu, and Haiyong Xie. 2022. RoSGAS: Adaptive Social Bot Detection with\nReinforced Self-Supervised GNN Architecture Search. ACM Transactions on the\nWeb (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.06757",
                        "Citation Paper Title": "Title:RoSGAS: Adaptive Social Bot Detection with Reinforced Self-Supervised GNN Architecture Search",
                        "Citation Paper Abstract": "Abstract:Social bots are referred to as the automated accounts on social networks that make attempts to behave like human. While Graph Neural Networks (GNNs) has been massively applied to the field of social bot detection, a huge amount of domain expertise and prior knowledge is heavily engaged in the state-of-the art approaches to design a dedicated neural network architecture for a specific classification task. Involving oversized nodes and network layers in the model design, however, usually causes the over-smoothing problem and the lack of embedding discrimination. In this paper, we propose RoSGAS, a novel Reinforced and Self-supervised GNN Architecture Search framework to adaptively pinpoint the most suitable multi-hop neighborhood and the number of layers in the GNN architecture. More specifically, we consider the social bot detection problem as a user-centric subgraph embedding and classification task. We exploit heterogeneous information network to present the user connectivity by leveraging account metadata, relationships, behavioral features and content features. RoSGAS uses a multi-agent deep reinforcement learning (RL) mechanism for navigating the search of optimal neighborhood and network layers to learn individually the subgraph embedding for each target user. A nearest neighbor mechanism is developed for accelerating the RL training process, and RoSGAS can learn more discriminative subgraph embedding with the aid of self-supervised learning. Experiments on 5 Twitter datasets show that RoSGAS outperforms the state-of-the-art approaches in terms of accuracy, training efficiency and stability, and has better generalization when handling unseen samples.",
                        "Citation Paper Authors": "Authors:Yingguang Yang, Renyu Yang, Yangyang Li, Kai Cui, Zhiqin Yang, Yue Wang, Jie Xu, Haiyong Xie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.11348v2": {
            "Paper Title": "In the Name of Fairness: Assessing the Bias in Clinical Record\n  De-identification",
            "Sentences": [
                {
                    "Sentence ID": 141,
                    "Sentence": "and mimic the existing\nbiases in text completions for clinical treatment decisions ",
                    "Citation Text": "Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew McDermott, and\nMarzyeh Ghassemi. 2020. Hurtful words: quantifying biases in clinical contex-\ntual word embeddings. In proceedings of the ACM Conference on Health, Inference,\nand Learning .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11515",
                        "Citation Paper Title": "Title:Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings",
                        "Citation Paper Abstract": "Abstract:In this work, we examine the extent to which embeddings may encode marginalized populations differently, and how this may lead to a perpetuation of biases and worsened performance on clinical tasks. We pretrain deep embedding models (BERT) on medical notes from the MIMIC-III hospital dataset, and quantify potential disparities using two approaches. First, we identify dangerous latent relationships that are captured by the contextual word embeddings using a fill-in-the-blank method with text from real clinical notes and a log probability bias score quantification. Second, we evaluate performance gaps across different definitions of fairness on over 50 downstream clinical prediction tasks that include detection of acute and chronic conditions. We find that classifiers trained from BERT representations exhibit statistically significant differences in performance, often favoring the majority group with regards to gender, language, ethnicity, and insurance status. Finally, we explore shortcomings of using adversarial debiasing to obfuscate subgroup information in contextual word embeddings, and recommend best practices for such deep embedding models in clinical settings.",
                        "Citation Paper Authors": "Authors:Haoran Zhang, Amy X. Lu, Mohamed Abdalla, Matthew McDermott, Marzyeh Ghassemi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "and tend to generate violent or negative-toned text\nwhen given \u201cMuslim\u201d as a demographic descriptor for input ",
                    "Citation Text": "Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim\nbias in large language models. In Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05783",
                        "Citation Paper Title": "Title:Persistent Anti-Muslim Bias in Large Language Models",
                        "Citation Paper Abstract": "Abstract:It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, \"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is mapped to \"money\" in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for \"Muslims\" from 66% to 20%, but which is still higher than for other religious groups.",
                        "Citation Paper Authors": "Authors:Abubakar Abid, Maheen Farooqi, James Zou"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "or\nusing names as a proxy for demographic [ 89,93,96]. For example,\nNLP models link the female gender to specific stereotypical occu-\npations ",
                    "Citation Text": "Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T\nKalai. 2016. Man is to computer programmer as woman is to homemaker?\ndebiasing word embeddings. Advances in neural information processing systems\n(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06520",
                        "Citation Paper Title": "Title:Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                        "Citation Paper Abstract": "Abstract:The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                        "Citation Paper Authors": "Authors:Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "practices.\nOther work has explored the biases learned by large language mod-\nels when the demographic context is varied directly in input ",
                    "Citation Text": "Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov.\n2021. Towards understanding and mitigating social biases in language models.\nInInternational Conference on Machine Learning .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.13219",
                        "Citation Paper Title": "Title:Towards Understanding and Mitigating Social Biases in Language Models",
                        "Citation Paper Abstract": "Abstract:As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
                        "Citation Paper Authors": "Authors:Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 139,
                    "Sentence": "as the clinical context and the Wikipedia articles in\nthe DocRED dataset ",
                    "Citation Text": "Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan\nLiu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A Large-Scale\nDocument-Level Relation Extraction Dataset. In Proceedings of ACL 2019 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.06127",
                        "Citation Paper Title": "Title:DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
                        "Citation Paper Abstract": "Abstract:Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research.",
                        "Citation Paper Authors": "Authors:Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, Maosong Sun"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "as instructed and view the outputs for the classes\nPATIENT andDOCTOR as the set of recognized names.\n3.6 Evaluation of Bias\nTo quantify the bias of each method along each dimension, we fol-\nlow ",
                    "Citation Text": "Courtney Mansfield, Amandalynne Paullada, and Kristen Howell. 2022. Behind\nthe Mask: Demographic bias in name detection for PII masking. In Proceedings\nof the Second Workshop on Language Technology for Equality, Diversity and\nInclusion .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.04505",
                        "Citation Paper Title": "Title:Behind the Mask: Demographic bias in name detection for PII masking",
                        "Citation Paper Abstract": "Abstract:Many datasets contain personally identifiable information, or PII, which poses privacy risks to individuals. PII masking is commonly used to redact personal information such as names, addresses, and phone numbers from text data. Most modern PII masking pipelines involve machine learning algorithms. However, these systems may vary in performance, such that individuals from particular demographic groups bear a higher risk for having their personal information exposed. In this paper, we evaluate the performance of three off-the-shelf PII masking systems on name detection and redaction. We generate data using names and templates from the customer service domain. We find that an open-source RoBERTa-based system shows fewer disparities than the commercial models we test. However, all systems demonstrate significant differences in error rate based on demographics. In particular, the highest error rates occurred for names associated with Black and Asian/Pacific Islander individuals.",
                        "Citation Paper Authors": "Authors:Courtney Mansfield, Amandalynne Paullada, Kristen Howell"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "(12.7k GitHub Stars) is a powerful NLP frame-\nwork. We employ its large four-class NER model variant\nbuilt on XLM-R embeddings ",
                    "Citation Text": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learn-\ning at Scale. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02116",
                        "Citation Paper Title": "Title:Unsupervised Cross-lingual Representation Learning at Scale",
                        "Citation Paper Abstract": "Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 108,
                    "Sentence": ",\nwhich is pre-trained on a massive general-purpose corpus,\nas the backbone of its NER pipeline.\n\u2022Stanza ",
                    "Citation Text": "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning.\n2020. Stanza: A Python Natural Language Processing Toolkit for Many Human\nLanguages. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07082",
                        "Citation Paper Title": "Title:Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
                        "Citation Paper Abstract": "Abstract:We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at this https URL.",
                        "Citation Paper Authors": "Authors:Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": "(25.9k GitHub Stars) is widely adopted for indus-\ntrial information extraction. We choose RoBERTa-base ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.12072v2": {
            "Paper Title": "SPEED: Speculative Pipelined Execution for Efficient Decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.08163v2": {
            "Paper Title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.19555v3": {
            "Paper Title": "Large Language Models Are Not Strong Abstract Reasoners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.05134v2": {
            "Paper Title": "TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.06035v3": {
            "Paper Title": "Multimodality and Attention Increase Alignment in Natural Language\n  Prediction Between Humans and Computational Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.07490v5": {
            "Paper Title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.15218v4": {
            "Paper Title": "Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and\n  Qualitative Analysis",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "(which is already pre-trained on TRC2-\nfinancialtextdata)fine-tunedonthelabeledfinancialdataset\nrespectively, DistilBERT and DistilRoBERTa fine-tuned\nrespectively. The results obtained by finetuning the afore-\nmentioned models are shown in Table(3)\nDistilBERT ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.\nDistilbert,adistilledversionofbert:smaller,faster,cheaperandlighter.\narXiv preprint arXiv:1910.01108 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "sentiments and events\nare integrated and exhibit combined influence on the stock\nmarket, making use of the tensor framework.\nA recent work by Zhang .et al ",
                    "Citation Text": "Xi Zhang, Yunjia Zhang, Senzhang Wang, Yuntao Yao, Binxing Fang,\nandPhilipS.Yu. Improvingstockmarketpredictionviaheterogeneous\ninformation fusion. Knowledge-Based Systems , 143:236\u2013247, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00588",
                        "Citation Paper Title": "Title:Improving Stock Market Prediction via Heterogeneous Information Fusion",
                        "Citation Paper Abstract": "Abstract:Traditional stock market prediction approaches commonly utilize the historical price-related data of the stocks to forecast their future trends. As the Web information grows, recently some works try to explore financial news to improve the prediction. Effective indicators, e.g., the events related to the stocks and the people's sentiments towards the market and stocks, have been proved to play important roles in the stocks' volatility, and are extracted to feed into the prediction models for improving the prediction accuracy. However, a major limitation of previous methods is that the indicators are obtained from only a single source whose reliability might be low, or from several data sources but their interactions and correlations among the multi-sourced data are largely ignored.\nIn this work, we extract the events from Web news and the users' sentiments from social media, and investigate their joint impacts on the stock price movements via a coupled matrix and tensor factorization framework. Specifically, a tensor is firstly constructed to fuse heterogeneous data and capture the intrinsic relations among the events and the investors' sentiments. Due to the sparsity of the tensor, two auxiliary matrices, the stock quantitative feature matrix and the stock correlation matrix, are constructed and incorporated to assist the tensor decomposition. The intuition behind is that stocks that are highly correlated with each other tend to be affected by the same event. Thus, instead of conducting each stock prediction task separately and independently, we predict multiple correlated stocks simultaneously through their commonalities, which are enabled via sharing the collaboratively factorized low rank matrices between matrices and the tensor. Evaluations on the China A-share stock data and the HK stock data in the year 2015 demonstrate the effectiveness of the proposed model.",
                        "Citation Paper Authors": "Authors:Xi Zhang, Yunjia Zhang, Senzhang Wang, Yuntao Yao, Binxing Fang, Philip S. Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2306.11300v5": {
            "Paper Title": "RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10741v2": {
            "Paper Title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.14587v2": {
            "Paper Title": "Large Search Model: Redefining Search Stack in the Era of LLMs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.04021v3": {
            "Paper Title": "A Study on the Calibration of In-context Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.16033v2": {
            "Paper Title": "ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question\n  Answering with Multimodal Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "to construct\nhighly compositional questions requiring spatial, logical,\nrelational, and comparative reasoning, and explicitly control-\nling the answer distribution for different groups of questions\nin order to prevent educated guesses using language and\nworld priors. 4) AOKVQA ",
                    "Citation Text": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A bench-\nmark for visual question answering using world knowledge.\n9InComputer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII ,\npages 146\u2013162. Springer, 2022. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.01718",
                        "Citation Paper Title": "Title:A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
                        "Citation Paper Abstract": "Abstract:The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models. Project page: this http URL",
                        "Citation Paper Authors": "Authors:Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "contains 12,578 questions paired with 398 images,\nusing the scene graphs of Visual Genome ",
                    "Citation Text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision ,\n123:32\u201373, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.07332",
                        "Citation Paper Title": "Title:Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                        "Citation Paper Abstract": "Abstract:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "apply grad-cam to the Trans-\nformer structure, identifying the most relevant image patches\nfrom BLIP ",
                    "Citation Text": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\nBootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In International Con-\nference on Machine Learning , pages 12888\u201312900. PMLR,\n2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.12086",
                        "Citation Paper Title": "Title:BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "tries to localize the salient\narea of visual classification by tracking the gradients of\nCNN features without any need for spatial annotation. PNP-\nVQA ",
                    "Citation Text": "Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio\nSavarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shot\nvqa by conjoining large pretrained models with zero training.\narXiv preprint arXiv:2210.08773 , 2022. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.08773",
                        "Citation Paper Title": "Title:Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training",
                        "Citation Paper Abstract": "Abstract:Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1% on GQA over FewVLM with 740M PLM parameters. Code is released at this https URL",
                        "Citation Paper Authors": "Authors:Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C.H. Hoi"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", rely heavily on rich spatial annota-\ntions. However, grad-cam ",
                    "Citation Text": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-\ncam: Visual explanations from deep networks via gradient-\nbased localization. In Proceedings of the IEEE international\nconference on computer vision , pages 618\u2013626, 2017. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02391",
                        "Citation Paper Title": "Title:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
                        "Citation Paper Abstract": "Abstract:We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2309.12269v4": {
            "Paper Title": "The Cambridge Law Corpus: A Dataset for Legal AI Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.04469v2": {
            "Paper Title": "On the Learnability of Watermarks for Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14557v2": {
            "Paper Title": "Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse\n  Mixture-of-Experts through Instruction-Tuning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "trained medical LLM for diagnosing diseases based on LLaMA. ChatLaw ",
                    "Citation Text": "Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language model\nwith integrated external knowledge bases, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.16092",
                        "Citation Paper Title": "Title:ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.\nIn this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at this https URL.",
                        "Citation Paper Authors": "Authors:Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, Li Yuan"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". To align the LLM with various domain functions,\nIvyGPT ",
                    "Citation Text": "Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu,\nPatrick Cheong-Iao Pang, and Tao Tan. Ivygpt: Interactive chinese pathway language model in medical domain,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.10512",
                        "Citation Paper Title": "Title:IvyGPT: InteractiVe Chinese pathwaY language model in medical domain",
                        "Citation Paper Abstract": "Abstract:General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.",
                        "Citation Paper Authors": "Authors:Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu, Patrick Cheong-Iao Pang, Tao Tan"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". T0 is a series of models trained on P3 via multitask prompted training ",
                    "Citation Text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,\nArnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization.\narXiv preprint arXiv:2110.08207 , 2021.\n7PRIME AI paper",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.08207",
                        "Citation Paper Title": "Title:Multitask Prompted Training Enables Zero-Shot Task Generalization",
                        "Citation Paper Abstract": "Abstract:Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". PromptSource contains a growing collection of prompts (which is\nalso called P3: Public Pool of Prompts) ",
                    "Citation Text": "Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V . Nayak, Abheesht Sharma,\nTaewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik\nBen-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma,\nUrmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush.\nPromptsource: An integrated development environment and repository for natural language prompts, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.01279",
                        "Citation Paper Title": "Title:PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                        "Citation Paper Abstract": "Abstract:PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at this https URL.",
                        "Citation Paper Authors": "Authors:Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-Jian Jiang, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17660v2": {
            "Paper Title": "Normalization of Lithuanian Text Using Regular Expressions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2304.02478v2": {
            "Paper Title": "Exploring AI-Generated Text in Student Writing: How Does AI Help?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00609v1": {
            "Paper Title": "A Survey of Personality, Persona, and Profile in Conversational Agents\n  and Chatbots",
            "Sentences": [
                {
                    "Sentence ID": 100,
                    "Sentence": "FriendsPersona Dialogues from TV shows with both Emotion and\nBig-5 labels\nIT-ConvAI221 ",
                    "Citation Text": "Y . Liu, W. Wei, J. Liu, X. Mao, R. Fang, and D. Chen. Improving personality consistency in conversation by\npersona extending. In Proceedings of the 31st ACM International Conference on Information & Knowledge\nManagement , pages 1350\u20131359, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.10816",
                        "Citation Paper Title": "Title:Improving Personality Consistency in Conversation by Persona Extending",
                        "Citation Paper Abstract": "Abstract:Endowing chatbots with a consistent personality plays a vital role for agents to deliver human-like interactions. However, existing personalized approaches commonly generate responses in light of static predefined personas depicted with textual description, which may severely restrict the interactivity of human and the chatbot, especially when the agent needs to answer the query excluded in the predefined personas, which is so-called out-of-predefined persona problem (named OOP for simplicity). To alleviate the problem, in this paper we propose a novel retrieval-to-prediction paradigm consisting of two subcomponents, namely, (1) Persona Retrieval Model (PRM), it retrieves a persona from a global collection based on a Natural Language Inference (NLI) model, the inferred persona is consistent with the predefined personas; and (2) Posterior-scored Transformer (PS-Transformer), it adopts a persona posterior distribution that further considers the actual personas used in the ground response, maximally mitigating the gap between training and inferring. Furthermore, we present a dataset called IT-ConvAI2 that first highlights the OOP problem in personalized dialogue. Extensive experiments on both IT-ConvAI2 and ConvAI2 demonstrate that our proposed model yields considerable improvements in both automatic metrics and human evaluations.",
                        "Citation Paper Authors": "Authors:Yifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang, Dangyang Chen"
                    }
                },
                {
                    "Sentence ID": 174,
                    "Sentence": ". The generative dialogue model comprises three components, an Image Encoder, a Style\nEncoder, which transforms a trait into a distributed vector, 500 dimensions for retrieval, 300 for generation, and a\nDialogue Encoder, based on Transformers ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention\nis all you need. Advances in neural information processing systems , 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 196,
                    "Sentence": "IMDB Quotes 15,608 quotes linked to tropes in the Character Tropes\nDataset (see above).\nPersona-Chat101112 ",
                    "Citation Text": "S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, and J. Weston. Personalizing dialogue agents: I have a dog,\ndo you have pets too? arXiv preprint arXiv:1801.07243 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07243",
                        "Citation Paper Title": "Title:Personalizing Dialogue Agents: I have a dog, do you have pets too?",
                        "Citation Paper Abstract": "Abstract:Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
                        "Citation Paper Authors": "Authors:Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 140,
                    "Sentence": "Crowdworkers Based on Persona-Chat approach but Japanese. 100\npersonas, 5 sentences each. 5,000 dialogues, 12-15\nturns.\nPersonageNLG24 ",
                    "Citation Text": "A. Ramirez, M. Alsalihy, K. Aggarwal, C. Li, L. Wu, and M. Walker. Controlling personality style in dialogue\nwith zero-shot prompt-based learning. arXiv preprint arXiv:2302.03848 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.03848",
                        "Citation Paper Title": "Title:Controlling Personality Style in Dialogue with Zero-Shot Prompt-Based Learning",
                        "Citation Paper Abstract": "Abstract:Prompt-based or in-context learning has achieved high zero-shot performance on many natural language generation (NLG) tasks. Here we explore the performance of prompt-based learning for simultaneously controlling the personality and the semantic accuracy of an NLG for task-oriented dialogue. We experiment with prompt-based learning on the PERSONAGE restaurant recommendation corpus to generate semantically and stylistically-controlled text for 5 different Big-5 personality types: agreeable, disagreeable, conscientious, unconscientious, and extravert. We test two different classes of discrete prompts to generate utterances for a particular personality style: (1) prompts that demonstrate generating directly from a meaning representation that includes a personality specification; and (2) prompts that rely on first converting the meaning representation to a textual pseudo-reference, and then using the pseudo-reference in a textual style transfer (TST) prompt. In each case, we show that we can vastly improve performance by over-generating outputs and ranking them, testing several ranking functions based on automatic metrics for semantic accuracy, personality-match, and fluency. We also test whether NLG personality demonstrations from the restaurant domain can be used with meaning representations for the video game domain to generate personality stylized utterances about video games. Our findings show that the TST prompts produces the highest semantic accuracy (78.46% for restaurants and 87.6% for video games) and personality accuracy (100% for restaurants and 97% for video games). Our results on transferring personality style to video game utterances are surprisingly good. To our knowledge, there is no previous work testing the application of prompt-based learning to simultaneously controlling both style and semantic accuracy in NLG.",
                        "Citation Paper Authors": "Authors:Angela Ramirez, Mamon Alsalihy, Kartik Aggarwal, Cecilia Li, Liren Wu, Marilyn Walker"
                    }
                },
                {
                    "Sentence ID": 181,
                    "Sentence": "Weibo Chinese dialogues with personality traits for each\nspeaker (recognised by classifiers they trained), in\nattribute-value form. 20.83 million dialogues, 56.25\nmillion utterances, 8.47M million speakers.\nPersuasion-ForGood18 ",
                    "Citation Text": "X. Wang, W. Shi, R. Kim, Y . Oh, S. Yang, J. Zhang, and Z. Yu. Persuasion for good: Towards a personalized\npersuasive dialogue system for social good. arXiv preprint arXiv:1906.06725 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.06725",
                        "Citation Paper Title": "Title:Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good",
                        "Citation Paper Abstract": "Abstract:Developing intelligent persuasive conversational agents to change people's opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals' personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.",
                        "Citation Paper Authors": "Authors:Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, Zhou Yu"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": "Wikipedia, Free-\nbase501 characters associated with 72 tropes (and much\nother information).\nSpeaker Model ",
                    "Citation Text": "J. Li, M. Galley, C. Brockett, G. Spithourakis, J. Gao, and W. B. Dolan. A persona-based neural conversation\nmodel. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers) , pages 994\u20131003, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06155",
                        "Citation Paper Title": "Title:A Persona-Based Neural Conversation Model",
                        "Citation Paper Abstract": "Abstract:We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.",
                        "Citation Paper Authors": "Authors:Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan"
                    }
                },
                {
                    "Sentence ID": 198,
                    "Sentence": "Crowdworkers Active characters from 617 film scripts in Cornell\nMovie-Dialogs Corpus16are labelled with Profession,\nGender, and Age. Also includes labels for some\nPersona-Chat personas, and for some Reddit users.\nPersonalDialog17 ",
                    "Citation Text": "Y . Zheng, G. Chen, M. Huang, S. Liu, and X. Zhu. Personalized dialogue generation with diversified traits. arXiv\npreprint arXiv:1901.09672 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.09672",
                        "Citation Paper Title": "Title:Personalized Dialogue Generation with Diversified Traits",
                        "Citation Paper Abstract": "Abstract:Endowing a dialogue system with particular personality traits is essential to deliver more human-like conversations. However, due to the challenge of embodying personality via language expression and the lack of large-scale persona-labeled dialogue data, this research problem is still far from well-studied. In this paper, we investigate the problem of incorporating explicit personality traits in dialogue generation to deliver personalized dialogues.\nTo this end, firstly, we construct PersonalDialog, a large-scale multi-turn dialogue dataset containing various traits from a large number of speakers. The dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers. Each utterance is associated with a speaker who is marked with traits like Age, Gender, Location, Interest Tags, etc. Several anonymization schemes are designed to protect the privacy of each speaker. This large-scale dataset will facilitate not only the study of personalized dialogue generation, but also other researches on sociolinguistics or social science.\nSecondly, to study how personality traits can be captured and addressed in dialogue generation, we propose persona-aware dialogue generation models within the sequence to sequence learning framework. Explicit personality traits (structured by key-value pairs) are embedded using a trait fusion module. During the decoding process, two techniques, namely persona-aware attention and persona-aware bias, are devised to capture and address trait-related information. Experiments demonstrate that our model is able to address proper traits in different contexts. Case studies also show interesting results for this challenging research problem.",
                        "Citation Paper Authors": "Authors:Yinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu, Xuan Zhu"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "Friends\nBig Bang Theory69,565 dialogue turns for 13 characters in TV shows.\nPrivate dataset.\nPersonalized-Dialog45 ",
                    "Citation Text": "C. K. Joshi, F. Mi, and B. Faltings. Personalization in goal-oriented dialog. arXiv preprint arXiv:1706.07503 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.07503",
                        "Citation Paper Title": "Title:Personalization in Goal-Oriented Dialog",
                        "Citation Paper Abstract": "Abstract:The main goal of modeling human conversation is to create agents which can interact with people in both open-ended and goal-oriented scenarios. End-to-end trained neural dialog systems are an important line of research for such generalized dialog models as they do not resort to any situation-specific handcrafting of rules. However, incorporating personalization into such systems is a largely unexplored topic as there are no existing corpora to facilitate such work. In this paper, we present a new dataset of goal-oriented dialogs which are influenced by speaker profiles attached to them. We analyze the shortcomings of an existing end-to-end dialog system based on Memory Networks and propose modifications to the architecture which enable personalization. We also investigate personalization in dialog as a multi-task learning problem, and show that a single model which shares features among various profiles outperforms separate models for each profile.",
                        "Citation Paper Authors": "Authors:Chaitanya K. Joshi, Fei Mi, Boi Faltings"
                    }
                },
                {
                    "Sentence ID": 173,
                    "Sentence": "Microsoft 100 scenarios, 5 personalities, 9 languages.\nMovie Character Attributes\n(MovieChAtt) Dataset15 ",
                    "Citation Text": "A. Tigunova, A. Yates, P. Mirza, and G. Weikum. Listening between the lines: Learning personal attributes from\nconversations. In The World Wide Web Conference , pages 1818\u20131828, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10887",
                        "Citation Paper Title": "Title:Listening between the Lines: Learning Personal Attributes from Conversations",
                        "Citation Paper Abstract": "Abstract:Open-domain dialogue agents must be able to converse about many topics while incorporating knowledge about the user into the conversation. In this work we address the acquisition of such knowledge, for personalization in downstream Web applications, by extracting personal attributes from conversations. This problem is more challenging than the established task of information extraction from scientific publications or Wikipedia articles, because dialogues often give merely implicit cues about the speaker. We propose methods for inferring personal attributes, such as profession, age or family status, from conversations using deep learning. Specifically, we propose several Hidden Attribute Models, which are neural networks leveraging attention mechanisms and embeddings. Our methods are trained on a per-predicate basis to output rankings of object values for a given subject-predicate combination (e.g., ranking the doctor and nurse professions high when speakers talk about patients, emergency rooms, etc). Experiments with various conversational texts including Reddit discussions, movie scripts and a collection of crowdsourced personal dialogues demonstrate the viability of our methods and their superior performance compared to state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Anna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "Synthesised Synthetic personalised dialogues for booking a restau-\nrant. Uses KB. Personas defined by gender, age,\nfavourite food. 6,000 dialogues, 20 turns.\nCharacter Trope Description\nDataset6 ",
                    "Citation Text": "E. Chu, P. Vijayaraghavan, and D. Roy. Learning personas from dialogue with attentive memory networks. arXiv\npreprint arXiv:1810.08717 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08717",
                        "Citation Paper Title": "Title:Learning Personas from Dialogue with Attentive Memory Networks",
                        "Citation Paper Abstract": "Abstract:The ability to infer persona from dialogue can have applications in areas ranging from computational narrative analysis to personalized dialogue generation. We introduce neural models to learn persona embeddings in a supervised character trope classification task. The models encode dialogue snippets from IMDB into representations that can capture the various categories of film characters. The best-performing models use a multi-level attention mechanism over a set of utterances. We also utilize prior knowledge in the form of textual descriptions of the different tropes. We apply the learned embeddings to find similar characters across different movies, and cluster movies according to the distribution of the embeddings. The use of short conversational text as input, and the ability to learn from prior knowledge using memory, suggests these methods could be applied to other domains.",
                        "Citation Paper Authors": "Authors:Eric Chu, Prashanth Vijayaraghavan, Deb Roy"
                    }
                },
                {
                    "Sentence ID": 168,
                    "Sentence": "PELD Chinese translation of Personality Emotion Lines\nDataset. Includes both emotion adn Big-5 labels.\nJPersonaChat23 ",
                    "Citation Text": "H. Sugiyama, M. Mizukami, T. Arimoto, H. Narimatsu, Y . Chiba, H. Nakajima, and T. Meguro. Empirical\nanalysis of training strategies of transformer-based japanese chit-chat systems. In 2022 IEEE Spoken Language\nTechnology Workshop (SLT) , pages 685\u2013691. IEEE, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05217",
                        "Citation Paper Title": "Title:Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems",
                        "Citation Paper Abstract": "Abstract:In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model. Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, they did not analyze how the differences of fine-tuning datasets affect on user's detailed impression. In addition, the Transformer-based approach has only been verified for English, not for such languages with large inter-language distances as Japanese. In this study, we develop large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets to examine the effectiveness of the Transformer-based approach for building chit-chat dialogue systems. We evaluated and analyzed the impressions of human dialogues in different fine-tuning datasets, model parameters, and the use of additional information.",
                        "Citation Paper Authors": "Authors:Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "Crowdworkers 1,155 5-line random personas. 10,907 dialogues, 12-\n18 turns, 162,064 utterances.\nConvAI213 ",
                    "Citation Text": "E. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Urbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe,\net al. The second conversational intelligence challenge (convai2). arXiv preprint arXiv:1902.00098 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.00098",
                        "Citation Paper Title": "Title:The Second Conversational Intelligence Challenge (ConvAI2)",
                        "Citation Paper Abstract": "Abstract:We describe the setting and results of the ConvAI2 NeurIPS competition that aims to further the state-of-the-art in open-domain chatbots. Some key takeaways from the competition are: (i) pretrained Transformer variants are currently the best performing models on this task, (ii) but to improve performance on multi-turn conversations with humans, future systems must go beyond single word metrics like perplexity to measure the performance across sequences of utterances (conversations) -- in terms of repetition, consistency and balance of dialogue acts (e.g. how many questions asked vs. answered).",
                        "Citation Paper Authors": "Authors:Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W Black, Alexander Rudnicky, Jason Williams, Joelle Pineau, Mikhail Burtsev, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "proposed the The Personalized-Dialog dataset which is based on the well-known bAbI dialogue dataset ",
                    "Citation Text": "A. Bordes, Y .-L. Boureau, and J. Weston. Learning end-to-end goal-oriented dialog. arXiv preprint\narXiv:1605.07683 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07683",
                        "Citation Paper Title": "Title:Learning End-to-End Goal-Oriented Dialog",
                        "Citation Paper Abstract": "Abstract:Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.",
                        "Citation Paper Authors": "Authors:Antoine Bordes, Y-Lan Boureau, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 199,
                    "Sentence": "Friends 711 dialogues from first four seasons of Friends TV\nshow. Tagged for Big-5.\nPersonality Emotion Lines\nDataset20 ",
                    "Citation Text": "W. Zhiyuan, C. Jiannong, Y . Ruosong, L. Shuaiqi, and S. Jiaxing. Automatically select emotion for response via\npersonality-affected emotion transition. arXiv preprint arXiv:2106.15846 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.15846",
                        "Citation Paper Title": "Title:Automatically Select Emotion for Response via Personality-affected Emotion Transition",
                        "Citation Paper Abstract": "Abstract:To provide consistent emotional interaction with users, dialog systems should be capable to automatically select appropriate emotions for responses like humans. However, most existing works focus on rendering specified emotions in responses or empathetically respond to the emotion of users, yet the individual difference in emotion expression is overlooked. This may lead to inconsistent emotional expressions and disinterest users. To tackle this issue, we propose to equip the dialog system with personality and enable it to automatically select emotions in responses by simulating the emotion transition of humans in conversation. In detail, the emotion of the dialog system is transitioned from its preceding emotion in context. The transition is triggered by the preceding dialog context and affected by the specified personality trait. To achieve this, we first model the emotion transition in the dialog system as the variation between the preceding emotion and the response emotion in the Valence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks to encode the preceding dialog context and the specified personality traits to compose the variation. Finally, the emotion for response is selected from the sum of the preceding emotion and the variation. We construct a dialog dataset with emotion and personality labels and conduct emotion prediction tasks for evaluation. Experimental results validate the effectiveness of the personality-affected emotion transition.",
                        "Citation Paper Authors": "Authors:Wen Zhiyuan, Cao Jiannong, Yang Ruosong, Liu Shuaiqi, Shen Jiaxing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00908v1": {
            "Paper Title": "DocLLM: A layout-aware generative language model for multimodal document\n  understanding",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "is a subsequent development that enables flexible\ngeneration at arbitrary positions, unconstrained by a predefined generation order. In contrast, approaches like GLM ",
                    "Citation Text": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10360",
                        "Citation Paper Title": "Title:GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                        "Citation Paper Abstract": "Abstract:There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
                        "Citation Paper Authors": "Authors:Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "in VRDU tasks. But\nsuch models require task- and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent\nmPLUG-DocOwl ",
                    "Citation Text": "Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li,\nJunfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model\nfor document understanding. CoRR , abs/2307.02499, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.02499",
                        "Citation Paper Title": "Title:mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
                        "Citation Paper Abstract": "Abstract:Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14335v2": {
            "Paper Title": "Context-aware Decoding Reduces Hallucination in Query-focused\n  Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00595v1": {
            "Paper Title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00907v1": {
            "Paper Title": "LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00579v1": {
            "Paper Title": "Exploring the Effectiveness of Instruction Tuning in Biomedical Language\n  Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00575v1": {
            "Paper Title": "Neural Networks Against (and For) Self-Training: Classification with\n  Small Labeled and Large Unlabeled Sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00504v1": {
            "Paper Title": "HSC-GPT: A Large Language Model for Human Settlements Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00448v1": {
            "Paper Title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "datasets, and subsequent work has replicated these\nscaling laws on other internet corpora and transformer variants ",
                    "Citation Text": "N. Dey, G. Gosal, Zhiming, Chen, H. Khachane, W. Marshall, R. Pathria, M. Tom, and\nJ. Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras\nwafer-scale cluster, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.03208",
                        "Citation Paper Title": "Title:Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
                        "Citation Paper Abstract": "Abstract:We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: this https URL.",
                        "Citation Paper Authors": "Authors:Nolan Dey, Gurpreet Gosal, Zhiming (Charles)Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "has discussed the training-inference compute trade-off [ 16,17,18,1,19]. Touvron et al. ",
                    "Citation Text": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\nefficient foundation language models, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00437v1": {
            "Paper Title": "BatchEval: Towards Human-like Text Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00434v1": {
            "Paper Title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "is a multimodal Geoscience Academic Knowledge Graph (GAKG) framework by fusing papers\u2019\nillustrations, text, and bibliometric data.\n\u2022DeepShovel . DeepShovel ",
                    "Citation Text": "Shao Zhang, Yuting Jia, Hui Xu, Ying Wen, Dakuo Wang, and Xinbing Wang. Deepshovel: An online collaborative\nplatform for data extraction in geoscience literature with ai assistance. arXiv preprint arXiv:2202.10163 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10163",
                        "Citation Paper Title": "Title:DeepShovel: An Online Collaborative Platform for Data Extraction in Geoscience Literature with AI Assistance",
                        "Citation Paper Abstract": "Abstract:Geoscientists, as well as researchers in many fields, need to read a huge amount of literature to locate, extract, and aggregate relevant results and data to enable future research or to build a scientific database, but there is no existing system to support this use case well. In this paper, based on the findings of a formative study about how geoscientists collaboratively annotate literature and extract and aggregate data, we proposed DeepShovel, a publicly-available AI-assisted data extraction system to support their needs. DeepShovel leverages the state-of-the-art neural network models to support researcher(s) easily and accurately annotate papers (in the PDF format) and extract data from tables, figures, maps, etc. in a human-AI collaboration manner. A follow-up user evaluation with 14 researchers suggested DeepShovel improved users' efficiency of data extraction for building scientific databases, and encouraged teams to form a larger scale but more tightly-coupled collaboration.",
                        "Citation Paper Authors": "Authors:Shao Zhang, Yuting Jia, Hui Xu, Ying Wen, Dakuo Wang, Xinbing Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00426v1": {
            "Paper Title": "keqing: knowledge-based question answering is a nature chain-of-thought\n  mentor of LLM",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "using dense document vectors embedded by a neural\nnetwork. Moving beyond retrieving on text corpus, recent works ",
                    "Citation Text": "Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. Few-shot in-context learning for\nknowledge base question answering. arXiv preprint arXiv:2305.01750 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.01750",
                        "Citation Paper Title": "Title:Few-shot In-context Learning for Knowledge Base Question Answering",
                        "Citation Paper Abstract": "Abstract:Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "asResponse\nGeneration module to summarize the execution log.\nThe version of ChatGPT in Keqing isgpt-3.5-turbo , and the pretrained LLaMA can be found in Huggingface ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language\nprocessing. arXiv preprint arXiv:1910.03771 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", \"args\": \"seed_entities\": [\"<GENERATED>-0\"]}}, {\"question\": \"who acted in the movie [mask]?\",\n\"id\": 2, \"dep\": ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877\u20131901, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "working with bag-of-words representations of\ndocuments and another one is the dense neural retriever ",
                    "Citation Text": "Akari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi. One question answering model for many languages\nwith cross-lingual dense passage retrieval. Advances in Neural Information Processing Systems , 34:7547\u20137560,\n2021.\n10Preprint",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.11976",
                        "Citation Paper Title": "Title:One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
                        "Citation Paper Abstract": "Abstract:We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. Combined with a multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings.",
                        "Citation Paper Authors": "Authors:Akari Asai, Xinyan Yu, Jungo Kasai, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "2.1 Retrieval-Augmented Language Generation\nTo avoid generating non-factual and out-of-data response, retrieval-augmented LMs ",
                    "Citation Text": "Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey.\narXiv preprint arXiv:2302.07842 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.07842",
                        "Citation Paper Title": "Title:Augmented Language Models: a Survey",
                        "Citation Paper Abstract": "Abstract:This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",
                        "Citation Paper Authors": "Authors:Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, Thomas Scialom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00424v1": {
            "Paper Title": "SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation\n  for Multi-modal Intent Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08793v3": {
            "Paper Title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00396v1": {
            "Paper Title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy\n  Retrieval-Augmented Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.05450v2": {
            "Paper Title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00383v1": {
            "Paper Title": "Predicting Evoked Emotions in Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00368v1": {
            "Paper Title": "Improving Text Embeddings with Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", to train text embeddings [ 36,5,12]. However, labeled data are often limited in\nterms of task diversity and language coverage. To address this challenge, methods like Contriever ",
                    "Citation Text": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive\nlearning. ArXiv preprint , abs/2112.09118, 2021. URL https://arxiv.org/abs/2112.\n09118 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.09118",
                        "Citation Paper Title": "Title:Unsupervised Dense Information Retrieval with Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
                        "Citation Paper Authors": "Authors:Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "75.2 44.5 86.0 56.6 50.6 82.1 30.2 62.3\nGTE large ",
                    "Citation Text": "Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.\nTowards general text embeddings with multi-stage contrastive learning. ArXiv preprint ,\nabs/2308.03281, 2023. URL https://arxiv.org/abs/2308.03281 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.03281",
                        "Citation Paper Title": "Title:Towards General Text Embeddings with Multi-stage Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:We present GTE, a general-purpose text embedding model trained with multi-stage contrastive learning. In line with recent advancements in unifying various NLP tasks into a single format, we train a unified text embedding model by employing contrastive learning over a diverse mixture of datasets from multiple sources. By significantly increasing the number of training data during both unsupervised pre-training and supervised fine-tuning stages, we achieve substantial performance gains over existing embedding models. Notably, even with a relatively modest parameter count of 110M, GTE$_\\text{base}$ outperforms the black-box embedding API provided by OpenAI and even surpasses 10x larger text embedding models on the massive text embedding benchmark. Furthermore, without additional fine-tuning on each programming language individually, our model outperforms previous best code retrievers of similar size by treating code as text. In summary, our model achieves impressive results by effectively harnessing multi-stage contrastive learning, offering a powerful and efficient text embedding model with broad applicability across various NLP and code-related tasks.",
                        "Citation Paper Authors": "Authors:Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "dataset.\nWe evaluate the trained model on the MTEB benchmark ",
                    "Citation Text": "Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text em-\nbedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics , pages 2014\u20132037, Dubrovnik, Croatia, 2023. Association\nfor Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.148 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.07316",
                        "Citation Paper Title": "Title:MTEB: Massive Text Embedding Benchmark",
                        "Citation Paper Abstract": "Abstract:Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at this https URL.",
                        "Citation Paper Authors": "Authors:Niklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, Nils Reimers"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". Note that the retrieval category in\nMTEB corresponds to the 15publicly available datasets in the BEIR benchmark ",
                    "Citation Text": "Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych.\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08663",
                        "Citation Paper Title": "Title:BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
                        "Citation Paper Abstract": "Abstract:Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "are proprietary and have little technical\n2details disclosed. To bridge the gap between proprietary and open-source LLMs, several notable\nefforts have been made, such as LLaMA-2 ",
                    "Citation Text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. ArXiv preprint , abs/2307.09288, 2023. URL https:\n//arxiv.org/abs/2307.09288 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.09288",
                        "Citation Paper Title": "Title:Llama 2: Open Foundation and Fine-Tuned Chat Models",
                        "Citation Paper Abstract": "Abstract:In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ".\nLarge Language Models With the popularization of ChatGPT, large language models (LLMs) have\ndemonstrated remarkable capabilities in instruction following and few-shot in-context learning ",
                    "Citation Text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-\ners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "is a synthetic instruction following\ndataset by prompting existing LLMs. Orca ",
                    "Citation Text": "Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Hassan Awadallah. Orca: Progressive learning from complex explanation traces of\ngpt-4. ArXiv preprint , abs/2306.02707, 2023. URL https://arxiv.org/abs/2306.02707 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.02707",
                        "Citation Paper Title": "Title:Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                        "Citation Paper Abstract": "Abstract:Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at this https URL), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
                        "Citation Paper Authors": "Authors:Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "employs a cross-encoder\nto produce pseudo-labels for query-document pairs. Similarly, Query2doc ",
                    "Citation Text": "Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language\nmodels. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing , pages 9414\u20139423, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.585. URL https://aclanthology.org/\n2023.emnlp-main.585 .\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.07678",
                        "Citation Paper Title": "Title:Query2doc: Query Expansion with Large Language Models",
                        "Citation Paper Abstract": "Abstract:This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
                        "Citation Paper Authors": "Authors:Liang Wang, Nan Yang, Furu Wei"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "generate synthetic queries for unlabeled documents, which\nare then leveraged for document expansion or retriever training. GPL ",
                    "Citation Text": "Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. GPL: Generative pseudo\nlabeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2345\u20132360, Seattle, United States, 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.168. URL https://\naclanthology.org/2022.naacl-main.168 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07577",
                        "Citation Paper Title": "Title:GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval",
                        "Citation Paper Abstract": "Abstract:Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets.\nIn this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods.\nWe further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Kexin Wang, Nandan Thakur, Nils Reimers, Iryna Gurevych"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ". More recent methods exploit supervision from\nnatural language inference ",
                    "Citation Text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing , pages 632\u2013642, Lisbon, Portugal,\n2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:\n//aclanthology.org/D15-1075 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.05326",
                        "Citation Paper Title": "Title:A large annotated corpus for learning natural language inference",
                        "Citation Paper Abstract": "Abstract:Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
                        "Citation Paper Authors": "Authors:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00366v1": {
            "Paper Title": "Argumentation in Waltz's \"Emerging Structure of International Politics''",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13401v2": {
            "Paper Title": "Time is Encoded in the Weights of Finetuned Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00290v1": {
            "Paper Title": "Red Teaming for Large Language Models At Scale: Tackling Hallucinations\n  on Mathematics Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12999v3": {
            "Paper Title": "Machine Mindset: An MBTI Exploration of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00284v1": {
            "Paper Title": "Evaluation is all you need. Prompting Generative Large Language Models\n  for Annotation Tasks in the Social Sciences. A Primer using Open Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00273v1": {
            "Paper Title": "Investigating Zero-Shot Generalizability on Mandarin-English\n  Code-Switched ASR and Speech-to-text Translation of Recent Foundation Models\n  with Self-Supervision and Weak Supervision",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". Thus, the model may not be\nstrong enough in Mandarin and likely has weak generalizabil ity to\nMandarin-English code-switching. In addition, as NTUML20 21 is\n3The data statistics in ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\nChristine Mcleavey, and Ilya Sutskever, \u201cRobust speech rec og-\nnition via large-scale weak supervision,\u201d in Proceedings of\nthe 40th International Conference on Machine Learning , An-\ndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-\ngelhardt, Sivan Sabato, and Jonathan Scarlett, Eds. 23\u201329 J ul\n2023, vol. 202 of Proceedings of Machine Learning Research ,\npp. 28492\u201328518, PMLR.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04356",
                        "Citation Paper Title": "Title:Robust Speech Recognition via Large-Scale Weak Supervision",
                        "Citation Paper Abstract": "Abstract:We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00246v1": {
            "Paper Title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00238v1": {
            "Paper Title": "How to Evaluate Coreference in Literary Texts?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.11380v2": {
            "Paper Title": "Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect\n  ChatGPT-Generated Text",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "\u2717 \u2713 Output String SHAP\nOurs \u2713 \u2713 Output String Polish Ratio\nTable 1: Summary of recent algorithms detecting ChatGPT-generated texts. ",
                    "Citation Text": "Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of\ngenerated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics:\nSystem Demonstrations , pages 111\u2013116, Florence, Italy, July 2019. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04043",
                        "Citation Paper Title": "Title:GLTR: Statistical Detection and Visualization of Generated Text",
                        "Citation Paper Abstract": "Abstract:The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs",
                        "Citation Paper Authors": "Authors:Sebastian Gehrmann, Hendrik Strobelt, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", we treat the detection as a binary classification task and build a black-box detector\nutilizing the Roberta model ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Along with the appearance of large language models such as ChatGPT, some detection algorithms are proposed to\nprevent the abuse of such powerful AI-generated text models. Recent detection methods can be roughly grouped\ninto two categories ",
                    "Citation Text": "Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. The science of detecting llm-generated texts. arXiv preprint\narXiv:2303.07205 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.07205",
                        "Citation Paper Title": "Title:The Science of Detecting LLM-Generated Texts",
                        "Citation Paper Abstract": "Abstract:The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.",
                        "Citation Paper Authors": "Authors:Ruixiang Tang, Yu-Neng Chuang, Xia Hu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00210v1": {
            "Paper Title": "The Problem of Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00177v1": {
            "Paper Title": "Principle Interference in Technical and Scientific Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00170v1": {
            "Paper Title": "L3Cube-MahaSocialNER: A Social Media based Marathi NER Dataset and BERT\n  models",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "havepresented\ntheMarathiSentimentAnalysisDataset-L3CubeMahaSent.\nTheyhavecalculatedresultsforsentimentpredictioninCNN,\nBi-LSTM,ULMFiT,mBERT,andIndicBERTmodels.TheCNN\nmodelcombined with trainable Indic fastText word embed-\ndings gave the best results in the 2-class classi\ufb01cation ex-\nperiments, slightly outperforming IndicBERT. ",
                    "Citation Text": "Onkar Litake, MaithiliSabane, Parth Patil,Aparna Ranade,and Ra vi-\nraj Joshi. 2023. Mono versus multilingual bert:A case study in hindi\nand marathi named entity recognition. In Proceedings of 3rd Interna-\ntional Conference on Recent Trends in Machine Learning, IoT , Smart\nCities andApplications: ICMISC2022 . Springer,607\u2013618.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.12907",
                        "Citation Paper Title": "Title:Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:Named entity recognition (NER) is the process of recognising and classifying important information (entities) in text. Proper nouns, such as a person's name, an organization's name, or a location's name, are examples of entities. The NER is one of the important modules in applications like human resources, customer support, search engines, content classification, and academia. In this work, we consider NER for low-resource Indian languages like Hindi and Marathi. The transformer-based models have been widely used for NER tasks. We consider different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark them on publicly available Hindi and Marathi NER datasets. We provide an exhaustive comparison of different monolingual and multilingual transformer-based models and establish simple baselines currently missing in the literature. We show that the monolingual MahaRoBERTa model performs the best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for Hindi NER. We also perform cross-language evaluation and present mixed observations.",
                        "Citation Paper Authors": "Authors:Onkar Litake, Maithili Sabane, Parth Patil, Aparna Ranade, Raviraj Joshi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "havein-\ntroduced a transformer library providing the APIs to sup-\nport cutting-edge TensorFlow or PyTorch models for com-\nplextaskslikephrasecomparison,question answering,ma-\nchinetranslation,andsummarization,amongothers.Named\nEntity Recognition for social media data is challenging be-\ncause of its inherent noisiness ",
                    "Citation Text": "Gustavo Aguilar, Suraj Maharjan, Adrian Pastor L\u00f3pez-Monro y,\nand Thamar Solorio. 2017. A Multi-task Approach for Named\nEntity Recognition in Social Media Data. In Proceedings of the\n3rd Workshop on Noisy User-generated Text . Association for\nComputational Linguistics, Copenhagen, Denmark, 148\u2013153.\nh/t_tps://doi.org/10.18653/v1/W17-4419",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04135",
                        "Citation Paper Title": "Title:A Multi-task Approach for Named Entity Recognition in Social Media Data",
                        "Citation Paper Abstract": "Abstract:Named Entity Recognition for social media data is challenging because of its inherent noisiness. In addition to improper grammatical structures, it contains spelling inconsistencies and numerous informal abbreviations. We propose a novel multi-task approach by employing a more general secondary task of Named Entity (NE) segmentation together with the primary task of fine-grained NE categorization. The multi-task neural network architecture learns higher order feature representations from word and character sequences along with basic Part-of-Speech tags and gazetteer information. This neural network acts as a feature extractor to feed a Conditional Random Fields classifier. We were able to obtain the first position in the 3rd Workshop on Noisy User-generated Text (WNUT-2017) with a 41.86% entity F1-score and a 40.24% surface F1-score.",
                        "Citation Paper Authors": "Authors:Gustavo Aguilar, Suraj Maharjan, Adrian Pastor L\u00f3pez-Monroy, Thamar Solorio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00165v1": {
            "Paper Title": "Mitigating the Impact of False Negatives in Dense Retrieval with\n  Contrastive Confidence Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00158v1": {
            "Paper Title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained\n  Language Models for Question Answering over Knowledge Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00139v1": {
            "Paper Title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.07931v2": {
            "Paper Title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17122v2": {
            "Paper Title": "Large Language Model for Causal Decision Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.05928v2": {
            "Paper Title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in\n  Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17710v1": {
            "Paper Title": "Principled Gradient-based Markov Chain Monte Carlo for Text Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17704v1": {
            "Paper Title": "TuPy-E: detecting hate speech in Brazilian Portuguese social media with\n  a novel dataset and comprehensive analysis of models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17673v1": {
            "Paper Title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "Yu, W., Pang, T., Liu, Q., Du, C., Kang, B., Huang, Y., Lin, M., Yan, S.:\nBag of Tricks for Training Data Extraction from Language Models (2023),\narXiv:2302.04460 5 ",
                    "Citation Text": "Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang,\nT., Wu, F., Wang, G.: Instruction Tuning for Large Language Models: A Survey\n(2023), arXiv:2308.10792 4Jatmo: Prompt Injection Defense by Task-Specific Finetuning 21",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.10792",
                        "Citation Paper Title": "Title:Instruction Tuning for Large Language Models: A Survey",
                        "Citation Paper Abstract": "Abstract:This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: this http URL",
                        "Citation Paper Authors": "Authors:Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "Xu, L., Chen, Y., Cui, G., Gao, H., Liu, Z.: Exploring the universal vulnerability of\nprompt-based learning paradigm. In: Findings of the Association for Computational\nLinguistics (2022) 1 ",
                    "Citation Text": "Yu, W., Pang, T., Liu, Q., Du, C., Kang, B., Huang, Y., Lin, M., Yan, S.:\nBag of Tricks for Training Data Extraction from Language Models (2023),\narXiv:2302.04460 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.04460",
                        "Citation Paper Title": "Title:Bag of Tricks for Training Data Extraction from Language Models",
                        "Citation Paper Abstract": "Abstract:With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the baseline by a large margin in most cases, providing a much stronger baseline for future research. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, Shuicheng Yan"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "Willison, S.: Delimiters won\u2019t save you from prompt injection (2023), URL https:\n//simonwillison.net/2023/May/11/delimiters-wont-save-you 5 ",
                    "Citation Text": "Xu, L., Chen, Y., Cui, G., Gao, H., Liu, Z.: Exploring the universal vulnerability of\nprompt-based learning paradigm. In: Findings of the Association for Computational\nLinguistics (2022) 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.05239",
                        "Citation Paper Title": "Title:Exploring the Universal Vulnerability of Prompt-based Learning Paradigm",
                        "Citation Paper Abstract": "Abstract:Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "Wei, A., Haghtalab, N., Steinhardt, J.: Jailbroken: How Does LLM Safety Training\nFail? (2023), arXiv:2307.02483 7, 9 ",
                    "Citation Text": "Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M.,\nLe, Q.V.: Finetuned language models are zero-shot learners (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01652",
                        "Citation Paper Title": "Title:Finetuned Language Models Are Zero-Shot Learners",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                        "Citation Paper Authors": "Authors:Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "Wang, J., Liang, Y., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., Zhou, J.: Is\nChatGPT a Good NLG Evaluator? A Preliminary Study (2023), arXiv:2303.04048\n13 ",
                    "Citation Text": "Wang, J., et al.: On the Robustness of ChatGPT: An Adversarial and Out-of-\ndistribution Perspective (2023), arXiv:2302.12095 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.12095",
                        "Citation Paper Title": "Title:On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
                        "Citation Paper Abstract": "Abstract:ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.",
                        "Citation Paper Authors": "Authors:Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "Wan, M., McAuley, J.: Item Recommendation on Monotonic Behavior Chains. In:\nProceedings of the 12th ACM Conference on Recommender Systems (2018) 12 ",
                    "Citation Text": "Wang, J., Liang, Y., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., Zhou, J.: Is\nChatGPT a Good NLG Evaluator? A Preliminary Study (2023), arXiv:2303.04048\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.04048",
                        "Citation Paper Title": "Title:Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                        "Citation Paper Abstract": "Abstract:Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.",
                        "Citation Paper Authors": "Authors:Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "See, A., Liu, P.J., Manning, C.D.: Get To The Point: Summarization with Pointer-\nGenerator Networks. In: Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL (2017) 12 ",
                    "Citation Text": "Toyer, S., et al.: Tensor Trust: Interpretable Prompt Injection Attacks from an\nOnline Game (2023), arXiv:2311.01011 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.01011",
                        "Citation Paper Title": "Title:Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
                        "Citation Paper Abstract": "Abstract:While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at this https URL",
                        "Citation Paper Authors": "Authors:Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "Schulhoff, S., et al.: Ignore This Title and HackAPrompt: Exposing Systemic\nVulnerabilities of LLMs through a Global Scale Prompt Hacking Competition\n(2023), arXiv:2311.16119 5, 13 ",
                    "Citation Text": "See, A., Liu, P.J., Manning, C.D.: Get To The Point: Summarization with Pointer-\nGenerator Networks. In: Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL (2017) 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "Perez, F., Ribeiro, I.: Ignore previous prompt: Attack techniques for language\nmodels. In: NeurIPS ML Safety Workshop (2022) 1, 4, 5, 7 ",
                    "Citation Text": "Piet, J., Sitawarin, C., Fang, V., Mu, N., Wagner, D.: Mark My Words: Analyzing\nand Evaluating Language Model Watermarks (2023), arXiv:2312.00273 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2312.00273",
                        "Citation Paper Title": "Title:Mark My Words: Analyzing and Evaluating Language Model Watermarks",
                        "Citation Paper Abstract": "Abstract:The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. In this context, the ability to distinguish machine-generated text from human-authored content becomes important. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on text watermarking techniques - as opposed to image watermarks - and proposes MARKMYWORDS, a comprehensive benchmark for them under different tasks as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. Current watermarking techniques are good enough to be deployed: Kirchenbauer et al. [1] can watermark Llama2-7B-chat with no perceivable loss in quality, the watermark can be detected with fewer than 100 tokens, and the scheme offers good tamper-resistance to simple attacks. We argue that watermark indistinguishability, a criteria emphasized in some prior works, is too strong a requirement: schemes that slightly modify logit distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. We publicly release our benchmark (this https URL)",
                        "Citation Paper Authors": "Authors:Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, David Wagner"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "OpenAI, A.P.: GPT-3 powers the next generation of apps. https://openai.com/\nblog/gpt-3-apps (2021) 3, 420 J. Piet, M. Alrashed et al. ",
                    "Citation Text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\nSimens,M.,Askell,A.,Welinder,P.,Christiano,P.,Leike,J.,Lowe,R.:Traininglan-\nguage models to follow instructions with human feedback (2022), arXiv:2203.02155\n1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02155",
                        "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                        "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "Naismith, B., Mulcaire, P., Burstein, J.: Automated evaluation of written discourse\ncoherence using GPT-4. In: Proceedings of the 18th Workshop on Innovative Use\nof NLP for Building Educational Applications (BEA 2023) (2023) 13 ",
                    "Citation Text": "Nasr, M., et al.: Scalable Extraction of Training Data from (Production) Language\nModels (2023), arXiv:2311.17035 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.17035",
                        "Citation Paper Title": "Title:Scalable Extraction of Training Data from (Production) Language Models",
                        "Citation Paper Abstract": "Abstract:This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.",
                        "Citation Paper Authors": "Authors:Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, Katherine Lee"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning\nWord Vectors for Sentiment Analysis. In: Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies\n(2011) 12 ",
                    "Citation Text": "Mao, R., Chen, G., Zhang, X., Guerin, F., Cambria, E.: GPTEval: A survey on\nassessments of ChatGPT and GPT-4 (2023), arXiv:2308.12488 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.12488",
                        "Citation Paper Title": "Title:GPTEval: A Survey on Assessments of ChatGPT and GPT-4",
                        "Citation Paper Abstract": "Abstract:The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.",
                        "Citation Paper Authors": "Authors:Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "Liu, Y., Jia, Y., Geng, R., Jia, J., Gong, N.Z.: Prompt Injection Attacks and\nDefenses in LLM-Integrated Applications (2023), arXiv:2310.12815 5 ",
                    "Citation Text": "Lukas, N., Salem, A., Sim, R., Tople, S., Wutschitz, L., Zanella-B\u00e9guelin, S.:\nAnalyzing Leakage of Personally Identifiable Information in Language Models. In:\nIEEE Symposium on Security and Privacy (2023) 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00539",
                        "Citation Paper Title": "Title:Analyzing Leakage of Personally Identifiable Information in Language Models",
                        "Citation Paper Abstract": "Abstract:Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at this https URL.",
                        "Citation Paper Authors": "Authors:Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B\u00e9guelin"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "Liu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., Wang, H., Zheng, Y.,\nLiu, Y.: Prompt Injection Attack against LLM-integrated Applications (2023),\narXiv:2306.05499 1 ",
                    "Citation Text": "Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C.: G-Eval: NLG Evaluation using\nGPT-4 with Better Human Alignment (2023), arXiv:2303.16634 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16634",
                        "Citation Paper Title": "Title:G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                        "Citation Paper Abstract": "Abstract:The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at this https URL",
                        "Citation Paper Authors": "Authors:Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Liu, X., Xu, N., Chen, M., Xiao, C.: AutoDAN: Generating Stealthy Jailbreak\nPrompts on Aligned Large Language Models (2023), arXiv:2310.04451 7 ",
                    "Citation Text": "Liu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., Wang, H., Zheng, Y.,\nLiu, Y.: Prompt Injection Attack against LLM-integrated Applications (2023),\narXiv:2306.05499 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.05499",
                        "Citation Paper Title": "Title:Prompt Injection attack against LLM-integrated Applications",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",
                        "Citation Paper Authors": "Authors:Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "Lewis, P., et al.: Retrieval-Augmented Generation for Knowledge-Intensive NLP\nTasks. Advances in Neural Information Processing Systems (2020) 8 ",
                    "Citation Text": "Li, H., Guo, D., Fan, W., Xu, M., Song, Y.: Multi-step Jailbreaking Privacy Attacks\non ChatGPT (2023), arXiv:2304.05197 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.05197",
                        "Citation Paper Title": "Title:Multi-step Jailbreaking Privacy Attacks on ChatGPT",
                        "Citation Paper Abstract": "Abstract:With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.",
                        "Citation Paper Authors": "Authors:Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Kocetkov, D., et al.: The Stack: 3 TB of permissively licensed source code.\nTransactions on Machine Learning Research (2023), ISSN 2835-8856, URL\nhttps://openreview.net/forum?id=pxpbTdUEpD 12 ",
                    "Citation Text": "Kocmi, T., Federmann, C.: Large Language Models Are State-of-the-Art Evaluators\nof Translation Quality (2023), arXiv:2302.14520 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.14520",
                        "Citation Paper Title": "Title:Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
                        "Citation Paper Abstract": "Abstract:We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT~3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.",
                        "Citation Paper Authors": "Authors:Tom Kocmi, Christian Federmann"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., yeh Chiang,\nP., Goldblum, M., Saha, A., Geiping, J., Goldstein, T.: Baseline Defenses for\nAdversarial Attacks Against Aligned Language Models (2023), arXiv:2309.00614 5 ",
                    "Citation Text": "Ji, J., et al.: AI Alignment: A Comprehensive Survey (2023), arXiv:2310.19852 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.19852",
                        "Citation Paper Title": "Title:AI Alignment: A Comprehensive Survey",
                        "Citation Paper Abstract": "Abstract:AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices.\nWe also release and continually update the website (this http URL) which features tutorials, collections of papers, blog posts, and other resources.",
                        "Citation Paper Authors": "Authors:Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Hackl, V., M\u00fcller, A.E., Granitzer, M., Sailer, M.: Is GPT-4 a reliable rater?\nEvaluating Consistency in GPT-4\u2019s Text Ratings. Frontiers in Education 8(2023)\n13 ",
                    "Citation Text": "Hermann, K.M., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay,\nW., Suleyman, M., Blunsom, P.: Teaching Machines to Read and\nComprehend. In: NIPS (2015), URL http://papers.nips.cc/paper/\n5945-teaching-machines-to-read-and-comprehend 12Jatmo: Prompt Injection Defense by Task-Specific Finetuning 19",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.03340",
                        "Citation Paper Title": "Title:Teaching Machines to Read and Comprehend",
                        "Citation Paper Abstract": "Abstract:Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
                        "Citation Paper Authors": "Authors:Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., Fritz, M.: Not what\nyou\u2019ve signed up for: Compromising real-world LLM-integrated applications with\nindirect prompt injection (2023), arXiv:2302.12173 1, 8 ",
                    "Citation Text": "Hackl, V., M\u00fcller, A.E., Granitzer, M., Sailer, M.: Is GPT-4 a reliable rater?\nEvaluating Consistency in GPT-4\u2019s Text Ratings. Frontiers in Education 8(2023)\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.02575",
                        "Citation Paper Title": "Title:Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings",
                        "Citation Paper Abstract": "Abstract:This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model (LLM) effectively distinguishes between these two criteria during evaluation. The prompt used in this study is furthermore presented and explained. Further research is necessary to assess the robustness and reliability of AI models in various use cases.",
                        "Citation Paper Authors": "Authors:Veronika Hackl, Alexandra Elena M\u00fcller, Michael Granitzer, Maximilian Sailer"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "cjadams, Sorensen, J., Elliott, J., Dixon, L., McDonald, M., nithum, Cukierski,\nW.: Toxic Comment Classification Challenge (2017), URL https://kaggle.com/\ncompetitions/jigsaw-toxic-comment-classification-challenge 12 ",
                    "Citation Text": "Dong, Y., Chen, H., Chen, J., Fang, Z., Yang, X., Zhang, Y., Tian, Y., Su, H.,\nZhu, J.: How Robust is Google\u2019s Bard to Adversarial Image Attacks? (2023),\narXiv:2309.11751 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.11751",
                        "Citation Paper Title": "Title:How Robust is Google's Bard to Adversarial Image Attacks?",
                        "Citation Paper Abstract": "Abstract:Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at this https URL.\nUpdate: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "Chiang, C.H., yi Lee, H.: Can Large Language Models Be an Alternative to Human\nEvaluations? (2023), arXiv:2305.01937 13 ",
                    "Citation Text": "Chung, H.W., et al.: Scaling Instruction-Finetuned Language Models (2022),\narXiv:2210.11416 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.11416",
                        "Citation Paper Title": "Title:Scaling Instruction-Finetuned Language Models",
                        "Citation Paper Abstract": "Abstract:Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
                        "Citation Paper Authors": "Authors:Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "Chen, Y., Wang, R., Jiang, H., Shi, S., Xu, R.: Exploring the Use of Large Language\nModels for Reference-Free Text Quality Evaluation: An Empirical Study (2023),\narXiv:2304.00723 13 ",
                    "Citation Text": "Chiang, C.H., yi Lee, H.: Can Large Language Models Be an Alternative to Human\nEvaluations? (2023), arXiv:2305.01937 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.01937",
                        "Citation Paper Title": "Title:Can Large Language Models Be an Alternative to Human Evaluations?",
                        "Citation Paper Abstract": "Abstract:Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
                        "Citation Paper Authors": "Authors:Cheng-Han Chiang, Hung-yi Lee"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "Chen, C., Shu, K.: Combating Misinformation in the Age of LLMs: Opportunities\nand Challenges (2023), arXiv:2311.05656 5 ",
                    "Citation Text": "Chen, Y., Wang, R., Jiang, H., Shi, S., Xu, R.: Exploring the Use of Large Language\nModels for Reference-Free Text Quality Evaluation: An Empirical Study (2023),\narXiv:2304.00723 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.00723",
                        "Citation Paper Title": "Title:Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
                        "Citation Paper Abstract": "Abstract:Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data.",
                        "Citation Paper Authors": "Authors:Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.J., Wong, E.: Jailbreak-\ning Black Box Large Language Models in Twenty Queries (2023), arXiv:2310.08419\n5 ",
                    "Citation Text": "Chen, C., Shu, K.: Combating Misinformation in the Age of LLMs: Opportunities\nand Challenges (2023), arXiv:2311.05656 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.05656",
                        "Citation Paper Title": "Title:Combating Misinformation in the Age of LLMs: Opportunities and Challenges",
                        "Citation Paper Abstract": "Abstract:Misinformation such as fake news and rumors is a serious threat on information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: how to utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation.",
                        "Citation Paper Authors": "Authors:Canyu Chen, Kai Shu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Carlini, N., et al.: Extracting Training Data from Large Language Models. In: 30th\nUSENIX Security Symposium (2021) 5 ",
                    "Citation Text": "Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.J., Wong, E.: Jailbreak-\ning Black Box Large Language Models in Twenty Queries (2023), arXiv:2310.08419\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.08419",
                        "Citation Paper Title": "Title:Jailbreaking Black Box Large Language Models in Twenty Queries",
                        "Citation Paper Abstract": "Abstract:There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
                        "Citation Paper Authors": "Authors:Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Bubeck, S., et al.: Sparks of Artificial General Intelligence: Early Experiments with\nGPT-4 (2023), arXiv:2303.12712 1 ",
                    "Citation Text": "Carlini, N., et al.: Extracting Training Data from Large Language Models. In: 30th\nUSENIX Security Symposium (2021) 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.07805",
                        "Citation Paper Title": "Title:Extracting Training Data from Large Language Models",
                        "Citation Paper Abstract": "Abstract:It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Bai, Y., et al.: Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback (2022), arXiv:2204.05862 4 ",
                    "Citation Text": "Bubeck, S., et al.: Sparks of Artificial General Intelligence: Early Experiments with\nGPT-4 (2023), arXiv:2303.12712 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.12712",
                        "Citation Paper Title": "Title:Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                        "Citation Paper Abstract": "Abstract:Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",
                        "Citation Paper Authors": "Authors:S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "Armstrong, S., Gorman, R.: Using GPT-Eliezer against ChatGPT Jailbreak-\ning (2022), URL https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/\nusing-gpt-eliezer-against-chatgpt-jailbreaking 5 ",
                    "Citation Text": "Bai, Y., et al.: Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback (2022), arXiv:2204.05862 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.05862",
                        "Citation Paper Title": "Title:Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                        "Citation Paper Abstract": "Abstract:We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",
                        "Citation Paper Authors": "Authors:Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17661v1": {
            "Paper Title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.15959v2": {
            "Paper Title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations\n  Conditioned on Clinical Notes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17642v1": {
            "Paper Title": "Research on the Laws of Multimodal Perception and Cognition from a\n  Cross-cultural Perspective -- Taking Overseas Chinese Gardens as an Example",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16374v2": {
            "Paper Title": "LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner\n  States Analysis",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "2.1 Large Language Models\nLarge Language Models (LLMs), predominantly structured\naround the transformer decoder architecture ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors,\nAdvances inNeural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". Thesemodels, typically comprising billions of parameters, are adept\nat capturing intricate language patterns ",
                    "Citation Text": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu\nLiu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.18223",
                        "Citation Paper Title": "Title:A Survey of Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
                        "Citation Paper Authors": "Authors:Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17591v1": {
            "Paper Title": "Towards Faithful Explanations for Text Classification with Robustness\n  Improvement and Explanation Guided Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17025v2": {
            "Paper Title": "Experiential Co-Learning of Software-Developing Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17581v1": {
            "Paper Title": "Action-Item-Driven Summarization of Long Meeting Transcripts",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "datasets to generate the general summaries for each chunk. The se are widely used\ndialoguedatasetsfortrainingdialoguesummarizationmodel s ",
                    "Citation Text": "Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2022. A Survey on Dialogue Summarization: Recent Advances and\nNewFrontiers. http://arxiv.org/abs/2107.03175 arXiv:2107.03175[cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.03175",
                        "Citation Paper Title": "Title:A Survey on Dialogue Summarization: Recent Advances and New Frontiers",
                        "Citation Paper Abstract": "Abstract:Dialogue summarization aims to condense the original dialogue into a shorter version covering salient information, which is a crucial way to reduce dialogue data overload. Recently, the promising achievements in both dialogue systems and natural language generation techniques drastically lead this task to a new landscape, which results in significant research attentions. However, there still remains a lack of a comprehensive survey for this task. To this end, we take the first step and present a thorough review of this research field carefully and widely. In detail, we systematically organize the current works according to the characteristics of each domain, covering meeting, chat, email thread, customer service and medical dialogue. Additionally, we provide an overview of publicly available research datasets as well as organize two leaderboards under unified metrics. Furthermore, we discuss some future directions, including faithfulness, multi-modal, multi-domain and multi-lingual dialogue summarization, and give our thoughts respectively. We hope that this first survey of dialogue summarization can provide the community with a quick access and a general picture to this task and motivate future researches.",
                        "Citation Paper Authors": "Authors:Xiachong Feng, Xiaocheng Feng, Bing Qin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.10019v5": {
            "Paper Title": "Translating Hanja Historical Documents to Contemporary Korean and\n  English",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17535v1": {
            "Paper Title": "Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of\n  LLMs",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "selects correct paths as augmented samples from various reasoning\npaths, thereby fine-tuning the LLMs. ",
                    "Citation Text": "Yuanzhen Xie, Tao Xie, Mingxiong Lin, WenTao Wei, Chenglin Li, Beibei Kong, Lei Chen,\nChengxiang Zhuo, Bo Hu, and Zang Li. 2023. Olagpt: Empowering llms with human-like\nproblem-solving abilities.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.16334",
                        "Citation Paper Title": "Title:OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities",
                        "Citation Paper Abstract": "Abstract:In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yuanzhen Xie, Tao Xie, Mingxiong Lin, WenTao Wei, Chenglin Li, Beibei Kong, Lei Chen, Chengxiang Zhuo, Bo Hu, Zang Li"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ", prompt-order ensemble [ 13,36], and input-\nrationale ensemble [ 24,27], and then takes a majority vote to get the final answer. Rejection-\nsampling Fine-Tuning (RFT) ",
                    "Citation Text": "Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou.\n2023. Scaling relationship on learning mathematical reasoning with large language models.\narXiv preprint arXiv:2308.01825 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.01825",
                        "Citation Paper Title": "Title:Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.",
                        "Citation Paper Authors": "Authors:Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "generates a diverse set of reasoning paths for each problem. This approach deviates from the\nconventional greedy search, instead opting for a majority vote to determine the final answer. The\nRationale-augmented Ensembles framework ",
                    "Citation Text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022.\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.00747",
                        "Citation Paper Title": "Title:Rationale-Augmented Ensembles in Language Models",
                        "Citation Paper Abstract": "Abstract:Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input -> output) prompts are expanded to (input, rationale -> output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.",
                        "Citation Paper Authors": "Authors:Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "further\noptimized the traditional CoT approach by breaking down the original problem into a series of simpler\nsub-problems, which are then solved in a sequential manner. Progressive-Hint Prompting (PHP) ",
                    "Citation Text": "Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint\nprompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.09797",
                        "Citation Paper Title": "Title:Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%), AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).",
                        "Citation Paper Authors": "Authors:Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "Recent studies have focused on improving the mathematical reasoning capabilities of LLMs through\noptimizing prompts, aggregating reasoning paths, and alignment training.\nOptimizing prompts. Chain-of-Thought (CoT) ",
                    "Citation Text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems , 35:24824\u201324837.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17522v1": {
            "Paper Title": "Overview of the PromptCBLUE Shared Task in CHIP2023",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06172v2": {
            "Paper Title": "Decoupling SQL Query Hardness Parsing for Text-to-SQL",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "propose using a syntax-based neural\nmodel (IRNet) to synthesize SemQL queries to bridge the\ngap between the intent expressed in natural language and\nthe implementation details in SQL, addressing challenges\nposed by numerous out-of-vocabulary words in predicting\ncolumns. NatSQL ",
                    "Citation Text": "Y. Gan, X. Chen, J. Xie, M. Purver, J. R. Woodward, J. Drake,\nQ. Zhang, Natural sql: Making sql easier to infer from natural lan-\nguage specifications., Findings of EMNLP 2021 (2021) 2030\u20132042.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05153",
                        "Citation Paper Title": "Title:Natural SQL: Making SQL Easier to Infer from Natural Language Specifications",
                        "Citation Paper Abstract": "Abstract:Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts for in the text descriptions; (2) removing the need for nested subqueries and set operators; and (3) making schema linking easier by reducing the required number of schema items. On Spider, a challenging text-to-SQL benchmark that contains complex and nested SQL queries, we demonstrate that NatSQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing models that do not support executable SQL generation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy.",
                        "Citation Paper Authors": "Authors:Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R. Woodward, John Drake, Qiaofu Zhang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "forencoding.Recognizingthemultipleassociations\nbetweenquestiontokens,schematables,andcolumnnames,\nthis approach treats question tokens and schema items as\nnodes and relationships as edges, constructing heteroge-\nneous graphs (e.g., RAT ",
                    "Citation Text": "T. Yu, C.-S. Wu, X. V. Lin, B. Wang, Y. C. Tan, X. Yang, D. Radev,\nR. Socher, C. Xiong, Grappa: Grammar-augmented pre-training for\ntable semantic parsing., 9th International Conference on Learning\nRepresentations, ICLR 2021. (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.13845",
                        "Citation Paper Title": "Title:GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing",
                        "Citation Paper Abstract": "Abstract:We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG) induced from existing text-to-SQL datasets. We pre-train our model on the synthetic data using a novel text-schema linking objective that predicts the syntactic role of a table field in the SQL for each question-SQL pair. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) over several existing table-and-language datasets to regularize the pre-training process. On four popular fully supervised and weakly supervised table semantic parsing benchmarks, GraPPa significantly outperforms RoBERTa-large as the feature representation layers and establishes new state-of-the-art results on all of them.",
                        "Citation Paper Authors": "Authors:Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Specifically, the question and serialized\nschema items are input into BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\nof deep bidirectional transformers for language understanding., Pro-\nceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019 (2019) 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", design decoders for decoding specific parts\nof SQL queries based on the SQL syntax structure. Syn-\ntaxSQLNet ",
                    "Citation Text": "T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, D. Radev,\nSyntaxsqlnet: syntax tree networks for complex and cross-domain\ntext-to-sql task., DOI:10.18653/v1/D18-1193. (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05237",
                        "Citation Paper Title": "Title:SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-DomainText-to-SQL Task",
                        "Citation Paper Abstract": "Abstract:Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on the Spider text-to-SQL task, which contains databases with multiple tables and complex SQL queries with multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 7.3% in exact matching accuracy. We also show that SyntaxSQLNet can further improve the performance by an additional 7.5% using a cross-domain augmentation method, resulting in a 14.8% improvement in total. To our knowledge, we are the first to study this complex and cross-domain text-to-SQL task.",
                        "Citation Paper Authors": "Authors:Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan Li, Dragomir Radev"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "incorporates an\nincremental parser into the auto-regressive decoder of the\nPLM. This parser is utilized to eliminate invalid segments\nof the generated SQL queries throughout the beam search\nprocess. Wang et al. ",
                    "Citation Text": "C. Wang, K. Tatwawadi, M. Brockschmidt, P.-S. Huang, Y. Mao,\nO. Polozov, R. Singh, Robust text-to- sql generation with execution-\nguided decoding., arXiv preprint arXiv:1807.03100. (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03100",
                        "Citation Paper Title": "Title:Robust Text-to-SQL Generation with Execution-Guided Decoding",
                        "Citation Paper Abstract": "Abstract:We consider the problem of neural semantic parsing, which translates natural language questions into executable SQL queries. We introduce a new mechanism, execution guidance, to leverage the semantics of SQL. It detects and excludes faulty programs during the decoding procedure by conditioning on the execution of partially generated program. The mechanism can be used with any autoregressive generative model, which we demonstrate on four state-of-the-art recurrent or template-based semantic parsing models. We demonstrate that execution guidance universally improves model performance on various text-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS, and GeoQuery. As a result, we achieve new state-of-the-art execution accuracy of 83.8% on WikiSQL.",
                        "Citation Paper Authors": "Authors:Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Mao, Oleksandr Polozov, Rishabh Singh"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "and depth constraints, Furthermore, GNNs are\nunabletoincorporatelanguagepatternslikePLMs,andtheir\narchitecture is constrained by relationship design, which\nhinders model robustness and generalization ",
                    "Citation Text": "C. Gao, B. Li, W. Zhang, W. Lam, B. Li, F. Huang, L. Si, Y. Li,\nTowards generalizable and robust text-to- sql parsing., Findings of\nEMNLP 2022 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.12674",
                        "Citation Paper Title": "Title:Towards Generalizable and Robust Text-to-SQL Parsing",
                        "Citation Paper Abstract": "Abstract:Text-to-SQL parsing tackles the problem of mapping natural language questions to executable SQL queries. In practice, text-to-SQL parsers often encounter various challenging scenarios, requiring them to be generalizable and robust. While most existing work addresses a particular generalization or robustness challenge, we aim to study it in a more comprehensive manner. In specific, we believe that text-to-SQL parsers should be (1) generalizable at three levels of generalization, namely i.i.d., zero-shot, and compositional, and (2) robust against input perturbations. To enhance these capabilities of the parser, we propose a novel TKK framework consisting of Task decomposition, Knowledge acquisition, and Knowledge composition to learn text-to-SQL parsing in stages. By dividing the learning process into multiple stages, our framework improves the parser's ability to acquire general SQL knowledge instead of capturing spurious patterns, making it more generalizable and robust. Experimental results under various generalization and robustness settings show that our framework is effective in all scenarios and achieves state-of-the-art performance on the Spider, SParC, and CoSQL datasets. Code can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Chang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Binhua Li, Fei Huang, Luo Si, Yongbin Li"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "). These graphs are then encoded using relation-\naware transformer networks or relational graph neural net-\nworkslikeRGCN ",
                    "Citation Text": "M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,\nM. Welling, Modeling relational data with graph convolutional net-\nworks., The Semantic Web-15th International Conference, ESWC\n2018 (2018) 593\u2013607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06103",
                        "Citation Paper Title": "Title:Modeling Relational Data with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
                        "Citation Paper Authors": "Authors:Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17515v1": {
            "Paper Title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in\n  the Avalon Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17505v1": {
            "Paper Title": "Leveraging Open-Vocabulary Diffusion to Camouflaged Instance\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.03022v2": {
            "Paper Title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "introduces an examiner LLM to validate claims produced by the\noriginal LLM, leveraging a distinct division of labor to uncover\nfactual errors. Hao et al . ",
                    "Citation Text": "Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. 2023.\nChatLLM Network: More brains, More intelligence. CoRR abs/2304.12998 (2023).\nhttps://doi.org/10.48550/ARXIV.2304.12998 arXiv:2304.12998",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.12998",
                        "Citation Paper Title": "Title:ChatLLM Network: More brains, More intelligence",
                        "Citation Paper Abstract": "Abstract:Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member.",
                        "Citation Paper Authors": "Authors:Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, Liqiang Nie"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "extends the exploration by integrating\nKG structural information into LLMs, employing self-supervised\nstructural embedding pre-training to imbue models with structural\nawareness. Wei et al . ",
                    "Citation Text": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang,\nPengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-Shot Information Extraction via Chatting with Chat-\nGPT. CoRR abs/2302.10205 (2023). https://doi.org/10.48550/ARXIV.2302.10205\narXiv:2302.10205",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.10205",
                        "Citation Paper Title": "Title:Zero-Shot Information Extraction via Chatting with ChatGPT",
                        "Citation Paper Abstract": "Abstract:Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.",
                        "Citation Paper Authors": "Authors:Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, Wenjuan Han"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", CoT [ 17,44,61], self-reflection [ 33,47], and\nfine-tuning [ 20,43,71]. Techniques such as RLHF [ 28,75], knowl-\nedge retrieval enhancement ",
                    "Citation Text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n2020. Retrieval Augmented Language Model Pre-Training. In Proceedings of\nthe 37th International Conference on Machine Learning, ICML 2020, 13-18 July\n2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) . PMLR,\n3929\u20133938. http://proceedings.mlr.press/v119/guu20a.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08909",
                        "Citation Paper Title": "Title:REALM: Retrieval-Augmented Language Model Pre-Training",
                        "Citation Paper Abstract": "Abstract:Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                        "Citation Paper Authors": "Authors:Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "delves into the application of LLMs in KG construc-\ntion and reasoning tasks, uncovering their commendable prowess\nin tasks such as reasoning and question answering. Building on this\nfoundation, Zhang et al . ",
                    "Citation Text": "Yichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen. 2023. Mak-\ning Large Language Models Perform Better in Knowledge Graph Comple-\ntion. CoRR abs/2310.06671 (2023). https://doi.org/10.48550/ARXIV.2310.06671\narXiv:2310.06671",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.06671",
                        "Citation Paper Title": "Title:Making Large Language Models Perform Better in Knowledge Graph Completion",
                        "Citation Paper Abstract": "Abstract:Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Yichi Zhang, Zhuo Chen, Wen Zhang, Huajun Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17484v1": {
            "Paper Title": "Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models\n  through Intervention without Tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17482v1": {
            "Paper Title": "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "document many examples where certain modifications to the BERT architecture and\ntraining recipe fail to improve accuracy. We also note here that there is an ongoing debate about the\nbenefits of RoPE (rotary positional embeddings) ",
                    "Citation Text": "J. Su, M. Ahmed, Y . Lu, S. Pan, W. Bo, and Y . Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing , 568:127063, 2024.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.09864",
                        "Citation Paper Title": "Title:RoFormer: Enhanced Transformer with Rotary Position Embedding",
                        "Citation Paper Abstract": "Abstract:Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "9suggested a clever change to the way encoder blocks are stacked so that computation is not wasted on\nunmasked tokens. Another recent study showed that dynamically masking the masking ratio during\npretraining leads to downstream accuracy gains in BERT ",
                    "Citation Text": "Z. Ankner, N. Saphra, D. Blalock, J. Frankle, and M. L. Leavitt. Dynamic masking rate\nschedules for mlm pretraining. arXiv preprint arXiv:2305.15096 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.15096",
                        "Citation Paper Title": "Title:Dynamic Masking Rate Schedules for MLM Pretraining",
                        "Citation Paper Abstract": "Abstract:Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.",
                        "Citation Paper Authors": "Authors:Zachary Ankner, Naomi Saphra, Davis Blalock, Jonathan Frankle, Matthew L. Leavitt"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "and\nGeGLU with SwiGLU [ 49,41]. Removing all biases from the linear layers can also potentially\nlead to slight speed ups in training time without a significant hit to accuracy. \u201cNarrowBERT\u201d ",
                    "Citation Text": "H. Li, P. Keung, D. Cheng, J. Kasai, and N. A. Smith. Narrowbert: Accelerating masked\nlanguage model pretraining and inference. arXiv preprint arXiv:2301.04761 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.04761",
                        "Citation Paper Title": "Title:NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
                        "Citation Paper Abstract": "Abstract:Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than $2\\times$. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as $3.5\\times$ with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
                        "Citation Paper Authors": "Authors:Haoxin Li, Phillip Keung, Daniel Cheng, Jungo Kasai, Noah A. Smith"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "Various studies rightly focus on a single method or modification that improves throughput, such as\nFlashAttention ",
                    "Citation Text": "T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems , 35:\n16344\u201316359, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.14135",
                        "Citation Paper Title": "Title:FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                        "Citation Paper Abstract": "Abstract:Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                        "Citation Paper Authors": "Authors:Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09085v3": {
            "Paper Title": "The Earth is Flat because...: Investigating LLMs' Belief towards\n  Misinformation via Persuasive Conversation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17475v1": {
            "Paper Title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.03109v9": {
            "Paper Title": "A Survey on Evaluation of Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 256,
                    "Sentence": "reveals that existing LMs have potential\nin ethical judgment, but still need improvement. ",
                    "Citation Text": "Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, and Mykola Pechenizkiy. 2023. CHBias: Bias Evaluation and\nMitigation of Chinese Conversational Language Models. arXiv:2305.11262 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.11262",
                        "Citation Paper Title": "Title:CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
                        "Citation Paper Abstract": "Abstract:\\textit{\\textbf{\\textcolor{red}{Warning}:} This paper contains content that may be offensive or upsetting.} Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models. Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models' conversational capabilities.",
                        "Citation Paper Authors": "Authors:Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, Mykola Pechenizkiy"
                    }
                },
                {
                    "Sentence ID": 203,
                    "Sentence": "is an early work that evaluated ChatGPT and other LLMs\nfrom both the adversarial and OOD perspectives using existing benchmarks such as AdvGLUE ",
                    "Citation Text": "Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo\nLi. 2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint\narXiv:2111.02840 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.02840",
                        "Citation Paper Title": "Title:Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models",
                        "Citation Paper Abstract": "Abstract:Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at this https URL.",
                        "Citation Paper Authors": "Authors:Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li"
                    }
                },
                {
                    "Sentence ID": 249,
                    "Sentence": "operated on two hypotheses regarding how LLMs generate factual\nor hallucinated responses. It proposed the use of three formulas (BERTScore ",
                    "Citation Text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09675",
                        "Citation Paper Title": "Title:BERTScore: Evaluating Text Generation with BERT",
                        "Citation Paper Abstract": "Abstract:We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
                        "Citation Paper Authors": "Authors:Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", but\nthe ability of ChatGPT to understand low-resource languages is limited ",
                    "Citation Text": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng\nYu, Willy Chung, et al .2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,\nand interactivity. arXiv preprint arXiv:2302.04023 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.04023",
                        "Citation Paper Title": "Title:A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                        "Citation Paper Abstract": "Abstract:This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn \"prompt engineering\" fashion. We also release codebase for evaluation set extraction.",
                        "Citation Paper Authors": "Authors:Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung"
                    }
                },
                {
                    "Sentence ID": 247,
                    "Sentence": "achieved the highest score\nin both scenarios, followed by OPT (175B) ",
                    "Citation Text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\nDiab, Xian Li, Xi Victoria Lin, et al .2022. Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01068",
                        "Citation Paper Title": "Title:OPT: Open Pre-trained Transformer Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                        "Citation Paper Authors": "Authors:Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 251,
                    "Sentence": ". In low-resource\nlearning environments, LLMs exhibit significant advantages over small language models ",
                    "Citation Text": "Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment Analysis in the Era of Large\nLanguage Models: A Reality Check. arXiv preprint arXiv:2305.15005 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.15005",
                        "Citation Paper Title": "Title:Sentiment Analysis in the Era of Large Language Models: A Reality Check",
                        "Citation Paper Abstract": "Abstract:Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 218,
                    "Sentence": ". In fine-grained sentiment and\nemotion cause analysis, ChatGPT also exhibits exceptional performance ",
                    "Citation Text": "Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is ChatGPT a Good Sentiment Analyzer? A\nPreliminary Study. arXiv:2304.04339 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.04339",
                        "Citation Paper Title": "Title:Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study",
                        "Citation Paper Abstract": "Abstract:Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.",
                        "Citation Paper Authors": "Authors:Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia"
                    }
                },
                {
                    "Sentence ID": 105,
                    "Sentence": "showed that ChatGPT outperforms GPT-3.5 for\nNLI tasks. They also found that ChatGPT excels in handling factual input that could be attributed\nto its RLHF training process in favoring human feedback. However, Lee et al . ",
                    "Citation Text": "Noah Lee, Na Min An, and James Thorne. 2023. Can Large Language Models Infer and Disagree Like Humans? arXiv\npreprint arXiv:2305.13788 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.13788",
                        "Citation Paper Title": "Title:Can Large Language Models Capture Dissenting Human Voices?",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population. The source code for the experiments is available at this https URL",
                        "Citation Paper Authors": "Authors:Noah Lee, Na Min An, James Thorne"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "\u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nBai et al. ",
                    "Citation Text": "Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu,\net al.2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner. arXiv preprint arXiv:2306.04181\n(2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.04181",
                        "Citation Paper Title": "Title:Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                        "Citation Paper Abstract": "Abstract:Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: this http URL.",
                        "Citation Paper Authors": "Authors:Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15166v2": {
            "Paper Title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective\n  Depth Up-Scaling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17429v1": {
            "Paper Title": "Commonsense for Zero-Shot Natural Language Video Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.15427v2": {
            "Paper Title": "Graph Neural Prompting with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17349v1": {
            "Paper Title": "Language Model as an Annotator: Unsupervised Context-aware Quality\n  Phrase Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17343v1": {
            "Paper Title": "AQUALLM: Audio Question Answering Data Generation Using Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17342v1": {
            "Paper Title": "SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language\n  Models for Private and Secure Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17337v1": {
            "Paper Title": "Exploring Nature: Datasets and Models for Analyzing Nature-Related\n  Disclosures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.12475v2": {
            "Paper Title": "PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with\n  Unassimilated Loanwords",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17249v1": {
            "Paper Title": "Do Androids Know They're Only Dreaming of Electric Sheep?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17244v1": {
            "Paper Title": "The LLM Surgeon",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17242v1": {
            "Paper Title": "Learning to Generate Text in Arbitrary Writing Styles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17180v1": {
            "Paper Title": "Virtual Scientific Companion for Synchrotron Beamlines: A Prototype",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". Prince et al employed GPT-3.5 and open-source model Vicuna to illustrate the applicability of LLMs on\nexperiment guidance ",
                    "Citation Text": "Michael H Prince, Henry Chan, Aikaterini Vriza, Tao Zhou, Varuni K Sastry, Matthew T Dearing, Ross J Harder,\nRama K Vasudevan, and Mathew J Cherukara. Opportunities for retrieval and tool augmented large language\nmodels in scientific facilities. arXiv preprint arXiv:2312.01291 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2312.01291",
                        "Citation Paper Title": "Title:Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities",
                        "Citation Paper Abstract": "Abstract:Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics. However, these facility and instrument upgrades come with a significant increase in complexity. Driven by more exacting scientific needs, instruments and experiments become more intricate each year. This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments. Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage. Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation. With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures. With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments. By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities' users and accelerate scientific output.",
                        "Citation Paper Authors": "Authors:Michael H. Prince, Henry Chan, Aikaterini Vriza, Tao Zhou, Varuni K. Sastry, Matthew T. Dearing, Ross J. Harder, Rama K. Vasudevan, Mathew J. Cherukara"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17172v1": {
            "Paper Title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,\n  Language, Audio, and Action",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". However, its\nperformance drops significantly in multi-object 3D detec-\ntion tasks, like those on nuScenes ",
                    "Citation Text": "Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuScenes: A mul-\ntimodal dataset for autonomous driving. In CVPR , 2020.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11027",
                        "Citation Paper Title": "Title:nuScenes: A multimodal dataset for autonomous driving",
                        "Citation Paper Abstract": "Abstract:Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
                        "Citation Paper Authors": "Authors:Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 109,
                    "Sentence": "\u221750.0 - - - - - - 27.8 - -\nBLIP-2 ",
                    "Citation Text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models. In\nICML , 2023. 3, 10, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12597",
                        "Citation Paper Title": "Title:BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Citation Paper Abstract": "Abstract:The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "to the\nqueries and keys before the dot-product attention computa-\ntion.\nScaled Cosine Attention. We use perceiver resampler ",
                    "Citation Text": "Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Daniel Zoran, Andrew Brock, Evan\nShelhamer, et al. Perceiver IO: A General Architecture for\nStructured Inputs & Outputs. In ICLR , 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.14795",
                        "Citation Paper Title": "Title:Perceiver IO: A General Architecture for Structured Inputs & Outputs",
                        "Citation Paper Abstract": "Abstract:A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",
                        "Citation Paper Authors": "Authors:Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\u0101o Carreira"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "gener-\nate features that a diffusion model can use, and JAM ",
                    "Citation Text": "Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan,\nand Barlas Oguz. Jointly Training Large Autoregressive\nMultimodal Models. arXiv preprint arXiv:2309.15564 ,\n2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.15564",
                        "Citation Paper Title": "Title:Jointly Training Large Autoregressive Multimodal Models",
                        "Citation Paper Abstract": "Abstract:In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                        "Citation Paper Authors": "Authors:Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": "train models to generate tokens that\na VQ-GAN [49, 179] can then decode into an image,\nwhile GILL ",
                    "Citation Text": "Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating Images with Multimodal Language Models. In\nNeurIPS , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.17216",
                        "Citation Paper Title": "Title:Generating Images with Multimodal Language Models",
                        "Citation Paper Abstract": "Abstract:We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                        "Citation Paper Authors": "Authors:Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": ". Other efforts have added new input modali-\nties. This includes video inputs [110, 126], audio ",
                    "Citation Text": "Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,\nXuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,\nJiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding\n14and Generating Speech, Music, Sound, and Talking Head.\narXiv preprint arXiv:2304.12995 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.12995",
                        "Citation Paper Title": "Title:AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", embodied\nAI tasks [17, 135, 140, 152] or leverage other expert sys-\ntems ",
                    "Citation Text": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al. LLaMA-Adapter V2: Parameter-\nEfficient Visual Instruction Model. arXiv preprint\narXiv:2304.15010 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.15010",
                        "Citation Paper Title": "Title:LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
                        "Citation Paper Abstract": "Abstract:How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset. During inference, we incorporate additional expert models (e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image understanding capability without incurring training costs. Compared to the original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. The newly designed framework also exhibits stronger language-only instruction-following capabilities and even excels in chat interactions. Our code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17120v1": {
            "Paper Title": "Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale\n  Pretraining Corpus for Math",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17115v1": {
            "Paper Title": "How Far Are We from Believable AI Agents? A Framework for Evaluating the\n  Believability of Human Behavior Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.03952v3": {
            "Paper Title": "Is ChatGPT a Good Personality Recognizer? A Preliminary Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17109v1": {
            "Paper Title": "MIVC: Multiple Instance Visual Component for Visual-Language Models",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "to ask the model to generate a short descriptive\nsentence of the product using multiple images. Instead of\nusing original product titles, which are often less descrip-\ntive, we leverage the manually annotated captions ",
                    "Citation Text": "Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models, 2023.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.07279",
                        "Citation Paper Title": "Title:Scalable 3D Captioning with Pretrained Models",
                        "Citation Paper Abstract": "Abstract:We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion.",
                        "Citation Paper Authors": "Authors:Tiange Luo, Chris Rockwell, Honglak Lee, Justin Johnson"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "utilizes a fully connected layer in MIL. Further-\nmore, AB-MIL ",
                    "Citation Text": "Maximilian Ilse, Jakub Tomczak, and Max Welling.\nAttention-based deep multiple instance learning. In Inter-\nnational conference on machine learning , pages 2127\u20132136.\nPMLR, 2018. 2, 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04712",
                        "Citation Paper Title": "Title:Attention-based Deep Multiple Instance Learning",
                        "Citation Paper Abstract": "Abstract:Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.",
                        "Citation Paper Authors": "Authors:Maximilian Ilse, Jakub M. Tomczak, Max Welling"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ",\nfor instance, embarked on a pretraining task where images\nwere tokenized, enabled text-to-image generation. Subse-\nquently, various vision-language models (VLMs) have been\nproposed to enhance the fusion of text and images. For\nexample, BLIP2 ",
                    "Citation Text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597 , 2023. 1, 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12597",
                        "Citation Paper Title": "Title:BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Citation Paper Abstract": "Abstract:The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "employs attention not only to consider instance-\nto-instance relationships but also instance-to-bag relation-\nships; DTFD-MIL ",
                    "Citation Text": "Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao,\nXiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-\nmil: Double-tier feature distillation multiple instance learn-\ning for histopathology whole slide image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 18802\u201318812, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.12081",
                        "Citation Paper Title": "Title:DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification",
                        "Citation Paper Abstract": "Abstract:Multiple instance learning (MIL) has been increasingly used in the classification of histopathology whole slide images (WSIs). However, MIL approaches for this specific classification problem still face unique challenges, particularly those related to small sample cohorts. In these, there are limited number of WSI slides (bags), while the resolution of a single WSI is huge, which leads to a large number of patches (instances) cropped from this slide. To address this issue, we propose to virtually enlarge the number of bags by introducing the concept of pseudo-bags, on which a double-tier MIL framework is built to effectively use the intrinsic features. Besides, we also contribute to deriving the instance probability under the framework of attention-based MIL, and utilize the derivation to help construct and analyze the proposed framework. The proposed method outperforms other latest methods on the CAMELYON-16 by substantially large margins, and is also better in performance on the TCGA lung cancer dataset. The proposed framework is ready to be extended for wider MIL applications. The code is available at: this https URL",
                        "Citation Paper Authors": "Authors:Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E. Coupland, Yalin Zheng"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "employs attention during the pooling\nprocess, allowing for better weighting of different instances.Another category of methods attempts to consider the re-\nlationships between different instances using graph neural\nnetworks or capsule neural networks. More recently, DS-\nMIL ",
                    "Citation Text": "Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple\ninstance learning network for whole slide image classifica-\ntion with self-supervised contrastive learning. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 14318\u201314328, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.08939",
                        "Citation Paper Title": "Title:Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems. GitHub repository: this https URL",
                        "Citation Paper Authors": "Authors:Bin Li, Yin Li, Kevin W. Eliceiri"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "employed triplet contrastive learning to si-\nmultaneously learn from text and images. FROMAGe ",
                    "Citation Text": "Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal gen-\neration. arXiv preprint arXiv:2301.13823 , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.13823",
                        "Citation Paper Title": "Title:Grounding Language Models to Images for Multimodal Inputs and Outputs",
                        "Citation Paper Abstract": "Abstract:We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
                        "Citation Paper Authors": "Authors:Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "introduced the use of a Q-Former\nto align images more effectively with the input space of\ntext. TCL ",
                    "Citation Text": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,\nLiqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou\nHuang. Vision-language pre-training with triple contrastive\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 15671\u2013\n15680, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10401",
                        "Citation Paper Title": "Title:Vision-Language Pre-Training with Triple Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g., InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual information (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in degraded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this paper, we propose triple contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive and achieves the new state of the art on various common down-stream vision-language tasks such as image-text retrieval and visual question answering.",
                        "Citation Paper Authors": "Authors:Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.08777v2": {
            "Paper Title": "Question-Answering System Extracts Information on Injection Drug Use\n  from Clinical Notes",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ", multi-label classification [34, 35, 36, 37],\nconcept extraction, and joint extraction of entities and relations have been employed to extract\ninformation about drug use ",
                    "Citation Text": "Kevin Lybarger, Nicholas J Dobbins, Ritche Long, Angad Singh, Patrick Wedgeworth, Ozlem\nOzuner, and Meliha Yetisgen. Leveraging natural language processing to augment struc-\ntured social determinants of health data in the electronic health record. arXiv preprint\narXiv:2212.07538 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.07538",
                        "Citation Paper Title": "Title:Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record",
                        "Citation Paper Abstract": "Abstract:Objective: Social determinants of health (SDOH) impact health outcomes and are documented in the electronic health record (EHR) through structured data and unstructured clinical notes. However, clinical notes often contain more comprehensive SDOH information, detailing aspects such as status, severity, and temporality. This work has two primary objectives: i) develop a natural language processing (NLP) information extraction model to capture detailed SDOH information and ii) evaluate the information gain achieved by applying the SDOH extractor to clinical narratives and combining the extracted representations with existing structured data.\nMaterials and Methods: We developed a novel SDOH extractor using a deep learning entity and relation extraction architecture to characterize SDOH across various dimensions. In an EHR case study, we applied the SDOH extractor to a large clinical data set with 225,089 patients and 430,406 notes with social history sections and compared the extracted SDOH information with existing structured data.\nResults: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the EHR case study, we found extracted SDOH information complements existing structured data with 32% of homeless patients, 19% of current tobacco users, and 10% of drug users only having these health risk factors documented in the clinical narrative.\nConclusions: Utilizing EHR data to identify SDOH health risk factors and social needs may improve patient care and outcomes. Semantic representations of text-encoded SDOH information can augment existing structured data, and this more comprehensive SDOH representation can assist health systems in identifying and addressing these social needs.",
                        "Citation Paper Authors": "Authors:Kevin Lybarger, Nicholas J Dobbins, Ritche Long, Angad Singh, Patrick Wedgeworth, Ozlem Ozuner, Meliha Yetisgen"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". Multi-label text classification and sequence\nlabeling have been used to identify sentences containing labeled arguments about drug use ",
                    "Citation Text": "Manabu Torii, Ian M Finn, Son Doan, Paul Wang, Elly W Yang, and Daniel S Zisook.\nTask formulation for extracting social determinants of health from clinical narratives. arXiv\npreprint arXiv:2301.11386 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.11386",
                        "Citation Paper Title": "Title:Task formulation for Extracting Social Determinants of Health from Clinical Narratives",
                        "Citation Paper Abstract": "Abstract:Objective: The 2022 n2c2 NLP Challenge posed identification of social determinants of health (SDOH) in clinical narratives. We present three systems that we developed for the Challenge and discuss the distinctive task formulation used in each of the three systems. Materials and Methods: The first system identifies target pieces of information independently using machine learning classifiers. The second system uses a large language model (LLM) to extract complete structured outputs per document. The third system extracts candidate phrases using machine learning and identifies target relations with hand-crafted rules. Results: The three systems achieved F1 scores of 0.884, 0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third, seventh, and eighth among the 15 participating teams. The review of the extraction results from our systems reveals characteristics of each approach and those of the SODH extraction task. Discussion: Phrases and relations annotated in the task is unique and diverse, not conforming to the conventional event extraction task. These annotations are difficult to model with limited training data. The system that extracts information independently, ignoring the annotated relations, achieves the highest F1 score. Meanwhile, LLM with its versatile capability achieves the high F1 score, while respecting the annotated relations. The rule-based system tackling relation extraction obtains the low F1 score, while it is the most explainable approach. Conclusion: The F1 scores of the three systems vary in this challenge setting, but each approach has advantages and disadvantages in a practical application. The selection of the approach depends not only on the F1 score but also on the requirements in the application.",
                        "Citation Paper Authors": "Authors:Manabu Torii, Ian M. Finn, Son Doan, Paul Wang, Elly W. Yang, Daniel S. Zisook"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Machine reading comprehension has been\napplied to extract some clinical concept categories and relation categories, such as relations of\nmedications with adverse drug events and SBDH ",
                    "Citation Text": "Cheng Peng, Xi Yang, Zehao Yu, Jiang Bian, William R Hogan, and Yonghui Wu. Clinical\nconcept and relation extraction using prompt-based machine reading comprehension. arXiv\npreprint arXiv:2303.08262 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.08262",
                        "Citation Paper Title": "Title:Clinical Concept and Relation Extraction Using Prompt-based Machine Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Objective: To develop a natural language processing system that solves both clinical concept extraction and relation extraction in a unified prompt-based machine reading comprehension (MRC) architecture with good generalizability for cross-institution applications.\nMethods: We formulate both clinical concept extraction and relation extraction using a unified prompt-based MRC architecture and explore state-of-the-art transformer models. We compare our MRC models with existing deep learning models for concept extraction and end-to-end relation extraction using two benchmark datasets developed by the 2018 National NLP Clinical Challenges (n2c2) challenge (medications and adverse drug events) and the 2022 n2c2 challenge (relations of social determinants of health [SDoH]). We also evaluate the transfer learning ability of the proposed MRC models in a cross-institution setting. We perform error analyses and examine how different prompting strategies affect the performance of MRC models.\nResults and Conclusion: The proposed MRC models achieve state-of-the-art performance for clinical concept and relation extraction on the two benchmark datasets, outperforming previous non-MRC transformer models. GatorTron-MRC achieves the best strict and lenient F1-scores for concept extraction, outperforming previous deep learning models on the two datasets by 1%~3% and 0.7%~1.3%, respectively. For end-to-end relation extraction, GatorTron-MRC and BERT-MIMIC-MRC achieve the best F1-scores, outperforming previous deep learning models by 0.9%~2.4% and 10%-11%, respectively. For cross-institution evaluation, GatorTron-MRC outperforms traditional GatorTron by 6.4% and 16% for the two datasets, respectively. The proposed method is better at handling nested/overlapped concepts, extracting relations, and has good portability for cross-institute applications.",
                        "Citation Paper Authors": "Authors:Cheng Peng, Xi Yang, Zehao Yu, Jiang Bian, William R. Hogan, Yonghui Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17295v1": {
            "Paper Title": "Optimizing watermarks for large language models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17090v1": {
            "Paper Title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined\n  Levels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17080v1": {
            "Paper Title": "Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil\n  Cognitive Depth in LLMs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.10062v2": {
            "Paper Title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.03549v3": {
            "Paper Title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language\n  Model through Expert Feedback and Real-world Multi-turn Dialogue",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17055v1": {
            "Paper Title": "Improving In-context Learning via Bidirectional Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16975v1": {
            "Paper Title": "Few-shot learning for automated content analysis: Efficient coding of\n  arguments and claims in the debate on arms deliveries to Ukraine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.11940v4": {
            "Paper Title": "Audio Generation with Multiple Conditional Diffusion Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.02772v3": {
            "Paper Title": "Hot or Cold? Adaptive Temperature Sampling for Code Generation with\n  Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17289v1": {
            "Paper Title": "AI Content Self-Detection for Transformer-based Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "in 2014. A milestone for text data was the development\nof the transformer architecture ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all\nyou need,\u201d CoRR , vol. abs/1706.03762, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1706.03762",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "indicate students could potentially use ChatGPT to bypass\nplagiarism detection. This indicates that plagiarism detection\nwill need to shift its focus to verifying the origin of the content.\nYu et al ",
                    "Citation Text": "P. Yu, J. Chen, X. Feng, and Z. Xia, \u201cCHEAT: A large-scale\ndataset for detecting ChatGPT-writtEn AbsTracts,\u201d arXiv preprint\narXiv:2304.12008 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.12008",
                        "Citation Paper Title": "Title:CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts",
                        "Citation Paper Abstract": "Abstract:The powerful ability of ChatGPT has caused widespread concern in the academic community. Malicious users could synthesize dummy academic content through ChatGPT, which is extremely harmful to academic rigor and originality. The need to develop ChatGPT-written content detection algorithms call for large-scale datasets. In this paper, we initially investigate the possible negative impact of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract dataset (CHEAT) to support the development of detection algorithms. In particular, the ChatGPT-written abstract dataset contains 35,304 synthetic abstracts, with Generation, Polish, and Mix as prominent representatives. Based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. We show that ChatGPT-written abstracts are detectable, while the detection difficulty increases with human involvement.",
                        "Citation Paper Authors": "Authors:Peipeng Yu, Jiahan Chen, Xuan Feng, Zhihua Xia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16917v1": {
            "Paper Title": "Unified Lattice Graph Fusion for Chinese Named Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09785v2": {
            "Paper Title": "RJUA-QA: A Comprehensive QA Dataset for Urology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16903v1": {
            "Paper Title": "Spike No More: Stabilizing the Pre-training of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16893v1": {
            "Paper Title": "BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16864v1": {
            "Paper Title": "OmniDialog: An Omnipotent Pre-training Model for Task-Oriented Dialogue\n  System",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ".\nd) Dialogue Summarization: : The considered baselines\ninclude Lead-3 ",
                    "Citation Text": "A. See, P. J. Liu, and C. D. Manning, \u201cGet to the point: Summarization with\npointer-generator networks,\u201d arXiv preprint arXiv:1704.04368 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04368",
                        "Citation Paper Title": "Title:Get To The Point: Summarization with Pointer-Generator Networks",
                        "Citation Paper Abstract": "Abstract:Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
                        "Citation Paper Authors": "Authors:Abigail See, Peter J. Liu, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "with 77 banking scenarios is considered.\nd) Dialogue Summarization: aims to extract key informa-\ntion and generate concise summaries from dialogues. In this\nwork, we select TODSum ",
                    "Citation Text": "L. Zhao, F. Zheng, K. He, W. Zeng, Y. Lei, H. Jiang, W. Wu, W. Xu, J. Guo,\nand F. Meng, \u201cTodsum: Task-oriented dialogue summarization with state\ntracking,\u201d arXiv preprint arXiv:2110.12680 , 2021.JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2023 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.12680",
                        "Citation Paper Title": "Title:TODSum: Task-Oriented Dialogue Summarization with State Tracking",
                        "Citation Paper Abstract": "Abstract:Previous dialogue summarization datasets mainly focus on open-domain chitchat dialogues, while summarization datasets for the broadly used task-oriented dialogue haven't been explored yet. Automatically summarizing such task-oriented dialogues can help a business collect and review needs to improve the service. Besides, previous datasets pay more attention to generate good summaries with higher ROUGE scores, but they hardly understand the structured information of dialogues and ignore the factuality of summaries. In this paper, we introduce a large-scale public Task-Oriented Dialogue Summarization dataset, TODSum, which aims to summarize the key points of the agent completing certain tasks with the user. Compared to existing work, TODSum suffers from severe scattered information issues and requires strict factual consistency, which makes it hard to directly apply recent dialogue summarization models. Therefore, we introduce additional dialogue state knowledge for TODSum to enhance the faithfulness of generated summaries. We hope a better understanding of conversational content helps summarization models generate concise and coherent summaries. Meanwhile, we establish a comprehensive benchmark for TODSum and propose a state-aware structured dialogue summarization model to integrate dialogue state information and dialogue history. Exhaustive experiments and qualitative analysis prove the effectiveness of dialogue structure guidance. Finally, we discuss the current issues of TODSum and potential development directions for future work.",
                        "Citation Paper Authors": "Authors:Lulu Zhao, Fujia Zheng, Keqing He, Weihao Zeng, Yuejie Lei, Huixing Jiang, Wei Wu, Weiran Xu, Jun Guo, Fanyu Meng"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ".\nb) DST Task: : the considered baselines are TRADE ",
                    "Citation Text": "C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong, R. Socher, and P. Fung,\n\u201cTransferable multi-domain state generator for task-oriented dialogue\nsystems,\u201d arXiv preprint arXiv:1905.08743 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08743",
                        "Citation Paper Title": "Title:Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:Over-dependence on domain ontology and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using a copy mechanism, facilitating knowledge transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art joint goal accuracy of 48.62% for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show its transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.",
                        "Citation Paper Authors": "Authors:Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, Pascale Fung"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": ".\na)Experimental Settings :The TODSum dataset com-\nprises five domains: train, taxi, restaurant, hotel, and attraction.\nLike previous approaches ",
                    "Citation Text": "L. Zhao, F. Zheng, W. Zeng, K. He, W. Xu, H. Jiang, W. Wu, and\nY. Wu, \u201cDomain-oriented prefix-tuning: Towards efficient and general-\nizable fine-tuning for zero-shot dialogue summarization,\u201d arXiv preprint\narXiv:2204.04362 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.04362",
                        "Citation Paper Title": "Title:Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization",
                        "Citation Paper Abstract": "Abstract:The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts discrete prompts to guide the model to focus on key contents of dialogues and enhance model generalization. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and qualitative analysis prove the effectiveness of our methods.",
                        "Citation Paper Authors": "Authors:Lulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He, Weiran Xu, Huixing Jiang, Wei Wu, Yanan Wu"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "refers to\npredicting the system response from a user\u2019s speech or text input\nin a conversational system. Here, we consider the MultiWOZ\n2.0 ",
                    "Citation Text": "P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes,\nO. Ramadan, and M. Ga \u02c7si\u00b4c, \u201cMultiwoz\u2013a large-scale multi-domain\nwizard-of-oz dataset for task-oriented dialogue modelling,\u201d arXiv preprint\narXiv:1810.00278 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00278",
                        "Citation Paper Title": "Title:MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
                        "Citation Paper Abstract": "Abstract:Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of $10$k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
                        "Citation Paper Authors": "Authors:Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, Milica Ga\u0161i\u0107"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "used a novel\nspan-based dynamic convolution to replace these self-attention\nheads in BERT, showing good performance in GLUE ",
                    "Citation Text": "A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n\u201cGlue: A multi-task benchmark and analysis platform for natural language\nunderstanding,\u201d arXiv preprint arXiv:1804.07461 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07461",
                        "Citation Paper Title": "Title:GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
                        "Citation Paper Abstract": "Abstract:For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.",
                        "Citation Paper Authors": "Authors:Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16862v1": {
            "Paper Title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "with 107,439 questions aimed at challenging visual understanding\nand reasoning in the context of icon images, encompassing three sub-tasks (multi-image-choice,\nmulti-text-choice, and filling-in-the-blank). VizWiz ",
                    "Citation Text": "Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo,\nand Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people.\nInProceedings of the IEEE conference on computer vision and pattern recognition , pages\n3608\u20133617, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08218",
                        "Citation Paper Title": "Title:VizWiz Grand Challenge: Answering Visual Questions from Blind People",
                        "Citation Paper Abstract": "Abstract:The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of over 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.",
                        "Citation Paper Authors": "Authors:Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P. Bigham"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "comprises over 10k natural text-image pairs in English, encompassing 66 types\nof spatial relations. IconQA ",
                    "Citation Text": "Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,\nand Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual\nlanguage reasoning, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.13214",
                        "Citation Paper Title": "Title:IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning",
                        "Citation Paper Abstract": "Abstract:Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at this https URL.",
                        "Citation Paper Authors": "Authors:Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "introduced an efficient\nQ-Former for aligning visual and language modalities. These groundbreaking studies have paved the\nway for further innovations in the field, leading to the development of models like LLaV A ",
                    "Citation Text": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08485",
                        "Citation Paper Title": "Title:Visual Instruction Tuning",
                        "Citation Paper Abstract": "Abstract:Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                        "Citation Paper Authors": "Authors:Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "leverage these\npowerful models to answer diverse questions and perform complex tasks such as coding. The\nintroduction of open-source LLMs like LLaMA ",
                    "Citation Text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.09288",
                        "Citation Paper Title": "Title:Llama 2: Open Foundation and Fine-Tuned Chat Models",
                        "Citation Paper Abstract": "Abstract:In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", which incorporated gated cross-attention mechanisms to align pre-trained vision\nencoders and language models, training on vast image-text pairs. BLIP-2 ",
                    "Citation Text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12597",
                        "Citation Paper Title": "Title:BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Citation Paper Abstract": "Abstract:The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", which utilized pre-trained language models for image\ncaptioning and visual question answering. This approach was further advanced by models such\nas Flamingo ",
                    "Citation Text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems ,\n35:23716\u201323736, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.14198",
                        "Citation Paper Title": "Title:Flamingo: a Visual Language Model for Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". These models fine-tune the LLaMA\nmodel with additional high-quality instruction datasets, showcasing the versatility and adaptability\n2of LLM frameworks.Among the most notable recent advancements are Phi ",
                    "Citation Text": "Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. Textbooks are all you need ii: phi-1.5 technical report, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.05463",
                        "Citation Paper Title": "Title:Textbooks Are All You Need II: phi-1.5 technical report",
                        "Citation Paper Abstract": "Abstract:We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.",
                        "Citation Paper Authors": "Authors:Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.18365v3": {
            "Paper Title": "What can Large Language Models do in chemistry? A comprehensive\n  benchmark on eight tasks",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "developed ChemCrow, which augmented LLMs with chem-expert designed tools for downstream\ntasks such as organic synthesis and drug discovery. Similarly, by leveraging the planning and\nexecution ability of multiple LLMs, Boiko et al ",
                    "Citation Text": "Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research\ncapabilities of large language models. arXiv preprint arXiv:2304.05332 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.05332",
                        "Citation Paper Title": "Title:Emergent autonomous scientific research capabilities of large language models",
                        "Citation Paper Abstract": "Abstract:Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.",
                        "Citation Paper Authors": "Authors:Daniil A. Boiko, Robert MacKnight, Gabe Gomes"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". Specifically, in healthcare, the utility and safety\nof LLMs in clinical settings were explored ",
                    "Citation Text": "Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-\nbilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.13375",
                        "Citation Paper Title": "Title:Capabilities of GPT-4 on Medical Challenge Problems",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
                        "Citation Paper Authors": "Authors:Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, Eric Horvitz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.10253v2": {
            "Paper Title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized\n  Image-Dialogue Data",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "35.5 - - 48.2\nIDEFICS-80B 36.0 - - 54.5\nQwen-VL ",
                    "Citation Text": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.12966",
                        "Citation Paper Title": "Title:Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                        "Citation Paper Abstract": "Abstract:In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", for example, uses GPT4 to\nconstruct datasets based on the annotations of the COCO\ndataset ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740\u2013755. Springer, 2014. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "introduces a lightweight adapter\nmodule for parameter-efficient tuning, and LaVIN ",
                    "Citation Text": "Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai\nSun, and Rongrong Ji. Cheap and quick: Efficient vision-\nlanguage instruction tuning for large language models. arXiv\npreprint arXiv:2305.15023 , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.15023",
                        "Citation Paper Title": "Title:Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
                        "Citation Paper Abstract": "Abstract:Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at this https URL.",
                        "Citation Paper Authors": "Authors:Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "have\nemployed learnable queries to extract information from the\nembeddings of other modalities, an approach first used in\nthe Flamingo model ",
                    "Citation Text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems , 35:23716\u201323736,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.14198",
                        "Citation Paper Title": "Title:Flamingo: a Visual Language Model for Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "learns a simple linear layer to translate the im-\nage features to language tokens, a strategy that has yielded\nsurprisingly good results. In contrast, other works ",
                    "Citation Text": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.14178",
                        "Citation Paper Title": "Title:mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at this https URL. The online demo is available at this https URL.",
                        "Citation Paper Authors": "Authors:Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.09772v2": {
            "Paper Title": "Rethinking Relation Classification with Graph Meaning Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16778v1": {
            "Paper Title": "Adversarial Representation with Intra-Modal and Inter-Modal Graph\n  Contrastive Learning for Multimodal Emotion Recognition",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "to capture\ndeeper image features in videos and reduce the introduction of\nnoisy information. (3) Audio Feature Extraction: The encoder\narchitecture is adopted ",
                    "Citation Text": "Y . Jia, Y . Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, z. Chen, P. Nguyen,\nR. Pang, I. Lopez Moreno, and Y . Wu, \u201cTransfer learning from speaker\nverification to multispeaker text-to-speech synthesis,\u201d in Advances in\nNeural Information Processing Systems , vol. 31. MIT, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04558",
                        "Citation Paper Title": "Title:Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.",
                        "Citation Paper Authors": "Authors:Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposed a Low-rank Fusion Network (LFN) to\nrealize dimensionality reduction of tensors through low-rank\ndecomposition operations, which has achieved performance\nimprovement in emotion recognition. Moreover, Hu et al. ",
                    "Citation Text": "J. Hu, Y . Liu, J. Zhao, and Q. Jin, \u201cMmgcn: Multimodal fusion via deep\ngraph convolution network for emotion recognition in conversation,\u201d\ninProceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers) , 2021, pp.\n5666\u20135675.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06779",
                        "Citation Paper Title": "Title:MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users' emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.",
                        "Citation Paper Authors": "Authors:Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16755v1": {
            "Paper Title": "Graph Neural Networks for Antisocial Behavior Detection on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07492v4": {
            "Paper Title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in\n  Generative Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17279v1": {
            "Paper Title": "Stateful FastConformer with Cache-based Inference for Streaming\n  Automatic Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". It\nis calculated as the average time needed for each word to get\npredicted by the model while ignoring the inference time of\nthe neural network.\nWe used FastEmit ",
                    "Citation Text": "Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang,\nTara N Sainath, Yanzhang He, Arun Narayanan, Wei\nHan, Anmol Gulati, Yonghui Wu, et al., \u201cFastemit:\nLow-latency streaming asr with sequence-level emis-\nsion regularization,\u201d in ICASSP , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11148",
                        "Citation Paper Title": "Title:FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization",
                        "Citation Paper Abstract": "Abstract:Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible. However, emitting fast without degrading quality, as measured by word error rate (WER), is highly challenging. Existing approaches including Early and Late Penalties and Constrained Alignments penalize emission delay by manipulating per-token or per-frame probability prediction in sequence transducer models. While being successful in reducing delay, these approaches suffer from significant accuracy regression and also require additional word alignment information from an existing model. In this work, we propose a sequence-level emission regularization method, named FastEmit, that applies latency regularization directly on per-sequence probability in training transducer models, and does not require any alignment. We demonstrate that FastEmit is more suitable to the sequence-level optimization of transducer models for streaming ASR by applying it on various end-to-end streaming ASR networks including RNN-Transducer, Transformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve 150-300 ms latency reduction with significantly better accuracy over previous techniques on a Voice Search test set. FastEmit also improves streaming ASR accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile latency from 210 ms to only 30 ms on LibriSpeech.",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N. Sainath, Yanzhang He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, Ruoming Pang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "with coefficients of\n5.0. We used checkpoint averaging of the best five check-\npoints based on the WER of the validation sets to get the final\nmodels. Mixed precision training with FP16 ",
                    "Citation Text": "Paulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, et al., \u201cMixed precision training,\u201d in ICLR ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.03740",
                        "Citation Paper Title": "Title:Mixed Precision Training",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.",
                        "Citation Paper Authors": "Authors:Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14183v2": {
            "Paper Title": "On Early Detection of Hallucinations in Factual Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "use un-\ncertainty estimates to derive quality measures for neural machine\ntranslation (NMT). ",
                    "Citation Text": "Nuno M. Guerreiro, Elena Voita, and Andr\u00e9 Martins. 2023. Looking for a needle\nin a haystack: a comprehensive study of hallucinations in neural machine\ntranslation. In Proceedings of the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics . Association for Computational\nLinguistics, Dubrovnik, Croatia, (May 2023), 1059\u20131075. https://aclanthology.o\nrg/2023.eacl-main.75.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.05309",
                        "Citation Paper Title": "Title:Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:Although the problem of hallucinations in neural machine translation (NMT) has received some attention, research on this highly pathological phenomenon lacks solid ground. Previous work has been limited in several ways: it often resorts to artificial settings where the problem is amplified, it disregards some (common) types of hallucinations, and it does not validate adequacy of detection heuristics. In this paper, we set foundations for the study of NMT hallucinations. First, we work in a natural setting, i.e., in-domain data without artificial noise neither in training nor in inference. Next, we annotate a dataset of over 3.4k sentences indicating different kinds of critical errors and hallucinations. Then, we turn to detection methods and both revisit methods used previously and propose using glass-box uncertainty-based detectors. Overall, we show that for preventive settings, (i) previously used methods are largely inadequate, (ii) sequence log-probability works best and performs on par with reference-based methods. Finally, we propose DeHallucinator, a simple method for alleviating hallucinations at test time that significantly reduces the hallucinatory rate. To ease future research, we release our annotated dataset for WMT18 German-English data, along with the model, training data, and code.",
                        "Citation Paper Authors": "Authors:Nuno M. Guerreiro, Elena Voita, Andr\u00e9 F.T. Martins"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16702v1": {
            "Paper Title": "Rethinking Tabular Data Understanding with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17278v1": {
            "Paper Title": "Large Language Models for Conducting Advanced Text Analytics Information\n  Systems Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16682v1": {
            "Paper Title": "Some things are more CRINGE than others: Preference Optimization with\n  the Pairwise Cringe Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16659v1": {
            "Paper Title": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.01234v2": {
            "Paper Title": "Frauds Bargain Attack: Generating Adversarial Text Samples via Word\n  Manipulation Process",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "sought synonyms based on WordNet synsets and ranked\nword replacement order via probability-weighted word\nsaliency (PWWS). Zang ",
                    "Citation Text": "Y. Zang, F. Qi, C. Yang, Z. Liu, M. Zhang, Q. Liu, and M. Sun,\n\u201cWord-level textual adversarial attacking as combinatorial opti-\nmization,\u201d in Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics , 2020, pp. 6066\u20136080.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.12196",
                        "Citation Paper Title": "Title:Word-level Textual Adversarial Attacking as Combinatorial Optimization",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on this https URL.",
                        "Citation Paper Authors": "Authors:Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2303.14582v2": {
            "Paper Title": "Identification of Negative Transfers in Multitask Learning Using\n  Surrogate Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.14628v2": {
            "Paper Title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.03719v3": {
            "Paper Title": "Assessing AI Chatbots Performance in Comprehensive Standardized Test\n  Preparation; A Case Study with GRE",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16549v1": {
            "Paper Title": "How Robust are LLMs to In-Context Majority Label Bias?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17276v1": {
            "Paper Title": "PanGu-$\u03c0$: Enhancing Language Model Architectures via Nonlinearity\n  Compensation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16511v1": {
            "Paper Title": "S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational\n  Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". Directly using single-turn and\nmulti-turn datasets will reduce task performance [18, 13]; the second\nmethod generates datasets for the target task. However, the work is\nrare, and only SIMSEEK ",
                    "Citation Text": "Gangwoo Kim, Sungdong Kim, Kang Min Yoo, and Jaewoo Kang,\n\u2018Generating information-seeking conversations from unlabeled docu-\nments\u2019, Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , 2362\u20132378, (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.12609",
                        "Citation Paper Title": "Title:Generating Information-Seeking Conversations from Unlabeled Documents",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a novel framework, SIMSEEK, (Simulating information-Seeking conversation from unlabeled documents), and compare its two variants. In our baseline SIMSEEK-SYM, a questioner generates follow-up questions upon the predetermined answer by an answerer. On the contrary, SIMSEEK-ASYM first generates the question and then finds its corresponding answer under the conversational context. Our experiments show that they can synthesize effective training resources for CQA and conversational search tasks. As a result, conversations from SIMSEEK-ASYM not only make more improvements in our experiments but also are favorably reviewed in a human evaluation. We finally release a large-scale resource of synthetic conversations, WIKI-SIMSEEK, containing 2 million CQA pairs built upon Wikipedia documents. With the dataset, our CQA model achieves state-of-the-art performance on a recent CQA benchmark, QuAC.",
                        "Citation Paper Authors": "Authors:Gangwoo Kim, Sungdong Kim, Kang Min Yoo, Jaewoo Kang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". With the pop-\nularity of large-scale pre-trained language models [4, 10, 17], more\nresearchers used pre-trained language models and data augmenta-\ntion methods, such as ROR ",
                    "Citation Text": "Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou, Youzheng Wu, Xi-\naodong He, and Bowen Zhou. Ror: Read-over-read for long document\nmachine reading comprehension., Nov 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.04780",
                        "Citation Paper Title": "Title:RoR: Read-over-Read for Long Document Machine Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Transformer-based pre-trained models, such as BERT, have achieved remarkable results on machine reading comprehension. However, due to the constraint of encoding length (e.g., 512 WordPiece tokens), a long document is usually split into multiple chunks that are independently read. It results in the reading field being limited to individual chunks without information collaboration for long document machine reading comprehension. To address this problem, we propose RoR, a read-over-read method, which expands the reading field from chunk to document. Specifically, RoR includes a chunk reader and a document reader. The former first predicts a set of regional answers for each chunk, which are then compacted into a highly-condensed version of the original document, guaranteeing to be encoded once. The latter further predicts the global answers from this condensed document. Eventually, a voting strategy is utilized to aggregate and rerank the regional and global answers for final prediction. Extensive experiments on two benchmarks QuAC and TriviaQA demonstrate the effectiveness of RoR for long document reading. Notably, RoR ranks 1st place on the QuAC leaderboard (this https URL) at the time of submission (May 17th, 2021).",
                        "Citation Paper Authors": "Authors:Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou, Youzheng Wu, Xiaodong He, Bowen Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.06435v7": {
            "Paper Title": "A Comprehensive Overview of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.16082v4": {
            "Paper Title": "EnrichEvent: Enriching Social Data with Contextual Information for\n  Emerging Event Extraction",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ", which considers averages of the\nword2vec embedding of words as their representation. While imple-\nmenting this baseline, we only utilized the word2vec embedding in\nthe Contextual Knowledge Enhancement component and did not use\nlexical knowledge. (2) Bert ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n\u2018BERT: Pre-training of deep bidirectional transformers for language\nunderstanding\u2019, in Proceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics . Associ-\nation for Computational Linguistics, (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16490v1": {
            "Paper Title": "Understanding News Creation Intents: Frame, Dataset, and Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16488v1": {
            "Paper Title": "Source Code is a Graph, Not a Sequence: A Cross-Lingual Perspective on\n  Code Clone Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16148v2": {
            "Paper Title": "The Media Bias Taxonomy: A Systematic Literature Review on the Forms and\n  Automated Detection of Media Bias",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16467v1": {
            "Paper Title": "Transfer and Alignment Network for Generalized Category Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11508v2": {
            "Paper Title": "Rethinking the Instruction Quality: LIFT is What You Need",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08078v4": {
            "Paper Title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "(2015) 18.60 43.10 56.10 18.13 43.20 55.97\nChauhan et al. ",
                    "Citation Text": "Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob An-\ndreas, Xin Wang, Seth Berkowitz, Steven Horng, Peter\nSzolovits, and Polina Golland. Joint modeling of chest radio-\ngraphs and radiology reports for pulmonary edema assess-\nment. In MICCAI , pages 529\u2013539. Springer, 2020. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.09884",
                        "Citation Paper Title": "Title:Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary Edema Assessment",
                        "Citation Paper Abstract": "Abstract:We propose and demonstrate a novel machine learning algorithm that assesses pulmonary edema severity from chest radiographs. While large publicly available datasets of chest radiographs and free-text radiology reports exist, only limited numerical edema severity labels can be extracted from radiology reports. This is a significant challenge in learning such models for image classification. To take advantage of the rich information present in the radiology reports, we develop a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs at inference time. Our experimental results suggest that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained on images only. We also show the use of the text for explaining the image classification by the joint model. To the best of our knowledge, our approach is the first to leverage free-text radiology reports for improving the image model performance in this application. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob Andreas, Xin Wang, Seth Berkowitz, Steven Horng, Peter Szolovits, Polina Golland"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "(2023) 0.2275 0.0878 0.0378 0.0166\nAdaMatch-Cyclic 0.3754 0.2303 0.1520 0.1058\n4.2.2 Report-to-CXR Generation\nTo quantitatively assess the performance, we compare our\nmethod with the text-to-image generation method ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR , pages 10684\u2013\n10695, 2022. 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "(2021) 0.3585 0.2266 0.1550 0.1021 0.1425 0.2788 0.3833 0.2409 0.1598 0.1078 0.1457 0.3293\nXProNet ",
                    "Citation Text": "Jun Wang, Abhir Bhalerao, and Yulan He. Cross-modal pro-\ntotype driven network for radiology report generation. In\nECCV , pages 563\u2013579. Springer, 2022. 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.04818",
                        "Citation Paper Title": "Title:Cross-modal Prototype Driven Network for Radiology Report Generation",
                        "Citation Paper Abstract": "Abstract:Radiology report generation (RRG) aims to describe automatically a radiology image with human-like language and could potentially support the work of radiologists, reducing the burden of manual reporting. Previous approaches often adopt an encoder-decoder architecture and focus on single-modal feature learning, while few studies explore cross-modal feature interaction. Here we propose a Cross-modal PROtotype driven NETwork (XPRONET) to promote cross-modal pattern learning and exploit it to improve the task of radiology report generation. This is achieved by three well-designed, fully differentiable and complementary modules: a shared cross-modal prototype matrix to record the cross-modal prototypes; a cross-modal prototype network to learn the cross-modal prototypes and embed the cross-modal information into the visual and textual features; and an improved multi-label contrastive loss to enable and enhance multi-label prototype learning. XPRONET obtains substantial improvements on the IU-Xray and MIMIC-CXR benchmarks, where its performance exceeds recent state-of-the-art approaches by a large margin on IU-Xray and comparable performance on MIMIC-CXR.",
                        "Citation Paper Authors": "Authors:Jun Wang, Abhir Bhalerao, Yulan He"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "to detect\nvisual objects and match ROI embeddings with textual em-\nbeddings. Moreover, FILIP ",
                    "Citation Text": "Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe\nNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\nChunjing Xu. Filip: Fine-grained interactive language-image\npre-training. arXiv preprint arXiv:2111.07783 , 2021. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.07783",
                        "Citation Paper Title": "Title:FILIP: Fine-grained Interactive Language-Image Pre-Training",
                        "Citation Paper Abstract": "Abstract:Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.",
                        "Citation Paper Authors": "Authors:Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.03236v2": {
            "Paper Title": "A Survey on Out-of-Distribution Detection in NLP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10967v2": {
            "Paper Title": "Knowledge Graphs and Pre-trained Language Models enhanced Representation\n  Learning for Conversational Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.07707v2": {
            "Paper Title": "CoLLD: Contrastive Layer-to-layer Distillation for Compressing\n  Multilingual Pre-trained Speech Encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06681v2": {
            "Paper Title": "Steering Llama 2 via Contrastive Activation Addition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15291v2": {
            "Paper Title": "Reverse Multi-Choice Dialogue Commonsense Inference with\n  Graph-of-Thought",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.08475v4": {
            "Paper Title": "Can We Edit Multimodal Large Language Models?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.04047v2": {
            "Paper Title": "CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual\n  Navigation in Noisy Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17269v1": {
            "Paper Title": "Conversational Question Answering with Reformulations over Knowledge\n  Graph",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", an encoder-\ndecoder model is used to transform natural language\nquestions into logical queries for finding answers. In ",
                    "Citation Text": "E Kacupaj, J Plepi, K Singh, H Thakkar. Conver-\nsational Question Answering over Knowledge Graphs\nwith Transformer and Graph Attention Networks. As-\nsociation for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.01569",
                        "Citation Paper Title": "Title:Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks",
                        "Citation Paper Abstract": "Abstract:This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEtworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baseline averages on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase in F1-score is more than 20% compared to the state of the art.",
                        "Citation Paper Authors": "Authors:Endri Kacupaj, Joan Plepi, Kuldeep Singh, Harsh Thakkar, Jens Lehmann, Maria Maleshkova"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "5.1 Conversational Question Answering Vari-\nous approaches have been used to develop ConvQA sys-\ntems. For instance, in ",
                    "Citation Text": "C Buck, J Bulian. Ask the Right Questions: Active\nQuestion Reformulation with Reinforcement Learning.\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07830",
                        "Citation Paper Title": "Title:Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.",
                        "Citation Paper Authors": "Authors:Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16337v1": {
            "Paper Title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16291v1": {
            "Paper Title": "Observable Propagation: A Data-Efficient Approach to Uncover Feature\n  Vectors in Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16171v1": {
            "Paper Title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2,\n  GPT-3.5/4",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ", emphasizing the importance of prompt\nengineering in enhancing LLM applications in software development and education. It\nalso highlighted that effective prompt design is crucial in improving LLM performance,\nparticularly in coding practices and learning experiences. Lastly, Directional Stimulus\nPrompting ",
                    "Citation Text": "Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan.\nGuiding large language models via directional stimulus prompting. arXiv preprint\narXiv:2302.11520 , 2023. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.11520",
                        "Citation Paper Title": "Title:Guiding Large Language Models via Directional Stimulus Prompting",
                        "Citation Paper Abstract": "Abstract:We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", finding that explanations can enhance LLM\u2019s\nlearning capabilities on complex tasks. Furthermore, a catalog of prompt engineering\ntechniques was examined with ChatGPT ",
                    "Citation Text": "Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf\nElnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. A prompt pattern catalog to\nenhance prompt engineering with chatgpt, 2023. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.11382",
                        "Citation Paper Title": "Title:A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
                        "Citation Paper Abstract": "Abstract:Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.",
                        "Citation Paper Authors": "Authors:Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C. Schmidt"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", where the model\ngenerates a series of intermediate reasoning steps to improve performance on complex\ntasks. Also, least-to-most prompting ",
                    "Citation Text": "Denny Zhou, Nathanael Sch \u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models, 2023. 4\n24",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.10625",
                        "Citation Paper Title": "Title:Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
                        "Citation Paper Authors": "Authors:Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "prompting introduced focusing on using multiple imperfect\nprompts and aggregating them to improve model performance, particularly in question-\nanswering formats. Another one, Chain-of-Thought method ",
                    "Citation Text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2023. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "which significantly expanded its parameter count to 1.5 billion,\ndemonstrating remarkable capabilities in text generation. Then, GPT-3 ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural information processing systems ,\n33:1877\u20131901, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", which proposed that smaller, optimally trained models\ncould achieve exceptional results. The latest in this series of innovations is Mistral ",
                    "Citation Text": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L \u00b4elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth \u00b4ee Lacroix, and William El Sayed. Mistral 7b, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.06825",
                        "Citation Paper Title": "Title:Mistral 7B",
                        "Citation Paper Abstract": "Abstract:We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",
                        "Citation Paper Authors": "Authors:Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "revolutionized context understanding through its bidirectional\ntraining approach, while T5 ",
                    "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. CoRR , abs/1910.10683, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10683",
                        "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                        "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                        "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "Large Language Models. The evolution of large language models (LLMs) has been\npivotal in advancing natural language processing (NLP). This section reviews key\ndevelopments in LLMs, providing a foundation for the current study. Beginning with\nGoogle\u2019s BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR , abs/1810.04805, 2018.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16159v1": {
            "Paper Title": "Zero-Shot Cross-Lingual Reranking with Large Language Models for\n  Low-Resource Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16156v1": {
            "Paper Title": "From Text to Multimodal: A Comprehensive Survey of Adversarial Example\n  Generation in Question Answering Systems",
            "Sentences": [
                {
                    "Sentence ID": 83,
                    "Sentence": ". It contains altered versions of\nthe original SQuAD examples to challenge the robustness of QA models.\n\u2022QA Testbed-Quizbowl: QA Testbed, called Quizbowl, is a dataset used in the\nQuizbowl competition ",
                    "Citation Text": "Wallace, E., Rodriguez, P., Feng, S., Yamada, I., Boyd-Graber, J.: Trick me if you\ncan: Human-in-the-loop generation of adversarial examples for question answer-\ning. Transactions of the Association for Computational Linguistics 7, 387\u2013401\n(2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.02701",
                        "Citation Paper Title": "Title:Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering",
                        "Citation Paper Abstract": "Abstract:Adversarial evaluation stress tests a model's understanding of natural language. While past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human-in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human--computer matches: although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.",
                        "Citation Paper Authors": "Authors:Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". The misleading texts are generated from answer sentences by removing\nthe answer tokens.\n\u2022AddSentDiverse (ASD): Drawing inspiration from AddSent, this approach enhanced\nthe SQuAD training dataset by incorporating specifically crafted adversarial\nexamples ",
                    "Citation Text": "Wang, Y., Bansal, M.: Robust machine comprehension models via adversarial\ntraining. arXiv preprint arXiv:1804.06473 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.06473",
                        "Citation Paper Title": "Title:Robust Machine Comprehension Models via Adversarial Training",
                        "Citation Paper Abstract": "Abstract:It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent's semantic perturbations (e.g., antonyms), we jointly improve the model's semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art model significantly more robust, achieving a 36.5% increase in F1 score under many different types of adversarial evaluation while maintaining performance on the regular SQuAD task.",
                        "Citation Paper Authors": "Authors:Yicheng Wang, Mohit Bansal"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": ".\n\u2022SVAMP: SVAMP is a challenge set focused on elementary-level Math Word\nProblems (MWPs) ",
                    "Citation Text": "Patel, A., Bhattamishra, S., Goyal, N.: Are nlp models really able to solve simple\nmath word problems? arXiv preprint arXiv:2103.07191 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07191",
                        "Citation Paper Title": "Title:Are NLP Models really able to Solve Simple Math Word Problems?",
                        "Citation Paper Abstract": "Abstract:The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered \"solved\" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",
                        "Citation Paper Authors": "Authors:Arkil Patel, Satwik Bhattamishra, Navin Goyal"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": ". It evaluates the models\u2019 ability to handle ambiguity.\n\u2022GSM8K: The dataset is intended for tasks involving multi-step mathematical reason-\ning, providing a diverse set of math problems that challenge learners\u2019 comprehension\nand problem-solving skills ",
                    "Citation Text": "Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,\nM., Tworek, J., Hilton, J., Nakano, R., et al.: Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.14168",
                        "Citation Paper Title": "Title:Training Verifiers to Solve Math Word Problems",
                        "Citation Paper Abstract": "Abstract:State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                        "Citation Paper Authors": "Authors:Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": ". It presents sit-\nuations and multiple alternatives, requiring models to select the most plausible\ncontinuation based on commonsense understanding.\n\u2022AmbigQA (Answering Ambiguous Open-domain Questions): AmbigQA provides\nquestions with ambiguous phrasing, leading to multiple interpretations and answers ",
                    "Citation Text": "Min, S., Michael, J., Hajishirzi, H., Zettlemoyer, L.: Ambigqa: Answering\nambiguous open-domain questions. arXiv preprint arXiv:2004.10645 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.10645",
                        "Citation Paper Title": "Title:AmbigQA: Answering Ambiguous Open-domain Questions",
                        "Citation Paper Abstract": "Abstract:Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at this https URL.",
                        "Citation Paper Authors": "Authors:Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": ". It contains questions of multiple passages, requiring models to merge\ninformation to reach the correct answer.\n\u2022SWAG: SWAG focuses on grounded, commonsense reasoning ",
                    "Citation Text": "Zellers, R., Bisk, Y., Schwartz, R., Choi, Y.: Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326\n(2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05326",
                        "Citation Paper Title": "Title:SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
                        "Citation Paper Abstract": "Abstract:Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.\nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
                        "Citation Paper Authors": "Authors:Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ". It challenges models to predict the ending of a\nsentence on commonsense understanding.\n\u2022DROP: DROP is a dataset that highlights complex reasoning and multi-step com-\nprehension ",
                    "Citation Text": "Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., Gardner, M.: Drop: A\nreading comprehension benchmark requiring discrete reasoning over paragraphs.\narXiv preprint arXiv:1903.00161 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.00161",
                        "Citation Paper Title": "Title:DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                        "Citation Paper Abstract": "Abstract:Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.",
                        "Citation Paper Authors": "Authors:Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner"
                    }
                },
                {
                    "Sentence ID": 85,
                    "Sentence": ". It contains adversarial exam-\nples where minor modifications to the question or image can end with incorrect\nanswers.\n\u2022HellaSwag: HellaSwag introduces questions that require commonsense reasoning\nbeyond statistical patterns ",
                    "Citation Text": "Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07830",
                        "Citation Paper Title": "Title:HellaSwag: Can a Machine Really Finish Your Sentence?",
                        "Citation Paper Abstract": "Abstract:Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?\nIn this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.\nOur construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
                        "Citation Paper Authors": "Authors:Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": ". VILLA\n18Table 6 Comparative Analysis of Adversarial Multimodal Question Generation \u2013 Baseline Models, Modalities, Objectives, Attack Methods, Evaluation Metrics,\nDatasets, and Limitation\nStudyBaseline\nModelSupported\nModalitiesAdversarial\nObjectiveAttack MethodEvaluation\nMetricsDataset Limitation ",
                    "Citation Text": "Tang, R., Ma, C., Zhang, W.E., Wu, Q., Yang, X.: Semantic equivalent adversarial\ndata augmentation for visual question answering. In: Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart XIX 16, pp. 437\u2013453 (2020). Springer",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.09592",
                        "Citation Paper Title": "Title:Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Visual Question Answering (VQA) has achieved great success thanks to the fast development of deep neural networks (DNN). On the other hand, the data augmentation, as one of the major tricks for DNN, has been widely used in many computer vision tasks. However, there are few works studying the data augmentation problem for VQA and none of the existing image based augmentation schemes (such as rotation and flipping) can be directly applied to VQA due to its semantic structure -- an $\\langle image, question, answer\\rangle$ triplet needs to be maintained correctly. For example, a direction related Question-Answer (QA) pair may not be true if the associated image is rotated or flipped. In this paper, instead of directly manipulating images and questions, we use generated adversarial examples for both images and questions as the augmented data. The augmented examples do not change the visual properties presented in the image as well as the \\textbf{semantic} meaning of the question, the correctness of the $\\langle image, question, answer\\rangle$ is thus still maintained. We then use adversarial learning to train a classic VQA model (BUTD) with our augmented data. We find that we not only improve the overall performance on VQAv2, but also can withstand adversarial attack effectively, compared to the baseline model. The source code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, Xiaokang Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16132v1": {
            "Paper Title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16119v1": {
            "Paper Title": "A bi-objective $\u03b5$-constrained framework for quality-cost\n  optimization in language model ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16104v1": {
            "Paper Title": "Dotless Representation of Arabic Text: Analysis and Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2304.14318v2": {
            "Paper Title": "q2d: Turning Questions into Dialogs to Teach Models How to Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.10400v4": {
            "Paper Title": "What You See is What You Read? Improving Text-Image Alignment Evaluation",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "which depend on human feedback and operate within\na discrete set of alignments, our approach produces automated scores for a broader range of text-\nimage alignments, facilitating efficient evaluation of vision-and-language models. Our approach also\nsurpasses the recently proposed TIFA ",
                    "Citation Text": "Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and\nNoah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with\nquestion answering. arXiv preprint arXiv:2303.11897 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.11897",
                        "Citation Paper Title": "Title:TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
                        "Citation Paper Abstract": "Abstract:Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.",
                        "Citation Paper Authors": "Authors:Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, Noah A Smith"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "often act as bag-of-words models, lacking a deep\ncomprehension of language compositionality ",
                    "Citation Text": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When\nand why vision-language models behave like bags-of-words, and what to do about it? In The\nEleventh International Conference on Learning Representations , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.01936",
                        "Citation Paper Title": "Title:When and why vision-language models behave like bags-of-words, and what to do about it?",
                        "Citation Paper Abstract": "Abstract:Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on retrieval over existing datasets without using the composition and order information. Given that contrastive pretraining optimizes for retrieval on datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality.",
                        "Citation Paper Authors": "Authors:Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "to\nassess the semantic alignment between images and text. Vision-and-language models like CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning ,\npages 8748\u20138763. PMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.06595v4": {
            "Paper Title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n  Inspired by Real-World Use",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "for evaluating instruction-following vision-language\nmodels. Win-rates* refers to the model win-rates against a reference output/model.\nMultiInstruct ",
                    "Citation Text": "Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal\nzero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10773",
                        "Citation Paper Title": "Title:MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
                        "Citation Paper Abstract": "Abstract:Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric - Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.",
                        "Citation Paper Authors": "Authors:Zhiyang Xu, Ying Shen, Lifu Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16070v1": {
            "Paper Title": "Can ChatGPT Read Who You Are?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17267v1": {
            "Paper Title": "Improving Low-resource Prompt-based Relation Representation with\n  Multi-view Decoupling Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16054v1": {
            "Paper Title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14187v2": {
            "Paper Title": "WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with\n  Refined Data Generation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "5.1 Code Large Language Models.\nNumerous researchers have proposed their Code Language Models (LLMs), including CodeGen ",
                    "Citation Text": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint\narXiv:2203.13474 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13474",
                        "Citation Paper Title": "Title:CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
                        "Citation Paper Abstract": "Abstract:Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.",
                        "Citation Paper Authors": "Authors:Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.01678v3": {
            "Paper Title": "Jellyfish: A Large Language Model for Data Preprocessing",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "are often used\nfor identifying candidates. Despite search speed being a key concern, LLMs are anticipated to be used\non top of their outcomes for automated integration in a data lake ",
                    "Citation Text": "S. Arora, B. Yang, S. Eyuboglu, A. Narayan, A. Hojel, I. Trummer, and C. R\u00b4 e. Language\nmodels enable simple systems for generating structured views of heterogeneous data lakes. arXiv\npreprint arXiv:2304.09433 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.09433",
                        "Citation Paper Title": "Title:Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes",
                        "Citation Paper Abstract": "Abstract:A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.\nWe propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision. EVAPORATE-CODE+ not only outperforms the state-of-the art systems, but does so using a sublinear pass over the documents with the LLM. This equates to a 110x reduction in the number of tokens the LLM needs to process, averaged across 16 real-world evaluation settings of 10k documents each.",
                        "Citation Paper Authors": "Authors:Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, Christopher R\u00e9"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ".\nWe use the publicly available version of these datasets ",
                    "Citation Text": "A. Narayan, I. Chami, L. Orr, and C. R\u00b4 e. Can foundation models wrangle your data? PVLDB ,\n16(4):738\u2013746, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09911",
                        "Citation Paper Title": "Title:Can Foundation Models Wrangle Your Data?",
                        "Citation Paper Abstract": "Abstract:Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: this https URL.",
                        "Citation Paper Authors": "Authors:Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, Christopher R\u00e9"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", basically\nemploying various prompt engineering techniques on frozen LLMs. Fine-tuning GPT-3.5 and\nChatGPT for a variety of table-related tasks has also been investigated ",
                    "Citation Text": "P. Li, Y. He, D. Yashar, W. Cui, S. Ge, H. Zhang, D. R. Fainman, D. Zhang, and S. Chaudhuri.\nTable-GPT: Table-tuned GPT for diverse table tasks. arXiv preprint arXiv:2310.09263 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.09263",
                        "Citation Paper Title": "Title:Table-GPT: Table-tuned GPT for Diverse Table Tasks",
                        "Citation Paper Abstract": "Abstract:Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \\emph{one-dimensional} natural-language texts, whereas relational tables are \\emph{two-dimensional} objects.\nIn this work, we propose a new \"\\emph{table-tuning}\" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \\emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \\emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.",
                        "Citation Paper Authors": "Authors:Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ", including feature engineering, clustering, and classification.\n\u2022DI: While rule-based solutions [75, 79] remain one of the prevalent approaches, another stream\nof works develops ML models for this task, including variational autoencoders ",
                    "Citation Text": "A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera. Handling incomplete heterogeneous\ndata using VAEs. Pattern Recognition , 107:107501, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03653",
                        "Citation Paper Title": "Title:Handling Incomplete Heterogeneous Data using VAEs",
                        "Citation Paper Abstract": "Abstract:Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data.",
                        "Citation Paper Authors": "Authors:Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, Isabel Valera"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "(reasoning-eliciting) prompt settings with a focus on reasoning\ntrace. Second, we select the Open-Platypus dataset ",
                    "Citation Text": "A. N. Lee, C. J. Hunter, and N. Ruiz. Platypus: Quick, cheap, and powerful refinement of LLMs.\narXiv preprint arXiv:2308.07317 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.07317",
                        "Citation Paper Title": "Title:Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
                        "Citation Paper Abstract": "Abstract:We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ";\n*Despite the same domain as the Amazon dataset used in DP-tuning, the entities belong to a different category of\nproducts.\n12\u2022CTA: RoBERTa ",
                    "Citation Text": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV. Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692 , 2019.\n23",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ") to quickly find a few matches and use them\nas few-shot examples.\nBatch prompting ",
                    "Citation Text": "Z. Cheng, J. Kasai, and T. Yu. Batch prompting: Efficient inference with large language model\nAPIs. arXiv preprint arXiv:2301.08721 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.08721",
                        "Citation Paper Title": "Title:Batch Prompting: Efficient Inference with Large Language Model APIs",
                        "Citation Paper Abstract": "Abstract:Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code can be found at the site this https URL.",
                        "Citation Paper Authors": "Authors:Zhoujun Cheng, Jungo Kasai, Tao Yu"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": ", which exploits the low\nintrinsic rank in the change of weights during fine-tuning. We apply LoRA in fine-tuning the Wq,Wk,\nWvandWomatrices in the Transformer architecture ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. NIPS , 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 102,
                    "Sentence": "for a survey on LLMs. Some LLMs are\nopen-source (e.g., Llama and Llama 2), and they can be fine-tuned with additional tasks to improve\ntheir abilities in logical reasoning, question answering, and so on. Among these fine-tuning approaches,\ninstruction tuning ",
                    "Citation Text": "S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, et al.\nInstruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.10792",
                        "Citation Paper Title": "Title:Instruction Tuning for Large Language Models: A Survey",
                        "Citation Paper Abstract": "Abstract:This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: this http URL",
                        "Citation Paper Authors": "Authors:Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": ". Due to their superb ability to process natural\nlanguage, LLMs have not only been used in NLP applications (e.g., ChatGPT and Claude), but also\ncatalyzed the rise of LLM-powered autonomous agents ",
                    "Citation Text": "L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al.\nA survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.11432",
                        "Citation Paper Title": "Title:A Survey on Large Language Model based Autonomous Agents",
                        "Citation Paper Abstract": "Abstract:Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at this https URL.",
                        "Citation Paper Authors": "Authors:Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10987v2": {
            "Paper Title": "Data Contamination Issues in Brain-to-Text Decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16023v1": {
            "Paper Title": "DocMSU: A Comprehensive Benchmark for Document-level Multimodal Sarcasm\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.14316v2": {
            "Paper Title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11562v4": {
            "Paper Title": "A Survey of Reasoning with Foundation Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15997v1": {
            "Paper Title": "Aligning Large Language Models with Human Preferences through\n  Representation Engineering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2302.08387v2": {
            "Paper Title": "LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with\n  Knowledge Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.15074v2": {
            "Paper Title": "Natural Language based Context Modeling and Reasoning for Ubiquitous\n  Computing with Large Language Models: A Tutorial",
            "Sentences": [
                {
                    "Sentence ID": 106,
                    "Sentence": "help in learning from past outcomes to optimize the trajectory of actions. More\nrecently, LLM+P ",
                    "Citation Text": "Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with\noptimal planning proficiency. arXiv preprint arXiv:2304.11477 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.11477",
                        "Citation Paper Title": "Title:LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "Misinformation Evaluating LLMs-generated misinformation.\nUser Interaction/User Experience (UI/UX)\nWang et al. ",
                    "Citation Text": "Bryan Wang, Gang Li, and Yang Li. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems , pages 1\u201317, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.08655",
                        "Citation Paper Title": "Title:Enabling Conversational Interaction with Mobile UI using Large Language Models",
                        "Citation Paper Abstract": "Abstract:Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.",
                        "Citation Paper Authors": "Authors:Bryan Wang, Gang Li, Yang Li"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "leverage\nprompt encoders to enhance tokens in the input sequence for text generations. P-tuning v2 ",
                    "Citation Text": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages\n61\u201368, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.07602",
                        "Citation Paper Title": "Title:P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                        "Citation Paper Abstract": "Abstract:Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at this https URL.",
                        "Citation Paper Authors": "Authors:Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "propose a small continuous task-specific vector , namely\nprefix, as virtual tokens prior to the input sequences of LLMs, while prompt tuning ",
                    "Citation Text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08691",
                        "Citation Paper Title": "Title:The Power of Scale for Parameter-Efficient Prompt Tuning",
                        "Citation Paper Abstract": "Abstract:In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",
                        "Citation Paper Authors": "Authors:Brian Lester, Rami Al-Rfou, Noah Constant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15922v1": {
            "Paper Title": "Towards Probing Contact Center Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08901v2": {
            "Paper Title": "Boosting LLM Reasoning: Push the Limits of Few-shot Learning with\n  Reinforced In-Context Pruning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15907v1": {
            "Paper Title": "Align on the Fly: Adapting Chatbot Behavior to Established Norms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15883v1": {
            "Paper Title": "Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large\n  Language Models",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "generates sequential reading notes for retrieved\nknowledge, enabling a thorough evaluation of their relevance to\nthe given question and integrating this information to formulate\nthe final answer. (6) Chain-of-Knowledge (CoK) ",
                    "Citation Text": "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Sou-\njanya Poria, and Lidong Bing. 2023. Chain-of-Knowledge: Grounding Large Lan-\nguage Models via Dynamic Knowledge Adapting over Heterogeneous Sources.\narXiv:2305.13269 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.13269",
                        "Citation Paper Title": "Title:Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
                        "Citation Paper Abstract": "Abstract:We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
                        "Citation Paper Authors": "Authors:Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "reformulate\nthe user\u2019s initial query by adding additional meaningful terms\nwith similar significance with the help of LLMs. (5) CHAIN-OF-\nNOTE (CoN) ",
                    "Citation Text": "Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and\nDong Yu. 2023. Chain-of-Note: Enhancing Robustness in Retrieval-Augmented\nLanguage Models. arXiv:2311.09210 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.09210",
                        "Citation Paper Title": "Title:Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
                        "Citation Paper Abstract": "Abstract:Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with \"unknown\" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
                        "Citation Paper Authors": "Authors:Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "finetunes LLaMA with 100,000 real-world data samples\nfrom online medical consultation platforms and employs a web\nengine search to assist in generating responses. KALA ",
                    "Citation Text": "Minki Kang, Jinheon Baek, and Sung Ju Hwang. 2022. KALA: Knowledge-\nAugmented Language Model Adaptation. arXiv:2204.10555 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.10555",
                        "Citation Paper Title": "Title:KALA: Knowledge-Augmented Language Model Adaptation",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM's performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Minki Kang, Jinheon Baek, Sung Ju Hwang"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "leverages a mixture\nof medical knowledge generated by ChatGPT and real-world med-\nical consultation data to achieve performance surpassing that of\nChatGPT in the field of Chinese medical knowledge QA. ChatDoc-\ntor ",
                    "Citation Text": "Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023.\nChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model\nMeta-AI (LLaMA) Using Medical Domain Knowledge. arXiv:2303.14070 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.14070",
                        "Citation Paper Title": "Title:ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
                        "Citation Paper Abstract": "Abstract:The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.",
                        "Citation Paper Authors": "Authors:Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "utilizes a hierarchical tree structure to generate reasoning steps.Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large Language Models Conference\u201917, July 2017, Washington, DC, USA\nGraph of Thoughts ",
                    "Citation Text": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,\nJoanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, and Torsten Hoefler. 2023. Graph of Thoughts: Solving Elaborate\nProblems with Large Language Models. arXiv:2308.09687 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.09687",
                        "Citation Paper Title": "Title:Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.",
                        "Citation Paper Authors": "Authors:Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "proposes\na sample method which votes on the kmost complex reasoning\npaths instead of using greedy decoding. Ling ",
                    "Citation Text": "Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memise-\nvic, and Hao Su. 2023. Deductive Verification of Chain-of-Thought Reasoning.\narXiv:2306.03872 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.03872",
                        "Citation Paper Title": "Title:Deductive Verification of Chain-of-Thought Reasoning",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at this https URL.",
                        "Citation Paper Authors": "Authors:Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "or \" Take a deep breath and work on this problem step-by-\nstep\" ",
                    "Citation Text": "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny\nZhou, and Xinyun Chen. 2023. Large Language Models as Optimizers.\narXiv:2309.03409 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.03409",
                        "Citation Paper Title": "Title:Large Language Models as Optimizers",
                        "Citation Paper Abstract": "Abstract:Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at this https URL.",
                        "Citation Paper Authors": "Authors:Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "is a prompting strategy to boost the\nperformance of LLMs on complex reasoning tasks, which enables\nLLMs to decompose complex problems into multiple intermediate\nreasoning steps. Simple instructions such as \" Let\u2019s think step by\nstep.\" ",
                    "Citation Text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and\nYusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners.\narXiv:2205.11916 [cs.CL]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11916",
                        "Citation Paper Title": "Title:Large Language Models are Zero-Shot Reasoners",
                        "Citation Paper Abstract": "Abstract:Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
                        "Citation Paper Authors": "Authors:Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15880v1": {
            "Paper Title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced\n  Reasoning over Knowledge Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15872v1": {
            "Paper Title": "Heterogeneous Encoders Scaling In The Transformer For Neural Machine\n  Translation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", and it was\napplied to recurrent models drastically improving the performances by allowing the encoder\ninformation to be distributed on each encoder input rather than one single state vector. The\nmethod was later refined in ",
                    "Citation Text": "Thang Luong, Hieu Pham, and Christopher D. Manning. \u201cEffective Approaches to\nAttention-based Neural Machine Translation\u201d. In: Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing . Lisbon, Portugal: Association for\nComputational Linguistics, Sept. 2015, pp. 1412\u20131421. doi:10.18653/v1/D15-1166 .url:\nhttps://aclanthology.org/D15-1166 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04025",
                        "Citation Paper Title": "Title:Effective Approaches to Attention-based Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15869v1": {
            "Paper Title": "Medical Report Generation based on Segment-Enhanced Contrastive\n  Representation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15867v1": {
            "Paper Title": "Punctuation Matters! Stealthy Backdoor Attack for Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11242v2": {
            "Paper Title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16257v1": {
            "Paper Title": "More than Correlation: Do Large Language Models Learn Causal\n  Representations of Space?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15842v1": {
            "Paper Title": "Knowledge Distillation of LLM for Education",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "discussed cross-domain KD for\ntext classification, highlighting the versatility of KD across\ndifferent domains. Similarly, Liu et al. ",
                    "Citation Text": "Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, and Xiaoming Zhai.\n2023. Context matters: A strategy to pre-train language model for\nscience education. arXiv preprint arXiv:2301.12031 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12031",
                        "Citation Paper Title": "Title:Context Matters: A Strategy to Pre-train Language Model for Science Education",
                        "Citation Paper Abstract": "Abstract:This study aims at improving the performance of scoring student responses in science education automatically. BERT-based language models have shown significant superiority over traditional NLP models in various language-related tasks. However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants. All these suggest that a domain-specific model pre-trained using science education data may improve model performance. However, the ideal type of data to contextualize pre-trained language model and improve the performance in automatically scoring student written responses remains unclear. Therefore, we employ different data in this study to contextualize both BERT and SciBERT models and compare their performance on automatic scoring of assessment tasks for scientific argumentation. We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks. Our experimental results show that in-domain training corpora constructed from science questions and responses improve language model performance on a wide variety of downstream tasks. Our study confirms the effectiveness of continual pre-training on domain-specific data in the education domain and demonstrates a generalizable strategy for automating science education tasks with high accuracy. We plan to release our data and SciEdBERT models for public use and community engagement.",
                        "Citation Paper Authors": "Authors:Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, Xiaoming Zhai"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", who demonstrated the efficacy of lifelong\nlanguage KD and unifying text recognition using KD, respec-\ntively.\nIn addition, our method is influenced by the work of Hof-\nstatter et al. ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete\nSertkan, and Allan Hanbury. 2020. Improving efficient neural ranking\nmodels with cross-architecture knowledge distillation. arXiv preprint\narXiv:2010.02666 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02666",
                        "Citation Paper Title": "Title:Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Retrieval and ranking models are the backbone of many applications such as web search, open domain QA, or text-based recommender systems. The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT passage ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of distilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves re-ranking effectiveness without compromising their efficiency. Additionally, we show our general distillation method to improve nearest neighbor based index retrieval with the BERT dot product model, offering competitive results with specialized and much more costly training methods. To benefit the community, we publish the teacher-score training files in a ready-to-use package.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "laid the foundational work in this field, illustrating\nthe basic principles of KD in neural networks. Building on\nthis, we incorporate methods from Chuang et al. ",
                    "Citation Text": "Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen. 2020. Life-\nlong language knowledge distillation. arXiv preprint arXiv:2010.02123\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02123",
                        "Citation Paper Title": "Title:Lifelong Language Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.",
                        "Citation Paper Authors": "Authors:Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposedthe LLM distillation technique to solve math word problems,\nwhich fosters a tailored learning experience by generating\ntargeted exercises aligned with educational science princi-\nples and able to achieve comparable accuracy as GPT-3.5\nand PaLLM. Lastly, ",
                    "Citation Text": "Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H\nLaradji. 2023. Promptmix: A class boundary augmentation method\nfor large language model distillation. arXiv preprint arXiv:2310.14192\n(2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.14192",
                        "Citation Paper Title": "Title:PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
                        "Citation Paper Abstract": "Abstract:Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM's abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "used GPT-4 to augment unbalanced data for\nmore effective automatic scoring and Lee et al. ",
                    "Citation Text": "Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, and\nXiaoming Zhai. 2023. Applying Large Language Models and Chain-\nof-Thought for Automatic Scoring. arXiv preprint arXiv:2312.03748\n(2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2312.03748",
                        "Citation Paper Title": "Title:Applying Large Language Models and Chain-of-Thought for Automatic Scoring",
                        "Citation Paper Abstract": "Abstract:This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT)in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of automatic assessment tools among researchers and educators. We used a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses. We employed six prompt engineering strategies, combining zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6\\% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = .60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44\\% increase for zero-shot; 3.7\\% increase for few-shot). Using a novel approach PPEAS, we found a more balanced accuracy across different proficiency categories, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. Additionally, we also found that GPT-4 demonstrated superior performance over GPT-3.5 in various scoring tasks, showing 8.64\\% difference. The study revealed that the single-call strategy with GPT-4, particularly using greedy sampling, outperformed other approaches, including ensemble voting strategies. This study demonstrates the potential of LLMs in facilitating automatic scoring, emphasizing that CoT enhances accuracy, particularly when used with item stem and scoring rubrics.",
                        "Citation Paper Authors": "Authors:Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "fine-tuned GPT-3.5 Turbo\nto automatically score students\u2019 written explanations, and\nFang et al. ",
                    "Citation Text": "Luyang Fang, Gyeong-Geon Lee, and Xiaoming Zhai. 2023. Using gpt-\n4 to augment unbalanced data for automatic scoring. arXiv preprint\narXiv:2310.18365 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.18365",
                        "Citation Paper Title": "Title:Using GPT-4 to Augment Unbalanced Data for Automatic Scoring",
                        "Citation Paper Abstract": "Abstract:Machine learning-based automatic scoring can be challenging if students' responses are unbalanced across scoring categories, as it introduces uncertainty in the machine training process. To meet this challenge, we introduce a novel text data augmentation framework using GPT-4, a generative large language model, specifically tailored for unbalanced datasets in automatic scoring. Our experimental dataset comprised student-written responses to two science items. We crafted prompts for GPT-4 to generate responses resembling student-written answers, particularly for the minority scoring classes, to augment the data. We then finetuned DistillBERT for automatic scoring based on the augmented and original datasets. Model performance was assessed using accuracy, precision, recall, and F1 score. We incorporate varied amounts of augmented data to examine scoring performance, and our findings revealed remarkedly improved model performance. The average maximum increase observed across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for recall, and 24.2% for F1 score. Notably, using just 5% of the augmented data led to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly, the extent of improvement varied depending on specific datasets. Moreover, we found that a varying amount of augmented data (5%-40%) was needed to obtain a stable improvement. We also compare models trained with GPT-4 augmented data and those trained with additional student-written responses. The findings indicate that former ones match or even exceed the performance of the latter. Specifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for four metrics separately. This research underscores the potential and effectiveness of data augmentation techniques utilizing GPT-4 in addressing unbalanced datasets within automated assessment.",
                        "Citation Paper Authors": "Authors:Luyang Fang, Gyeong-Geon Lee, Xiaoming Zhai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.10072v3": {
            "Paper Title": "Fine-tuning ChatGPT for Automatic Scoring",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15838v1": {
            "Paper Title": "SecQA: A Concise Question-Answering Dataset for Evaluating Large\n  Language Models in Computer Security",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.03752v2": {
            "Paper Title": "Automatic Scoring of Students' Science Writing Using Hybrid Neural\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15835v1": {
            "Paper Title": "ShallowBlocker: Improving Set Similarity Joins for Blocking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15816v1": {
            "Paper Title": "TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15815v1": {
            "Paper Title": "Compositional Generalization in Spoken Language Understanding",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "have been widely\nsuccessful for a variety of natural language understanding tasks [9, 10, 37, 38]. The pre-trained model BERT\nwhen fine-tuned on joint intent classification and slot tagging task has achieved state-of-the-art performance on\nbenchmark datasets ",
                    "Citation Text": "Q. Chen, Z. Zhuo, and W. Wang, \u201cBERT for joint intent classification and slot filling,\u201d CoRR , vol.\nabs/1902.10909, 2019. [Online]. Available: http://arxiv.org/abs/1902.10909",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10909",
                        "Citation Paper Title": "Title:BERT for Joint Intent Classification and Slot Filling",
                        "Citation Paper Abstract": "Abstract:Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.",
                        "Citation Paper Authors": "Authors:Qian Chen, Zhu Zhuo, Wen Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13933v2": {
            "Paper Title": "Structured Probabilistic Coding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.09889v3": {
            "Paper Title": "Language Generation from Brain Recordings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09781v2": {
            "Paper Title": "GSQA: An End-to-End Model for Generative Spoken Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15784v1": {
            "Paper Title": "AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.11730v3": {
            "Paper Title": "Knowledge Graph Prompting for Multi-Document Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.18652v2": {
            "Paper Title": "EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health\n  Records with Chest X-ray Images",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", which\nare outperforming models in the fields of semantic parsing ( e.g., Text-to-Query) and medical VQA\n(e.g., MIMIC-CXR-VQA), respectively. For ChatGPT, we conduct in-context learning ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems , 33:1877\u20131901, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposed Symphony, a QA system for multi-modal data lakes, particularly\ndesigned to handle text and tables by using a unified representation for multi-modal datasets. Drawing\ninspiration from recent studies like Binder ",
                    "Citation Text": "Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir\nRadev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv\npreprint arXiv:2210.02875 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02875",
                        "Citation Paper Title": "Title:Binding Language Models in Symbolic Languages",
                        "Citation Paper Abstract": "Abstract:Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", there remains a noticeable gap in efforts to view patient images as an integral part\nof the EHR database or to synchronize them effectively with the structured tabular data in EHRs.\n2Presently, MIMIC-CXR ",
                    "Citation Text": "Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,\nChih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database\nof chest radiographs with free-text reports. Scientific data , 6(1):317, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.07042",
                        "Citation Paper Title": "Title:MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs",
                        "Citation Paper Abstract": "Abstract:Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.",
                        "Citation Paper Authors": "Authors:Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, Steven Horng"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "introduced\nMIMICSQL dataset for the text-to-SQL generation task on MIMIC-III, employing slot-filling for pre-\ndefined templates and using crowd-sourced paraphrasing. Pampari et al. ",
                    "Citation Text": "Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. emrqa: A large corpus for question\nanswering on electronic medical records. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing , pages 2357\u20132368, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00732",
                        "Citation Paper Title": "Title:emrQA: A Large Corpus for Question Answering on Electronic Medical Records",
                        "Citation Paper Abstract": "Abstract:We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.",
                        "Citation Paper Authors": "Authors:Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15751v1": {
            "Paper Title": "Solving Label Variation in Scientific Information Extraction via\n  Multi-Task Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15713v1": {
            "Paper Title": "PersianLLaMA: Towards Building First Persian Large Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15710v1": {
            "Paper Title": "Alleviating Hallucinations of Large Language Models through Induced\n  Hallucinations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15696v1": {
            "Paper Title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models\n  with Semi-structured Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.07516v2": {
            "Paper Title": "Voting-based Multimodal Automatic Deception Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15645v1": {
            "Paper Title": "Conditional Variational Autoencoder for Sign Language Translation with\n  Cross-Modal Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.04031v2": {
            "Paper Title": "Multiple Representation Transfer from Large Language Models to\n  End-to-End ASR Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.07695v5": {
            "Paper Title": "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "and T5-base with schema\nserialization [ 30,12], as baseline models for our task. This choice aligns with a recent finding\nthat transfer learning from pre-trained language models surpasses healthcare-specific text-to-SQL\nmodels ",
                    "Citation Text": "Seongsu Bae, Daeyoung Kim, Jiho Kim, and Edward Choi. Question answering for complex electronic\nhealth records database using unified encoder-decoder architecture. In Proceedings of Machine Learning\nfor Health , volume 158 of Proceedings of Machine Learning Research , pages 13\u201325. PMLR, 04 Dec 2021.\nURLhttps://proceedings.mlr.press/v158/bae21a.html .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14703",
                        "Citation Paper Title": "Title:Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture",
                        "Citation Paper Abstract": "Abstract:An intelligent machine that can answer human questions based on electronic health records (EHR-QA) has a great practical value, such as supporting clinical decisions, managing hospital administration, and medical chatbots. Previous table-based QA studies focusing on translating natural questions into table queries (NLQ2SQL), however, suffer from the unique nature of EHR data due to complex and specialized medical terminology, hence increased decoding difficulty. In this paper, we design UniQA, a unified encoder-decoder architecture for EHR-QA where natural language questions are converted to queries such as SQL or SPARQL. We also propose input masking (IM), a simple and effective method to cope with complex medical terms and various typos and better learn the SQL/SPARQL syntax. Combining the unified architecture with an effective auxiliary training objective, UniQA demonstrated a significant performance improvement against the previous state-of-the-art model for MIMICSQL* (14.2% gain), the most complex NLQ2SQL dataset in the EHR domain, and its typo-ridden versions (approximately 28.8% gain). In addition, we confirmed consistent results for the graph-based EHR-QA dataset, MIMICSPARQL*.",
                        "Citation Paper Authors": "Authors:Seongsu Bae, Daeyoung Kim, Jiho Kim, Edward Choi"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "trained on the Quora duplicate\nquestion detection dataset.\n4.The paraphrases are ranked by the perplexity score from GPT-Neo 1.3B ",
                    "Citation Text": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language\nmodeling. arXiv preprint arXiv:2101.00027 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.00027",
                        "Citation Paper Title": "Title:The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.",
                        "Citation Paper Authors": "Authors:Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". Yet, in semantic parsing, most works assume that all input questions are valid\nand can be answered; however, this is not true in practice ",
                    "Citation Text": "Yusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao Yu, Peng Shi, and Rui Zhang. Did you ask a good\nquestion? a cross-domain question intention classification benchmark for text-to-sql. arXiv preprint\narXiv:2010.12634 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12634",
                        "Citation Paper Title": "Title:Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL",
                        "Citation Paper Abstract": "Abstract:Neural models have achieved significant results on the text-to-SQL task, in which most current work assumes all the input questions are legal and generates a SQL query for any input. However, in the real scenario, users can input any text that may not be able to be answered by a SQL query. In this work, we propose TriageSQL, the first cross-domain text-to-SQL question intention classification benchmark that requires models to distinguish four types of unanswerable questions from answerable questions. The baseline RoBERTa model achieves a 60% F1 score on the test set, demonstrating the need for further improvement on this task. Our dataset is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao Yu, Peng Shi, Rui Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14890v2": {
            "Paper Title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language\n  Models via Complexity Classes",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "both utilize human exam\nquestions in their automated evaluations. Additionally, datasets such as the French National Math\nExam, Hungarian National High School Exam1, and GHOST (Graduate-Level High-Order Skill\nTests) ",
                    "Citation Text": "Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz,\nPhilipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of\nchatgpt. arXiv preprint arXiv:2301.13867 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.13867",
                        "Citation Paper Title": "Title:Mathematical Capabilities of ChatGPT",
                        "Citation Paper Abstract": "Abstract:We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!",
                        "Citation Paper Authors": "Authors:Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Julius Berner"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". In robotic contexts, LLMs\n3demonstrate enhanced performance with environment feedback, creating an internal \u201cmonologue\u201d to\nassist decision-making ",
                    "Citation Text": "Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied\nreasoning through planning with language models. arXiv preprint arXiv:2207.05608 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.05608",
                        "Citation Paper Title": "Title:Inner Monologue: Embodied Reasoning through Planning with Language Models",
                        "Citation Paper Abstract": "Abstract:Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
                        "Citation Paper Authors": "Authors:Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". Moreover, the interplay of\nreasoning and action showcases LLMs\u2019 outstanding synergistic ability. For instance, the \u201cReAct\u201d\napproach highlights that reasoning can enhance action plans, while actions can help the model\ninterface with external sources for better reasoning ",
                    "Citation Text": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.03629",
                        "Citation Paper Title": "Title:ReAct: Synergizing Reasoning and Acting in Language Models",
                        "Citation Paper Abstract": "Abstract:While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: this https URL",
                        "Citation Paper Authors": "Authors:Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ".\nSimilarly, backward verification proposes an intuitive human-like mechanism for LLMs to self-check\nand improve their conclusions, reducing errors in reasoning tasks ",
                    "Citation Text": "Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are\nreasoners with self-verification. arXiv preprint arXiv:2212.09561 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.09561",
                        "Citation Paper Title": "Title:Large Language Models are Better Reasoners with Self-Verification",
                        "Citation Paper Abstract": "Abstract:Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: this https URL.",
                        "Citation Paper Authors": "Authors:Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ".\nLater, various self-critique methods have been proposed to enhance LLM\u2019s reasoning performance.\nThe Recursively Criticizes and Improves (RCI) approach, for example, iteratively refines outputs,\nproving more effective in automating computer tasks and elevating reasoning capabilities ",
                    "Citation Text": "Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.17491",
                        "Citation Paper Title": "Title:Language Models can Solve Computer Tasks",
                        "Citation Paper Abstract": "Abstract:Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: this https URL.",
                        "Citation Paper Authors": "Authors:Geunwoo Kim, Pierre Baldi, Stephen McAleer"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nThis has sparked considerable interest in the community since reasoning ability is a hallmark of human\nintelligence. Various variations of chain-of-thought have been developed to prompt models\u2019 reasoning\nability[20, 21], such as tree of thought ",
                    "Citation Text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10601",
                        "Citation Paper Title": "Title:Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.",
                        "Citation Paper Authors": "Authors:Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17264v1": {
            "Paper Title": "ESGReveal: An LLM-based approach for extracting structured data from ESG\n  reports",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.12267v6": {
            "Paper Title": "Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid\n  Essay in Education",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15614v1": {
            "Paper Title": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Software Engineering Tasks",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ", the Eq. 6 can be rewrote as:\n\u210e\u2190( 1\u2212\ud835\udf06(\ud835\udc65))\u210e+\ud835\udf06(\ud835\udc65)\u0394\u210e,\u0394\u210e:=\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc65\ud835\udc4a\ud835\udc5e\ud835\udc43\u22a4\n\ud835\udc58)\ud835\udc43\ud835\udc63 (7)\nwhere\ud835\udf06(\ud835\udc65)is a scalar representing the sum of normalized attention weights over prefixes and\ncan be represented by:\n\ud835\udf06(\ud835\udc65)=\u00cd\n\ud835\udc56\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc65\ud835\udc4a\ud835\udc5e\ud835\udc43\u22a4\n\ud835\udc58)\ud835\udc56\u00cd\n\ud835\udc56\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc65\ud835\udc4a\ud835\udc5e\ud835\udc43\u22a4\n\ud835\udc58)\ud835\udc56+\u00cd\n\ud835\udc57\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc65\ud835\udc4a\ud835\udc5e\ud835\udc4a\u22a4\n\ud835\udc58\ud835\udc4b\u22a4)\ud835\udc57(8)\nLoRA ",
                    "Citation Text": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09685",
                        "Citation Paper Title": "Title:LoRA: Low-Rank Adaptation of Large Language Models",
                        "Citation Paper Abstract": "Abstract:An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL.",
                        "Citation Paper Authors": "Authors:Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ", and Section 7 concludes this paper.\n2 PRELIMINARIES\n2.1 Transformer and Pre-Trained Models\nTransformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.14403v2": {
            "Paper Title": "O3D: Offline Data-driven Discovery and Distillation for Sequential\n  Decision-Making with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15603v1": {
            "Paper Title": "A Split-and-Privatize Framework for Large Language Model Fine-Tuning",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "requires retraining the PLM based on the designed masked LM objective on the publicly\navailable corpora, which incurs significant computation costs. Besides, considering the high cost of fine-tuning the\nentire model on private data, Li et al. ",
                    "Citation Text": "Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language model services.\narXiv preprint arXiv:2305.06212 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.06212",
                        "Citation Paper Title": "Title:Privacy-Preserving Prompt Tuning for Large Language Model Services",
                        "Citation Paper Abstract": "Abstract:Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. \\textsc{rapt} adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.",
                        "Citation Paper Authors": "Authors:Yansong Li, Zhixing Tan, Yang Liu"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "proposed a local differential privacy (LDP) scheme,\nwhere the customer encodes its input embedding vectors in binary and then randomly flip some of the positions to\nintroduce perturbations. Chen et al. ",
                    "Citation Text": "Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork. Natural language\nunderstanding with privacy-preserving bert. In Proceedings of the 30th ACM International Conference on\nInformation & Knowledge Management , pages 1488\u20131497, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07504",
                        "Citation Paper Title": "Title:Natural Language Understanding with Privacy-Preserving BERT",
                        "Citation Paper Abstract": "Abstract:Privacy preservation remains a key challenge in data mining and Natural Language Understanding (NLU). Previous research shows that the input text or even text embeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying dx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs.",
                        "Citation Paper Authors": "Authors:Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, Marc Najork"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "published by Huggingface1as the pre-\ntrained model, which consists of a total of 355 million parameters. We evaluate the SAP framework on the Financial\nPhrasebank (FP) ",
                    "Citation Text": "Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or bad debt: Detecting\nsemantic orientations in economic texts. Journal of the Association for Information Science and Technology ,\n65(4):782\u2013796, 2014.\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1307.5336",
                        "Citation Paper Title": "Title:Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts",
                        "Citation Paper Abstract": "Abstract:The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to investigate how financial sentiments relate to future company performance. However, based on experience from other fields, where sentiment analysis is commonly applied, it is well-known that the overall semantic orientation of a sentence may differ from the prior polarity of individual words. The objective of this article is to investigate how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase-structure information and domain-specific use of language. Our three main contributions are: (1) establishment of a human-annotated finance phrase-bank, which can be used as benchmark for training and evaluating alternative models; (2) presentation of a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect overall sentiment; (3) development of a linearized phrase-structure model for detecting contextual semantic orientations in financial and economic news texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning-algorithm are demonstrated in a comparative study against previously used general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature-space caused by the use of conventional n-gram features.",
                        "Citation Paper Authors": "Authors:Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposed a method based on differential privacy (DP)\nand word dropout, which protects data privacy during the inference phase by randomly dropping some words and\nadding Gaussian noise to the text representation. The work ",
                    "Citation Text": "Lingjuan Lyu, Yitong Li, Xuanli He, and Tong Xiao. Towards differentially private text representations. In\nProceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information\nRetrieval , pages 1813\u20131816, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.14170",
                        "Citation Paper Title": "Title:Towards Differentially Private Text Representations",
                        "Citation Paper Abstract": "Abstract:Most deep learning frameworks require users to pool their local data or model updates to a trusted server to train or maintain a global model. The assumption of a trusted server who has access to user information is ill-suited in many applications. To tackle this problem, we develop a new deep learning framework under an untrusted server setting, which includes three modules: (1) embedding module, (2) randomization module, and (3) classifier module. For the randomization module, we propose a novel local differentially private (LDP) protocol to reduce the impact of privacy parameter $\\epsilon$ on accuracy, and provide enhanced flexibility in choosing randomization probabilities for LDP. Analysis and experiments show that our framework delivers comparable or even better performance than the non-private framework and existing LDP protocols, demonstrating the advantages of our LDP protocol.",
                        "Citation Paper Authors": "Authors:Lingjuan Lyu, Yitong Li, Xuanli He, Tong Xiao"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "injects trainable rank\ndecomposition matrices into each attention layer of the transformer architecture. BitFit ",
                    "Citation Text": "Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers) , pages 1\u20139, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10199",
                        "Citation Paper Title": "Title:BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
                        "Citation Paper Abstract": "Abstract:We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
                        "Citation Paper Authors": "Authors:Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.13961v2": {
            "Paper Title": "Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing\n  Idiomatic Translation with Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17263v1": {
            "Paper Title": "TACIT: A Target-Agnostic Feature Disentanglement Framework for\n  Cross-Domain Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16233v1": {
            "Paper Title": "Chatbot is Not All You Need: Information-rich Prompting for More\n  Realistic Responses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.09361v2": {
            "Paper Title": "MFAS: Emotion Recognition through Multiple Perspectives Fusion\n  Architecture Search Emulating Human Cognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15576v1": {
            "Paper Title": "Reducing LLM Hallucinations using Epistemic Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15561v1": {
            "Paper Title": "README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15550v1": {
            "Paper Title": "Multi-level biomedical NER through multi-granularity embeddings and\n  enhanced labeling",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "presented the ClinicalBERT  model which trained on the MIMIC-III dataset, \nwhich consists of clinical reports of patients for use in clinical text mining. The BLUE model ",
                    "Citation Text": "Y. Peng, S. Yan and Z. Lu, \"Transfer Learning in Bi omedical Natural Language Processing: An Evaluation  \nof BERT and ELMo on Ten Benchmarking Datasets,\" in Proceedings of the 18th BioNLP Workshop and \nShared Task , Florence, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05474",
                        "Citation Paper Title": "Title:Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
                        "Citation Paper Abstract": "Abstract:Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Yifan Peng, Shankai Yan, Zhiyong Lu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "proposes a model based on CNNs \nand word embeddings, which classifies text and sent ences. Text sentence vectors with candidate \nwindows, have been represented with CNN models. \n\u2022 Transfer learning-based : Lee et al. ",
                    "Citation Text": "J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. So and J. Kang, \"BioBERT: a pre- trained biomedical \nlanguage representation model for biomedical text m ining,\" Bioinformatics (Oxford, England), vol. 36, \nSeptember 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08746",
                        "Citation Paper Title": "Title:BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
                        "Citation Paper Abstract": "Abstract:Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at this https URL, and the source code for fine-tuning BioBERT available at this https URL.",
                        "Citation Paper Authors": "Authors:Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15548v1": {
            "Paper Title": "YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal\n  Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15523v1": {
            "Paper Title": "The Persuasive Power of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15503v1": {
            "Paper Title": "Making Large Language Models A Better Foundation For Dense Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12021v2": {
            "Paper Title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11152v2": {
            "Paper Title": "Prompt Based Tri-Channel Graph Convolution Neural Network for Aspect\n  Sentiment Triplet Extraction",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "constructed the\nASTE task as a multi-step machine reading comprehen-\nsion (MRC) problem. However, these pipeline methods\nare limited by error propagation and its cumbersome\nprocess.\nAnother research line is joint extraction. Xu ",
                    "Citation Text": "Lu Xu, Hao Li, Wei Lu, and Lidong Bing. Position-\naware tagging for aspect sentiment triplet extraction.\nInEMNLP , pages 2339\u20132349, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02609",
                        "Citation Paper Title": "Title:Position-Aware Tagging for Aspect Sentiment Triplet Extraction",
                        "Citation Paper Abstract": "Abstract:Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.",
                        "Citation Paper Authors": "Authors:Lu Xu, Hao Li, Wei Lu, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "first transformed ASTE into a unified\ntable-filling task and proposed the Grid Tagging Scheme\n(GTS) to address it. To further explore this method,\nJing ",
                    "Citation Text": "Hongjiang Jing, Zuchao Li, Hai Zhao, and Shu Jiang.\nSeeking common but distinguishing difference, a joint\naspect-based sentiment analysis model. In EMNLP\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09634",
                        "Citation Paper Title": "Title:Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model",
                        "Citation Paper Abstract": "Abstract:Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three subtasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However, most of the existing joint models only focus on the benefits of encoder sharing between subtasks but ignore the difference. Therefore, we propose a joint ABSA model, which not only enjoys the benefits of encoder sharing but also focuses on the difference to improve the effectiveness of the model. In detail, we introduce a dual-encoder design, in which a pair encoder especially focuses on candidate aspect-opinion pair classification, and the original encoder keeps attention on sequence labeling. Empirical results show that our proposed model shows robustness and significantly outperforms the previous state-of-the-art on four benchmark datasets.",
                        "Citation Paper Authors": "Authors:Hongjiang Jing, Zuchao Li, Hai Zhao, Shu Jiang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". 3) Table-filling methods\nencoder the word relations into a two-dimensional table:\nS3E2 ",
                    "Citation Text": "Zhexue Chen, Hong Huang, Bang Liu, Xuanhua Shi,\nand Hai Jin. Semantic and syntactic enhanced aspect\nsentiment triplet extraction. In Findings of ACL-\nIJCNLP , pages 1474\u20131483, 2021.\nCopyright \u00a92024 by SIAM\nUnauthorized reproduction of this article is prohibited",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03315",
                        "Citation Paper Title": "Title:Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction",
                        "Citation Paper Abstract": "Abstract:Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from sentences, where each triplet includes an entity, its associated sentiment, and the opinion span explaining the reason for the sentiment. Most existing research addresses this problem in a multi-stage pipeline manner, which neglects the mutual information between such three elements and has the problem of error propagation. In this paper, we propose a Semantic and Syntactic Enhanced aspect Sentiment triplet Extraction model (S3E2) to fully exploit the syntactic and semantic relationships between the triplet elements and jointly extract them. Specifically, we design a Graph-Sequence duel representation and modeling paradigm for the task of ASTE: we represent the semantic and syntactic relationships between word pairs in a sentence by graph and encode it by Graph Neural Networks (GNNs), as well as modeling the original sentence by LSTM to preserve the sequential information. Under this setting, we further apply a more efficient inference strategy for the extraction of triplets. Extensive evaluations on four benchmark datasets show that S3E2 significantly outperforms existing approaches, which proves our S3E2's superiority and flexibility in an end-to-end fashion.",
                        "Citation Paper Authors": "Authors:Zhexue Chen, Hong Huang, Bang Liu, Xuanhua Shi, Hai Jin"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "designed two types of paradigms, named\nannotation-style and extraction-style modeling, tackling\nseveral sentiment analysis tasks in a unified generative\nframework. Xu ",
                    "Citation Text": "Lu Xu, Yew Ken Chia, and Lidong Bing. Learning\nspan-level interactions for aspect sentiment triplet ex-\ntraction. In ACL-IJCNLP , pages 4755\u20134766, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.12214",
                        "Citation Paper Title": "Title:Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction",
                        "Citation Paper Abstract": "Abstract:Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions.",
                        "Citation Paper Authors": "Authors:Lu Xu, Yew Ken Chia, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ",\nand make progress on many NLP tasks, such as Text\nClassification ",
                    "Citation Text": "Shengding Hu, Ning Ding, Huadong Wang, and et al.\nKnowledgeable prompt-tuning: Incorporating knowl-\nedge into prompt verbalizer for text classification. In\nACL, pages 2225\u20132240, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.02035",
                        "Citation Paper Title": "Title:Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
                        "Citation Paper Abstract": "Abstract:Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompt-tuning (KPT), to improve and stabilize prompt-tuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.",
                        "Citation Paper Authors": "Authors:Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed a boundary detection model, which\ncan fully exploit both word-to-word and relation-to-\nrelation interactions.\nPrompt-tuning. Prompt-tuning is a fine-tuning\nparadigm proposed in GPT-3 ",
                    "Citation Text": "Tom B. Brown, Benjamin Mann, Nick Ryder, and et al.\nLanguage models are few-shot learners. In NeurIPS\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.02317v2": {
            "Paper Title": "GNN2R: Weakly-Supervised Rationale-Providing Question Answering over\n  Knowledge Graphs",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "show that even the current state-of-the-art\nLLMs, including ChatGPT2and LLaMA (33B) ",
                    "Citation Text": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample, \u201cLlama: Open and efficient\nfoundation language models,\u201d CoRR , vol. abs/2302.13971, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2302.13971",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15478v1": {
            "Paper Title": "A Group Fairness Lens for Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": ". BiasAsker introduces an\nautomated framework for identifying both absolute and correlated\nsocial biases in conversational AI ",
                    "Citation Text": "Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, and Michael R.\nLyu. 2023. BiasAsker: Measuring the Bias in Conversational AI System. ArXiv\nabs/2305.12434 (2023). https://api.semanticscholar.org/CorpusID:258833296",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.12434",
                        "Citation Paper Title": "Title:BiasAsker: Measuring the Bias in Conversational AI System",
                        "Citation Paper Abstract": "Abstract:Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to the data-driven, black-box nature of modern AI techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on 8 commercial systems and 2 famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.",
                        "Citation Paper Authors": "Authors:Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, Michael Lyu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15472v1": {
            "Paper Title": "Towards Consistent Language Models Using Declarative Constraints",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". Nonetheless, it has shown that these methods may\nalso lead to many inaccurate answers ",
                    "Citation Text": "Ximing Lu, Peter West, Rowan Zellers, Ronan\nLe Bras, Chandra Bhagavatula, and Yejin Choi. Neu-\nroLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies , Online, June 2021. As-\nsociation for Computational Linguistics. 1,2,5,7,8,\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12884",
                        "Citation Paper Title": "Title:NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints",
                        "Citation Paper Abstract": "Abstract:Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples.\nWe propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search.\nEmpirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.",
                        "Citation Paper Authors": "Authors:Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Domain Speci\ufb01c Fine-Tuning. One can \ufb01ne-tune an\nLLM on a set of domain-speci\ufb01c data sources to improve\nthe quality of its answers for questions in a given domain ",
                    "Citation Text": "Helena H. Lee, Ke Shu, Palakorn Achananuparp,\nPhilips Kokoh Prasetyo, Yue Liu, Ee-Peng Lim, and\nLav R. Varshney. Recipegpt: Generative pre-training\nbased cooking recipe generation and evaluation sys-\ntem. In Companion Proceedings of the Web Confer-\nence 2020 , WWW \u201920, page 181\u2013184, New York, NY ,\nUSA, 2020. Association for Computing Machinery. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.02498",
                        "Citation Paper Title": "Title:RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and Evaluation System",
                        "Citation Paper Abstract": "Abstract:Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at this https URL.",
                        "Citation Paper Authors": "Authors:Helena H. Lee, Ke Shu, Palakorn Achananuparp, Philips Kokoh Prasetyo, Yue Liu, Ee-Peng Lim, Lav R. Varshney"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08642v2": {
            "Paper Title": "Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "Few-shot Prompting. With the rapid development of LLMs,\nmore and more researchers attempted to develop and optimize\ndifferent prompting strategies to effectively employ LLMs in\nvarious downstream tasks (e.g., sentiment analysis ",
                    "Citation Text": "Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng,\nand Rui Xia, \u201cIs chatgpt a good sentiment analyzer?\na preliminary study,\u201d arXiv preprint arXiv:2304.04339 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.04339",
                        "Citation Paper Title": "Title:Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study",
                        "Citation Paper Abstract": "Abstract:Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.",
                        "Citation Paper Authors": "Authors:Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". Similarly, we conduct\nexperiments on SemEval-2014 Datasets2(i.e., 14-Laptop and\n14-Restaurant). The statistics of the two datasets are shown\nin Table 1. Besides, we use Accuracy andMacro F1 as evalu-\nation metrics ",
                    "Citation Text": "Margherita Grandini, CRIF SpA, Enrico Bagli, and\nGiorgio Visani, \u201cMetrics for multi-class classification:\nAn overview,\u201d stat, vol. 1050, pp. 13, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.05756",
                        "Citation Paper Title": "Title:Metrics for Multi-Class Classification: an Overview",
                        "Citation Paper Abstract": "Abstract:Classification tasks in machine learning involving more than two classes are known by the name of \"multi-class classification\". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.",
                        "Citation Paper Authors": "Authors:Margherita Grandini, Enrico Bagli, Giorgio Visani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2302.14691v2": {
            "Paper Title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for\n  Instruction Following",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.14089v2": {
            "Paper Title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with\n  Electronic Medical Records",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". Unfortunately, requiring EHRs as context\nfor question generation limits scalability, as medical institutions restrict access to patient data to preserve\npatient privacy. Pampari et al. ",
                    "Citation Text": "A. Pampari, P. Raghavan, J. Liang, and J. Peng. emrQA: A large corpus for question answering on\nelectronic medical records. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing , pages 2357\u20132368, Brussels,\nBelgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1258. URL\nhttps://aclanthology.org/D18-1258 .\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00732",
                        "Citation Paper Title": "Title:emrQA: A Large Corpus for Question Answering on Electronic Medical Records",
                        "Citation Paper Abstract": "Abstract:We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.",
                        "Citation Paper Authors": "Authors:Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13533v2": {
            "Paper Title": "Automated Clinical Coding for Outpatient Departments",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". For inpatient settings, conflict-\ning claims regarding the efficacy of pretrained language models\nhave been put forward ",
                    "Citation Text": "Z. Yuan, C. Tan, and S. Huang, \u201cCode Synonyms Do Matter: Multiple\nSynonyms Matching Network for Automatic ICD Coding,\u201d Proceedings\nof the Annual Meeting of the Association for Computational Linguistics ,\nvol. 2, pp. 808\u2013814, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.01515",
                        "Citation Paper Title": "Title:Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding",
                        "Citation Paper Abstract": "Abstract:Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs). Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Zheng Yuan, Chuanqi Tan, Songfang Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06635v3": {
            "Paper Title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15407v1": {
            "Paper Title": "A Comprehensive Analysis of the Effectiveness of Large Language Models\n  as Automatic Dialogue Evaluators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15398v1": {
            "Paper Title": "Fairness-Aware Structured Pruning in Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.17842v2": {
            "Paper Title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15395v1": {
            "Paper Title": "Prompt Valuation Based on Shapley Values",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.05722v3": {
            "Paper Title": "Exploring Large Language Model for Graph Data Understanding in Online\n  Job Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11518v2": {
            "Paper Title": "User Modeling in the Era of Large Language Models: Current Research and\n  Future Directions",
            "Sentences": [
                {
                    "Sentence ID": 293,
                    "Sentence": "Y . Zhu, H. Wang, Y . Wang, Y . Li, Y . Yuan, and J. Qiang. Clickbait detection via large language models. arXiv\npreprint arXiv:2306.09597 , 2023. ",
                    "Citation Text": "C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, and D. Yang. Can large language models transform computational\nsocial science? arXiv preprint arXiv:2305.03514 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.03514",
                        "Citation Paper Title": "Title:Can Large Language Models Transform Computational Social Science?",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in} social science analysis in partnership with humans.",
                        "Citation Paper Authors": "Authors:Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang"
                    }
                },
                {
                    "Sentence ID": 291,
                    "Sentence": "C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206 , 2023. ",
                    "Citation Text": "D. Zhou, N. Sch \u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al.\nLeast-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.10625",
                        "Citation Paper Title": "Title:Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
                        "Citation Paper Authors": "Authors:Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi"
                    }
                },
                {
                    "Sentence ID": 290,
                    "Sentence": "A. Zhiyuli, Y . Chen, X. Zhang, and X. Liang. Bookgpt: A general framework for book recommendation empowered\nby large language model. arXiv preprint arXiv:2305.15673 , 2023. ",
                    "Citation Text": "C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.11206",
                        "Citation Paper Title": "Title:LIMA: Less Is More for Alignment",
                        "Citation Paper Abstract": "Abstract:Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
                        "Citation Paper Authors": "Authors:Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy"
                    }
                },
                {
                    "Sentence ID": 289,
                    "Sentence": "Z. Zheng, Z. Qiu, X. Hu, L. Wu, H. Zhu, and H. Xiong. Generative job recommendations with large language\nmodel. arXiv preprint arXiv:2307.02157 , 2023. ",
                    "Citation Text": "A. Zhiyuli, Y . Chen, X. Zhang, and X. Liang. Bookgpt: A general framework for book recommendation empowered\nby large language model. arXiv preprint arXiv:2305.15673 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.15673",
                        "Citation Paper Title": "Title:BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model",
                        "Citation Paper Abstract": "Abstract:With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities. This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice. By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios. At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios.",
                        "Citation Paper Authors": "Authors:Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, Xun Liang"
                    }
                },
                {
                    "Sentence ID": 288,
                    "Sentence": "Z. Zheng, L. Liao, Y . Deng, and L. Nie. Building emotional support chatbots in the era of llms. arXiv preprint\narXiv:2308.11584 , 2023. ",
                    "Citation Text": "Z. Zheng, Z. Qiu, X. Hu, L. Wu, H. Zhu, and H. Xiong. Generative job recommendations with large language\nmodel. arXiv preprint arXiv:2307.02157 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.02157",
                        "Citation Paper Title": "Title:Generative Job Recommendations with Large Language Model",
                        "Citation Paper Abstract": "Abstract:The rapid development of online recruitment services has encouraged the utilization of recommender systems to streamline the job seeking process. Predominantly, current job recommendations deploy either collaborative filtering or person-job matching strategies. However, these models tend to operate as \"black-box\" systems and lack the capacity to offer explainable guidance to job seekers. Moreover, conventional matching-based recommendation methods are limited to retrieving and ranking existing jobs in the database, restricting their potential as comprehensive career AI advisors. To this end, here we present GIRL (GeneratIve job Recommendation based on Large language models), a novel approach inspired by recent advancements in the field of Large Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker. Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator. This aligns the generator with recruiter feedback, tailoring the output to better meet employer preferences. In particular, GIRL serves as a job seeker-centric generative model, providing job suggestions without the need of a candidate set. This capability also enhances the performance of existing job recommendation models by supplementing job seeking features with generated content. With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach. We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more personalized and comprehensive job-seeking experience.",
                        "Citation Paper Authors": "Authors:Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong"
                    }
                },
                {
                    "Sentence ID": 287,
                    "Sentence": "L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023. ",
                    "Citation Text": "Z. Zheng, L. Liao, Y . Deng, and L. Nie. Building emotional support chatbots in the era of llms. arXiv preprint\narXiv:2308.11584 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.11584",
                        "Citation Paper Title": "Title:Building Emotional Support Chatbots in the Era of LLMs",
                        "Citation Paper Abstract": "Abstract:The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, examining the impact of diverse training strategies, ultimately yielding an LLM meticulously optimized for emotional support interactions. An exhaustive assessment of the resultant model showcases its proficiency in offering emotional support, marking a pivotal step in the realm of emotional support bots and paving the way for subsequent research and implementations.",
                        "Citation Paper Authors": "Authors:Zhonghua Zheng, Lizi Liao, Yang Deng, Liqiang Nie"
                    }
                },
                {
                    "Sentence ID": 286,
                    "Sentence": "C. Zheng, S. Sabour, J. Wen, and M. Huang. Augesc: Large-scale data augmentation for emotional support conver-\nsation with pre-trained language models. arXiv preprint arXiv:2202.13047 , 2022. ",
                    "Citation Text": "L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.05685",
                        "Citation Paper Title": "Title:Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
                        "Citation Paper Abstract": "Abstract:Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica"
                    }
                },
                {
                    "Sentence ID": 285,
                    "Sentence": "Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of\nlanguage models. In International Conference on Machine Learning , pages 12697\u201312706. PMLR, 2021. ",
                    "Citation Text": "C. Zheng, S. Sabour, J. Wen, and M. Huang. Augesc: Large-scale data augmentation for emotional support conver-\nsation with pre-trained language models. arXiv preprint arXiv:2202.13047 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.13047",
                        "Citation Paper Title": "Title:AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
                        "Citation Paper Abstract": "Abstract:Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models' generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks.",
                        "Citation Paper Authors": "Authors:Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang, Minlie Huang"
                    }
                },
                {
                    "Sentence ID": 284,
                    "Sentence": "W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223 , 2023. ",
                    "Citation Text": "Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of\nlanguage models. In International Conference on Machine Learning , pages 12697\u201312706. PMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.09690",
                        "Citation Paper Title": "Title:Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                        "Citation Paper Abstract": "Abstract:GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
                        "Citation Paper Authors": "Authors:Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh"
                    }
                },
                {
                    "Sentence ID": 283,
                    "Sentence": "T. Zhao, B. Ni, W. Yu, Z. Guo, N. Shah, and M. Jiang. Action sequence augmentation for early graph-based anomaly\ndetection. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management ,\npages 2668\u20132678, 2021. ",
                    "Citation Text": "W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.18223",
                        "Citation Paper Title": "Title:A Survey of Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
                        "Citation Paper Authors": "Authors:Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen"
                    }
                },
                {
                    "Sentence ID": 282,
                    "Sentence": "Q. Zhao, Y . Zhang, D. Friedman, and F. Tan. E-commerce recommendation with personalized promotion. In\nProceedings of the 9th ACM Conference on Recommender Systems , pages 219\u2013226, 2015. ",
                    "Citation Text": "T. Zhao, B. Ni, W. Yu, Z. Guo, N. Shah, and M. Jiang. Action sequence augmentation for early graph-based anomaly\ndetection. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management ,\npages 2668\u20132678, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10016",
                        "Citation Paper Title": "Title:Action Sequence Augmentation for Early Graph-based Anomaly Detection",
                        "Citation Paper Abstract": "Abstract:The proliferation of web platforms has created incentives for online abuse. Many graph-based anomaly detection techniques are proposed to identify the suspicious accounts and behaviors. However, most of them detect the anomalies once the users have performed many such behaviors. Their performance is substantially hindered when the users' observed data is limited at an early stage, which needs to be improved to minimize financial loss. In this work, we propose Eland, a novel framework that uses action sequence augmentation for early anomaly detection. Eland utilizes a sequence predictor to predict next actions of every user and exploits the mutual enhancement between action sequence augmentation and user-action graph anomaly detection. Experiments on three real-world datasets show that Eland improves the performance of a variety of graph-based anomaly detection methods. With Eland, anomaly detection performance at an earlier stage is better than non-augmented methods that need significantly more observed data by up to 15% on the Area under the ROC curve.",
                        "Citation Paper Authors": "Authors:Tong Zhao, Bo Ni, Wenhao Yu, Zhichun Guo, Neil Shah, Meng Jiang"
                    }
                },
                {
                    "Sentence ID": 280,
                    "Sentence": "S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open\npre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. ",
                    "Citation Text": "Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y . Zhang, Y . Chen, et al. Siren\u2019s song in the ai\nocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.01219",
                        "Citation Paper Title": "Title:Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
                        "Citation Paper Abstract": "Abstract:While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
                        "Citation Paper Authors": "Authors:Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi"
                    }
                },
                {
                    "Sentence ID": 279,
                    "Sentence": "R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y . Qiao. Llama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention. arXiv preprint arXiv:2303.16199 , 2023. ",
                    "Citation Text": "S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open\npre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01068",
                        "Citation Paper Title": "Title:OPT: Open Pre-trained Transformer Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                        "Citation Paper Authors": "Authors:Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 278,
                    "Sentence": "J. Zhang, R. Xie, Y . Hou, W. X. Zhao, L. Lin, and J.-R. Wen. Recommendation as instruction following: A large\nlanguage model empowered recommendation approach. arXiv preprint arXiv:2305.07001 , 2023. ",
                    "Citation Text": "R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y . Qiao. Llama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention. arXiv preprint arXiv:2303.16199 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16199",
                        "Citation Paper Title": "Title:LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
                        "Citation Paper Abstract": "Abstract:We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao"
                    }
                },
                {
                    "Sentence ID": 276,
                    "Sentence": "J. Zhang, K. Bao, Y . Zhang, W. Wang, F. Feng, and X. He. Is chatgpt fair for recommendation? evaluating fairness\nin large language model recommendation. arXiv preprint arXiv:2305.07609 , 2023.\n39 ",
                    "Citation Text": "J. Zhang, K. Sun, A. Jagadeesh, M. Ghahfarokhi, D. Gupta, A. Gupta, V . Gupta, and Y . Guo. The poten-\ntial and pitfalls of using a large language model such as chatgpt or gpt-4 as a clinical assistant. arXiv preprint\narXiv:2307.08152 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.08152",
                        "Citation Paper Title": "Title:The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant",
                        "Citation Paper Abstract": "Abstract:Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications.",
                        "Citation Paper Authors": "Authors:Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo"
                    }
                },
                {
                    "Sentence ID": 275,
                    "Sentence": "J. Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv\npreprint arXiv:2304.11116 , 2023. ",
                    "Citation Text": "J. Zhang, K. Bao, Y . Zhang, W. Wang, F. Feng, and X. He. Is chatgpt fair for recommendation? evaluating fairness\nin large language model recommendation. arXiv preprint arXiv:2305.07609 , 2023.\n39",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.07609",
                        "Citation Paper Title": "Title:Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
                        "Citation Paper Abstract": "Abstract:The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He"
                    }
                },
                {
                    "Sentence ID": 272,
                    "Sentence": "A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al. Glm-130b: An open\nbilingual pre-trained model. In The Eleventh International Conference on Learning Representations , 2022. ",
                    "Citation Text": "B. Zhang, D. Ding, and L. Jing. How would stance detection techniques evolve after the launch of chatgpt? arXiv\npreprint arXiv:2212.14548 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.14548",
                        "Citation Paper Title": "Title:How would Stance Detection Techniques Evolve after the Launch of ChatGPT?",
                        "Citation Paper Abstract": "Abstract:Stance detection refers to the task of extracting the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the proliferation of social media contents. The conventional framework of handling stance detection is converting it into text classification tasks. Deep learning models have already replaced rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are facing two main challenges which are insufficient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing model. The explanations for the cases it cannot provide classification results are especially useful. ChatGPT has the potential to be the best AI model for stance detection tasks in NLP, or at least change the research paradigm of this field. ChatGPT also opens up the possibility of building explanatory AI for stance detection.",
                        "Citation Paper Authors": "Authors:Bowen Zhang, Daijun Ding, Liwen Jing"
                    }
                },
                {
                    "Sentence ID": 271,
                    "Sentence": "S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y . Zhou, Y . Xiao, S. Yun, W. Lin, et al. Disc-lawllm: Fine-tuning\nlarge language models for intelligent legal services. arXiv preprint arXiv:2309.11325 , 2023. ",
                    "Citation Text": "A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al. Glm-130b: An open\nbilingual pre-trained model. In The Eleventh International Conference on Learning Representations , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02414",
                        "Citation Paper Title": "Title:GLM-130B: An Open Bilingual Pre-trained Model",
                        "Citation Paper Abstract": "Abstract:We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 269,
                    "Sentence": "Y . Yu, Y . Zhuang, J. Zhang, Y . Meng, A. Ratner, R. Krishna, J. Shen, and C. Zhang. Large language model as\nattributed training data generator: A tale of diversity and bias. arXiv preprint arXiv:2306.15895 , 2023. ",
                    "Citation Text": "J. Yuan, R. Tang, X. Jiang, and X. Hu. Llm for patient-trial matching: Privacy-aware data augmentation towards\nbetter performance and generalizability. arXiv preprint arXiv:2303.16756 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16756",
                        "Citation Paper Title": "Title:Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching",
                        "Citation Paper Abstract": "Abstract:The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.",
                        "Citation Paper Authors": "Authors:Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, Xia Hu"
                    }
                },
                {
                    "Sentence ID": 268,
                    "Sentence": "X. Yu, Y . Qi, K. Chen, G. Chen, X. Yang, P. Zhu, W. Zhang, and N. Yu. Gpt paternity test: Gpt generated text\ndetection with gpt genetic inheritance. arXiv preprint arXiv:2305.12519 , 2023. ",
                    "Citation Text": "Y . Yu, Y . Zhuang, J. Zhang, Y . Meng, A. Ratner, R. Krishna, J. Shen, and C. Zhang. Large language model as\nattributed training data generator: A tale of diversity and bias. arXiv preprint arXiv:2306.15895 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.15895",
                        "Citation Paper Title": "Title:Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\% of the querying cost of ChatGPT associated with the latter. The data and code are available on \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang"
                    }
                },
                {
                    "Sentence ID": 267,
                    "Sentence": "X. Yu, Z. Chen, Y . Ling, S. Dong, Z. Liu, and Y . Lu. Temporal data meets llm\u2013explainable financial time series\nforecasting. arXiv preprint arXiv:2306.11025 , 2023. ",
                    "Citation Text": "X. Yu, Y . Qi, K. Chen, G. Chen, X. Yang, P. Zhu, W. Zhang, and N. Yu. Gpt paternity test: Gpt generated text\ndetection with gpt genetic inheritance. arXiv preprint arXiv:2305.12519 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.12519",
                        "Citation Paper Title": "Title:GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating fake social media postings that can sway election results. Detecting whether a text is machine-generated has thus become increasingly important. While machine-learning-based detection strategies exhibit superior performance, they often lack generalizability, limiting their practicality. In this work, we introduce GPT Paternity Test (GPT-Pat), which reliably detects machine-generated text across varied datasets. Given a text under scrutiny, we leverage ChatGPT to generate a corresponding question and provide a re-answer to the question. By comparing the similarity between the original text and the generated re-answered text, it can be determined whether the text is machine-generated. GPT-Pat consists of a Siamese network to compute the similarity between the original text and the generated re-answered text and a binary classifier. Our method achieved an average accuracy of 94.57% on four generalization test sets, surpassing the state-of-the-art RoBERTa-based method by 12.34%. The accuracy drop of our method is only about half of that of the RoBERTa-based method when it is attacked by re-translation and polishing.",
                        "Citation Paper Authors": "Authors:Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, Nenghai Yu"
                    }
                },
                {
                    "Sentence ID": 266,
                    "Sentence": "W. Yu, C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. A survey of knowledge-enhanced text generation. ACM\nComputing Surveys , 54(11s):1\u201338, 2022. ",
                    "Citation Text": "X. Yu, Z. Chen, Y . Ling, S. Dong, Z. Liu, and Y . Lu. Temporal data meets llm\u2013explainable financial time series\nforecasting. arXiv preprint arXiv:2306.11025 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.11025",
                        "Citation Paper Title": "Title:Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting",
                        "Citation Paper Abstract": "Abstract:This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",
                        "Citation Paper Authors": "Authors:Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, Yanbin Lu"
                    }
                },
                {
                    "Sentence ID": 265,
                    "Sentence": "W. Yu, M. Yu, T. Zhao, and M. Jiang. Identifying referential intention with heterogeneous contexts. In Proceedings\nof The Web Conference 2020 , pages 962\u2013972, 2020. ",
                    "Citation Text": "W. Yu, C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. A survey of knowledge-enhanced text generation. ACM\nComputing Surveys , 54(11s):1\u201338, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04389",
                        "Citation Paper Title": "Title:A Survey of Knowledge-Enhanced Text Generation",
                        "Citation Paper Abstract": "Abstract:The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.",
                        "Citation Paper Authors": "Authors:Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang"
                    }
                },
                {
                    "Sentence ID": 262,
                    "Sentence": "B. Yin, J. Xie, Y . Qin, Z. Ding, Z. Feng, X. Li, and W. Lin. Heterogeneous knowledge fusion: A novel approach for\npersonalized recommendation via llm. arXiv preprint arXiv:2308.03333 , 2023. ",
                    "Citation Text": "R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec. Graph convolutional neural net-\nworks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on\nknowledge discovery & data mining , pages 974\u2013983, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 261,
                    "Sentence": "R. Ye, C. Zhang, R. Wang, S. Xu, and Y . Zhang. Natural language is all a graph needs. arXiv preprint\narXiv:2308.07134 , 2023. ",
                    "Citation Text": "B. Yin, J. Xie, Y . Qin, Z. Ding, Z. Feng, X. Li, and W. Lin. Heterogeneous knowledge fusion: A novel approach for\npersonalized recommendation via llm. arXiv preprint arXiv:2308.03333 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.03333",
                        "Citation Paper Title": "Title:Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM",
                        "Citation Paper Abstract": "Abstract:The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior information. In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations. The experimental results demonstrate that our method can effectively integrate user heterogeneous behavior and significantly improve recommendation performance.",
                        "Citation Paper Authors": "Authors:Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, Wei Lin"
                    }
                },
                {
                    "Sentence ID": 260,
                    "Sentence": "Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang. Editing large language models:\nProblems, methods, and opportunities. arXiv preprint arXiv:2305.13172 , 2023. ",
                    "Citation Text": "R. Ye, C. Zhang, R. Wang, S. Xu, and Y . Zhang. Natural language is all a graph needs. arXiv preprint\narXiv:2308.07134 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.07134",
                        "Citation Paper Title": "Title:Natural Language is All a Graph Needs",
                        "Citation Paper Abstract": "Abstract:The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.",
                        "Citation Paper Authors": "Authors:Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang"
                    }
                },
                {
                    "Sentence ID": 259,
                    "Sentence": "Y . Yao, Z. Li, and H. Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models.\narXiv preprint arXiv:2305.16582 , 2023. ",
                    "Citation Text": "Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang. Editing large language models:\nProblems, methods, and opportunities. arXiv preprint arXiv:2305.13172 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.13172",
                        "Citation Paper Title": "Title:Editing Large Language Models: Problems, Methods, and Opportunities",
                        "Citation Paper Abstract": "Abstract:Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang"
                    }
                },
                {
                    "Sentence ID": 258,
                    "Sentence": "S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem\nsolving with large language models. arXiv preprint arXiv:2305.10601 , 2023. ",
                    "Citation Text": "Y . Yao, Z. Li, and H. Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models.\narXiv preprint arXiv:2305.16582 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.16582",
                        "Citation Paper Title": "Title:Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT(large) with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.",
                        "Citation Paper Authors": "Authors:Yao Yao, Zuchao Li, Hai Zhao"
                    }
                },
                {
                    "Sentence ID": 257,
                    "Sentence": "S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y . Jia, and H. Zan. Zhongjing: Enhancing the chinese medical ca-\npabilities of large language model through expert feedback and real-world multi-turn dialogue. arXiv preprint\narXiv:2308.03549 , 2023. ",
                    "Citation Text": "S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem\nsolving with large language models. arXiv preprint arXiv:2305.10601 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10601",
                        "Citation Paper Title": "Title:Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.",
                        "Citation Paper Authors": "Authors:Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan"
                    }
                },
                {
                    "Sentence ID": 256,
                    "Sentence": "L. Yang, F. Jiang, and H. Li. Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text.\narXiv preprint arXiv:2307.11380 , 2023. ",
                    "Citation Text": "S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y . Jia, and H. Zan. Zhongjing: Enhancing the chinese medical ca-\npabilities of large language model through expert feedback and real-world multi-turn dialogue. arXiv preprint\narXiv:2308.03549 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.03549",
                        "Citation Paper Title": "Title:Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue",
                        "Citation Paper Abstract": "Abstract:Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot align responses with experts' intentions. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from continuous pre-training, SFT, to Reinforcement Learning from Human Feedback (RLHF). Additionally, we construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We also define a refined annotation rule and evaluation criteria given the unique characteristics of the biomedical domain. Extensive experimental results show that Zhongjing outperforms baselines in various capacities and matches the performance of ChatGPT in some abilities, despite the 100x parameters. Ablation studies also demonstrate the contributions of each component: pre-training enhances medical knowledge, and RLHF further improves instruction-following ability and safety. Our code, datasets, and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan"
                    }
                },
                {
                    "Sentence ID": 255,
                    "Sentence": "K.-C. Yang and F. Menczer. Large language models can rate news outlet credibility. arXiv preprint\narXiv:2304.00228 , 2023. ",
                    "Citation Text": "L. Yang, F. Jiang, and H. Li. Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text.\narXiv preprint arXiv:2307.11380 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.11380",
                        "Citation Paper Title": "Title:Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text",
                        "Citation Paper Abstract": "Abstract:The remarkable capabilities of large-scale language models, such as ChatGPT, in text generation have impressed readers and spurred researchers to devise detectors to mitigate potential risks, including misinformation, phishing, and academic dishonesty. Despite this, most previous studies have been predominantly geared towards creating detectors that differentiate between purely ChatGPT-generated texts and human-authored texts. This approach, however, fails to work on discerning texts generated through human-machine collaboration, such as ChatGPT-polished texts. Addressing this gap, we introduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts), facilitating the construction of more robust detectors. It diverges from extant corpora by comprising pairs of human-written and ChatGPT-polished abstracts instead of purely ChatGPT-generated texts. Additionally, we propose the \"Polish Ratio\" method, an innovative measure of the degree of modification made by ChatGPT compared to the original human-written text. It provides a mechanism to measure the degree of ChatGPT influence in the resulting text. Our experimental results show our proposed model has better robustness on the HPPT dataset and two existing datasets (HC3 and CDB). Furthermore, the \"Polish Ratio\" we proposed offers a more comprehensive explanation by quantifying the degree of ChatGPT involvement.",
                        "Citation Paper Authors": "Authors:Lingyi Yang, Feng Jiang, Haizhou Li"
                    }
                },
                {
                    "Sentence ID": 251,
                    "Sentence": "Q. Yan, Y . Zhang, Q. Liu, S. Wu, and L. Wang. Relation-aware heterogeneous graph for user profiling. In Pro-\nceedings of the 30th ACM International Conference on Information & Knowledge Management , pages 3573\u20133577,\n2021. ",
                    "Citation Text": "C. Yang, X. Wang, Y . Lu, H. Liu, Q. V . Le, D. Zhou, and X. Chen. Large language models as optimizers. arXiv\npreprint arXiv:2309.03409 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.03409",
                        "Citation Paper Title": "Title:Large Language Models as Optimizers",
                        "Citation Paper Abstract": "Abstract:Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at this https URL.",
                        "Citation Paper Authors": "Authors:Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15316v1": {
            "Paper Title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "as the speech encoder.\nWav2vec 2.0 has been shown to encode rich prosodic information\nuseful for speech sentiment and sarcasm detection ",
                    "Citation Text": "Guan-Ting Lin, Chi-Luen Feng, Wei-Ping Huang, Yuan Tseng, Tzu-\nHan Lin, Chen-An Li, Hung-yi Lee, and Nigel G Ward, \u201cOn the utility\nof self-supervised models for prosody-related tasks,\u201d in 2022 IEEE\nSpoken Language Technology Workshop (SLT) . IEEE, 2023, pp. 1104\u2013\n1111.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.07185",
                        "Citation Paper Title": "Title:On the Utility of Self-supervised Models for Prosody-related Tasks",
                        "Citation Paper Abstract": "Abstract:Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, SUPERB-prosody, consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks.",
                        "Citation Paper Authors": "Authors:Guan-Ting Lin, Chi-Luen Feng, Wei-Ping Huang, Yuan Tseng, Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Nigel G. Ward"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "leverage paired, aligned transcripts and speech\nfor ASR and TTS tasks. However, discrete audio/speech units\nmainly capture content information and lack prosodic and paralin-\nguistic information. pGSLM ",
                    "Citation Text": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet,\nKushal Lakhotia, Tu-Anh Nguyen, Morgane Riviere, Abdelrahman\nMohamed, Emmanuel Dupoux, et al., \u201cText-free prosody-aware gener-\native spoken language modeling,\u201d in ACL 2022 , 2022, pp. 8666\u20138681.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.03264",
                        "Citation Paper Title": "Title:Text-Free Prosody-Aware Generative Spoken Language Modeling",
                        "Citation Paper Abstract": "Abstract:Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at this https URL. Codes and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Morgane Rivi\u00e8re, Abdelrahman Mohamed, Emmanuel Dupoux, Wei-Ning Hsu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.14910v2": {
            "Paper Title": "From Shortcuts to Triggers: Backdoor Defense with Denoised PoE",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.13245v3": {
            "Paper Title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head\n  Checkpoints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15304v1": {
            "Paper Title": "Evaluating the Capability of ChatGPT on Ancient Chinese",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15272v1": {
            "Paper Title": "Detecting anxiety from short clips of free-form speech",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.10420v2": {
            "Paper Title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.09596v2": {
            "Paper Title": "RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal\n  Sentiment Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.02437v3": {
            "Paper Title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
            "Sentences": [
                {
                    "Sentence ID": 99,
                    "Sentence": ", open domain\nquestion answering [12, 33] and semantic parsing ",
                    "Citation Text": "Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu,\nSumit Sanghai, and Fei Sha. Generate-and-retrieve: Use your predictions to improve retrieval\nfor semantic parsing. In Proceedings of the 29th International Conference on Computational\nLinguistics , pages 4946\u20134951, Gyeongju, Republic of Korea, October 2022. International\nCommittee on Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14899",
                        "Citation Paper Title": "Title:Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing",
                        "Citation Paper Abstract": "Abstract:A common recent approach to semantic parsing augments sequence-to-sequence models by retrieving and appending a set of training samples, called exemplars. The effectiveness of this recipe is limited by the ability to retrieve informative exemplars that help produce the correct parse, which is especially challenging in low-resource settings. Existing retrieval is commonly based on similarity of query and exemplar inputs. We propose GandR, a retrieval procedure that retrieves exemplars for which outputs are also similar. GandRfirst generates a preliminary prediction with input-based retrieval. Then, it retrieves exemplars with outputs similar to the preliminary prediction which are used to generate a final prediction. GandR sets the state of the art on multiple low-resource semantic parsing tasks.",
                        "Citation Paper Authors": "Authors:Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, Fei Sha"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", paraphrase\ngeneration [ 34,83], language modeling [ 36,105], counterfactual data generation ",
                    "Citation Text": "Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A\nretrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.04873",
                        "Citation Paper Title": "Title:CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
                        "Citation Paper Abstract": "Abstract:Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present COunterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations",
                        "Citation Paper Authors": "Authors:Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Other\napplications include abstractive summarization [ 64,14,18,15], code generation ",
                    "Citation Text": "Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit\nframework for predicting structured outputs. In Proc. of NeurIPS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.01194",
                        "Citation Paper Title": "Title:A Retrieve-and-Edit Framework for Predicting Structured Outputs",
                        "Citation Paper Abstract": "Abstract:For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.",
                        "Citation Paper Authors": "Authors:Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". For dialogue response generation tasks, employing exemplar/template\n2retrieval as an intermediate step has proven advantageous for generating informative responses [ 89,\n91,6,7]. In-context learning example retrieval also aids in controllable dialogue ",
                    "Citation Text": "Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable\ndialogue simulation with in-context learning. In Proc. of EMNLP Findings , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.04185",
                        "Citation Paper Title": "Title:Controllable Dialogue Simulation with In-Context Learning",
                        "Citation Paper Abstract": "Abstract:Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic} automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero \\textit{human involvement} and \\textit{parameter update} and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialogues as a seed. When enough data is available, our method can still serve as an effective data augmentation method. Human evaluation results also show that our simulated dialogues have near-human fluency and annotation accuracy. The code and data are available at \\textbf{\\url{this https URL}}.",
                        "Citation Paper Authors": "Authors:Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, Xifeng Yan"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ". For\nfew-shot LLM generation, strategies for in-context example selection have been proposed to improve\ntranslation quality ",
                    "Citation Text": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.\nIn-context examples selection for machine translation. CoRR , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.02437",
                        "Citation Paper Title": "Title:In-context Examples Selection for Machine Translation",
                        "Citation Paper Abstract": "Abstract:Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
                        "Citation Paper Authors": "Authors:Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15194v1": {
            "Paper Title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15185v1": {
            "Paper Title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion\n  Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15181v1": {
            "Paper Title": "Multilingual Bias Detection and Mitigation for Indian Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.06415v3": {
            "Paper Title": "Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.17280v4": {
            "Paper Title": "Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.14901v3": {
            "Paper Title": "Chain-of-Questions Training with Latent Answers for Robust Multistep\n  Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.10944v2": {
            "Paper Title": "Deception Detection from Linguistic and Physiological Data Streams Using\n  Bimodal Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15159v1": {
            "Paper Title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large\n  Language Model Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15156v1": {
            "Paper Title": "Large Language Models as Zero-Shot Keyphrase Extractor: A Preliminary\n  Empirical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.15408v5": {
            "Paper Title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical\n  Perspective",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "demonstrated that autoregressive Transformers can\nin-context learn basic function classes (e.g., linear functions, MLPs, and decision trees) via input\nsample sequences. Subsequent works further revealed that Transformers can implement learning\nalgorithms such as linear regression ",
                    "Citation Text": "Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learn-\ning algorithm is in-context learning? investigations with linear models. In The Eleventh\nInternational Conference on Learning Representations , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.15661",
                        "Citation Paper Title": "Title:What learning algorithm is in-context learning? Investigations with linear models",
                        "Citation Paper Abstract": "Abstract:Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at this https URL.",
                        "Citation Paper Authors": "Authors:Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "considered a restricted setting of learning automata,\nfor which a shallow non-recursive Transformer provably suffices. Yao et al. ",
                    "Citation Text": "Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention\nnetworks can process bounded hierarchical languages. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers) , pages 3770\u20133785, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.11115",
                        "Citation Paper Title": "Title:Self-Attention Networks Can Process Bounded Hierarchical Languages",
                        "Citation Paper Abstract": "Abstract:Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested parentheses of $k$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process $\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by $D$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with $D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes $\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and $O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show that self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.",
                        "Citation Paper Authors": "Authors:Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "showed\nthat finite-precision encoder-decoder Transformers can approximately simulate Turing machines\nwith bounded computation time. Liu et al. ",
                    "Citation Text": "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-\nformers learn shortcuts to automata. In The Eleventh International Conference on Learning\nRepresentations , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.10749",
                        "Citation Paper Title": "Title:Transformers Learn Shortcuts to Automata",
                        "Citation Paper Abstract": "Abstract:Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.",
                        "Citation Paper Authors": "Authors:Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "demonstrated that a constant-depth\nlooped Transformer encoder can simulate practical computer programs. Wei et al. ",
                    "Citation Text": "Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case\nstudy on approximating turing machines with transformers. In Advances in Neural Information\nProcessing Systems , volume 35, pages 12071\u201312083, 2022.\n14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.13163",
                        "Citation Paper Title": "Title:Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers",
                        "Citation Paper Abstract": "Abstract:A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\\log (T)$. We also introduce new tools for analyzing generalization which provide much tighter sample complexities than the typical VC-dimension or norm-based bounds, which may be of independent interest.",
                        "Citation Paper Authors": "Authors:Colin Wei, Yining Chen, Tengyu Ma"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "and looped Transformer encoders are Turing-complete [ 49,47,20,8]. However, these results\ndepend on the unreasonable assumption of infinite precision, yielding a quite unrealistic construction\nthat does not match practical scenarios. Recently, Giannou ",
                    "Citation Text": "Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-\nitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint\narXiv:2301.13196 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.13196",
                        "Citation Paper Title": "Title:Looped Transformers as Programmable Computers",
                        "Citation Paper Abstract": "Abstract:We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.",
                        "Citation Paper Authors": "Authors:Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ".\n9More relevant to this paper, another line of works investigated the power of Transformers from a\ncomputation perspective. Early results have shown that both standard encoder-decoder Transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems , volume 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "proved that Transformers\nwith sufficient size can universally approximate arbitrary continuous sequence-to-sequence functions\non a compact domain. Recently, universality results have been extended to model variants such as\nSparse Transformers ",
                    "Citation Text": "Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi,\nand Sanjiv Kumar. O (n) connections are expressive enough: Universal approximability of\nsparse transformers. In Advances in Neural Information Processing Systems , volume 33, pages\n13783\u201313794, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04862",
                        "Citation Paper Title": "Title:$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
                        "Citation Paper Abstract": "Abstract:Recently, Transformer networks have redefined the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length $n$ to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, fundamental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufficient conditions under which we prove that a sparse attention model can universally approximate any sequence-to-sequence function. Surprisingly, our results show that sparse Transformers with only $O(n)$ connections per attention layer can approximate the same function class as the dense model with $n^2$ connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks.",
                        "Citation Paper Authors": "Authors:Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15099v1": {
            "Paper Title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in\n  Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": ".\nLLMs based on this prompting style have been shown to per-\nform better on reasoning tasks, such as arithmetic problem\nsolving ",
                    "Citation Text": "S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\nK. Narasimhan, \u201cTree of thoughts: Deliberate problem solving with\nlarge language models,\u201d arXiv preprint arXiv:2305.10601 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10601",
                        "Citation Paper Title": "Title:Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.",
                        "Citation Paper Authors": "Authors:Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": ", minimizing their dependency on\nextensive data. Additionally, CoT could be instrumental\nin generating varied fuzzing inputs across multiple\nprogramming languages ",
                    "Citation Text": "C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang,\n\u201cUniversal fuzzing via large language models,\u201d arXiv preprint\narXiv:2308.04748 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.04748",
                        "Citation Paper Title": "Title:Universal Fuzzing via Large Language Models",
                        "Citation Paper Abstract": "Abstract:Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 76 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 47 bugs already confirmed by developers as previously unknown.",
                        "Citation Paper Authors": "Authors:Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "). Following existing approaches that\npropose to use human moderators to collect a limited, initial\nseed dataset of tweets ",
                    "Citation Text": "P. Paudel, J. Blackburn, E. De Cristofaro, S. Zannettou, and\nG. Stringhini, \u201cLambretta: learning to rank for Twitter soft modera-\ntion,\u201d in 2023 IEEE Symposium on Security and Privacy (SP) . IEEE,\n2023, pp. 311\u2013326.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.05926",
                        "Citation Paper Title": "Title:LAMBRETTA: Learning to Rank for Twitter Soft Moderation",
                        "Citation Paper Abstract": "Abstract:To curb the problem of false information, social media platforms like Twitter started adding warning labels to content discussing debunked narratives, with the goal of providing more context to their audiences. Unfortunately, these labels are not applied uniformly and leave large amounts of false content unmoderated. This paper presents LAMBRETTA, a system that automatically identifies tweets that are candidates for soft moderation using Learning To Rank (LTR). We run LAMBRETTA on Twitter data to moderate false claims related to the 2020 US Election and find that it flags over 20 times more tweets than Twitter, with only 3.93% false positives and 18.81% false negatives, outperforming alternative state-of-the-art methods based on keyword extraction and semantic search. Overall, LAMBRETTA assists human moderators in identifying and flagging false information on social media.",
                        "Citation Paper Authors": "Authors:Pujan Paudel, Jeremy Blackburn, Emiliano De Cristofaro, Savvas Zannettou, Gianluca Stringhini"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". As\na result, there is a need for reasoning-based approaches to\nidentify new hateful content, involving decision-making to\ndetermine whether the content is hateful or not. Recently,\nin-context learning (ICL) ",
                    "Citation Text": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V .\nLe, D. Zhou et al. , \u201cChain-of-thought prompting elicits reasoning in\nlarge language models,\u201d Advances in Neural Information Processing\nSystems , vol. 35, pp. 24 824\u201324 837, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.01046v4": {
            "Paper Title": "Deep Manifold Learning for Reading Comprehension and Logical Reasoning\n  Tasks with Polytuplet Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15068v1": {
            "Paper Title": "Refining GPT-3 Embeddings with a Siamese Structure for Technical Post\n  Duplicate Detection",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "first proposed neural embeddings in\nthe form of a feed-forward neural network language model.\nNeural embeddings are dense vector representations of texts\nlearnt by different neural network structures and objective\nfunctions. Word embedding techniques (e.g., Word2Vec ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff\nDean. Distributed representations of words and phrases and their\ncompositionality. Advances in neural information processing systems ,\n26, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13772v2": {
            "Paper Title": "On Task Performance and Model Calibration with Supervised and\n  Self-Ensembled In-Context Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15033v1": {
            "Paper Title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable\n  Inference-Time Intervention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15021v1": {
            "Paper Title": "Towards a Unified Multimodal Reasoning Framework",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nAnticipated problems included limitations with com-\nputational resources, and we encountered some models\nreturning surprisingly poor performance (RoBERTa) con-\ntrary to expectations ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach, 2019. Available at: https:\n//arxiv.org/pdf/1907.11692.pdf . 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "; 2) integrating visual embeddings withtextual embeddings for the baseline textual model ",
                    "Citation Text": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks, 2019. Available at: https://\narxiv.org/pdf/1908.02265.pdf . 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.02265",
                        "Citation Paper Title": "Title:ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
                        "Citation Paper Abstract": "Abstract:We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.",
                        "Citation Paper Authors": "Authors:Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "We set up the following text embedding models for our ex-\nperiments: 1) a simple QA model using DistilBERT as a\nbaseline ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter, 2019. Available at: https:\n//arxiv.org/pdf/1910.01108v4.pdf . 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". ScienceQA contains 21,208 multimodal multiple-\nchoice science questions collected from elementary and\nhigh school science curricula, covering a wide range of sub-\njects and offering a rich diversity of domains ",
                    "Citation Text": "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan\nRoth, and Jonathan Berant. Did aristotle use a laptop?\na question answering benchmark with implicit reasoning\nstrategies. Transactions of the Association for Computa-\ntional Linguistics , 9:346\u2013361, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.02235",
                        "Citation Paper Title": "Title:Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
                        "Citation Paper Abstract": "Abstract:A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of $\\sim$66%.",
                        "Citation Paper Authors": "Authors:Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14877v1": {
            "Paper Title": "Robust Knowledge Extraction from Large Language Models using Social\n  Choice Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.14743v6": {
            "Paper Title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze\n  Foundation Models Under Distribution Shift",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.15774v2": {
            "Paper Title": "Next Steps for Human-Centered Generative AI: A Technical Perspective",
            "Sentences": [
                {
                    "Sentence ID": 123,
                    "Sentence": "to reducing the chance\nof training models on artists\u2019 work ( e.g., by adding adversarial\nnoises ",
                    "Citation Text": "Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and\nBen Y. Zhao. 2023. GLAZE: Protecting Artists from Style Mimicry by Text-to-\nImage Models. https://doi.org/10.48550/arXiv.2302.04222 arXiv:2302.04222\n[cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.04222",
                        "Citation Paper Title": "Title:Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
                        "Citation Paper Abstract": "Abstract:Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after \"fine-tuning\" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply \"style cloaks\" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
                        "Citation Paper Authors": "Authors:Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben Y. Zhao"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": ") and a bottom-up approach\nwhere AI acts as a \u201ccopilot\u201d to assist individual steps performed\nby a human ( e.g., recommending where to examine on a medical\nimage ",
                    "Citation Text": "Hongyan Gu, Chunxu Yang, Mohammad Haeri, Jing Wang, Shirley Tang,\nWenzhong Yan, Shujin He, Christopher Kazu Williams, Shino Magaki, and\nXiang \u2019Anthony\u2019 Chen. 2023. Augmenting Pathologists with NaviPath: De-\nsign and Evaluation of a Human-AI Collaborative Navigation System. In Pro-\nceedings of the 2023 CHI Conference on Human Factors in Computing Sys-\ntems (CHI \u201923) . Association for Computing Machinery, New York, NY, USA.\nhttps://doi.org/10.1145/3544548.3580694 event-place: Hamburg, Germany.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.07309",
                        "Citation Paper Title": "Title:Augmenting Pathologists with NaviPath: Design and Evaluation of a Human-AI Collaborative Navigation System",
                        "Citation Paper Abstract": "Abstract:Artificial Intelligence (AI) brings advancements to support pathologists in navigating high-resolution tumor images to search for pathology patterns of interest. However, existing AI-assisted tools have not realized this promised potential due to a lack of insight into pathology and HCI considerations for pathologists' navigation workflows in practice. We first conducted a formative study with six medical professionals in pathology to capture their navigation strategies. By incorporating our observations along with the pathologists' domain knowledge, we designed NaviPath -- a human-AI collaborative navigation system. An evaluation study with 15 medical professionals in pathology indicated that: (i) compared to the manual navigation, participants saw more than twice the number of pathological patterns in unit time with NaviPath, and (ii) participants achieved higher precision and recall against the AI and the manual navigation on average. Further qualitative analysis revealed that navigation was more consistent with NaviPath, which can improve the overall examination quality.",
                        "Citation Paper Authors": "Authors:Hongyan Gu, Chunxu Yang, Mohammad Haeri, Jing Wang, Shirley Tang, Wenzhong Yan, Shujin He, Christopher Kazu Williams, Shino Magaki, Xiang 'Anthony' Chen"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "to construct a concept\ndataset from images to texts.\nThe recent development on image segmentation ",
                    "Citation Text": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDoll\u00e1r, and Ross Girshick. 2023. Segment Anything. https://doi.org/10.48550/\narXiv.2304.02643 arXiv:2304.02643 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.02643",
                        "Citation Paper Title": "Title:Segment Anything",
                        "Citation Paper Abstract": "Abstract:We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": ". Meanwhile, concept teaching in Generative AI\nremains a nascent research topic with some next-steps as follows.\nSimilar to how Concept Activation Vector ",
                    "Citation Text": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda\nViegas, and Rory Sayres. 2018. Interpretability Beyond Feature Attribution:\nQuantitative Testing with Concept Activation Vectors (TCAV). In Proceedings\nof the 35th International Conference on Machine Learning . PMLR, 2668\u20132677.\nhttps://proceedings.mlr.press/v80/kim18d.html ISSN: 2640-3498.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11279",
                        "Citation Paper Title": "Title:Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
                        "Citation Paper Abstract": "Abstract:The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
                        "Citation Paper Authors": "Authors:Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, Rory Sayres"
                    }
                },
                {
                    "Sentence ID": 150,
                    "Sentence": ", such as\nspecifying regions on the generated images that should be disentan-\ngled. Similarly, we can employ other techniques to support direct\nfixes, such as using image inpainting techniques ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S.\nHuang. 2018. Generative Image Inpainting With Contextual Attention. 5505\u2013\n5514. https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Generative_\nImage_Inpainting_CVPR_2018_paper.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07892",
                        "Citation Paper Title": "Title:Generative Image Inpainting with Contextual Attention",
                        "Citation Paper Abstract": "Abstract:Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": "shows promises of\nproviding such semantic controls as the outcome of the generative\nprocess. Relatedly, Videomap lets a user perform video editing bynavigating on a 2D view of the latent space ",
                    "Citation Text": "David Chuan-En Lin, Fabian Caba Heilbron, Joon-Young Lee, Oliver Wang, and\nNikolas Martelaro. 2022. VideoMap: Video Editing in Latent Space. https:\n//doi.org/10.48550/arXiv.2211.12492 arXiv:2211.12492 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.12492",
                        "Citation Paper Title": "Title:VideoMap: Video Editing in Latent Space",
                        "Citation Paper Abstract": "Abstract:Video has become a dominant form of media. However, video editing interfaces have remained largely unchanged over the past two decades. Such interfaces typically consist of a grid-like asset management panel and a linear editing timeline. When working with a large number of video clips, it can be difficult to sort through them all and identify patterns within (e.g. opportunities for smooth transitions and storytelling). In this work, we imagine a new paradigm for video editing by mapping videos into a 2D latent space and building a proof-of-concept interface.",
                        "Citation Paper Authors": "Authors:David Chuan-En Lin, Fabian Caba Heilbron, Joon-Young Lee, Oliver Wang, Nikolas Martelaro"
                    }
                },
                {
                    "Sentence ID": 102,
                    "Sentence": ". One next-step is to explore how\nusers can control other parameters in text generation using 2D\ninput techniques, including the usage of a dashboard that presents\ncomprehensive key parameters of generated text using data visual-\nization techniques ",
                    "Citation Text": "Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon,\nand Chris Bryan. 2023. PromptAid: Prompt Exploration, Perturbation, Testing\nand Iteration using Visual Analytics for Large Language Models. https://doi.\norg/10.48550/arXiv.2304.01964 arXiv:2304.01964 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.01964",
                        "Citation Paper Title": "Title:PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.",
                        "Citation Paper Authors": "Authors:Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, Chris Bryan"
                    }
                },
                {
                    "Sentence ID": 149,
                    "Sentence": "and over half the tokens in a\nprompt can be removed while maintaining or even improving model\nperformance ",
                    "Citation Text": "Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-\nSheng Jason Wu. 2023. Did You Read the Instructions? Rethinking the Effec-\ntiveness of Task Definitions in Instruction Learning. https://doi.org/10.48550/\narXiv.2306.01150 arXiv:2306.01150 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.01150",
                        "Citation Paper Title": "Title:Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60\\% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks.",
                        "Citation Paper Authors": "Authors:Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Jason Wu"
                    }
                },
                {
                    "Sentence ID": 135,
                    "Sentence": ") to indicate which partsof the input is \u201cresponsible\u201d for certain output. Analogously, we can\ndevelop and integrate similar approaches for Generative AI, e.g., the\nCross Attention approach ",
                    "Citation Text": "Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun\nKumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. 2022. What the DAAM:\nInterpreting Stable Diffusion Using Cross Attention. https://doi.org/10.48550/\narXiv.2210.04885 arXiv:2210.04885 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.04885",
                        "Citation Paper Title": "Title:What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
                        "Citation Paper Abstract": "Abstract:Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research. Our code is at this https URL.",
                        "Citation Paper Authors": "Authors:Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, Ferhan Ture"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", it remains unclear how to implement such techniques at in-\nteractive speed without requiring pre-generated examples.\nSimilar to feedforward, borrowing the autocompletion approach\nin text entry ",
                    "Citation Text": "Mia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin\nLu, Jackie Tsay, Yinan Wang, Andrew M. Dai, Zhifeng Chen, Timothy Sohn,\nand Yonghui Wu. 2019. Gmail Smart Compose: Real-Time Assisted Writing.\nInProceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining (KDD \u201919) . 2287\u20132295. https://doi.org/10.1145/3292500.\n3330723",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00080",
                        "Citation Paper Title": "Title:Gmail Smart Compose: Real-Time Assisted Writing",
                        "Citation Paper Abstract": "Abstract:In this paper, we present Smart Compose, a novel system for generating interactive, real-time suggestions in Gmail that assists users in writing mails by reducing repetitive typing. In the design and deployment of such a large-scale and complicated system, we faced several challenges including model selection, performance evaluation, serving and other practical issues. At the core of Smart Compose is a large-scale neural language model. We leveraged state-of-the-art machine learning techniques for language model training which enabled high-quality suggestion prediction, and constructed novel serving infrastructure for high-throughput and real-time inference. Experimental results show the effectiveness of our proposed system design and deployment approach. This system is currently being served in Gmail.",
                        "Citation Paper Authors": "Authors:Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M. Dai, Zhifeng Chen, Timothy Sohn, Yonghui Wu"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "to visualize what cer-\ntain edits in a text prompt might lead to changes in the generated\ncontents. Although there have been studies on the effects and trade-\noffs of such feedforward controls (using pre-generated examples) ",
                    "Citation Text": "Hai Dang, Lukas Mecke, and Daniel Buschek. 2022. GANSlider: How Users\nControl Generative Models for Images using Multiple Sliders with and without\nFeedforward Information. In Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems (CHI \u201922) . Association for Computing Machinery,\nNew York, NY, USA, 1\u201315. https://doi.org/10.1145/3491102.3502141",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.00965",
                        "Citation Paper Title": "Title:GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information",
                        "Citation Paper Abstract": "Abstract:We investigate how multiple sliders with and without feedforward visualizations influence users' control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.",
                        "Citation Paper Authors": "Authors:Hai Dang, Lukas Mecke, Daniel Buschek"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". Some next-steps for HGAI\ninclude the following.\nMore studies should be conducted to understand how humans\nuse prompts to interact with Generative AI, such as when writing\ntext ",
                    "Citation Text": "Hai Dang, Sven Goller, Florian Lehmann, and Daniel Buschek. 2023. Choice\nOver Control: How Users Write with Large Language Models using Diegetic and\nNon-Diegetic Prompting. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems (CHI \u201923) . Association for Computing Machinery,\nNew York, NY, USA, 1\u201317. https://doi.org/10.1145/3544548.3580969",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.03199",
                        "Citation Paper Title": "Title:Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
                        "Citation Paper Abstract": "Abstract:We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \"Once upon a time, I saw a fox...\"), and (2) non-diegetic prompts (external, e.g. \"Write about the adventures of the fox.\"). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.",
                        "Citation Paper Authors": "Authors:Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14870v1": {
            "Paper Title": "Numerical Reasoning for Financial Reports",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14867v1": {
            "Paper Title": "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis\n  Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15006v1": {
            "Paper Title": "Assessing the Impact of Prompting, Persona, and Chain of Thought Methods\n  on ChatGPT's Arithmetic Capabilities",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "Additionally, we employed\nGSM8K, which is a dataset comprising 8.5K high-quality, linguistically diverse grade school math word problems, to\nassess ChatGPT\u2019s performance. ",
                    "Citation Text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\nmath word problems, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.14168",
                        "Citation Paper Title": "Title:Training Verifiers to Solve Math Word Problems",
                        "Citation Paper Abstract": "Abstract:State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                        "Citation Paper Authors": "Authors:Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.12794v2": {
            "Paper Title": "Are Structural Concepts Universal in Transformer Language Models?\n  Towards Interpretable Cross-Lingual Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.19228v2": {
            "Paper Title": "Unsupervised Melody-to-Lyric Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14737v1": {
            "Paper Title": "Computational Semantics and Evaluation Benchmark for Interrogative\n  Sentences via Combinatory Categorial Grammar",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14708v1": {
            "Paper Title": "Balancing the Style-Content Trade-Off in Sentiment Transfer Using\n  Polarity-Aware Denoising",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "propose to perform text style transfer\nthrough back-translation. In a recent work, He et al. ",
                    "Citation Text": "He, J., Wang, X., Neubig, G., Berg-Kirkpatrick, T.: A Probabilistic Formulation of\nUnsupervised Text Style Transfer. In: Proc. ICLR (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.03912",
                        "Citation Paper Title": "Title:A Probabilistic Formulation of Unsupervised Text Style Transfer",
                        "Citation Paper Abstract": "Abstract:We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.",
                        "Citation Paper Authors": "Authors:Junxian He, Xinyi Wang, Graham Neubig, Taylor Berg-Kirkpatrick"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "93.5 0.308 0.504 0.9 15.2 -61.0 10.3 53.0\nWang et al. ",
                    "Citation Text": "Wang, K., Hua, H., Wan, X.: Controllable unsupervised text attribute transfer via\nediting entangled latent representation. In: Proc. NeurIPS, pp. 11034\u201311044 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12926",
                        "Citation Paper Title": "Title:Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation",
                        "Citation Paper Abstract": "Abstract:Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow to transfer over multiple aspects at the same time.",
                        "Citation Paper Authors": "Authors:Ke Wang, Hang Hua, Xiaojun Wan"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "propose a cross-aligned auto-encoder\nwith adversarial training. Prabhumoye et al. ",
                    "Citation Text": "Prabhumoye, S., Tsvetkov, Y ., Salakhutdinov, R., Black, A.W.: Style Transfer\nThrough Back-Translation. In: Proc. ACL, pp. 866\u2013876 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09000",
                        "Citation Paper Title": "Title:Style Transfer Through Back-Translation",
                        "Citation Paper Abstract": "Abstract:Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",
                        "Citation Paper Authors": "Authors:Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, Alan W Black"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ") have been masked. This allows measuring content preservation while\nignoring the parts of the sentences that need to be changed.\n5.4 Human Evaluation\nAs automated metrics for language generation do not correlate well with human judge-\nments ",
                    "Citation Text": "Novikova, J., Dusek, O., Curry, A.C., Rieser, V .: Why We Need New Evaluation\nMetrics for NLG. In: Proc. EMNLP, pp. 2241\u20132252 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06875",
                        "Citation Paper Title": "Title:Why We Need New Evaluation Metrics for NLG",
                        "Citation Paper Abstract": "Abstract:The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.",
                        "Citation Paper Authors": "Authors:Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, Verena Rieser"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "compare a multi-decoder model with a setup using a single\ndecoder and style embeddings. Shen et al. ",
                    "Citation Text": "Shen, T., Lei, T., Barzilay, R., Jaakkola, T.S.: Style Transfer from Non-Parallel\nText by Cross-Alignment. In: Proc. NeurIPS, pp. 6830\u20136841 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.09655",
                        "Citation Paper Title": "Title:Style Transfer from Non-Parallel Text by Cross-Alignment",
                        "Citation Paper Abstract": "Abstract:This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.",
                        "Citation Paper Authors": "Authors:Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". However,\nthese metric cannot evaluate style transfer specifically with respect to preservation\nof content ",
                    "Citation Text": "Toshevska, M., Gievska, S.: A Review of Text Style Transfer using Deep Learning.\nIEEE Transactions on Artificial Intelligence (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.15144",
                        "Citation Paper Title": "Title:A Review of Text Style Transfer using Deep Learning",
                        "Citation Paper Abstract": "Abstract:Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence.\nA systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field.",
                        "Citation Paper Authors": "Authors:Martina Toshevska, Sonja Gievska"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "between the transferred sentence and its source. Higher BLEU indicates higher n-\ngram overlap between the sentences, which is generally viewed as proxy for content\npreservation. We also compute Sentence BERT ",
                    "Citation Text": "Reimers, N., Gurevych, I.: Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks. In: Proc. EMNLP-IJCNLP, pp. 3980\u20133990 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.10084",
                        "Citation Paper Title": "Title:Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                        "Citation Paper Abstract": "Abstract:BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.\nIn this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                        "Citation Paper Authors": "Authors:Nils Reimers, Iryna Gurevych"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nWhile the Amazon Review data is originally intended for recommendation, it lends\nitself easily to our task. We have split the reviews into sentences and then used a pre-\ntrained transformer-based sentiment classifier ",
                    "Citation Text": "Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault,\nT., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY ., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush, A.M.:\nTransformers: State-of-the-art natural language processing. In: Proc. EMNLP, pp.\n38\u201345 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "use a\nvariational autoencoder and factor its latent representation into a style-independent andSentiment Transfer Using Polarity-Aware Denoising 3\nstylistic parts. Fu et al. ",
                    "Citation Text": "Fu, Z., Tan, X., Peng, N., Zhao, D., Yan, R.: Style Transfer in Text: Exploration\nand Evaluation. In: Proc. AAAI, pp. 663\u2013670 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.06861",
                        "Citation Paper Title": "Title:Style Transfer in Text: Exploration and Evaluation",
                        "Citation Paper Abstract": "Abstract:Style transfer is an important problem in natural language processing (NLP). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and principle evaluation metrics. In this paper, we propose to learn style transfer with non-parallel data. We explore two models to achieve this goal, and the key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. We also propose novel evaluation metrics which measure two aspects of style transfer: transfer strength and content preservation. We access our models and the evaluation metrics on two tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with higher style transfer strength and similar content preservation score comparing to auto-encoder.",
                        "Citation Paper Authors": "Authors:Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Sentiment Transfer A common approach to the sentiment transfer task is to separate\ncontent and style in a latent space, and then adjust the separated style. Hu et al. ",
                    "Citation Text": "Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., Xing, E.P.: Toward Controlled\nGeneration of Text. In: Proc. ICML, vol. 70, pp. 1587\u20131596 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00955",
                        "Citation Paper Title": "Title:Toward Controlled Generation of Text",
                        "Citation Paper Abstract": "Abstract:Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
                        "Citation Paper Authors": "Authors:Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.14171v3": {
            "Paper Title": "In-Context Probing: Toward Building Robust Classifiers via Probing Large\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.05782v2": {
            "Paper Title": "Aligning Language Models with Human Preferences via a Bayesian Approach",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "Developing human-centric systems, which ensure that human stakeholders benefit from system\noutcomes ",
                    "Citation Text": "Bhushan Kotnis, Kiril Gashteovski, Julia Gastinger, Giuseppe Serra, Francesco Alesiani, Timo Sztyler,\nAmmar Shaker, Na Gong, Carolin Lawrence, and Zhao Xu. Human-centric research for nlp: Towards a\ndefinition and guiding questions. arXiv preprint arXiv:2207.04447 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.04447",
                        "Citation Paper Title": "Title:Human-Centric Research for NLP: Towards a Definition and Guiding Questions",
                        "Citation Paper Abstract": "Abstract:With Human-Centric Research (HCR) we can steer research activities so that the research outcome is beneficial for human stakeholders, such as end users. But what exactly makes research human-centric? We address this question by providing a working definition and define how a research pipeline can be split into different stages in which human-centric components can be added. Additionally, we discuss existing NLP with HCR components and define a series of guiding questions, which can serve as starting points for researchers interested in exploring human-centric research approaches. We hope that this work would inspire researchers to refine the proposed definition and to pose other questions that might be meaningful for achieving HCR.",
                        "Citation Paper Authors": "Authors:Bhushan Kotnis, Kiril Gashteovski, Julia Gastinger, Giuseppe Serra, Francesco Alesiani, Timo Sztyler, Ammar Shaker, Na Gong, Carolin Lawrence, Zhao Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14646v1": {
            "Paper Title": "Collaborative Synthesis of Patient Records through Multi-Visit Health\n  State Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14609v1": {
            "Paper Title": "BLSTM-Based Confidence Estimation for End-to-End Speech Recognition",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ",\nwhich is a large scale lecture speech corpus. In the ESPnet CS J ASR\nrecipe ",
                    "Citation Text": "S. Watanabe, \u201cESPnet: End-to-end speech processing to olkit,\u201d\nhttps://github.com/espnet/espnet.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00015",
                        "Citation Paper Title": "Title:ESPnet: End-to-End Speech Processing Toolkit",
                        "Citation Paper Abstract": "Abstract:This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
                        "Citation Paper Authors": "Authors:Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, Tsubasa Ochiai"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ". We also used an LSTMLM\nwith shallow fusion fashion ",
                    "Citation Text": "T. Hori, S. Watanabe, Y . Zhang, and W. Chan, \u201cAdvances in\njoint CTC-attention based end-to-end speech recognition w ith\na deep CNN encoder and RNN-LM,\u201d in Proc. Interspeech ,\n2017, pp. 949\u2013953.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02737",
                        "Citation Paper Title": "Title:Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
                        "Citation Paper Abstract": "Abstract:We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR) model. We learn to listen and write characters with a joint Connectionist Temporal Classification (CTC) and attention-based encoder-decoder network. The encoder is a deep Convolutional Neural Network (CNN) based on the VGG network. The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder. During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model. We achieve a 5-10\\% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems.",
                        "Citation Paper Authors": "Authors:Takaaki Hori, Shinji Watanabe, Yu Zhang, William Chan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14590v1": {
            "Paper Title": "SIG: Speaker Identification in Literature via Prompt-Based Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14488v1": {
            "Paper Title": "Language Model is a Branch Predictor for Simultaneous Machine\n  Translation",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ", we adopted the same data\nprocessing approach as described in ",
                    "Citation Text": "Xutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon,\nand Jiatao Gu, \u201cMonotonic multihead attention,\u201d in\nICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.12406",
                        "Citation Paper Title": "Title:Monotonic Multihead Attention",
                        "Citation Paper Abstract": "Abstract:Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approaches for this task either apply a fixed policy on a state-of-the art Transformer model, or a learnable monotonic attention on a weaker recurrent neural network-based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which extends the monotonic attention mechanism to multihead attention. We also introduce two novel and interpretable approaches for latency control that are specifically designed for multiple attentions heads. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach. We also analyze how the latency controls affect the attention span and we motivate the introduction of our model by analyzing the effect of the number of decoder layers and heads on quality and latency.",
                        "Citation Paper Authors": "Authors:Xutai Ma, Juan Pino, James Cross, Liezl Puzon, Jiatao Gu"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". We conduct exper-\niments on following systems. 1) Wait-k ",
                    "Citation Text": "Mingbo Ma, Liang Huang, Hao Xiong, Renjie\nZheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,\nZhongjun He, Hairong Liu, Xing Li, Hua Wu, and\nHaifeng Wang, \u201cSTACL: Simultaneous Translation with\nImplicit Anticipation and Controllable Latency using\nPrefix-to-Prefix Framework,\u201d in ACL, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08398",
                        "Citation Paper Title": "Title:STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework",
                        "Citation Paper Abstract": "Abstract:Simultaneous translation, which translates sentences before they are finished, is useful in many scenarios but is notoriously difficult due to word-order differences. While the conventional seq-to-seq framework is only suitable for full-sentence translation, we propose a novel prefix-to-prefix framework for simultaneous translation that implicitly learns to anticipate in a single translation model. Within this framework, we present a very simple yet surprisingly effective wait-k policy trained to generate the target sentence concurrently with the source sentence, but always k words behind. Experiments show our strategy achieves low latency and reasonable quality (compared to full-sentence translation) on 4 directions: zh<->en and de<->en.",
                        "Citation Paper Authors": "Authors:Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, Haifeng Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10104v2": {
            "Paper Title": "ICD-LM: Configuring Vision-Language In-Context Demonstrations by\n  Language Modeling",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ": MS-COCO is widely used in IC, which is\ndivided into 118,287training, 5,000validation, and 5,000\ntest image-caption pairs. Notably, each training image is\nassociated with five distinct human-annotated captions.\nVQA V2 ",
                    "Citation Text": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the V in VQA matter: Ele-\nvating the role of image understanding in Visual Question\nAnswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2017. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00837",
                        "Citation Paper Title": "Title:Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
                        "Citation Paper Abstract": "Abstract:Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.\nWe propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at this http URL as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).\nWe further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.\nFinally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",
                        "Citation Paper Authors": "Authors:Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nRegarding the ordering of ICDs, researchers propose\nsome principles to measure the quality of ICD configura-\ntions, e.g., the Minimal Description Length ",
                    "Citation Text": "Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng\nKong. Self-adaptive in-context learning. arXiv preprint\narXiv:2212.10375 , 2022. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10375",
                        "Citation Paper Title": "Title:Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering",
                        "Citation Paper Abstract": "Abstract:Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: this https URL",
                        "Citation Paper Authors": "Authors:Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.11997v2": {
            "Paper Title": "Prompt-Based Editing for Text Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13545v2": {
            "Paper Title": "Developing Interactive Tourism Planning: A Dialogue Robot System Powered\n  by a Large Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17257v1": {
            "Paper Title": "Evolving Large Language Model Assistant with Long-Term Conditional\n  Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14327v1": {
            "Paper Title": "Parameter Efficient Tuning Allows Scalable Personalization of LLMs for\n  Text Entry: A Case Study on Abbreviation Expansion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17256v1": {
            "Paper Title": "From Bytes to Biases: Investigating the Cultural Self-Perception of\n  Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14302v1": {
            "Paper Title": "Exploiting Novel GPT-4 APIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14279v1": {
            "Paper Title": "Characterizing and Classifying Developer Forum Posts with their\n  Intentions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11462v2": {
            "Paper Title": "Cascade Speculative Drafting for Even Faster LLM Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.14859v3": {
            "Paper Title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for\n  Embodied Turn-Taking Prediction",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ".  The text \nembeddings  are 300-dimensional  features  obtained  by using  \nthe transcripts  by FastText\u2019s  Crawl ",
                    "Citation Text": "T. Mikolov,  E. Grave,  P. Bojanowski,  C. Puhrsch,  and \nA. Joulin.  Advances in pre -training distributed word  \nrepresentations.  In Proc. of the International Confer - \nence on Language  Resources  and Evaluation , pages  52\u2013 \n55, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09405",
                        "Citation Paper Title": "Title:Advances in Pre-Training Distributed Word Representations",
                        "Citation Paper Abstract": "Abstract:Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".  The \nvideo  embeddings  are 2048 -dimensional  visual  features  cor- \nresponding to the  last average  pooling layer  of R(2+1)D -101 \nmodel ",
                    "Citation Text": "D. Tran,  H. Wang,  L. Torresani,  J. Ray, Y. LeCun,  and \nM. Paluri.  A closer  look at spatiotemporal  convolutions  \nfor action recognition. In Proc. of CVPR , pages 6450 \u2013 \n6459,  2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11248",
                        "Citation Paper Title": "Title:A Closer Look at Spatiotemporal Convolutions for Action Recognition",
                        "Citation Paper Abstract": "Abstract:In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\" which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.",
                        "Citation Paper Authors": "Authors:Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14069v1": {
            "Paper Title": "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in\n  Speech-to-Speech Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.14367v2": {
            "Paper Title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and\n  Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14226v1": {
            "Paper Title": "Deep de Finetti: Recovering Topic Distributions from Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.11032v2": {
            "Paper Title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13951v1": {
            "Paper Title": "Typhoon: Thai Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ". Third, we evaluate the question-answering\nperformance on XQuAD (i.e., reading comprehension task) ",
                    "Citation Text": "Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of\nmonolingual representations. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4623\u20134637, Online, July 2020. Association for Computational\nLinguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11856",
                        "Citation Paper Title": "Title:On the Cross-lingual Transferability of Monolingual Representations",
                        "Citation Paper Abstract": "Abstract:State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
                        "Citation Paper Authors": "Authors:Mikel Artetxe, Sebastian Ruder, Dani Yogatama"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "(using Google API) into Thai and we filter\nout poorly translated examples such as coding instructions, resulting in a total of 68 examples. The\ntranslated MT-Bench is used to evaluate the multi-turn ability of the models.\n\u2022Sea-bench (Thai subset) ",
                    "Citation Text": "Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng,\nGuanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, et al. Seallms\u2013large language models for\nsoutheast asia. arXiv preprint arXiv:2312.00738 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2312.00738",
                        "Citation Paper Title": "Title:SeaLLMs -- Large Language Models for Southeast Asia",
                        "Citation Paper Abstract": "Abstract:Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.",
                        "Citation Paper Authors": "Authors:Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "with 7.5 billion parameters.\nWangChanGLM is adapted to Thai by fine-tuning XGLM using LoRA ",
                    "Citation Text": "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09685",
                        "Citation Paper Title": "Title:LoRA: Low-Rank Adaptation of Large Language Models",
                        "Citation Paper Abstract": "Abstract:An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL.",
                        "Citation Paper Authors": "Authors:Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "Training language models on monolingual data (as well as bilingual data where the target language\nand English data are mixed) has demonstrated gains over multilingual models in both predictive\nand generative modelling tasks [ 28,30,49,29,41,50]. For Thai, WangChangBERTa ",
                    "Citation Text": "Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, and Sarana Nutanong.\nWangchanberta: Pretraining transformer-based thai language models. arXiv preprint\narXiv:2101.09635 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.09635",
                        "Citation Paper Title": "Title:WangchanBERTa: Pretraining transformer-based Thai Language Models",
                        "Citation Paper Abstract": "Abstract:Transformer-based language models, more specifically BERT-based architectures have achieved state-of-the-art performance in many downstream tasks. However, for a relatively low-resource language such as Thai, the choices of models are limited to training a BERT-based model based on a much smaller dataset or finetuning multi-lingual models, both of which yield suboptimal downstream performance. Moreover, large-scale multi-lingual pretraining does not take into account language-specific features for Thai. To overcome these limitations, we pretrain a language model based on RoBERTa-base architecture on a large, deduplicated, cleaned training set (78GB in total size), curated from diverse domains of social media posts, news articles and other publicly available datasets. We apply text processing rules that are specific to Thai most importantly preserving spaces, which are important chunk and sentence boundaries in Thai before subword tokenization. We also experiment with word-level, syllable-level and SentencePiece tokenization with a smaller dataset to explore the effects on tokenization on downstream performance. Our model wangchanberta-base-att-spm-uncased trained on the 78.5GB dataset outperforms strong baselines (NBSVM, CRF and ULMFit) and multi-lingual models (XLMR and mBERT) on both sequence classification and token classification tasks in human-annotated, mono-lingual contexts.",
                        "Citation Paper Authors": "Authors:Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana Nutanong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13881v1": {
            "Paper Title": "Diversifying Knowledge Enhancement of Biomedical Language Models using\n  Adapter Modules and Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13876v1": {
            "Paper Title": "Capture the Flag: Uncovering Data Insights with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13871v1": {
            "Paper Title": "Evaluating Task-oriented Dialogue Systems: A Systematic Review of\n  Measures, Constructs and their Operationalisations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13816v1": {
            "Paper Title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking\n  in a Travel Agent Spoken Dialogue System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07661v2": {
            "Paper Title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". Fol-\nlowing previous work, we use the standard mIoU metric.\nApart from referring image segmentation, we also set up a\nnew baseline for zero-shot referring video segmentation\nonRef-DA VIS 2017 ",
                    "Citation Text": "Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video\nobject segmentation with language referring expressions. In\nComputer Vision\u2013ACCV 2018: 14th Asian Conference on\nComputer Vision, Perth, Australia, December 2\u20136, 2018, Re-\nvised Selected Papers, Part IV 14 , pages 123\u2013141. Springer,\n2019. 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08006",
                        "Citation Paper Title": "Title:Video Object Segmentation with Language Referring Expressions",
                        "Citation Paper Abstract": "Abstract:Most state-of-the-art semi-supervised video object segmentation methods rely on a pixel-accurate mask of a target object provided for the first frame of a video. However, obtaining a detailed segmentation mask is expensive and time-consuming. In this work we explore an alternative way of identifying a target object, namely by employing language referring expressions. Besides being a more practical and natural way of pointing out a target object, using language specifications can help to avoid drift as well as make the system more robust to complex dynamics and appearance variations. Leveraging recent advances of language grounding models designed for images, we propose an approach to extend them to video data, ensuring temporally coherent predictions. To evaluate our method we augment the popular video object segmentation benchmarks, DAVIS'16 and DAVIS'17 with language descriptions of target objects. We show that our language-supervised approach performs on par with the methods which have access to a pixel-level mask of the target object on DAVIS'16 and is competitive to methods using scribbles on the challenging DAVIS'17 dataset.",
                        "Citation Paper Authors": "Authors:Anna Khoreva, Anna Rohrbach, Bernt Schiele"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "25.95 - - 22.62 - - 23.41 - -\nzero-shot\nGL CLIP ",
                    "Citation Text": "Seonghoon Yu, Paul Hongsuck Seo, and Jeany Son. Zero-\nshot referring image segmentation with global-local context\nfeatures. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 19456\u2013\n19465, 2023. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.17811",
                        "Citation Paper Title": "Title:Zero-shot Referring Image Segmentation with Global-Local Context Features",
                        "Citation Paper Abstract": "Abstract:Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed Istance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed method outperforms several zero-shot baselines of the task and even the weakly supervised referring expression segmentation method with substantial margins. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Seonghoon Yu, Paul Hongsuck Seo, Jeany Son"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "dataset and the masks are paired with descriptive language\nannotations. In RefCOCO+, the use of location word in de-\nscriptions is prohibited, making the task more challenging.\nThere are two separate splits of the RefCOCOg dataset, one\nby UMD (U) ",
                    "Citation Text": "Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Mod-\neling context between objects for referring expression under-\nstanding. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11\u201314,\n2016, Proceedings, Part IV 14 , pages 792\u2013807. Springer,\n2016. 8, 13\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00525",
                        "Citation Paper Title": "Title:Modeling Context Between Objects for Referring Expression Understanding",
                        "Citation Paper Abstract": "Abstract:Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.",
                        "Citation Paper Authors": "Authors:Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ", and RefCOCOg [42, 46]\nfor the referring image segmentation task. Images usedModels RefCOCO RefCOCO+ RefCOCOg\nval testA testB val testA testB val test(U) val(G)\nweakly-supervised\nTSEG ",
                    "Citation Text": "Robin Strudel, Ivan Laptev, and Cordelia Schmid. Weakly-\nsupervised segmentation of referring expressions. arXiv\npreprint arXiv:2205.04725 , 2022. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.04725",
                        "Citation Paper Title": "Title:Weakly-supervised segmentation of referring expressions",
                        "Citation Paper Abstract": "Abstract:Visual grounding localizes regions (boxes or segments) in the image corresponding to given referring expressions. In this work we address image segmentation from referring expressions, a problem that has so far only been addressed in a fully-supervised setting. A fully-supervised setup, however, requires pixel-wise supervision and is hard to scale given the expense of manual annotation. We therefore introduce a new task of weakly-supervised image segmentation from referring expressions and propose Text grounded semantic SEGgmentation (TSEG) that learns segmentation masks directly from image-level referring expressions without pixel-level annotations. Our transformer-based method computes patch-text similarities and guides the classification objective during training with a new multi-label patch assignment mechanism. The resulting visual grounding model segments image regions corresponding to given natural language expressions. Our approach TSEG demonstrates promising results for weakly-supervised referring expression segmentation on the challenging PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.",
                        "Citation Paper Authors": "Authors:Robin Strudel, Ivan Laptev, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "for\npost-processing, and SAM is notinvolved for all experi-\nments in this section for fair comparison. Please refer to the\nsupplementary material for the implementation details.\nDatasets. Following [71, 76], we evaluate on Ref-\nCOCO ",
                    "Citation Text": "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14 , pages 69\u201385. Springer, 2016.\n8, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00272",
                        "Citation Paper Title": "Title:Modeling Context in Referring Expressions",
                        "Citation Paper Abstract": "Abstract:Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.",
                        "Citation Paper Authors": "Authors:Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L. Berg"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "sets the previous high-\nest record on COCO Object but also requires extensive data\nfor fine-tuning. Concretely, it first utilizes a ViT-based im-\nage encoder pre-trained with DINO ",
                    "Citation Text": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv \u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision , pages 9650\u20139660, 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.14294",
                        "Citation Paper Title": "Title:Emerging Properties in Self-Supervised Vision Transformers",
                        "Citation Paper Abstract": "Abstract:In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
                        "Citation Paper Authors": "Authors:Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "\u2713 \u2713 - CC12M 12M text 47.9 - 23.9\nCLIPpy ",
                    "Citation Text": "Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi,\nYinfei Yang, Alexander Toshev, and Jonathon Shlens. Per-\nceptual grouping in vision-language models. arXiv preprint\narXiv:2210.09996 , 2022. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.09996",
                        "Citation Paper Title": "Title:Perceptual Grouping in Contrastive Vision-Language Models",
                        "Citation Paper Abstract": "Abstract:Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robustness analyses. We find that the resulting model achieves state-of-the-art results in terms of unsupervised segmentation, and demonstrate that the learned representations are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models.",
                        "Citation Paper Authors": "Authors:Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, Jonathon Shlens"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "\u2713 \u2713 - IN-1K 1.3M self 40.8 20.2 20.4\nViewCo ",
                    "Citation Text": "Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guan-\ngrun Wang, Jianzhuang Liu, Xiaojun Chang, and Xiaodan\nLiang. Viewco: Discovering text-supervised segmentation\nmasks via multi-view semantic consistency. arXiv preprint\narXiv:2302.10307 , 2023. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.10307",
                        "Citation Paper Title": "Title:ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency",
                        "Citation Paper Abstract": "Abstract:Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-\\textbf{View} \\textbf{Co}nsistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders. The text-to-views consistency benefits the dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9\\%, 1.6\\%, and 2.4\\% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively.",
                        "Citation Paper Authors": "Authors:Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guangrun Wang, Jianzhuang Liu, Xiaojun Chang, Xiaodan Liang"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "\u00d7 \u2713 - CC12M+RedCaps 24M text 50.8 27.5 23.7\nSegCLIP ",
                    "Citation Text": "Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,\nand Tianrui Li. Segclip: Patch aggregation with learnable\ncenters for open-vocabulary semantic segmentation. In In-\nternational Conference on Machine Learning , pages 23033\u2013\n23044. PMLR, 2023. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.14813",
                        "Citation Paper Title": "Title:SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves comparable or superior segmentation accuracy on the PASCAL VOC 2012 (+0.3% mIoU), PASCAL Context (+2.3% mIoU), and COCO (+2.2% mIoU) compared with baselines. We release the code at this https URL.",
                        "Citation Paper Authors": "Authors:Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, Tianrui Li"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "\u2713 \u00d7 - HQITP-134M 134M text+self 52.2 32.0 -\nOVSegmenter ",
                    "Citation Text": "Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu\nQiao, and Weidi Xie. Learning open-vocabulary semantic\nsegmentation models from natural language supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 2935\u20132944, 2023. 2,\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.09121",
                        "Citation Paper Title": "Title:Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:In this paper, we consider the problem of open-vocabulary semantic segmentation (OVS), which aims to segment objects of arbitrary classes instead of pre-defined, closed-set categories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annotations. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention based binding module, and aligns the group tokens to the corresponding caption embedding. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given the group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consistent mask predictions between images that contain shared entities, which encourages the model to learn visual invariance. Third, we construct CC4M dataset for pre-training by filtering CC12M with frequently appeared entities, which significantly improves training efficiency. Fourth, we perform zero-shot transfer on three benchmark datasets, PASCAL VOC 2012, PASCAL Context, and COCO Object. Our model achieves superior segmentation results over the state-of-the-art method by using only 3\\% data (4M vs 134M) for pre-training. Code and pre-trained models will be released for future research.",
                        "Citation Paper Authors": "Authors:Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, Weidi Xie"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "\u2713 \u2713 - CC12M+YFCC 26M text+self 52.4 23.5 23.0\nMixReorg ",
                    "Citation Text": "Kaixin Cai, Pengzhen Ren, Yi Zhu, Hang Xu, Jianzhuang\nLiu, Changlin Li, Guangrun Wang, and Xiaodan Liang.\nMixreorg: Cross-modal mixed patch reorganization is a good\nmask learner for open-world semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 1196\u20131205, 2023. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.04829",
                        "Citation Paper Title": "Title:MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Recently, semantic segmentation models trained with image-level text supervision have shown promising results in challenging open-world scenarios. However, these models still face difficulties in learning fine-grained semantic alignment at the pixel level and predicting accurate object masks. To address this issue, we propose MixReorg, a novel and straightforward pre-training paradigm for semantic segmentation that enhances a model's ability to reorganize patches mixed across images, exploring both local visual relevance and global semantic coherence. Our approach involves generating fine-grained patch-text pairs data by mixing image patches while preserving the correspondence between patches and text. The model is then trained to minimize the segmentation loss of the mixed images and the two contrastive losses of the original and restored features. With MixReorg as a mask learner, conventional text-supervised semantic segmentation models can achieve highly generalizable pixel-semantic alignment ability, which is crucial for open-world segmentation. After training with large-scale image-text data, MixReorg models can be applied directly to segment visual objects of arbitrary categories, without the need for further fine-tuning. Our proposed framework demonstrates strong performance on popular zero-shot semantic segmentation benchmarks, outperforming GroupViT by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012, PASCAL Context, MS COCO, and ADE20K, respectively.",
                        "Citation Paper Authors": "Authors:Kaixin Cai, Pengzhen Ren, Yi Zhu, Hang Xu, Jianzhuang Liu, Changlin Li, Guangrun Wang, Xiaodan Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14215v1": {
            "Paper Title": "SimLM: Can Language Models Infer Parameters of Physical Systems?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11779v2": {
            "Paper Title": "Are you talking to ['xem'] or ['x', 'em']? On Tokenization and\n  Addressing Misgendering in LLMs with Pronoun Tokenization Parity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13766v1": {
            "Paper Title": "Exploiting Contextual Target Attributes for Target Sentiment\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13764v1": {
            "Paper Title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "UperNet IN-22K, BEiTv2 512 58.0\nProLab (Ours) UperNet IN-22K, BEiT 640 58.2\nProLab (Ours) UperNet IN-22K, BEiTv2 896 58.7\nSwinV2-G ",
                    "Citation Text": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\nSwin transformer v2: Scaling up capacity and resolution. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 12009\u201312019, 2022. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09883",
                        "Citation Paper Title": "Title:Swin Transformer V2: Scaling Up Capacity and Resolution",
                        "Citation Paper Abstract": "Abstract:Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": ".\nTraining details. The ViT backbone in our model is ini-\ntialized with the DeiT-Base weights ",
                    "Citation Text": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv \u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through atten-\ntion. In International conference on machine learning , pages\n10347\u201310357. PMLR, 2021. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.12877",
                        "Citation Paper Title": "Title:Training data-efficient image transformers & distillation through attention",
                        "Citation Paper Abstract": "Abstract:Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.\nIn this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.\nMore importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": ".\nThe output feature dimension is set to 384 or 768. All pre-\ntrained weights are imported from HuggineFace ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R \u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s\ntransformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771 , 2019. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "proposes a label unifica-\ntion approach to systematically resolve the label definition\nconflicts, while recent works also resort to language models\nto automate this process ",
                    "Citation Text": "Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin\nWang, and Fan Wang. Lmseg: Language-guided multi-dataset\nsegmentation. In International Conference on Learning Rep-\nresentations , 2023. 1, 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13495",
                        "Citation Paper Title": "Title:LMSeg: Language-guided Multi-dataset Segmentation",
                        "Citation Paper Abstract": "Abstract:It's a meaningful and attractive topic to build a general and inclusive segmentation model that can recognize more categories in various scenarios. A straightforward way is to combine the existing fragmented segmentation datasets and train a multi-dataset network. However, there are two major issues with multi-dataset segmentation: (1) the inconsistent taxonomy demands manual reconciliation to construct a unified taxonomy; (2) the inflexible one-hot common taxonomy causes time-consuming model retraining and defective supervision of unlabeled categories. In this paper, we investigate the multi-dataset segmentation and propose a scalable Language-guided Multi-dataset Segmentation framework, dubbed LMSeg, which supports both semantic and panoptic segmentation. Specifically, we introduce a pre-trained text encoder to map the category names to a text embedding space as a unified taxonomy, instead of using inflexible one-hot label. The model dynamically aligns the segment queries with the category embeddings. Instead of relabeling each dataset with the unified taxonomy, a category-guided decoding module is designed to dynamically guide predictions to each datasets taxonomy. Furthermore, we adopt a dataset-aware augmentation strategy that assigns each dataset a specific image augmentation pipeline, which can suit the properties of images from different datasets. Extensive experiments demonstrate that our method achieves significant improvements on four semantic and three panoptic segmentation datasets, and the ablation study evaluates the effectiveness of each component.",
                        "Citation Paper Authors": "Authors:Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Wang, Fan Wang"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", where the language modules are\ntrained to be visually aligned such that they are capable to\nmap the visual input with open vocabularies.\nRecent works such as ",
                    "Citation Text": "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-\nvocabulary object detection via vision and language knowl-\n9edge distillation. arXiv preprint arXiv:2104.13921 , 2021.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13921",
                        "Citation Paper Title": "Title:Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "2.1. Open-vocabulary recognition\nOpen-vocabulary recognition aims to address visual recogni-\ntion problems in an open world by extending the semantic\nspace to unlimited vocabularies. To address this problem,\na universal trend is to leverage pre-trained vision-languagemodels such as CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748\u20138763. PMLR, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.07919v2": {
            "Paper Title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified\n  Large-Scale Audio-Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07069v2": {
            "Paper Title": "Context Matters: Data-Efficient Augmentation of Large Language Models\n  for Scientific Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13694v1": {
            "Paper Title": "Data Transformation to Construct a Dataset for Generating\n  Entity-Relationship Model from Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.05203v2": {
            "Paper Title": "From Artificially Real to Real: Leveraging Pseudo Data from Large\n  Language Models for Low-Resource Molecule Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13671v1": {
            "Paper Title": "Text2Analysis: A Benchmark of Table Question Answering with Advanced\n  Data Analysis and Unclear Queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13655v1": {
            "Paper Title": "Compositional Zero-Shot Learning for Attribute-Based Object Reference in\n  Human-Robot Interaction",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Some recent\nworks use graph structure to leverage information transfer between seen to unseen pairs using graph\nconvolutional networks ",
                    "Citation Text": "M. Mancini, M. F. Naeem, Y . Xian, and Z. Akata. Learning graph embeddings for open\nworld compositional zero-shot learning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01017",
                        "Citation Paper Title": "Title:Learning Graph Embeddings for Open World Compositional Zero-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embeddings (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.",
                        "Citation Paper Authors": "Authors:Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "suggests encoding objects as vectors and states as linear operators that transform\nthese vectors. Similarly, a recent work ",
                    "Citation Text": "Y .-L. Li, Y . Xu, X. Mao, and C. Lu. Symmetry and group in attribute-object compositions.\n2020 ieee. In The IEEE Conference on Computer Vision and Pattern Recognition , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00587",
                        "Citation Paper Title": "Title:Symmetry and Group in Attribute-Object Compositions",
                        "Citation Paper Abstract": "Abstract:Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an end-to-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.10799v2": {
            "Paper Title": "Layer-wise Representation Fusion for Compositional Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.14034v2": {
            "Paper Title": "Confucius: Iterative Tool Learning from Introspection Feedback by\n  Easy-to-Difficult Curriculum",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13608v1": {
            "Paper Title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.17564v3": {
            "Paper Title": "BloombergGPT: A Large Language Model for Finance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13594v1": {
            "Paper Title": "Towards More Faithful Natural Language Explanation Using Multi-Level\n  Contrastive Learning in VQA",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13585v1": {
            "Paper Title": "Speech Translation with Large Language Models: An Industrial Practice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14203v1": {
            "Paper Title": "Shai: A large language model for asset management",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14202v1": {
            "Paper Title": "Illuminating the Black Box: A Psychometric Investigation into the\n  Multifaceted Nature of Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12655v2": {
            "Paper Title": "Can Transformers Learn Sequential Function Classes In Context?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.16502v3": {
            "Paper Title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning\n  Benchmark for Expert AGI",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "to mini-\nmize the training sample complexity. Later on, CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748\u20138763. PMLR, 2021. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "(inter alia) have been proposed to\ntrain visual representation using ViT ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "are among the earliest work\nto train universal vision-language models to tackle many\nmultimodal tasks. This work relies on pre-trained visual\nrepresentations like Faster RCNN features ",
                    "Citation Text": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems , 28, 2015. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01497",
                        "Citation Paper Title": "Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.10045v2": {
            "Paper Title": "An Empirical Study of CLIP for Text-based Person Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13558v1": {
            "Paper Title": "The Truth is in There: Improving Reasoning in Language Models with\n  Layer-Selective Rank Reduction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13547v1": {
            "Paper Title": "How to Prune Your Language Model: Recovering Accuracy on the \"Sparsity\n  May Cry'' Benchmark",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "investigate\nmore than 50 different hyper-parameter settings, or they make use of carefully crafted schedulers\nforeachsetupindependentlywhichmayincludewarm-upphaseswithandwithoutrewinds[2,5].\nThis may lead to high specialization to the target task/model, which is undesirable in practice and\nmakesithardtodistinguishbenefitsfromthepruningtechniqueitself. Weproposetosimply repli-\ncatethestandarddensefine-tuningschedule ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL) , 2019.\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12464v2": {
            "Paper Title": "Towards Better Serialization of Tabular Data for Few-shot Classification\n  with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14197v1": {
            "Paper Title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": ". Recent studies show that LLMs\naccumulate knowledge in the pre-training stage ",
                    "Citation Text": "C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat,\nP. Yu, L. Yu et al. , \u201cLima: Less is more for alignment,\u201d arXiv preprint\narXiv:2305.11206 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.11206",
                        "Citation Paper Title": "Title:LIMA: Less Is More for Alignment",
                        "Citation Paper Abstract": "Abstract:Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
                        "Citation Paper Authors": "Authors:Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "from Google, which is used to provide service for Bard12, and\nClaude13from Anthropic, which proposes to align LLMs with\nConstitutional AI ",
                    "Citation Text": "Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,\nA. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et al. , \u201cConstitutional\nai: Harmlessness from ai feedback,\u201d arXiv preprint arXiv:2212.08073 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.08073",
                        "Citation Paper Title": "Title:Constitutional AI: Harmlessness from AI Feedback",
                        "Citation Paper Abstract": "Abstract:As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",
                        "Citation Paper Authors": "Authors:Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "for external content and user\ninstructions, respectively.\nThe summarization task aims to generate concise sum-\nmaries of extensive texts for text reader applications. The\n1,000 articles from the XSum dataset ",
                    "Citation Text": "S. Narayan, S. B. Cohen, and M. Lapata, \u201cDon\u2019t give me the details,\njust the summary! topic-aware convolutional neural networks for\nextreme summarization,\u201d in EMNLP , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.08745",
                        "Citation Paper Title": "Title:Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
                        "Citation Paper Abstract": "Abstract:We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question \"What is the article about?\". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
                        "Citation Paper Authors": "Authors:Shashi Narayan, Shay B. Cohen, Mirella Lapata"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "deep learning models with a large number of\nparameters, designed for natural language processing (NLP)\ntasks. They have recently achieved remarkable performance\nin various NLP tasks, such as logic reasoning ",
                    "Citation Text": "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\nhery, and D. Zhou, \u201cSelf-consistency improves chain of thought\nreasoning in language models,\u201d arXiv preprint arXiv:2203.11171 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.11171",
                        "Citation Paper Title": "Title:Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                        "Citation Paper Abstract": "Abstract:Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
                        "Citation Paper Authors": "Authors:Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". LLM-specific\nattacks at the inference stage can be broadly classified into\nthree categories: jailbreak attacks, prompt leakage attacks,\nand prompt injection attacks.\nJailbreak attacks ",
                    "Citation Text": "D. Kang, X. Li, I. Stoica, C. Guestrin, M. Zaharia, and T. Hashimoto,\n\u201cExploiting programmatic behavior of llms: Dual-use through standard\nsecurity attacks,\u201d arXiv preprint arXiv:2302.05733 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.05733",
                        "Citation Paper Title": "Title:Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
                        "Citation Paper Abstract": "Abstract:Recent advances in instruction-following large language models (LLMs) have led to dramatic improvements in a range of NLP tasks. Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models. Dual-use is difficult to prevent as instruction-following capabilities now enable standard attacks from computer security. The capabilities of these instruction-following LLMs provide strong economic incentives for dual-use by malicious actors. In particular, we show that instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors. Our analysis shows that this content can be generated economically and at cost likely lower than with human effort alone. Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks, and addressing these attacks may require new approaches to mitigations.",
                        "Citation Paper Authors": "Authors:Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "is an LLM-based planner that assembles\ndifferent tools (e.g., off-the-shelf vision models, web search\nengines, Python functions, and heuristic-based modules).\nTaskmatrix.AI ",
                    "Citation Text": "Y . Liang, C. Wu, T. Song, W. Wu, Y . Xia, Y . Liu, Y . Ou, S. Lu, L. Ji,\nS. Mao et al. , \u201cTaskmatrix. ai: Completing tasks by connecting foun-\ndation models with millions of apis,\u201d arXiv preprint arXiv:2303.16434 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16434",
                        "Citation Paper Title": "Title:TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs",
                        "Citation Paper Abstract": "Abstract:Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce this http URL as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, this http URL focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.",
                        "Citation Paper Authors": "Authors:Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, Nan Duan"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "enables LLMs to connect with\nvarious models in the AI community (e.g., Huggingface).\nChameleon ",
                    "Citation Text": "P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y . N. Wu, S.-C.\nZhu, and J. Gao, \u201cChameleon: Plug-and-play compositional reasoning\nwith large language models,\u201d arXiv preprint arXiv:2304.09842 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.09842",
                        "Citation Paper Title": "Title:Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at this https URL.",
                        "Citation Paper Authors": "Authors:Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ". One of the most popular open-sourced LLMs is\nLLAMA from Meta ",
                    "Citation Text": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , \u201cLlama\n2: Open foundation and fine-tuned chat models,\u201d arXiv preprint\narXiv:2307.09288 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.09288",
                        "Citation Paper Title": "Title:Llama 2: Open Foundation and Fine-Tuned Chat Models",
                        "Citation Paper Abstract": "Abstract:In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "2.1. Large Language Models\nLarge language models (LLMs) are transformer-\nbased ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nNIPS , vol. 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.06762v3": {
            "Paper Title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A\n  Comprehensive Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13495v1": {
            "Paper Title": "Decoupling Representation and Knowledge for Few-Shot Intent\n  Classification and Slot Filling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.18260v3": {
            "Paper Title": "Consensus, dissensus and synergy between clinicians and specialist\n  foundation models in radiology report generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11326v3": {
            "Paper Title": "Towards Faithful Model Explanation in NLP: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.05964v2": {
            "Paper Title": "ConSequence: Synthesizing Logically Constrained Sequences for Electronic\n  Health Record Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13423v1": {
            "Paper Title": "VADIS -- a VAriable Detection, Interlinking and Summarization system",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.10512v2": {
            "Paper Title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.15043v2": {
            "Paper Title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13382v1": {
            "Paper Title": "DSPy Assertions: Computational Constraints for Self-Refining Language\n  Model Pipelines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13264v1": {
            "Paper Title": "dIR -- Discrete Information Retrieval: Conversational Search over\n  Unstructured (and Structured) Data with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12037v2": {
            "Paper Title": "Founder-GPT: Self-play to evaluate the Founder-Idea fit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13219v1": {
            "Paper Title": "Interactive Visual Task Learning for Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13211v1": {
            "Paper Title": "DSFormer: Effective Compression of Text-Transformers by Dense-Sparse\n  Weight Factorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13208v1": {
            "Paper Title": "LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent\n  Sentence Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.16307v3": {
            "Paper Title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation\n  Models for all 22 Scheduled Indian Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13193v1": {
            "Paper Title": "HCDIR: End-to-end Hate Context Detection, and Intensity Reduction model\n  for online comments",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "tokenization and dynamic embeddings for improved\ncontextualization. Our study employs three types of pre-trained transformer models:\n\u2022 Multilingual model: Google-MuRIL ",
                    "Citation Text": "Khanuja, S., Bansal, D., Mehtani, S., Khosla, S., Dey , A., Gopalan, B., Margam,\nD.K., Aggarwal, P ., Nagipogu, R.T., Dave, S., Gupta, S., Gali, S.C.B., Sub-\nramanian, V., T alukdar, P .P .: Muril: Multilingual representations for indian\nlanguages. CoRR abs/2103.10730 (2021) 2103.10730",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10730",
                        "Citation Paper Title": "Title:MuRIL: Multilingual Representations for Indian Languages",
                        "Citation Paper Abstract": "Abstract:India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011). India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today's state-of-the-art multilingual systems perform suboptimally on Indian (IN) languages. This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn't help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both translated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native to Latin script) test sets of the chosen datasets and demonstrate the efficacy of MuRIL in handling transliterated data.",
                        "Citation Paper Authors": "Authors:Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, Subhash Chandra Bose Gali, Vish Subramanian, Partha Talukdar"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". It visualizes the gradient for the\ninput image to capture pixels detected by the neurons.\nSundararajan et al. ",
                    "Citation Text": "Sundararajan, M., T aly , A., Y an, Q.: Axiomatic attribution for deep networks.\nCoRR abs/1703.01365 (2017) 1703.01365",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01365",
                        "Citation Paper Title": "Title:Axiomatic Attribution for Deep Networks",
                        "Citation Paper Abstract": "Abstract:We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
                        "Citation Paper Authors": "Authors:Mukund Sundararajan, Ankur Taly, Qiqi Yan"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": ",\nwhich can explain any classifier\u2019s predictions in an interpretable and faithful manner.\nAnother approach is SHAP ",
                    "Citation Text": "Lundberg, S.M., Lee, S.: A unified approach to interpreting model predictions.\nCoRR abs/1705.07874 (2017) 1705.07874",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07874",
                        "Citation Paper Title": "Title:A Unified Approach to Interpreting Model Predictions",
                        "Citation Paper Abstract": "Abstract:Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
                        "Citation Paper Authors": "Authors:Scott Lundberg, Su-In Lee"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "have been introduced to ex-\nplain the black-box nature of deep-learning models. One such technique is LIME ",
                    "Citation Text": "Ribeiro, M.T., Singh, S., Guestrin, C.: \u201dwhy should I trust you?\u201d: Explaining\nthe predictions of any classifier. CoRR abs/1602.04938 (2016) 1602.04938",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "uses\nhierarchically organized prototypes to classify the objects at every level in a prede-\nfined taxonomy , giving distinct explanations at each level in a predefined taxonomy .\nAttention-based models are first introduced by Bahdanau et al. ",
                    "Citation Text": "Bahdanau, D., Cho, K., Bengio, Y.: Neural Machine T ranslation by Jointly\nLearning to Align and T ranslate. arXiv (2014). https://doi.org/10.48550/\nARXIV.1409.0473 .https://arxiv.org/abs/1409.0473",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "use the Risk scores to detect the \u2018at-risk\u2019 patents in\na real-time environment. Anphi et al. ",
                    "Citation Text": "Nguyen, A., Mart\u00ednez, M.R.: Mononet: T owards interpretable models by learning\nmonotonic features. CoRR abs/1909.13611 (2019) 1909.13611",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13611",
                        "Citation Paper Title": "Title:MonoNet: Towards Interpretable Models by Learning Monotonic Features",
                        "Citation Paper Abstract": "Abstract:Being able to interpret, or explain, the predictions made by a machine learning model is of fundamental importance. This is especially true when there is interest in deploying data-driven models to make high-stakes decisions, e.g. in healthcare. While recent years have seen an increasing interest in interpretable machine learning research, this field is currently lacking an agreed-upon definition of interpretability, and some researchers have called for a more active conversation towards a rigorous approach to interpretability. Joining this conversation, we claim in this paper that the difficulty of interpreting a complex model stems from the existing interactions among features. We argue that by enforcing monotonicity between features and outputs, we are able to reason about the effect of a single feature on an output independently from other features, and consequently better understand the model. We show how to structurally introduce this constraint in deep learning models by adding new simple layers. We validate our model on benchmark datasets, and compare our results with previously proposed interpretable models.",
                        "Citation Paper Authors": "Authors:An-phi Nguyen, Mar\u00eda Rodr\u00edguez Mart\u00ednez"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", the best submission was achieved the highest Macro\nF1 fine-tuning Multilingual-BER T. In the paper ",
                    "Citation Text": "Bhardwaj, M., Akhtar, M.S., Ekbal, A., Das, A., Chakraborty , T.: Hostility\ndetection dataset in hindi. CoRR abs/2011.03588 (2020) 2011.03588",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03588",
                        "Citation Paper Title": "Title:Hostility Detection Dataset in Hindi",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a novel hostility detection dataset in Hindi language. We collect and manually annotate ~8200 online posts. The annotated dataset covers four hostility dimensions: fake news, hate speech, offensive, and defamation posts, along with a non-hostile label. The hostile posts are also considered for multi-label tags due to a significant overlap among the hostile classes. We release this dataset as part of the CONSTRAINT-2021 shared task on hostile post detection.",
                        "Citation Paper Authors": "Authors:Mohit Bhardwaj, Md Shad Akhtar, Asif Ekbal, Amitava Das, Tanmoy Chakraborty"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "use LSTM and CNN\nwith and without word embedding to classify abusive comments. Bashar et al. ",
                    "Citation Text": "Bashar, M.A., Nayak, R.: Qutnocturnal@hasoc\u201919: CNN for hate speech and\noffensive content identification in hindi language. CoRR abs/2008.12448 (2020)\n2008.12448",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.12448",
                        "Citation Paper Title": "Title:QutNocturnal@HASOC'19: CNN for Hate Speech and Offensive Content Identification in Hindi Language",
                        "Citation Paper Abstract": "Abstract:We describe our top-team solution to Task 1 for Hindi in the HASOC contest organised by FIRE 2019. The task is to identify hate speech and offensive language in Hindi. More specifically, it is a binary classification problem where a system is required to classify tweets into two classes: (a) \\emph{Hate and Offensive (HOF)} and (b) \\emph{Not Hate or Offensive (NOT)}. In contrast to the popular idea of pretraining word vectors (a.k.a. word embedding) with a large corpus from a general domain such as Wikipedia, we used a relatively small collection of relevant tweets (i.e. random and sarcasm tweets in Hindi and Hinglish) for pretraining. We trained a Convolutional Neural Network (CNN) on top of the pretrained word vectors. This approach allowed us to be ranked first for this task out of all teams. Our approach could easily be adapted to other applications where the goal is to predict class of a text when the provided context is limited.",
                        "Citation Paper Authors": "Authors:Md Abul Bashar, Richi Nayak"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "use RNN to classify hate\nspeech in the text. The Disadvantage of RNN is that the gradient descent problem\ndoes not improve accuracy after a certain time for the longer texts. Das et al. used\nlong short-term memory networks (LSTM) ",
                    "Citation Text": "Das, A.K., Asif, A.A., Paul, A., Hossain, M.N.: Bangla hate speech detection on\nsocial media using attention-based recurrent neural network. Journal of Intelli-\ngent Systems 30 (1), 578\u2013591 (2021) https://doi.org/10.1515/jisys-2020-0060",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.16775",
                        "Citation Paper Title": "Title:Bangla hate speech detection on social media using attention-based recurrent neural network",
                        "Citation Paper Abstract": "Abstract:Hate speech has spread more rapidly through the daily use of technology and, most notably, by sharing your opinions or feelings on social media in a negative aspect. Although numerous works have been carried out in detecting hate speeches in English, German, and other languages, very few works have been carried out in the context of the Bengali language. In contrast, millions of people communicate on social media in Bengali. The few existing works that have been carried out need improvements in both accuracy and interpretability. This article proposed encoder decoder based machine learning model, a popular tool in NLP, to classify user's Bengali comments on Facebook pages. A dataset of 7,425 Bengali comments, consisting of seven distinct categories of hate speeches, was used to train and evaluate our model. For extracting and encoding local features from the comments, 1D convolutional layers were used. Finally, the attention mechanism, LSTM, and GRU based decoders have been used for predicting hate speech categories. Among the three encoder decoder algorithms, the attention-based decoder obtained the best accuracy (77%).",
                        "Citation Paper Authors": "Authors:Amit Kumar Das, Abdullah Al Asif, Anik Paul, Md. Nur Hossain"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "can determine whether a user\u2019s comment contains abusive or appropriate lan-\nguage and guess the message\u2019s main word using F astT ext ",
                    "Citation Text": "Badjatiya, P ., Gupta, S., Gupta, M., V arma, V.: Deep learning for hate\nspeech detection in tweets. In: Proceedings of the 26th International Confer-\nence on W orld Wide W eb Companion. WWW \u201917 Companion, pp. 759\u2013760.\nInternational W orld Wide W eb Conferences Steering Committee, Republic and\nCanton of Geneva, CHE (2017). https://doi.org/10.1145/3041021.3054223 .\nhttps://doi.org/10.1145/3041021.3054223",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00188",
                        "Citation Paper Title": "Title:Deep Learning for Hate Speech Detection in Tweets",
                        "Citation Paper Abstract": "Abstract:Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by ~18 F1 points.",
                        "Citation Paper Authors": "Authors:Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, Vasudeva Varma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11517v2": {
            "Paper Title": "Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based\n  Classification and Mode-Based Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13119v1": {
            "Paper Title": "Prometheus: Infrastructure Security Posture Analysis with AI-generated\n  Attack Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13096v1": {
            "Paper Title": "In Generative AI we Trust: Can Chatbots Effectively Verify Political\n  Information?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13040v1": {
            "Paper Title": "Retrieval-augmented Multilingual Knowledge Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13026v1": {
            "Paper Title": "FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous\n  Self-Supervised Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13010v1": {
            "Paper Title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and\n  Optimisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.11662v3": {
            "Paper Title": "Separating form and meaning: Using self-consistency to quantify task\n  understanding across multiple senses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12989v1": {
            "Paper Title": "Benchmarking and Analyzing In-context Learning, Fine-tuning and\n  Supervised Learning for Biomedical Knowledge Curation: a focused study on\n  chemical entities of biological interest",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "Lin Xu, Qixian Zhou, Ke Gong, Xiaodan Liang, Jianheng Tang, and Liang Lin. End-to-end knowledge-routed relational dialogue system for automatic diagnosis.\nInProceedings of the AAAI conference on artificial intelligence , volume 33, pages 7346\u20137353, 2019. ",
                    "Citation Text": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-End Open-Domain Question Answering with. In\nProceedings of the 2019 Conference of the North , pages 72\u201377, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01718",
                        "Citation Paper Title": "Title:End-to-End Open-Domain Question Answering with BERTserini",
                        "Citation Paper Abstract": "Abstract:We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",
                        "Citation Paper Authors": "Authors:Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "Bin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, and C-C Jay Kuo. Evaluating word embedding models: Methods and experimental results. APSIPA\ntransactions on signal and information processing , 8:e19, 2019. ",
                    "Citation Text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al.\nTransformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system\ndemonstrations , pages 38\u201345, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.03771",
                        "Citation Paper Title": "Title:HuggingFace's Transformers: State-of-the-art Natural Language Processing",
                        "Citation Paper Abstract": "Abstract:Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in neural information processing systems , 30, 2017. ",
                    "Citation Text": "Bin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, and C-C Jay Kuo. Evaluating word embedding models: Methods and experimental results. APSIPA\ntransactions on signal and information processing , 8:e19, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.09785",
                        "Citation Paper Title": "Title:Evaluating Word Embedding Models: Methods and Experimental Results",
                        "Citation Paper Abstract": "Abstract:Extensive evaluation on a large number of word embedding models for language processing applications is conducted in this work. First, we introduce popular word embedding models and discuss desired properties of word models and evaluation methods (or evaluators). Then, we categorize evaluators into intrinsic and extrinsic two types. Intrinsic evaluators test the quality of a representation independent of specific natural language processing tasks while extrinsic evaluators use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. We report experimental results of intrinsic and extrinsic evaluators on six word embedding models. It is shown that different evaluators focus on different aspects of word models, and some are more correlated with natural language processing tasks. Finally, we adopt correlation analysis to study performance consistency of extrinsic and intrinsic evalutors.",
                        "Citation Paper Authors": "Authors:Bin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, C.-C. Jay Kuo"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in neural information processing systems , 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "OpenAI. Chatgpt: Get instant answers, find inspiration, learn something new, 2022. ",
                    "Citation Text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\nTraining language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730\u201327744, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02155",
                        "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                        "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "Yuqing Mao and Kin Wah Fung. Use of word and graph embedding to measure semantic relatedness between unified medical language system concepts. Journal\nof the American Medical Informatics Association , 27(10):1538\u20131546, 2020. ",
                    "Citation Text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781 [cs] , September\n2013.\n17Benchmarking and Analyzing In-context Learning, Fine-tuning and Supervised Learning for Biomedical Knowledge\nCuration: a focused study on chemical entities of biological interest",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12436v2": {
            "Paper Title": "A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning\ndeep features for scene recognition using places database. NeurIPS , 2014. ",
                    "Citation Text": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 , 2023.\n128",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.10592",
                        "Citation Paper Title": "Title:MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
                        "Citation Paper Abstract": "Abstract:The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg.\nMattnet: Modular attention network for referring expression comprehension. In CVPR , 2018. ",
                    "Citation Text": "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16199",
                        "Citation Paper Title": "Title:LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
                        "Citation Paper Abstract": "Abstract:We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey\non multimodal large language models. arXiv preprint arXiv:2306.13549 , 2023. ",
                    "Citation Text": "Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg.\nMattnet: Modular attention network for referring expression comprehension. In CVPR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08186",
                        "Citation Paper Title": "Title:MAttNet: Modular Attention Network for Referring Expression Comprehension",
                        "Citation Paper Abstract": "Abstract:In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo and code are provided.",
                        "Citation Paper Authors": "Authors:Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L.Berg"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178 , 2023. ",
                    "Citation Text": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey\non multimodal large language models. arXiv preprint arXiv:2306.13549 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.13549",
                        "Citation Paper Title": "Title:A Survey on Multimodal Large Language Models",
                        "Citation Paper Abstract": "Abstract:Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421 , 2023. ",
                    "Citation Text": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.14178",
                        "Citation Paper Title": "Title:mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at this https URL. The online demo is available at this https URL.",
                        "Citation Paper Authors": "Authors:Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection\nbenchmark. In CVPR , 2016. ",
                    "Citation Text": "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.17421",
                        "Citation Paper Title": "Title:The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
                        "Citation Paper Abstract": "Abstract:Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: this https URL",
                        "Citation Paper Authors": "Authors:Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a\nlarge-scale benchmark for instance-level recognition and retrieval. In CVPR , 2020. ",
                    "Citation Text": "Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any\nmultimodal llm. arXiv preprint arXiv:2309.05519 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.05519",
                        "Citation Paper Title": "Title:NExT-GPT: Any-to-Any Multimodal LLM",
                        "Citation Paper Abstract": "Abstract:While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nNeurIPS , 2022.\n127 ",
                    "Citation Text": "Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a\nlarge-scale benchmark for instance-level recognition and retrieval. In CVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.01804",
                        "Citation Paper Title": "Title:Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval",
                        "Citation Paper Abstract": "Abstract:While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Tobias Weyand, Andre Araujo, Bingyi Cao, Jack Sim"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR , 2022. ",
                    "Citation Text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nNeurIPS , 2022.\n127",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object\ntracking and segmentation: A unifying approach. In CVPR , 2019. ",
                    "Citation Text": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01652",
                        "Citation Paper Title": "Title:Finetuned Language Models Are Zero-Shot Learners",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                        "Citation Paper Authors": "Authors:Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. NeurIPS , 2021. ",
                    "Citation Text": "Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object\ntracking and segmentation: A unifying approach. In CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05050",
                        "Citation Paper Title": "Title:Fast Online Object Tracking and Segmentation: A Unifying Approach",
                        "Citation Paper Abstract": "Abstract:In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is this http URL.",
                        "Citation Paper Authors": "Authors:Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, and Ran He. Pareidolia\nface reenactment. In CVPR , 2021. ",
                    "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training\nenables zero-shot task generalization. In ICLR , 2021. ",
                    "Citation Text": "Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, and Ran He. Pareidolia\nface reenactment. In CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03061",
                        "Citation Paper Title": "Title:Everything's Talkin': Pareidolia Face Reenactment",
                        "Citation Paper Abstract": "Abstract:We present a new application direction named Pareidolia Face Reenactment, which is defined as animating a static illusory face to move in tandem with a human face in the video. For the large differences between pareidolia face reenactment and traditional human face reenactment, two main challenges are introduced, i.e., shape variance and texture variance. In this work, we propose a novel Parametric Unsupervised Reenactment Algorithm to tackle these two challenges. Specifically, we propose to decompose the reenactment into three catenate processes: shape modeling, motion transfer and texture synthesis. With the decomposition, we introduce three crucial components, i.e., Parametric Shape Modeling, Expansionary Motion Transfer and Unsupervised Texture Synthesizer, to overcome the problems brought by the remarkably variances on pareidolia faces. Extensive experiments show the superior performance of our method both qualitatively and quantitatively. Code, model and data are available on our project page.",
                        "Citation Paper Authors": "Authors:Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, Ran He"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\nreal-time object detection. In CVPR , 2016. ",
                    "Citation Text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training\nenables zero-shot task generalization. In ICLR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.08207",
                        "Citation Paper Title": "Title:Multitask Prompted Training Enables Zero-Shot Task Generalization",
                        "Citation Paper Abstract": "Abstract:Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at this https URL and all prompts are available at this https URL.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid\nSigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR , 2023. ",
                    "Citation Text": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\nreal-time object detection. In CVPR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02640",
                        "Citation Paper Title": "Title:You Only Look Once: Unified, Real-Time Object Detection",
                        "Citation Paper Abstract": "Abstract:We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\nOur unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
                        "Citation Paper Authors": "Authors:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana\nLazebnik. Phrase localization and visual relationship detection with comprehensive image-\nlanguage cues. In ICCV , 2017. ",
                    "Citation Text": "Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid\nSigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.13319",
                        "Citation Paper Title": "Title:Make-A-Story: Visual Memory Conditioned Consistent Story Generation",
                        "Citation Paper Abstract": "Abstract:There has been a recent explosion of impressive generative models that can produce high quality images (or videos) conditioned on text descriptions. However, all such approaches rely on conditional sentences that contain unambiguous descriptions of scenes and main actors in them. Therefore employing such models for more complex task of story visualization, where naturally references and co-references exist, and one requires to reason about when to maintain consistency of actors and backgrounds across frames/scenes, and when not to, based on story progression, remains a challenge. In this work, we address the aforementioned challenges and propose a novel autoregressive diffusion-based framework with a visual memory module that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft attention over the memories enables effective reference resolution and learns to maintain scene and actor consistency when needed. To validate the effectiveness of our approach, we extend the MUGEN dataset and introduce additional characters, backgrounds and referencing in multi-sentence storylines. Our experiments for story generation on the MUGEN, the PororoSV and the FlintstonesSV dataset show that our method not only outperforms prior state-of-the-art in generating frames with high visual quality, which are consistent with the story, but also models appropriate correspondences between the characters and the background.",
                        "Citation Paper Authors": "Authors:Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, Leonid Sigal"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. NeurIPS , 2022. ",
                    "Citation Text": "Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana\nLazebnik. Phrase localization and visual relationship detection with comprehensive image-\nlanguage cues. In ICCV , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.06641",
                        "Citation Paper Title": "Title:Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues",
                        "Citation Paper Abstract": "Abstract:This paper presents a framework for localization or grounding of phrases in images using a large collection of linguistic and visual cues. We model the appearance, size, and position of entity bounding boxes, adjectives that contain attribute information, and spatial relationships between pairs of entities connected by verbs or prepositions. Special attention is given to relationships between people and clothing or body part mentions, as they are useful for distinguishing individuals. We automatically learn weights for combining these cues and at test time, perform joint inference over all phrases in a caption. The resulting system produces state of the art performance on phrase localization on the Flickr30k Entities dataset and visual relationship detection on the Stanford VRD dataset.",
                        "Citation Paper Authors": "Authors:Bryan A. Plummer, Arun Mallya, Christopher M. Cervantes, Julia Hockenmaier, Svetlana Lazebnik"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "Hui Mao, Ming Cheung, and James She. Deepart: Learning joint representations of visual arts.\nInACM MM , 2017. ",
                    "Citation Text": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In ACL, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08773",
                        "Citation Paper Title": "Title:Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
                        "Citation Paper Abstract": "Abstract:Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
                        "Citation Paper Authors": "Authors:Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2309.01431v2": {
            "Paper Title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.12976v2": {
            "Paper Title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.13198v2": {
            "Paper Title": "Journey to the Center of the Knowledge Neurons: Discoveries of\n  Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12881v1": {
            "Paper Title": "Big Tech influence over AI research revisited: memetic analysis of\n  attribution of ideas to affiliation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11276v3": {
            "Paper Title": "Compositional Generalization for Multi-label Text Classification: A\n  Data-Augmentation Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2304.01246v3": {
            "Paper Title": "Safety Analysis in the Era of Large Language Models: A Case Study of\n  STPA using ChatGPT",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ".\nThis finding has spurred the development of LLMs such as GPT-3 ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in\nNeural Information Processing Systems , volume 33, pages 1877\u20131901. Curran Associates, Inc.,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12853v1": {
            "Paper Title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks\n  for Chinese Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12832v1": {
            "Paper Title": "Turning Dust into Gold: Distilling Complex Reasoning Capabilities from\n  LLMs by Leveraging Negative Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12806v1": {
            "Paper Title": "MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.14747v3": {
            "Paper Title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12783v1": {
            "Paper Title": "Stable Distillation: Regularizing Continued Pre-training for\n  Low-Resource Automatic Speech Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12764v1": {
            "Paper Title": "Lattice Rescoring Based on Large Ensemble of Complementary Neural\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12754v1": {
            "Paper Title": "Spectral Prompt Tuning:Unveiling Unseen Classes for Zero-Shot Semantic\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12747v1": {
            "Paper Title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12746v1": {
            "Paper Title": "ChatFDA: Medical Records Risk Assessment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12430v2": {
            "Paper Title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12740v1": {
            "Paper Title": "Fine-tuning Large Language Models for Adaptive Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12736v1": {
            "Paper Title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.08742v4": {
            "Paper Title": "PMET: Precise Model Editing in a Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11681v2": {
            "Paper Title": "Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows",
            "Sentences": [
                {
                    "Sentence ID": 116,
                    "Sentence": ". Another implements\nthree strategies from crowdsourcing using LLMs for data process-\ning tasks ",
                    "Citation Text": "Aditya G Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, and Yujie\nWang. 2023. Revisiting Prompt Engineering via Declarative Crowdsourcing.\narXiv preprint arXiv:2308.03854 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.03854",
                        "Citation Paper Title": "Title:Revisiting Prompt Engineering via Declarative Crowdsourcing",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach",
                        "Citation Paper Authors": "Authors:Aditya G. Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, Yujie Wang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "on LLMs and crowdsourcing\nConcurrent to this paper, a few other groups investigate the in-\ntersection of crowdsourcing and LLMs. One paper explores the\nimpact of incorporating LLMs into crowdsourcing workflows on\nrequester and crowdworker stakeholders ",
                    "Citation Text": "GARRETT ALLEN, GAOLE HE, and UJWAL GADIRAJU. 2023. Power-up! What\nCan Generative Models Do for Human Computation Workflows? (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.02243",
                        "Citation Paper Title": "Title:Power-up! What Can Generative Models Do for Human Computation Workflows?",
                        "Citation Paper Abstract": "Abstract:We are amidst an explosion of artificial intelligence research, particularly around large language models (LLMs). These models have a range of applications across domains like medicine, finance, commonsense knowledge graphs, and crowdsourcing. Investigation into LLMs as part of crowdsourcing workflows remains an under-explored space. The crowdsourcing research community has produced a body of work investigating workflows and methods for managing complex tasks using hybrid human-AI methods. Within crowdsourcing, the role of LLMs can be envisioned as akin to a cog in a larger wheel of workflows. From an empirical standpoint, little is currently understood about how LLMs can improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated. In this work, we present a vision for exploring this gap from the perspectives of various stakeholders involved in the crowdsourcing paradigm -- the task requesters, crowd workers, platforms, and end-users. We identify junctures in typical crowdsourcing workflows at which the introduction of LLMs can play a beneficial role and propose means to augment existing design patterns for crowd work.",
                        "Citation Paper Authors": "Authors:Garrett Allen, Gaole He, Ujwal Gadiraju"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.03898v3": {
            "Paper Title": "The Short Text Matching Model Enhanced with Knowledge via Contrastive\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", OTE model used\nSoftLexicon to provide more detailed information at different levels. They used LaserTagger ",
                    "Citation Text": "Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, and Aliaksei Severyn. Encode,\ntag, realize: High-precision text editing. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 5054\u20135065, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.01187",
                        "Citation Paper Title": "Title:Encode, Tag, Realize: High-Precision Text Editing",
                        "Citation Paper Abstract": "Abstract:We propose LaserTagger - a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.",
                        "Citation Paper Authors": "Authors:Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.04312v6": {
            "Paper Title": "Word-Graph2vec: An efficient word embedding approach on word\n  co-occurrence graph using random walk technique",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12716v1": {
            "Paper Title": "BloomVQA: Assessing Hierarchical Multi-modal Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12713v1": {
            "Paper Title": "Response Enhanced Semi-Supervised Dialogue Query Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12683v1": {
            "Paper Title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12682v1": {
            "Paper Title": "Mini-GPTs: Efficient Large Language Models through Contextual Pruning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.08206v2": {
            "Paper Title": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ". The GPT models are accessed on-\nline, whereas Llama models offer an on-board solution,\nwith both their code and pretrained weights open-sourced.\nCodeLlama-34b-Instruct is an enhanced version of the orig-\ninal Llama ",
                    "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth\u00b4 ee\nLacroix, Baptiste Rozi` ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. Llama: Open and ef\ufb01-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023. 3\n7A. Qualitative Results of GPT4\nThere are three examples of the GPT4 real results with explan ations for different user commands in Tab. 5, Tab. 6, Tab. 7.\nSome user command examples\na) LLM Conditioning [Set as in Tab. 1.]\nb) User Assistant [Provide few-shots for LLM to learn shown in Tab. 1.]\n[Test on real commands. ]\nc) User Share the vehicle\u2019s location to my dad.\nGPT-4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". However, the\nsuccess of adapting language models to new tasks largely\ndepends on the prompting strategy and the quality of the\nprompts ",
                    "Citation Text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The powe r\nof scale for parameter-ef\ufb01cient prompt tuning. arXiv preprint\narXiv:2104.08691 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08691",
                        "Citation Paper Title": "Title:The Power of Scale for Parameter-Efficient Prompt Tuning",
                        "Citation Paper Abstract": "Abstract:In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",
                        "Citation Paper Authors": "Authors:Brian Lester, Rami Al-Rfou, Noah Constant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12634v1": {
            "Paper Title": "MotionScript: Natural Language Descriptions for Expressive 3D Human\n  Motions",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "proposed a neuro-\nsymbolic, program-like representation that described mo-\ntions as a composition of high-level parametric primitives\ni.e. circular, linear, or stationary and further extended by\nadding general spline primitives ",
                    "Citation Text": "Sumith Kulal, Jiayuan Mao, Alex Aiken, and Jiajun Wu. Pro-\ngrammatic concept learning for human motion description\nand synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 13843\u2013\n13852, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.13502",
                        "Citation Paper Title": "Title:Programmatic Concept Learning for Human Motion Description and Synthesis",
                        "Citation Paper Abstract": "Abstract:We introduce Programmatic Motion Concepts, a hierarchical motion representation for human actions that captures both low-level motion and high-level description as motion concepts. This representation enables human motion description, interactive editing, and controlled synthesis of novel video sequences within a single framework. We present an architecture that learns this concept representation from paired video and action sequences in a semi-supervised manner. The compactness of our representation also allows us to present a low-resource training recipe for data-efficient learning. By outperforming established baselines, especially in the small data regime, we demonstrate the efficiency and effectiveness of our framework for multiple applications.",
                        "Citation Paper Authors": "Authors:Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed\na rule-based Gesture Description Language (GDL) to repre-\nsent human body skeleton data with synthetic descriptions.\nHierarchical Motion Understanding ",
                    "Citation Text": "Sumith Kulal, Jiayuan Mao, Alex Aiken, and Jiajun Wu. Hi-\nerarchical motion understanding via motion programs. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 6568\u20136576, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11216",
                        "Citation Paper Title": "Title:Hierarchical Motion Understanding via Motion Programs",
                        "Citation Paper Abstract": "Abstract:Current approaches to video analysis of human motion focus on raw pixels or keypoints as the basic units of reasoning. We posit that adding higher-level motion primitives, which can capture natural coarser units of motion such as backswing or follow-through, can be used to improve downstream analysis tasks. This higher level of abstraction can also capture key features, such as loops of repeated primitives, that are currently inaccessible at lower levels of representation. We therefore introduce Motion Programs, a neuro-symbolic, program-like representation that expresses motions as a composition of high-level primitives. We also present a system for automatically inducing motion programs from videos of human motion and for leveraging motion programs in video synthesis. Experiments show that motion programs can accurately describe a diverse set of human motions and the inferred programs contain semantically meaningful motion primitives, such as arm swings and jumping jacks. Our representation also benefits downstream tasks such as video interpolation and video prediction and outperforms off-the-shelf models. We further demonstrate how these programs can detect diverse kinds of repetitive motion and facilitate interactive video editing.",
                        "Citation Paper Authors": "Authors:Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "introduced bi-\nnary captions describing articulation angle or relative po-\nsition of joints while ",
                    "Citation Text": "Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.\nOrdinal depth supervision for 3d human pose estimation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 7307\u20137316, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04095",
                        "Citation Paper Title": "Title:Ordinal Depth Supervision for 3D Human Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.",
                        "Citation Paper Authors": "Authors:Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed a method to generate fine-grained human\nbody motions by analyzing the linguistics-structure in the\nmotion caption. To elevate data for compositional actions,\nSINC ",
                    "Citation Text": "Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG\u00a8ul Varol. Sinc: Spatial composition of 3d human motions\nfor simultaneous action generation supplementary material.\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.10417",
                        "Citation Paper Title": "Title:SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation",
                        "Citation Paper Abstract": "Abstract:Our goal is to synthesize 3D human motions given textual inputs describing simultaneous actions, for example 'waving hand' while 'walking' at the same time. We refer to generating such simultaneous movements as performing 'spatial compositions'. In contrast to temporal compositions that seek to transition from one action to another, spatial compositing requires understanding which body parts are involved in which action, to be able to move them simultaneously. Motivated by the observation that the correspondence between actions and body parts is encoded in powerful language models, we extract this knowledge by prompting GPT-3 with text such as \"what are the body parts involved in the action <action name>?\", while also providing the parts list and few-shot examples. Given this action-part mapping, we combine body parts from two motions together and establish the first automated method to spatially compose two actions. However, training data with compositional actions is always limited by the combinatorics. Hence, we further create synthetic data with this approach, and use it to train a new state-of-the-art text-to-motion generation model, called SINC (\"SImultaneous actioN Compositions for 3D human motions\"). In our experiments, that training with such GPT-guided synthetic data improves spatial composition generation over baselines. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Nikos Athanasiou, Mathis Petrovich, Michael J. Black, G\u00fcl Varol"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "gen-\nerates arbitrarily long dances, by enforcing temporal conti-\nnuity between batches of multiple sequences. Make-An-\nAnimation (MAA) ",
                    "Citation Text": "Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh,\nand Sonal Gupta. Make-an-animation: Large-scale text-\nconditional 3d human motion generation. arXiv preprint\narXiv:2305.09662 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.09662",
                        "Citation Paper Title": "Title:Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation",
                        "Citation Paper Abstract": "Abstract:Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation.",
                        "Citation Paper Authors": "Authors:Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "proposed a method to generate motions conditioned on\nprevious time step and current text description in an autore-\ngressive fashion to manage the scarcity of human motion-\ncaptured data for long prompts. Likewise, EDGE ",
                    "Citation Text": "Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 448\u2013458, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.10658",
                        "Citation Paper Title": "Title:EDGE: Editable Dance Generation From Music",
                        "Citation Paper Abstract": "Abstract:Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.",
                        "Citation Paper Authors": "Authors:Jonathan Tseng, Rodrigo Castellon, C. Karen Liu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "introduced uni-\nfying diverse skeletal data to extend co-speech gesture\n2datasets and MCM ",
                    "Citation Text": "Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkan-\nhalli, and Weidong Geng. Mcm: Multi-condition mo-\ntion synthesis framework for multi-scenario. arXiv preprint\narXiv:2309.03031 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.03031",
                        "Citation Paper Title": "Title:MCM: Multi-condition Motion Synthesis Framework for Multi-scenario",
                        "Citation Paper Abstract": "Abstract:The objective of the multi-condition human motion synthesis task is to incorporate diverse conditional inputs, encompassing various forms like text, music, speech, and more. This endows the task with the capability to adapt across multiple scenarios, ranging from text-to-motion and music-to-dance, among others. While existing research has primarily focused on single conditions, the multi-condition human motion generation remains underexplored. In this paper, we address these challenges by introducing MCM, a novel paradigm for motion synthesis that spans multiple scenarios under diverse conditions. The MCM framework is able to integrate with any DDPM-like diffusion model to accommodate multi-conditional information input while preserving its generative capabilities. Specifically, MCM employs two-branch architecture consisting of a main branch and a control branch. The control branch shares the same structure as the main branch and is initialized with the parameters of the main branch, effectively maintaining the generation ability of the main branch and supporting multi-condition input. We also introduce a Transformer-based diffusion model MWNet (DDPM-like) as our main branch that can capture the spatial complexity and inter-joint correlations in motion sequences through a channel-dimension self-attention module. Quantitative comparisons demonstrate that our approach achieves SoTA results in both text-to-motion and competitive results in music-to-dance tasks, comparable to task-specific methods. Furthermore, the qualitative evaluation shows that MCM not only streamlines the adaptation of methodologies originally designed for text-to-motion tasks to domains like music-to-dance and speech-to-gesture, eliminating the need for extensive network re-configurations but also enables effective multi-condition modal control, realizing \"once trained is motion need\".",
                        "Citation Paper Authors": "Authors:Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkanhalli, Weidong Geng"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ", a large collection of mo-\ntion datasets. Several deterministic methods [1, 16] as well\nas probabilistic approaches such as transformers [42, 69],\nGANs ",
                    "Citation Text": "Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng\nFang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xi-\naokang Yang, et al. Actformer: A gan-based transformer\ntowards general action-conditioned 3d human motion gener-\nation. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 2228\u20132238, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.07706",
                        "Citation Paper Title": "Title:ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation",
                        "Citation Paper Abstract": "Abstract:We present a GAN-based Transformer for general action-conditioned 3D human motion generation, including not only single-person actions but also multi-person interactive actions. Our approach consists of a powerful Action-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Process latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superiority in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alternately modeling temporal correlations and human interactions with Transformer encoders. To further facilitate research on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat behaviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion representations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step towards a general human motion generator.",
                        "Citation Paper Authors": "Authors:Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng, Wei Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.09156v2": {
            "Paper Title": "Characterizing Information Seeking Events in Health-Related Social\n  Discourse",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.15413v5": {
            "Paper Title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "proposes\nthe score-distillation sampling (SDS) method that uses text-to-image diffusion models to optimize\nneural fields ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM ,\n65(1):99\u2013106, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "Diffusion models. Denoising diffusion models [ 5,23] generate images through progressive denois-\ning process. During training, denoising diffusion probabilistic models (DDPM) ",
                    "Citation Text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS , 33:6840\u2013\n6851, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11239",
                        "Citation Paper Title": "Title:Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Ajay Jain, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14185v1": {
            "Paper Title": "Auto311: A Confidence-guided Automated System for Non-emergency Call",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12588v1": {
            "Paper Title": "An Empirical study of Unsupervised Neural Machine Translation: analyzing\n  NMT output, model's behavior and sentences' contribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17254v1": {
            "Paper Title": "Faithful Model Evaluation for Model-Based Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.11698v3": {
            "Paper Title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 122,
                    "Sentence": "to evaluate the model\u2019s memorization problem of training data [ 31,152]; 2)\nevaluating the information extraction accuracy of different types of Personally Identifiable Information\n(PII) introduced during the inference stage ",
                    "Citation Text": "J. X. Morris, J. T. Chiu, R. Zabih, and A. M. Rush. Unsupervised text deidentification.\narXiv:2210.11528v1, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.11528",
                        "Citation Paper Title": "Title:Unsupervised Text Deidentification",
                        "Citation Paper Abstract": "Abstract:Deidentification seeks to anonymize textual data prior to distribution. Automatic deidentification primarily uses supervised named entity recognition from human-labeled data points. We propose an unsupervised deidentification method that masks words that leak personally-identifying information. The approach utilizes a specially trained reidentification model to identify individuals from redacted personal documents. Motivated by K-anonymity based privacy, we generate redactions that ensure a minimum reidentification rank for the correct profile of the document. To evaluate this approach, we consider the task of deidentifying Wikipedia Biographies, and evaluate using an adversarial reidentification metric. Compared to a set of unsupervised baselines, our approach deidentifies documents more completely while removing fewer words. Qualitatively, we see that the approach eliminates many identifying aspects that would fall outside of the common named entity based approach.",
                        "Citation Paper Authors": "Authors:John X. Morris, Justin T. Chiu, Ramin Zabih, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 176,
                    "Sentence": ", and NaturalInstructions [ 121,185]. Beyond\nperformance evaluation in isolation, researchers have also developed benchmarks and platforms to\ntest other properties of LLMs, such as robustness with AdvGLUE ",
                    "Citation Text": "B. Wang, C. Xu, S. Wang, Z. Gan, Y . Cheng, J. Gao, A. H. Awadallah, and B. Li. Adversarial\nGLUE: A multi-task benchmark for robustness evaluation of language models. In J. Van-\nschoren and S. Yeung, editors, Proceedings oftheNeural Information Processing Systems\nTrack onDatasets andBenchmarks 1,NeurIPS Datasets andBenchmarks 2021, December\n2021, virtual , 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/\npaper/2021/hash/335f5352088d7d9bf74191e006d8e24c-Abstract-round2.html .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.02840",
                        "Citation Paper Title": "Title:Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models",
                        "Citation Paper Abstract": "Abstract:Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at this https URL.",
                        "Citation Paper Authors": "Authors:Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li"
                    }
                },
                {
                    "Sentence ID": 214,
                    "Sentence": ". For instance, potential adversaries may exploit the dialogue\ncontext or system instructions to execute adversarial attacks ",
                    "Citation Text": "K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y . Wang, L. Yang, W. Ye, N. Z. Gong, Y . Zhang,\net al. Promptbench: Towards evaluating the robustness of large language models on adversarial\nprompts. arXiv preprint arXiv:2306.04528, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.04528",
                        "Citation Paper Title": "Title:PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                        "Citation Paper Abstract": "Abstract:The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, Xing Xie"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": "). However, as we highlight (and others have shown), these new capabilities also result in\nnew trustworthiness concerns ",
                    "Citation Text": "N. Maus, P. Chao, E. Wong, and J. Gardner. Adversarial prompting for black box foundation\nmodels. arXiv preprint arXiv:2302.04237, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.04237",
                        "Citation Paper Title": "Title:Black Box Adversarial Prompting for Foundation Models",
                        "Citation Paper Abstract": "Abstract:Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.",
                        "Citation Paper Authors": "Authors:Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner"
                    }
                },
                {
                    "Sentence ID": 166,
                    "Sentence": "), from different perspectives, including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations,\nprivacy, machine ethics, and fairness under different settings. We further extend our evaluation\nto recent open LLMs, including llama ",
                    "Citation Text": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re,\nN. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "have been introduced to evaluate general-purpose language understanding.\nWith advances in the capabilities of LLMs, benchmarks have been proposed to evaluate more difficult\ntasks, such as CodeXGLUE ",
                    "Citation Text": "S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang,\nD. Tang, et al. Codexglue: A machine learning benchmark dataset for code understanding and\ngeneration. In Thirty-fifth Conference onNeural Information Processing Systems Datasets\nandBenchmarks Track (Round 1).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.04664",
                        "Citation Paper Title": "Title:CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.",
                        "Citation Paper Authors": "Authors:Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu"
                    }
                },
                {
                    "Sentence ID": 174,
                    "Sentence": ". In order to evaluate language models and better understand their capabilities and\nlimitations, different benchmarks have been proposed. For instance, benchmarks such as GLUE ",
                    "Citation Text": "A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. In ICLR, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07461",
                        "Citation Paper Title": "Title:GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
                        "Citation Paper Abstract": "Abstract:For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.",
                        "Citation Paper Authors": "Authors:Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2303.06854v2": {
            "Paper Title": "Robust Contrastive Language-Image Pre-training against Data Poisoning\n  and Backdoor Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.08456v3": {
            "Paper Title": "PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in\n  Poetry Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14184v1": {
            "Paper Title": "Large Language Models in Medical Term Classification and Unexpected\n  Misalignment Between Response and Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12334v1": {
            "Paper Title": "PowMix: A Versatile Regularizer for Multimodal Sentiment Analysis",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "across modalities. Coupling different\nlearning recipes with pre-trained models has been a popular\nchoice among researchers. Yu et al . ",
                    "Citation Text": "W. Yu, H. Xu, Z. Yuan, and J. Wu, \u201cLearning modality-specific\nrepresentations with self-supervised multi-task learning for mul-\ntimodal sentiment analysis,\u201d in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 35, no. 12, 2021, pp. 10 790\u201310 797.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.04830",
                        "Citation Paper Title": "Title:Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
                        "Citation Paper Abstract": "Abstract:Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annotation, existing methods are restricted in capturing differentiated information. However, additional uni-modal annotations are high time- and labor-cost. In this paper, we design a label generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions. Then, joint training the multi-modal and uni-modal tasks to learn the consistency and difference, respectively. Moreover, during the training stage, we design a weight-adjustment strategy to balance the learning progress among different subtasks. That is to guide the subtasks to focus on samples with a larger difference between modality supervisions. Last, we conduct extensive experiments on three public multimodal baseline datasets. The experimental results validate the reliability and stability of auto-generated unimodal supervisions. On MOSI and MOSEI datasets, our method surpasses the current state-of-the-art methods. On the SIMS dataset, our method achieves comparable performance than human-annotated unimodal labels. The full codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Wenmeng Yu, Hua Xu, Ziqi Yuan, Jiele Wu"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "introduce a uni-\nmodal pseudo-labeling module that backpropagates three\nadditional losses. Hazarika et al. ",
                    "Citation Text": "D. Hazarika, R. Zimmermann, and S. Poria, \u201cMisa: Modality-\ninvariant and-specific representations for multimodal sentiment\nanalysis,\u201d in Proceedings of the 28th ACM International Conference on\nMultimedia , 2020, pp. 1122\u20131131.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03545",
                        "Citation Paper Title": "Title:MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
                        "Citation Paper Abstract": "Abstract:Multimodal Sentiment Analysis is an active area of research that leverages multimodal signals for affective understanding of user-generated videos. The predominant approach, addressing this task, has been to develop sophisticated fusion techniques. However, the heterogeneous nature of the signals creates distributional modality gaps that pose significant challenges. In this paper, we aim to learn effective modality representations to aid the process of fusion. We propose a novel framework, MISA, which projects each modality to two distinct subspaces. The first subspace is modality-invariant, where the representations across modalities learn their commonalities and reduce the modality gap. The second subspace is modality-specific, which is private to each modality and captures their characteristic features. These representations provide a holistic view of the multimodal data, which is used for fusion that leads to task predictions. Our experiments on popular sentiment analysis benchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art models. We also consider the task of Multimodal Humor Detection and experiment on the recently proposed UR_FUNNY dataset. Here too, our model fares better than strong baselines, establishing MISA as a useful multimodal framework.",
                        "Citation Paper Authors": "Authors:Devamanyu Hazarika, Roger Zimmermann, Soujanya Poria"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ", a state-of-the-art CV technique,\nproposing pixel-wise reweighting of mixing factors based\non their attention map values.\nLatent space mixing: Algorithms in this category\nfocus on manipulating latent representations. Manifold\nMixUp ",
                    "Citation Text": "V . Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-\nPaz, and Y. Bengio, \u201cManifold mixup: Better representations by\ninterpolating hidden states,\u201d in International conference on machine\nlearning . PMLR, 2019, pp. 6438\u20136447.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05236",
                        "Citation Paper Title": "Title:Manifold Mixup: Better Representations by Interpolating Hidden States",
                        "Citation Paper Abstract": "Abstract:Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.",
                        "Citation Paper Authors": "Authors:Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "extends\nthis concept with a non-linear interpolation scheme in the\ntext embedding space. ReMix ",
                    "Citation Text": "H.-P . Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, \u201cRemix:\nRebalanced mixup,\u201d in Computer Vision \u2013 ECCV 2020 Workshops,\n2020, Proceedings, Part VI , 2020, p. 95\u2013110.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.03943",
                        "Citation Paper Title": "Title:Remix: Rebalanced Mixup",
                        "Citation Paper Abstract": "Abstract:Deep image classifiers often perform poorly when training data are heavily class-imbalanced. In this work, we propose a new regularization technique, Remix, that relaxes Mixup's formulation and enables the mixing factors of features and labels to be disentangled. Specifically, when mixing two samples, while features are mixed in the same fashion as Mixup, Remix assigns the label in favor of the minority class by providing a disproportionately higher weight to the minority class. By doing so, the classifier learns to push the decision boundaries towards the majority classes and balance the generalization error between majority and minority classes. We have studied the state-of-the art regularization techniques such as Mixup, Manifold Mixup and CutMix under class-imbalanced regime, and shown that the proposed Remix significantly outperforms these state-of-the-arts and several re-weighting and re-sampling techniques, on the imbalanced datasets constructed by CIFAR-10, CIFAR-100, and CINIC-10. We have also evaluated Remix on a real-world large-scale imbalanced dataset, iNaturalist 2018. The experimental results confirmed that Remix provides consistent and significant improvements over the previous methods.",
                        "Citation Paper Authors": "Authors:Hsin-Ping Chou, Shih-Chieh Chang, Jia-Yu Pan, Wei Wei, Da-Cheng Juan"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "propose a transformer architecture leveraging\ndual-level reconstruction loss and an attraction loss in a\nSiamese setup between complete and incomplete data. Hu\net al. ",
                    "Citation Text": "G. Hu, T.-E. Lin, Y. Zhao, G. Lu, Y. Wu, and Y. Li, \u201cUniMSE: To-\nwards unified multimodal sentiment analysis and emotion recog-\nnition,\u201d in Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing . Association for Computational\nLinguistics, Dec. 2022, pp. 7837\u20137851.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.11256",
                        "Citation Paper Title": "Title:UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, Yongbin Li"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "propose a meta-learning framework that learns each\nunimodal network and then adapts them for the MSA task.\nSun et al. ",
                    "Citation Text": "L. Sun, Z. Lian, B. Liu, and J. Tao, \u201cEfficient multimodal trans-\nformer with dual-level feature restoration for robust multimodal\nsentiment analysis,\u201d IEEE Transactions on Affective Computing , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.07589",
                        "Citation Paper Title": "Title:Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis",
                        "Citation Paper Abstract": "Abstract:With the proliferation of user-generated online videos, Multimodal Sentiment Analysis (MSA) has attracted increasing attention recently. Despite significant progress, there are still two major challenges on the way towards robust MSA: 1) inefficiency when modeling cross-modal interactions in unaligned multimodal data; and 2) vulnerability to random modality feature missing which typically occurs in realistic settings. In this paper, we propose a generic and unified framework to address them, named Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR). Concretely, EMT employs utterance-level representations from each modality as the global multimodal context to interact with local unimodal features and mutually promote each other. It not only avoids the quadratic scaling cost of previous local-local cross-modal interaction methods but also leads to better performance. To improve model robustness in the incomplete modality setting, on the one hand, DLFR performs low-level feature reconstruction to implicitly encourage the model to learn semantic information from incomplete data. On the other hand, it innovatively regards complete and incomplete data as two different views of one sample and utilizes siamese representation learning to explicitly attract their high-level representations. Comprehensive experiments on three popular datasets demonstrate that our method achieves superior performance in both complete and incomplete modality settings.",
                        "Citation Paper Authors": "Authors:Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "augment the learning\nobjective with feature reconstruction loss as well as attract-\ning and repelling objectives.\nA two-step hierarchical learning recipe based on mutual\ninformation maximization is proposed in ",
                    "Citation Text": "W. Han, H. Chen, and S. Poria, \u201cImproving multimodal fusion\nwith hierarchical mutual information maximization for multi-\nmodal sentiment analysis,\u201d in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing , 2021, pp. 9180\u2013\n9192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.00412",
                        "Citation Paper Title": "Title:Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
                        "Citation Paper Abstract": "Abstract:In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain task-related information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach. The implementation of this work is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Wei Han, Hui Chen, Soujanya Poria"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12321v1": {
            "Paper Title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12299v1": {
            "Paper Title": "Instruct-SCTG: Guiding Sequential Controlled Text Generation through\n  Instructions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10493v2": {
            "Paper Title": "Debiasing Multimodal Sarcasm Detection with Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12253v1": {
            "Paper Title": "Geo-located Aspect Based Sentiment Analysis (ABSA) for Crowdsourced\n  Evaluation of Urban Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12241v1": {
            "Paper Title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric\n  Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.14160v4": {
            "Paper Title": "Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.13023v2": {
            "Paper Title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 63,
                    "Sentence": "and Chain-of-Thought [ 47,57]. With the rise\nof open-source LLMs, such as Llama [ 36,37], ChatGLM ",
                    "Citation Text": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, et al .\n2023. GLM-130B: An Open Bilingual Pre-trained Model. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02414",
                        "Citation Paper Title": "Title:GLM-130B: An Open Bilingual Pre-trained Model",
                        "Citation Paper Abstract": "Abstract:We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ") have gained widespread attention for their remark-\nable capabilities in various NLP tasks [ 18,46]. Based on these unique\ncapabilities of LLMs, many tuning-free prompting techniques have\nbeen explored to enhance their generative abilities, such as in-\ncontext learning ",
                    "Citation Text": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations:\nWhat Makes In-Context Learning Work?. In EMNLP . 11048\u201311064.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.12837",
                        "Citation Paper Title": "Title:Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                        "Citation Paper Abstract": "Abstract:Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
                        "Citation Paper Authors": "Authors:Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "aims to handle down-\nstream tasks by integrating pre-training and downstream tasks\ninto a unified task template. Additionally, Sun et al. ",
                    "Citation Text": "Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:\nMulti-Task Prompting for Graph Neural Networks. In KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.01504",
                        "Citation Paper Title": "Title:All in One: Multi-task Prompting for Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
                        "Citation Paper Authors": "Authors:Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "is a trans-\nfer learning paradigm, where GNNs are first pre-trained on masked\nedge prediction and then prompted with token pairs for down-\nstream node classification. GraphPrompt ",
                    "Citation Text": "Zemin Liu, Xingtong Yu, et al .2023. Graphprompt: Unifying pre-training and\ndownstream tasks for graph neural networks. In WWW . 417\u2013428.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.08043",
                        "Citation Paper Title": "Title:GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train, prompt\" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
                        "Citation Paper Authors": "Authors:Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ".\nRecent advancements in contrastive SSL include automated con-\ntrastive augmentation ( i.e., JOAO ",
                    "Citation Text": "Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph\ncontrastive learning automated. In ICML . PMLR, 12121\u201312132.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.07594",
                        "Citation Paper Title": "Title:Graph Contrastive Learning Automated",
                        "Citation Paper Abstract": "Abstract:Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous \"best practices\" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at this https URL.",
                        "Citation Paper Authors": "Authors:Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12141v1": {
            "Paper Title": "Exploring the Residual Stream of Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.10405v8": {
            "Paper Title": "Editing Language Model-based Knowledge Graph Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.09767v2": {
            "Paper Title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.07924v4": {
            "Paper Title": "Communicative Agents for Software Development",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "find that multiple agents can improve each other in a negotiation\ngame like buyer-seller dealing by role-playing and learning from the agent feedback. Liu et al. ",
                    "Citation Text": "Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang,\nand Soroush V osoughi. Training socially aligned language models in simulated human society.\nCoRR , abs/2305.16960, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.16960",
                        "Citation Paper Title": "Title:Training Socially Aligned Language Models on Simulated Social Interactions",
                        "Citation Paper Abstract": "Abstract:Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
                        "Citation Paper Authors": "Authors:Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "use multiple agents to debate each other to solve the degeneration-of-thought problem\nin self-reflection. Fu et al. ",
                    "Citation Text": "Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation\nwith self-play and in-context learning from AI feedback. CoRR , abs/2305.10142, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10142",
                        "Citation Paper Title": "Title:Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
                        "Citation Paper Abstract": "Abstract:We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement. (2) Models' abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.",
                        "Citation Paper Authors": "Authors:Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "improve the factual correctness and reasoning accuracy by leveraging multi-agent debate.\nLiang et al. ",
                    "Citation Text": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang,\nZhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models\nthrough multi-agent debate. CoRR , abs/2305.19118, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.19118",
                        "Citation Paper Title": "Title:Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                        "Citation Paper Abstract": "Abstract:Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: this https URL",
                        "Citation Paper Authors": "Authors:Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "find that asking the agent to take on different roles can improve their performance. Du et\nal. ",
                    "Citation Text": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving\nfactuality and reasoning in language models through multiagent debate. CoRR , abs/2305.14325,\n2023.\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.14325",
                        "Citation Paper Title": "Title:Improving Factuality and Reasoning in Language Models through Multiagent Debate",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
                        "Citation Paper Authors": "Authors:Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "propose a role-playing framework which leverages agents to generate\ndiverse and detailed instructions for complicated tasks. (3) Performance improvement : Salewski et\nal. ",
                    "Citation Text": "Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context\nimpersonation reveals large language models\u2019 strengths and biases. CoRR , abs/2305.14930,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.14930",
                        "Citation Paper Title": "Title:In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
                        "Citation Paper Abstract": "Abstract:In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.",
                        "Citation Paper Authors": "Authors:Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "assign agents with different roles to collect and evaluate multi-party\nconversations. Li et al. ",
                    "Citation Text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. CAMEL: communicative agents for \"mind\" exploration of large scale language model\nsociety. CoRR , abs/2303.17760, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.17760",
                        "Citation Paper Title": "Title:CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society",
                        "Citation Paper Abstract": "Abstract:The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their \"cognitive\" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: this https URL.",
                        "Citation Paper Authors": "Authors:Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "use multiple agents to simulate the user behaviours in the recommendation scenario. (2)\nData construction : Wei et al. ",
                    "Citation Text": "Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, and Mojtaba Komeili.\nMulti-party chat: Conversational agents in group settings with humans and models. CoRR ,\nabs/2304.13835, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.13835",
                        "Citation Paper Title": "Title:Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
                        "Citation Paper Abstract": "Abstract:Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.",
                        "Citation Paper Authors": "Authors:Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "create\nmultiple generative agents with a sandbox environment to simulate believable human behavior. Wang\net al. ",
                    "Citation Text": "Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong\nWen. Recagent: A novel simulation paradigm for recommender systems. CoRR , abs/2306.02552,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.02552",
                        "Citation Paper Title": "Title:When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm",
                        "Citation Paper Abstract": "Abstract:User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a virtual simulator called RecAgent. For comprehensively simulation, we not only consider the behaviors within the recommender system (\\emph{e.g.}, item browsing and clicking), but also accounts for external influential factors, such as, friend chatting and social advertisement. Our simulator contains at most 1000 agents, and each agent is composed of a profiling module, a memory module and an action module, enabling it to behave consistently, reasonably and reliably. In addition, to more flexibly operate our simulator, we also design two global functions including real-human playing and system intervention. To evaluate the effectiveness of our simulator, we conduct extensive experiments from both agent and system perspectives. In order to advance this direction, we have released our project at {this https URL}.",
                        "Citation Paper Authors": "Authors:Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12108v1": {
            "Paper Title": "Knowledge Graph Error Detection with Contrastive Confidence Adaption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.18313v2": {
            "Paper Title": "FP8-LM: Training FP8 Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.06453v2": {
            "Paper Title": "Narrowing the Gap between Supervised and Unsupervised Sentence\n  Representation Learning with Large Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.05161v4": {
            "Paper Title": "Recurrent Neural Language Models as Probabilistic Finite-state Automata",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12009v1": {
            "Paper Title": "Active Preference Inference using Language Models and Probabilistic\n  Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "looks at\nthe problem of asking good clarifying questions from an information retrieval perspective, and train\nmodels with question-answer pairs data to retrieve good questions from a crowdsourced dataset. ",
                    "Citation Text": "Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, and Yoav Artzi. Interactive classification by\nasking informative questions. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 2664\u20132680, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.237. URL https://aclanthology.org/2020.\nacl-main.237 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.03598",
                        "Citation Paper Title": "Title:Interactive Classification by Asking Informative Questions",
                        "Citation Paper Abstract": "Abstract:We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification prediction.The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowdsourcing tasks. We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",
                        "Citation Paper Authors": "Authors:Lili Yu, Howard Chen, Sida Wang, Tao Lei, Yoav Artzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11997v1": {
            "Paper Title": "Coreference Graph Guidance for Mind-Map Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11983v1": {
            "Paper Title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11970v1": {
            "Paper Title": "Large Language Models Empowered Agent-based Modeling and Simulation: A\n  Survey and Perspectives",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ", a LLM for the medical field, is trained based on a foundational model PaLM ",
                    "Citation Text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1\u2013\n113, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02311",
                        "Citation Paper Title": "Title:PaLM: Scaling Language Modeling with Pathways",
                        "Citation Paper Abstract": "Abstract:Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
                        "Citation Paper Authors": "Authors:Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": ". In addition,\nconstructing modules such as reflection and memory can improve agents\u2019 planning and reasoning\ncapabilities, thereby giving them stronger gaming capabilities and creating a possible path towards\nhuman-intelligent gaming ",
                    "Citation Text": "Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo.\nSuspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. arXiv\npreprint arXiv:2309.17277 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.17277",
                        "Citation Paper Title": "Title:Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
                        "Citation Paper Abstract": "Abstract:Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.",
                        "Citation Paper Authors": "Authors:Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.04877v2": {
            "Paper Title": "Generating Explanations to Understand and Repair Embedding-based Entity\n  Alignment",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". Second,\nthis method has exponential time complexity since it needs to\nanalyze all combinations of relation triples ",
                    "Citation Text": "B. Rozemberczki, L. Watson, P. Bayer, H. Yang, O. Kiss, S. Nilsson,and R. Sarkar, \u201cThe Shapley value in machine learning,\u201d in IJCAI , 2022,\npp. 5572\u20135579.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05594",
                        "Citation Paper Title": "Title:The Shapley Value in Machine Learning",
                        "Citation Paper Abstract": "Abstract:Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.",
                        "Citation Paper Authors": "Authors:Benedek Rozemberczki, Lauren Watson, P\u00e9ter Bayer, Hao-Tsung Yang, Oliv\u00e9r Kiss, Sebastian Nilsson, Rik Sarkar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11947v1": {
            "Paper Title": "Emotion Rendering for Conversational Speech Synthesis with Heterogeneous\n  Graph-Based Context Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11945v1": {
            "Paper Title": "Multi-Granularity Information Interaction Framework for Incomplete\n  Utterance Rewriting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.04766v2": {
            "Paper Title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment\n  to Cultural Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11922v1": {
            "Paper Title": "Relation-Aware Question Answering for Heterogeneous Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11075v2": {
            "Paper Title": "Split and Rephrase with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.11608v2": {
            "Paper Title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse\n  Biomedical Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11890v1": {
            "Paper Title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a\n  Large Language Model-Based Difficulty Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11882v1": {
            "Paper Title": "ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for\n  Accelerating Language Models Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11875v1": {
            "Paper Title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11870v1": {
            "Paper Title": "A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10793v2": {
            "Paper Title": "Understanding the Instruction Mixture for Large Language Model\n  Fine-tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11852v1": {
            "Paper Title": "Predicting Human Translation Difficulty with Neural Machine Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11111v2": {
            "Paper Title": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11828v1": {
            "Paper Title": "TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with\n  Decentralized Natural Language Understanding Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11805v1": {
            "Paper Title": "Gemini: A Family of Highly Capable Multimodal Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11803v1": {
            "Paper Title": "Designing Guiding Principles for NLP for Healthcare: A Case Study of\n  Maternal Health",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "(both applications work and\nhigh-level ethics guidelines like those by Chen et al. ",
                    "Citation Text": "Irene Y. Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh\nGhassemi. Ethical machine learning in healthcare. Annual Review of Biomedical Data Science , 4\n(1):123\u2013144, 2021. doi: 10.1146/annurev-biodatasci-092820-114757. URL https://doi.org/10.\n1146/annurev-biodatasci-092820-114757 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.10576",
                        "Citation Paper Title": "Title:Ethical Machine Learning in Health Care",
                        "Citation Paper Abstract": "Abstract:The use of machine learning (ML) in health care raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of health care. Specifically, we frame ethics of ML in health care through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to post-deployment considerations. We close by summarizing recommendations to address these challenges.",
                        "Citation Paper Authors": "Authors:Irene Y. Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, Marzyeh Ghassemi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11795v1": {
            "Paper Title": "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11792v1": {
            "Paper Title": "COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.12276v2": {
            "Paper Title": "LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "API for the object\n\ud835\udc5c, generating a \"target image\" \ud835\udc47. Concurrently, the same object-prompt \ud835\udc5cis used to download \ud835\udc41screenshots of 3D\nobjects freely available on Sketchfab, denoted as \ud835\udc7a={\ud835\udc601,\ud835\udc602,...,\ud835\udc60 \ud835\udc41}. We then employ CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack\nClark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 [cs.CV]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", which focuses on explainable AI in augmented reality. A comprehensive\nreview of Neural Radiance Field (NeRF) models by Gao et al. ",
                    "Citation Text": "Kyle Gao, Yina Gao, Hongjie He, Denning Lu, Linlin Xu, and Jonathan Li. 2022. Nerf: Neural radiance field in 3d vision, a comprehensive review.\narXiv preprint arXiv:2210.00379 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.00379",
                        "Citation Paper Title": "Title:NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Field (NeRF) has recently become a significant development in the field of Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Due to the growing popularity of NeRF and its expanding research area, we present a comprehensive survey of NeRF papers from the past two years. Our survey is organized into architecture and application-based taxonomies and provides an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.",
                        "Citation Paper Authors": "Authors:Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "further\nextended these concepts to include demonstrations and task planning.\n2.5 Interpreting Non-Linguistic Information\nLastly, the work of Zhang et al. with MotionGPT ",
                    "Citation Text": "Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. 2023. MotionGPT: Finetuned\nLLMs are General-Purpose Motion Generators. arXiv preprint arXiv:2306.10900 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.10900",
                        "Citation Paper Title": "Title:MotionGPT: Finetuned LLMs are General-Purpose Motion Generators",
                        "Citation Paper Abstract": "Abstract:Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Codes shall be released upon acceptance.",
                        "Citation Paper Authors": "Authors:Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "focus on the automatic adaptation of\nMR interfaces, a line of work that is relevant for multi-user XR experiences, as shown by Mandi et al. with RoCo ",
                    "Citation Text": "Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. RoCo: Dialectic Multi-Robot Collaboration with Large Language Models. arXiv preprint\narXiv:2307.04738 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.04738",
                        "Citation Paper Title": "Title:RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
                        "Citation Paper Abstract": "Abstract:We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website this https URL for videos and code.",
                        "Citation Paper Authors": "Authors:Zhao Mandi, Shreeya Jain, Shuran Song"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", a system that focuses on universal dialogues for 3D scenes, which is further augmented by the work of\nHong et al. with 3D-LLM ",
                    "Citation Text": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 2023. 3D-LLM: Injecting the 3D World into\nLarge Language Models. arXiv preprint arXiv:2307.12981 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.12981",
                        "Citation Paper Title": "Title:3D-LLM: Injecting the 3D World into Large Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : this https URL.",
                        "Citation Paper Authors": "Authors:Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "furthered the conversation by employing diffusion models for 3D generative modeling. The Instruct-NeRF2NeRF\nmethod ",
                    "Citation Text": "Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. 2023. Instruct-nerf2nerf: Editing 3d scenes with\ninstructions. arXiv preprint arXiv:2303.12789 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.12789",
                        "Citation Paper Title": "Title:Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions",
                        "Citation Paper Abstract": "Abstract:We propose a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, our method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. We demonstrate that our proposed method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work.",
                        "Citation Paper Authors": "Authors:Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ",\na high-resolution text-to-3D content creation framework that addresses the limitations of slow optimization and\nlow-resolution output inherent in existing methods like DreamFusion. Recently, Holodiffusion by Karnewar et al. ",
                    "Citation Text": "Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. 2023. Holodiffusion: Training a 3D diffusion model using 2D images. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18423\u201318433.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.16509",
                        "Citation Paper Title": "Title:HoloDiffusion: Training a 3D Diffusion Model using 2D Images",
                        "Citation Paper Abstract": "Abstract:Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. We address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling.",
                        "Citation Paper Authors": "Authors:Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy Mitra"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "have demonstrated the potential of\ntext guidance and generative models in creating complex and diverse 3D objects. Lin et al. introduce Magic3D ",
                    "Citation Text": "Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.\n2023. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n300\u2013309.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.10440",
                        "Citation Paper Title": "Title:Magic3D: High-Resolution Text-to-3D Content Creation",
                        "Citation Paper Abstract": "Abstract:DreamFusion has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.",
                        "Citation Paper Authors": "Authors:Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11785v1": {
            "Paper Title": "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14180v1": {
            "Paper Title": "Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.08774v4": {
            "Paper Title": "GPT-4 Technical Report",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "in this area and discussion of use of words like \u201cfactual\u201d and \u201ctruthful\u201d in, e.g. ",
                    "Citation Text": "O. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and\nW. Saunders, \u201cTruthful AI: Developing and governing AI that does not lie,\u201d Oct. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.06674",
                        "Citation Paper Title": "Title:Truthful AI: Developing and governing AI that does not lie",
                        "Citation Paper Abstract": "Abstract:In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI \"lies\" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures.\nEstablishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful.\nOur initial proposals for these areas include: (1) a standard of avoiding \"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction.\nA concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",
                        "Citation Paper Authors": "Authors:Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, William Saunders"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.02775v3": {
            "Paper Title": "AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using\n  Open-Source LLMs",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". Performance on specialized tasks and\n2alignment with user goals can be significantly enhanced by fine-tuning them on specific datasets\ncomprising instructions and human-generated text ",
                    "Citation Text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.11416",
                        "Citation Paper Title": "Title:Scaling Instruction-Finetuned Language Models",
                        "Citation Paper Abstract": "Abstract:Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
                        "Citation Paper Authors": "Authors:Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11720v1": {
            "Paper Title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.03498v2": {
            "Paper Title": "In-Context Exemplars as Clues to Retrieving from Large Associative\n  Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11703v1": {
            "Paper Title": "Shaping Political Discourse using multi-source News Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11701v1": {
            "Paper Title": "Opportunities and Challenges of Applying Large Language Models in\n  Building Energy Efficiency and Decarbonization Studies: An Exploratory\n  Overview",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.17333v2": {
            "Paper Title": "Arabic Fine-Grained Entity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11441v1": {
            "Paper Title": "Social Learning: Towards Collaborative Learning with Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11399v1": {
            "Paper Title": "News Signals: An NLP Library for Text and Time Series",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11370v1": {
            "Paper Title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11361v1": {
            "Paper Title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual\n  Retrieval-Augmented Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11356v1": {
            "Paper Title": "The Problem of Coherence in Natural Language Explanations of\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". Additionally, the binary assessment of coherence al-\nlowed for simpler annotation rules, minimizing the inconsistencies\nthat may occur in manual evaluation ",
                    "Citation Text": "Qingkai Zeng, Mengxia Yu, Wenhao Yu, Tianwen Jiang, and Meng\nJiang, \u2018Validating label consistency in ner data annotation\u2019, arXiv\npreprint arXiv:2101.08698 , (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08698",
                        "Citation Paper Title": "Title:Validating Label Consistency in NER Data Annotation",
                        "Citation Paper Abstract": "Abstract:Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with annotation. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an empirical method to explore the relationship between label (in-)consistency and NER model performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in the corrected version of both datasets.",
                        "Citation Paper Authors": "Authors:Qingkai Zeng, Mengxia Yu, Wenhao Yu, Tianwen Jiang, Meng Jiang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11345v1": {
            "Paper Title": "Implicit Affordance Acquisition via Causal Action-Effect Modeling in the\n  Video Domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11344v1": {
            "Paper Title": "Muted: Multilingual Targeted Offensive Speech Identification and\n  Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.16456v2": {
            "Paper Title": "Camoscio: an Italian Instruction-tuned LLaMA",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "in the variants unigram (R1),\nbigram (R2), and Longest Common Subsequence (RL). To\ngauge semantic correspondence, we employ the trained\nBERTScore metric ",
                    "Citation Text": "T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger,\nY. Artzi, Bertscore: Evaluating text generation with\nbert, in: International Conference on Learning Rep-\nresentations, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09675",
                        "Citation Paper Title": "Title:BERTScore: Evaluating Text Generation with BERT",
                        "Citation Paper Abstract": "Abstract:We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
                        "Citation Paper Authors": "Authors:Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ", hereafter referred to as\nXFORMAL-IT. The dataset consists of forum messages\nfrom the GYAFC corpus ",
                    "Citation Text": "S. Rao, J. Tetreault, Dear sir or madam, may i in-\ntroduce the gyafc dataset: Corpus, benchmarks and\nmetrics for formality style transfer, in: Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), 2018, pp. 129\u2013140.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06535",
                        "Citation Paper Title": "Title:Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer",
                        "Citation Paper Abstract": "Abstract:Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics.",
                        "Citation Paper Authors": "Authors:Sudha Rao, Joel Tetreault"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". This dataset is composed of sets\nof paragraphs, questions, and answers derived from the\noriginal SQuAD dataset ",
                    "Citation Text": "P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad:\n100,000+ questions for machine comprehension of\ntext, in: Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing,\n2016, pp. 2383\u20132392.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "for their finetuning\napproach.\n3.1. Dataset\nStanford Alpaca is an instruction-tuning dataset con-\nstructed using the self-instruct method ",
                    "Citation Text": "Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,\nD. Khashabi, H. Hajishirzi, Self-instruct: Aligning\nlanguage models with self-generated instructions,\nin: Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), Association for Computational Lin-\nguistics, Toronto, Canada, 2023, pp. 13484\u201313508.\nURL: https://aclanthology.org/2023.acl-long.754.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10560",
                        "Citation Paper Title": "Title:Self-Instruct: Aligning Language Models with Self-Generated Instructions",
                        "Citation Paper Abstract": "Abstract:Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "released on their GitHub repository an instruction-\ntuned version of LLaMA on a translation to Italian of the\nGPT-4-LLM dataset ",
                    "Citation Text": "B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction\ntuning with gpt-4, arXiv preprint arXiv:2304.03277\n(2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.03277",
                        "Citation Paper Title": "Title:Instruction Tuning with GPT-4",
                        "Citation Paper Abstract": "Abstract:Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
                        "Citation Paper Authors": "Authors:Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "a T5 model tailored for Ital-\nian using a refined version of the mC4 corpus ",
                    "Citation Text": "L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-\nRfou, A. Siddhant, A. Barua, C. Raffel, mT5:A massively multilingual pre-trained text-to-text\ntransformer, in: Proceedings of the 2021 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Association for Com-\nputational Linguistics, Online, 2021, pp. 483\u2013498.\nURL: https://aclanthology.org/2021.naacl-main.41.\ndoi:10.18653/v1/2021.naacl-main.41 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11934",
                        "Citation Paper Title": "Title:mT5: A massively multilingual pre-trained text-to-text transformer",
                        "Citation Paper Abstract": "Abstract:The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
                        "Citation Paper Authors": "Authors:Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.15452v6": {
            "Paper Title": "When Do Program-of-Thoughts Work for Reasoning?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11312v1": {
            "Paper Title": "APE-then-QE: Correcting then Filtering Pseudo Parallel Corpora for MT\n  Training Data Creation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.08796v2": {
            "Paper Title": "Chinese Spelling Correction as Rephrasing Language Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11282v1": {
            "Paper Title": "LLM-ARK: Knowledge Graph Reasoning Using Large Language Models via Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11272v1": {
            "Paper Title": "Disentangling continuous and discrete linguistic signals in\n  transformer-based sentence embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.12439v2": {
            "Paper Title": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". More recent research has delved into various\ntypes of backdoor attacks that utilize prompt learning, such as\nBToP ",
                    "Citation Text": "L. Xu, Y . Chen, G. Cui, H. Gao, and Z. Liu, \u201cExplor-\ning the universal vulnerability of prompt-based learning\nparadigm,\u201d arXiv preprint arXiv:2204.05239 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.05239",
                        "Citation Paper Title": "Title:Exploring the Universal Vulnerability of Prompt-based Learning Paradigm",
                        "Citation Paper Abstract": "Abstract:Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11142v1": {
            "Paper Title": "Efficiency-oriented approaches for self-supervised speech representation\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.19972v2": {
            "Paper Title": "VILAS: Exploring the Effects of Vision and Language Context in Automatic\n  Speech Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11135v1": {
            "Paper Title": "Linear Attention via Orthogonal Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.05173v3": {
            "Paper Title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.04498v4": {
            "Paper Title": "NExT-Chat: An LMM for Chat, Detection and Segmentation",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nSegmentation. Similar to the detection process, we utilize\nthe hidden states tof the <trigger> as input for the mask\nhead. Inspired by LISA ",
                    "Citation Text": "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692 ,\n2023. 3, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.00692",
                        "Citation Paper Title": "Title:LISA: Reasoning Segmentation via Large Language Model",
                        "Citation Paper Abstract": "Abstract:Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at this https URL.",
                        "Citation Paper Authors": "Authors:Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "70.5 73.2 66.1 65.3 68.1 53.7 59.9 60.4\nLA VT ",
                    "Citation Text": "Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip HS Torr. Lavt: Language-awarevision transformer for referring image segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 18155\u201318165, 2022. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.02244",
                        "Citation Paper Title": "Title:LAVT: Language-Aware Vision Transformer for Referring Image Segmentation",
                        "Citation Paper Abstract": "Abstract:Referring image segmentation is a fundamental vision-language task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the referring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (\"cross-modal\") decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer's overwhelming success in many other vision-language tasks. Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder network. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the well-proven correlation modeling power of a Transformer encoder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.",
                        "Citation Paper Authors": "Authors:Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, Philip H.S. Torr"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "during training:\nLdet=\u03b1L1(b,bgt) +\u03b2GIoU (b,bgt), (2)\nwhere bgtrepresents the ground truth coordinates, and \u03b1=\n2,\u03b2= 0.8follows the ratio utilized in DETR ",
                    "Citation Text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision , pages 213\u2013229. Springer, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12872",
                        "Citation Paper Title": "Title:End-to-End Object Detection with Transformers",
                        "Citation Paper Abstract": "Abstract:We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "further ex-\ntend the idea to vision&language tasks like visual ground-\ning [26, 40]. Following this line, Vision-LLM ",
                    "Citation Text": "Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, and Jifeng Dai. Visionllm: Large language model is\nalso an open-ended decoder for vision-centric tasks. arXiv\npreprint arXiv:2305.11175 , 2023. 1, 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.11175",
                        "Citation Paper Title": "Title:VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on this https URL. The code shall be released at this https URL.",
                        "Citation Paper Authors": "Authors:Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "first\npropose to represent object bounding box coordinates as\ntext tokens and thus the language model can output the\nobject locations in a token classification manner. How-\never, pix2seq only validate its idea on traditional object\ndetection tasks. UniTab ",
                    "Citation Text": "Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling. In European Conference on Computer\nVision , pages 521\u2013539. Springer, 2022. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12085",
                        "Citation Paper Title": "Title:UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling",
                        "Citation Paper Abstract": "Abstract:We propose UniTAB that Unifies Text And Box outputs for grounded vision-language (VL) modeling. Grounded VL tasks such as grounded captioning require the model to generate a text description and align predicted words with object regions. To achieve this, models must generate desired text and box outputs together, and meanwhile indicate the alignments between words and boxes. In contrast to existing solutions that use multiple separate modules for different outputs, UniTAB represents both text and box outputs with a shared token sequence, and introduces a special <obj> token to naturally indicate word-box alignments in the sequence. UniTAB thus could provide a more comprehensive and interpretable image description, by freely grounding generated words to object regions. On grounded captioning, UniTAB presents a simpler solution with a single output head, and significantly outperforms state of the art in both grounding and captioning evaluations. On general VL tasks that have different desired output formats (i.e., text, box, or their combination), UniTAB with a single network achieves better or comparable performance than task-specific state of the art. Experiments cover 7 VL benchmarks, including grounded captioning, visual grounding, image captioning, and visual question answering. Furthermore, UniTAB's unified multi-task network and the task-agnostic output sequence design make the model parameter efficient and generalizable to new tasks.",
                        "Citation Paper Authors": "Authors:Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposes to encode the regions as features\nand thus can accept the region as input. Pix2seq ",
                    "Citation Text": "Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework for\nobject detection. arXiv preprint arXiv:2109.10852 , 2021. 1,\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.10852",
                        "Citation Paper Title": "Title:Pix2seq: A Language Modeling Framework for Object Detection",
                        "Citation Paper Abstract": "Abstract:We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",
                        "Citation Paper Authors": "Authors:Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". However, these\nLMMs [1, 21, 22] can only take the whole image/video as\ninput and output text, and are incapable of handling region\nunderstanding tasks.\n2.2. LMM for Region Reasoning\nGPT4ROI ",
                    "Citation Text": "Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601 , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.03601",
                        "Citation Paper Title": "Title:GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
                        "Citation Paper Abstract": "Abstract:Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code, dataset, and demo can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "fine-tunes its model with less than 5,000\nself-instruct image-text pairs and turns the model into bet-\nter conversation robot. Different from MiniGPT-4\u2019s self-\ninstruct, LLaV A ",
                    "Citation Text": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning, 2023. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08485",
                        "Citation Paper Title": "Title:Visual Instruction Tuning",
                        "Citation Paper Abstract": "Abstract:Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                        "Citation Paper Authors": "Authors:Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "directly feed the visual features into the LLMs as soft\nprompts. Following BLIP-2, MiniGPT-4 ",
                    "Citation Text": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592 , 2023. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.10592",
                        "Citation Paper Title": "Title:MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
                        "Citation Paper Abstract": "Abstract:The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "tries\nto extract vision information by a pre-trained vision back-\nbone with a resampler, and incorporate them into the text\nfeatures with a cross-attention mechanism. Instead of us-\ning cross-attention layers, BLIP-2 ",
                    "Citation Text": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597 , 2023. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.12597",
                        "Citation Paper Title": "Title:BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Citation Paper Abstract": "Abstract:The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "2.1. LMM\nLarge multimodal models (LMMs) are typically built on\nlarge language models (LLMs) and equipped with visual\nperception modules to enable the multimodal perception\nability, which can generate captions or answer questions\nbased on the given multimodal content. Flamingo ",
                    "Citation Text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems , 35:23716\u201323736,\n2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.14198",
                        "Citation Paper Title": "Title:Flamingo: a Visual Language Model for Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14966v1": {
            "Paper Title": "Dynamic Syntax Mapping: A New Approach to Unsupervised Syntax Parsing",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "linking\nsyntactic dependencies to statistical measures of mutual i nformation.\nWhile the application of non-parametric substitutability for syntactic parsing is novel, its conceptual roots can be\ntraced to studies in language model interpretability. Papa dimitriou et al. ",
                    "Citation Text": "Isabel Papadimitriou, Richard Futrell, and Kyle Mahow ald. When classifying grammatical role, BERT\ndoesn\u2019t care about word order... except when it matters. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 2 : Short Papers) , pages 636\u2013643, Dublin, Ire-\nland, May 2022. Association for Computational Linguistics . doi:10.18653/v1/2022.acl-short.71. URL\nhttps://aclanthology.org/2022.acl-short.71 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.06204",
                        "Citation Paper Title": "Title:When classifying grammatical role, BERT doesn't care about word order... except when it matters",
                        "Citation Paper Abstract": "Abstract:Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey \"The chef chopped the onion,\" not \"The onion chopped the chef.\" Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like \"The onion chopped the chef\". We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.",
                        "Citation Paper Authors": "Authors:Isabel Papadimitriou, Richard Futrell, Kyle Mahowald"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "trained a supervised probe to projec t vectors into a \u2019syntactic\u2019 space. Another innovative\ndirection is leveraging BERT\u2019s masked language modeling ob jective for calculating syntactic parsing scores, as ex-\nplored by Hoover et al. ",
                    "Citation Text": "Jacob Louis Hoover, Wenyu Du, Alessandro Sordoni, and T imothy J. O\u2019Donnell. Linguistic depen-\ndencies and statistical dependence. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing , pages 2941\u20132963, Online and Punta Cana, Dominican Republi c, Novem-\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.234. URL\nhttps://aclanthology.org/2021.emnlp-main.234 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08685",
                        "Citation Paper Title": "Title:Linguistic Dependencies and Statistical Dependence",
                        "Citation Paper Abstract": "Abstract:Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most $\\approx 0.5$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.",
                        "Citation Paper Authors": "Authors:Jacob Louis Hoover, Alessandro Sordoni, Wenyu Du, Timothy J. O'Donnell"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "developed a methodology predi-\ncated on syntactic similarity between word pairs, derived f rom attention distributions. This approach aligns with Cla rk\net al. ",
                    "Citation Text": "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christ opher D. Manning. What does BERT look at? an\nanalysis of BERT\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzin g and Inter-\npreting Neural Networks for NLP , pages 276\u2013286, Florence, Italy, August 2019. Association for Computational\nLinguistics. doi:10.18653/v1/W19-4828. URL https://aclanthology.org/W19-4828 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04341",
                        "Citation Paper Title": "Title:What Does BERT Look At? An Analysis of BERT's Attention",
                        "Citation Paper Abstract": "Abstract:Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.",
                        "Citation Paper Authors": "Authors:Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ", who leveraged BERT. For constituency p arsing, Kim et al. ",
                    "Citation Text": "Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Le e. Are pre-trained language models aware of phrases?\nsimple but strong baselines for grammar induction. In International Conference on Learning Representations ,\n2020. URL https://openreview.net/forum?id=H1xPR3NtPB .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.00737",
                        "Citation Paper Title": "Title:Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction",
                        "Citation Paper Abstract": "Abstract:With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.",
                        "Citation Paper Authors": "Authors:Taeuk Kim, Jihun Choi, Daniel Edmiston, Sang-goo Lee"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", who employed neural mach ine translation models, and Htut et al. ",
                    "Citation Text": "Phu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R. B owman. Do attention heads in BERT track syntactic\ndependencies? CoRR , abs/1911.12246, 2019. URL http://arxiv.org/abs/1911.12246 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.12246",
                        "Citation Paper Title": "Title:Do Attention Heads in BERT Track Syntactic Dependencies?",
                        "Citation Paper Abstract": "Abstract:We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods---taking the maximum attention weight and computing the maximum spanning tree---to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the semantics-oriented MNLI---to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.",
                        "Citation Paper Authors": "Authors:Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "and adv anced formulations like the Compound Probabilistic\nContext Free Grammar (PCFG) model ",
                    "Citation Text": "Yoon Kim, Chris Dyer, and Alexander Rush. Compound prob abilistic context-free grammars for grammar in-\nduction. In Proceedings of the 57th Annual Meeting of the Association fo r Computational Linguistics , pages\n2369\u20132385, Florence, Italy, July 2019. Association for Com putational Linguistics. doi:10.18653/v1/P19-1228.\nURLhttps://aclanthology.org/P19-1228 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.10225",
                        "Citation Paper Title": "Title:Compound Probabilistic Context-Free Grammars for Grammar Induction",
                        "Citation Paper Abstract": "Abstract:We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context-free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our grammar's rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized out with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods when evaluated on unsupervised parsing.",
                        "Citation Paper Authors": "Authors:Yoon Kim, Chris Dyer, Alexander M. Rush"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11062v1": {
            "Paper Title": "Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11043v1": {
            "Paper Title": "TDeLTA: A Light-weight and Robust Table Detection Method based on\n  Learning Text Arrangement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11036v1": {
            "Paper Title": "UniGen: A Unified Generative Framework for Retrieval and Question\n  Answering with Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.11374v3": {
            "Paper Title": "DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models\n  for Emotion Recognition in Conversations",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "applies contrast-aware\nadversarial training to generate worst-case\nsamples and uses a joint class-spread con-\ntrastive learning objective on both original\nand adversarial samples.\n\u2022LLMs based approaches:\n(1) LLaMA ",
                    "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. 2023. Llama: Open and\nefficient foundation language models. arXiv preprint\narXiv:2302.13971 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "is a module adept at\nmodeling context and interaction information\nin ERC tasks with high efficiency. It leverages\nmultiple extractors and PairCC strategies to\neffectively tackle the heterogeneity present in\nmultimodal fusion.\n(15) SACL-LSTM ",
                    "Citation Text": "Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, and\nSonglin Hu. 2023. Supervised adversarial contrastive\nlearning for emotion recognition in conversations.\narXiv preprint arXiv:2306.01505 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.01505",
                        "Citation Paper Title": "Title:Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations",
                        "Citation Paper Abstract": "Abstract:Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model's context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.",
                        "Citation Paper Authors": "Authors:Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, Songlin Hu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is a conversational interactive\nmodel to mitigate the problem of overlook-\ning the immediate mutual interaction between\ndifferent speakers by applying state mutual\ninteraction within history contexts.\n(14) GraphCFC ",
                    "Citation Text": "Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang\nZeng. 2022. Graphcfc: A directed graph based cross-\nmodal feature complementation approach for multi-\nmodal conversational emotion recognition. CoRR ,\nabs/2207.12261.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.12261",
                        "Citation Paper Title": "Title:GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. We extract various types of edges from the constructed graph for encoding, thus enabling GNNs to extract crucial contextual and interactive information more accurately when performing message passing. Furthermore, we design a GNN structure called GAT-MLP, which can provide a new unified network framework for multimodal learning. The experimental results on two benchmark datasets show that our GraphCFC outperforms the state-of-the-art (SOTA) approaches.",
                        "Citation Paper Authors": "Authors:Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "designs a method\nbased on recurrent neural networks (RNN)\nthat keeps track of the individual party states\nthroughout the conversation and uses this in-\nformation for emotion classification.\n(8) DialogueGCN ",
                    "Citation Text": "Deepanway Ghosal, Navonil Majumder, Soujanya\nPoria, Niyati Chhaya, and Alexander Gelbukh. 2019.\nDialoguegcn: A graph convolutional neural net-\nwork for emotion recognition in conversation. arXiv\npreprint arXiv:1908.11540 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.11540",
                        "Citation Paper Title": "Title:DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.",
                        "Citation Paper Authors": "Authors:Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, Alexander Gelbukh"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "leverages self and inter-\nspeaker dependency of the interlocutors to\nmodel conversational context for emotion\nrecognition.\n(9) DialogueCRN ",
                    "Citation Text": "Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021.\nDialoguecrn: Contextual reasoning networks for\nemotion recognition in conversations. arXiv preprint\narXiv:2106.01978 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.01978",
                        "Citation Paper Title": "Title:DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
                        "Citation Paper Abstract": "Abstract:Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.",
                        "Citation Paper Authors": "Authors:Dou Hu, Lingwei Wei, Xiaoyong Huai"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "presented the conversational memory\nnetwork (CMN), which harnessed contextual infor-\nmation from the conversation history to improve\nERC. Another approach by Majumder et al. ",
                    "Citation Text": "Navonil Majumder, Soujanya Poria, Devamanyu\nHazarika, Rada Mihalcea, Alexander Gelbukh, and\nErik Cambria. 2019. Dialoguernn: An attentive rnn\nfor emotion detection in conversations. In Proceed-\nings of the AAAI conference on artificial intelligence ,\nvolume 33, pages 6818\u20136825.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00405",
                        "Citation Paper Title": "Title:DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
                        "Citation Paper Abstract": "Abstract:Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, etc. Currently, systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state of the art by a significant margin on two different datasets.",
                        "Citation Paper Authors": "Authors:Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, Erik Cambria"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "modifies the recurrence\nmechanism of XLNet to store longer historical\ncontext and dialog-aware self-attention to deal\nwith the multi-party structures.\n(5) TODKAT ",
                    "Citation Text": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou,\nand Yulan He. 2021. Topic-driven and knowledge-\naware transformer for dialogue emotion detection.\nInProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1571\u20131582, Online. Association for Computational\nLinguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.01071",
                        "Citation Paper Title": "Title:Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
                        "Citation Paper Abstract": "Abstract:Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories.",
                        "Citation Paper Authors": "Authors:Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, Yulan He"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "hierarchically models the self-\nand inter-speaker emotional influences into\nglobal memories, and generates contextual\nsummaries.\n(3) MTL ",
                    "Citation Text": "Jingye Li, Meishan Zhang, Donghong Ji, and Yi-\njiang Liu. 2020. Multi-task learning with auxil-\niary speaker identification for conversational emotion\nrecognition. ArXiv , abs/2003.01478.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01478",
                        "Citation Paper Title": "Title:Multi-Task Learning with Auxiliary Speaker Identification for Conversational Emotion Recognition",
                        "Citation Paper Abstract": "Abstract:Conversational emotion recognition (CER) has attracted increasing interests in the natural language processing (NLP) community. Different from the vanilla emotion recognition, effective speaker-sensitive utterance representation is one major challenge for CER. In this paper, we exploit speaker identification (SI) as an auxiliary task to enhance the utterance representation in conversations. By this method, we can learn better speaker-aware contextual representations from the additional SI corpus. Experiments on two benchmark datasets demonstrate that the proposed architecture is highly effective for CER, obtaining new state-of-the-art results on two datasets.",
                        "Citation Paper Authors": "Authors:Jingye Li, Meishan Zhang, Donghong Ji, Yijiang Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "introduced the DialogueRNN model, which metic-\nulously monitored the states of individual partic-\nipants throughout the conversation, utilizing this\ninformation for ERC. In terms of multimodal ad-\nvancements, Poria et al. ",
                    "Citation Text": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2018. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations.\narXiv preprint arXiv:1810.02508 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.02508",
                        "Citation Paper Title": "Title:MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
                        "Citation Paper Abstract": "Abstract:Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http:// this http URL.",
                        "Citation Paper Authors": "Authors:Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, Rada Mihalcea"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "combined self-\nattention with BERT to model intra-modal corre-\nspondence and optimal transport for cross-modal\ncorrespondence, aiming to discover sarcasm and\nhumor. Lei et al. ",
                    "Citation Text": "Shanglin Lei, Guanting Dong, Xiaoping Wang,\nKeheng Wang, and Sirui Wang. 2023. Instruc-\nterc: Reforming emotion recognition in conversation\nwith a retrieval multi-task llms framework. CoRR ,\nabs/2309.11911.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.11911",
                        "Citation Paper Title": "Title:InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework",
                        "Citation Paper Abstract": "Abstract:The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely\nInstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based plug-and-play plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provide empirical guidance for applying InstructERC in practical scenarios. Our code will be released after blind review.",
                        "Citation Paper Authors": "Authors:Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "built\na mutual-enhanced incongruity learning network\nupon RoBERTa and graph convolution networks to\nidentify sarcasm. Pramanick ",
                    "Citation Text": "Shraman Pramanick, Aniket Roy, and Vishal M Pa-\ntel. 2022. Multimodal learning using optimal trans-\nport for sarcasm and humor detection. In Proceed-\nings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision , pages 3930\u20133940.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.10949",
                        "Citation Paper Title": "Title:Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection",
                        "Citation Paper Abstract": "Abstract:Multimodal learning is an emerging yet challenging research area. In this paper, we deal with multimodal sarcasm and humor detection from conversational videos and image-text pairs. Being a fleeting action, which is reflected across the modalities, sarcasm detection is challenging since large datasets are not available for this task in the literature. Therefore, we primarily focus on resource-constrained training, where the number of training samples is limited. To this end, we propose a novel multimodal learning system, MuLOT (Multimodal Learning using Optimal Transport), which utilizes self-attention to exploit intra-modal correspondence and optimal transport for cross-modal correspondence. Finally, the modalities are combined with multimodal attention fusion to capture the inter-dependencies across modalities. We test our approach for multimodal sarcasm and humor detection on three benchmark datasets - MUStARD (video, audio, text), UR-FUNNY (video, audio, text), MST (image, text) and obtain 2.1%, 1.54%, and 2.34% accuracy improvements over state-of-the-art.",
                        "Citation Paper Authors": "Authors:Shraman Pramanick, Aniket Roy, Vishal M. Patel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11020v1": {
            "Paper Title": "Information Type Classification with Contrastive Task-Specialized\n  Sentence Encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11011v1": {
            "Paper Title": "VinaLLaMA: LLaMA-based Vietnamese Foundation Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.15786v4": {
            "Paper Title": "YUAN 2.0: A Large Language Model with Localized Filtering-based\n  Attention",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "applies the Evol -Instruct to evolve  CodeAlpaca -20k into a dataset consisting  of approximately 78k \nsamples. Phi -1 ",
                    "Citation Text": "Gunasekar, Suriya, et al. \"Textbooks Are All You Need.\" arXiv preprint arXiv:2306.11644 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.11644",
                        "Citation Paper Title": "Title:Textbooks Are All You Need",
                        "Citation Paper Abstract": "Abstract:We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
                        "Citation Paper Authors": "Authors:Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "builds a code \ninstruction -following dataset terms as CodeAlpaca -20k. WizardLM ",
                    "Citation Text": "Xu, Can, et al. \"Wizardlm: Empowering large language models to follow complex instructions.\"  arXiv preprint \narXiv:2304.12244  (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.12244",
                        "Citation Paper Title": "Title:WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                        "Citation Paper Abstract": "Abstract:Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",
                        "Citation Paper Authors": "Authors:Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "2.1 From Yuan 1.0 to Yuan 2.0  \n \nYuan 1.0 with 245B parameters is unveiled 2 years ago ",
                    "Citation Text": "Wu, Shaohua, et al. \"Yuan 1.0: Large -scale pre -trained language model in zero -shot and few -shot learning.\" arXiv \npreprint arXiv:2110.04725 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.04725",
                        "Citation Paper Title": "Title:Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",
                        "Citation Paper Authors": "Authors:Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.13177v2": {
            "Paper Title": "How to Evaluate the Generalization of Detection? A Benchmark for\n  Comprehensive Open-Vocabulary Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10961v1": {
            "Paper Title": "Aspect-Based Sentiment Analysis with Explicit Sentiment Augmentations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10952v1": {
            "Paper Title": "Soft Alignment of Modality Space for End-to-end Speech Translation",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "which is a self-supervised pre-trained model. The textual\nencoder and decoder contain 6 layers which are initialized by the\npre-trained MT model. The MT training process was followed by\nYeet al. ",
                    "Citation Text": "Rong Ye, Mingxuan Wang, and Lei Li, \u201cCross-modal con-\ntrastive learning for speech translation,\u201d in Proc. of NAACL ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.02444",
                        "Citation Paper Title": "Title:Cross-modal Contrastive Learning for Speech Translation",
                        "Citation Paper Abstract": "Abstract:How can we learn unified representations for spoken utterances and their written text? Learning similar representations for semantically similar speech and text is important for speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms the previous methods on, and achieves an average BLEU of 29.4. The analysis further verifies that ConST indeed closes the representation gap of different modalities -- its learned representation improves the accuracy of cross-modal speech-text retrieval from 4% to 88%. Code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Rong Ye, Mingxuan Wang, Lei Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10945v1": {
            "Paper Title": "LaViP:Language-Grounded Visual Prompts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08583v2": {
            "Paper Title": "ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric\n  Strategy for Diverse Generative Tasks",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": ".\nAlongside FGQ or CGQ, specific algorithms are employed for precision mapping in quantization. Given\nthe focus on 4-bit quantization and the demonstrated efficacy of the INT4 format over FP4 (as detailed in the\nappendix) ",
                    "Citation Text": "Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8\nquantization using floating-point formats. arXiv preprint arXiv:2307.09782 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.09782",
                        "Citation Paper Title": "Title:ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats",
                        "Citation Paper Abstract": "Abstract:In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.",
                        "Citation Paper Authors": "Authors:Xiaoxia Wu, Zhewei Yao, Yuxiong He"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "and their open-source implementation5, we adapted non-greedy\ngeneration settings (n=20, t=0.2, top_p=0.95). To mitigate variance, nine random seeds {111,222,...,\n888, 1111} were employed. The models evaluated included CodeGeeX2-6B, StarCoder-15B ",
                    "Citation Text": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh\nShliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi,\nJian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman,\nSiva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy,\nUrvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson,\nBrendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite,\nCarlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm\nde Vries. Starcoder: may the source be with you! 2305.06161 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.06161",
                        "Citation Paper Title": "Title:StarCoder: may the source be with you!",
                        "Citation Paper Abstract": "Abstract:The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
                        "Citation Paper Authors": "Authors:Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ", The\nexperiments were deterministic, using the seed 123.\n\u2022Code Generation. Following ",
                    "Citation Text": "Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi\nWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation\nwith multilingual evaluations on humaneval-x. In KDD, 2023.\nA Background of Quantization\nThroughout this work, we focus on post-training quantization (PTQ), i.e., no or minimal training effort\nis applied after quantization, for which large accuracy degradation usually exhibits for coarse-grained\nquantization (per matrix/tensor) due to their large quantization error. Particularly, we use the per-row\nquantization (one row of the weight matrix) from",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.17568",
                        "Citation Paper Title": "Title:CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X",
                        "Citation Paper Abstract": "Abstract:Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at this https URL.",
                        "Citation Paper Authors": "Authors:Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "). Calibration for GPTQ used 128 (32) samples for LLaMa-1B/13B (-65B) models.4.\nWe believe the results generalize to other models family such sh OPT ",
                    "Citation Text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01068",
                        "Citation Paper Title": "Title:OPT: Open Pre-trained Transformer Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                        "Citation Paper Authors": "Authors:Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.01040v2": {
            "Paper Title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.03453v4": {
            "Paper Title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large\n  Language Model Signals for Science Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.09936v3": {
            "Paper Title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual\n  Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.08189v3": {
            "Paper Title": "All Data on the Table: Novel Dataset and Benchmark for Cross-Modality\n  Scientific Information Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.10144v2": {
            "Paper Title": "ExpeL: LLM Agents Are Experiential Learners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10897v1": {
            "Paper Title": "Generalized Category Discovery with Large Language Models in the Loop",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09494v2": {
            "Paper Title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.03360v2": {
            "Paper Title": "Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10868v1": {
            "Paper Title": "From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the\n  Generative Artificial Intelligence (AI) Research Landscape",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08579v2": {
            "Paper Title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.14865v2": {
            "Paper Title": "Improving Cross-Domain Hate Speech Generalizability with Emotion\n  Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.07723v3": {
            "Paper Title": "Generalization Analogies: A Testbed for Generalizing AI Oversight to\n  Hard-To-Measure Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10813v1": {
            "Paper Title": "Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model\n  within 0.5K Parameters",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "to use serial linear lay-\ners and activation functions to adapt for downstream\ntasks. It is simple yet effective in few-shot learning.\n5.ProGrad ",
                    "Citation Text": "Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Han-\nwang Zhang. Prompt-aligned gradient for prompt tuning. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 15659\u201315669, 2023. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.14865",
                        "Citation Paper Title": "Title:Prompt-aligned Gradient for Prompt Tuning",
                        "Citation Paper Abstract": "Abstract:Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we can craft a zero-shot classifier by \"prompt\", e.g., the confidence score of an image being \"[CLASS]\" can be obtained by using the VLM provided similarity measure between the image and the prompt sentence \"a photo of a [CLASS]\". Therefore, prompt shows a great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the prompt-based similarity measure. However, we find a common failure that improper fine-tuning may not only undermine the prompt's inherent prediction for the task-related classes, but also for other classes in the VLM vocabulary. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompt. We present Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from forgetting the the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the \"general direction\", which is represented as the gradient of the KL loss of the pre-defined prompt prediction. Extensive experiments demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", one dataset for the action recog-\nnition, i.e., UCF101 ",
                    "Citation Text": "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402 , 2012. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.0402",
                        "Citation Paper Title": "Title:UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
                        "Citation Paper Abstract": "Abstract:We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.",
                        "Citation Paper Authors": "Authors:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10771v1": {
            "Paper Title": "kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest\n  Neighbor In-Context Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10770v1": {
            "Paper Title": "Identification of Knowledge Neurons in Protein Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.14196v3": {
            "Paper Title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10750v1": {
            "Paper Title": "Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and\n  Statistical Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10748v1": {
            "Paper Title": "Multi-Label Classification of COVID-Tweets Using Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10746v1": {
            "Paper Title": "Knowledge Trees: Gradient Boosting Decision Trees on Knowledge Neurons\n  as Probing Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10730v1": {
            "Paper Title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.08152v2": {
            "Paper Title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review\n  Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10700v1": {
            "Paper Title": "Cross-Domain Robustness of Transformer-based Keyphrase Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10679v1": {
            "Paper Title": "Bengali Intent Classification with Generative Adversarial BERT",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". A solitary\nGAN model is enhanced by a semi-supervised GAN ",
                    "Citation Text": "T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and\nX. Chen, \u201cImproved techniques for training gans,\u201d Advances in neural\ninformation processing systems , vol. 29, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "is the first fine-tuned model that can\nbe applied to a variety of downstream tasks, including sen-\ntiment analysis, categorization, and question-answering. With\nan attention mechanism ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in\nneural information processing systems , vol. 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "used it to detect\nfake news and hate speech in Bengali, achieving 75.4% and\n92.6% accuracy on the task while addressing the issue of a\nsmall number of data in the datasets. In the article ",
                    "Citation Text": "M. T. R. Shawon, G. M. Shahariar, F. M. Shah, M. S. Alam, and\nM. S. Mahbub, \u201cBengali fake review detection using semi-supervised\ngenerative adversarial networks,\u201d in 2023 5th International Conference\non Natural Language Processing (ICNLP) , 2023, pp. 12\u201316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.02739",
                        "Citation Paper Title": "Title:Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:This paper investigates the potential of semi-supervised Generative Adversarial Networks (GANs) to fine-tune pretrained language models in order to classify Bengali fake reviews from real reviews with a few annotated data. With the rise of social media and e-commerce, the ability to detect fake or deceptive reviews is becoming increasingly important in order to protect consumers from being misled by false information. Any machine learning model will have trouble identifying a fake review, especially for a low resource language like Bengali. We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews as the experimental results suggest that even with only 1024 annotated samples, BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and a f1-score of 84.89% outperforming other pretrained language models - BanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and 10% respectively in terms of accuracy. The experiments were conducted on a manually labeled food review dataset consisting of total 6014 real and fake reviews collected from various social media groups. Researchers that are experiencing difficulty recognizing not just fake reviews but other classification issues owing to a lack of labeled data may find a solution in our proposed methodology.",
                        "Citation Paper Authors": "Authors:Md. Tanvir Rouf Shawon, G. M. Shahariar, Faisal Muhammad Shah, Mohammad Shafiul Alam, Md. Shahriar Mahbub"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.15494v2": {
            "Paper Title": "ETHER: Aligning Emergent Communication for Hindsight Experience Replay",
            "Sentences": [
                {
                    "Sentence ID": 68,
                    "Sentence": "with 1024 hidden units, which feeds into the\nadvantage and value heads of a 1-layer dueling network ",
                    "Citation Text": "Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network\narchitectures for deep reinforcement learning. In International conference on machine learning ,\npages 1995\u20132003. PMLR, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06581",
                        "Citation Paper Title": "Title:Dueling Network Architectures for Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
                        "Citation Paper Authors": "Authors:Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "and notation in Section 2. After delineating our\nmethods in Section 3.1, we present experimental results on the PickUpDist instruction-following\ntask of the BabyAI benchmark ",
                    "Citation Text": "M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. H. Nguyen, and\nY . Bengio. BabyAI: First Steps Towards Grounded Language Learning With a Human In the\nLoop. oct 2018. URL http://arxiv.org/abs/1810.08272 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08272",
                        "Citation Paper Title": "Title:BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
                        "Citation Paper Abstract": "Abstract:Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.",
                        "Citation Paper Authors": "Authors:Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ", or because they have been\nfound to further some forms of disentanglement [ 26,34,12,44] in learned representations [ 71,17].\nSuch properties can enable \u201cbetter up-stream performance\u201d ",
                    "Citation Text": "S. van Steenkiste, F. Locatello, J. Schmidhuber, and O. Bachem. Are disentangled representa-\ntions helpful for abstract visual reasoning? May 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12506",
                        "Citation Paper Title": "Title:Are Disentangled Representations Helpful for Abstract Visual Reasoning?",
                        "Citation Paper Abstract": "Abstract:A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",
                        "Citation Paper Authors": "Authors:Sjoerd van Steenkiste, Francesco Locatello, J\u00fcrgen Schmidhuber, Olivier Bachem"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", defined by Q\u03c0(s, a, g ) =E\u03c0[P\nt\u03b3tr(st, at, st+1, g)|s0=s, a0=a, st+1\u223cT]\nfor all (s, a, g )\u2208 S \u00d7 A \u00d7 G . While previous works makes use of Deep Q-learning (DQN) ",
                    "Citation Text": "V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Ried-\nmiller. Playing atari with deep reinforcement learning. CoRR , abs/1312.5602, 2013. URL\nhttp://arxiv.org/abs/1312.5602 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.5602",
                        "Citation Paper Title": "Title:Playing Atari with Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
                        "Citation Paper Authors": "Authors:Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10665v1": {
            "Paper Title": "Silkie: Preference Distillation for Large Visual Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07398v2": {
            "Paper Title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10645v1": {
            "Paper Title": "FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph\n  Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11556v1": {
            "Paper Title": "StarVector: Generating Scalable Vector Graphics Code from Images",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "use an interme-\ndiate mapping module to convert image features into fixed-\nsize token embeddings. Similar to ours, Llava ",
                    "Citation Text": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\n2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08485",
                        "Citation Paper Title": "Title:Visual Instruction Tuning",
                        "Citation Paper Abstract": "Abstract:Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                        "Citation Paper Authors": "Authors:Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "have\nbeen also explored [25, 57, 64], that aim to preserve\nmore detailed features from images. Some models like\nFlamingo ",
                    "Citation Text": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems , 35:23716\u201323736,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.14198",
                        "Citation Paper Title": "Title:Flamingo: a Visual Language Model for Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": "uses a CLIP distance\nloss to iteratively refine SVG from sketches. Both these\nsolutions can be slow due to their iterative nature. Similar\nto ours, IconShop ",
                    "Citation Text": "Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Icon-\nshop: Text-based vector icon synthesis with autoregressive\ntransformers. arXiv preprint arXiv:2304.14400 , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.14400",
                        "Citation Paper Title": "Title:IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers",
                        "Citation Paper Abstract": "Abstract:Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text -> raster image -> vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text -> vector graphics script) through pretrained large language models. However, these methods still suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to fully exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively and qualitatively. Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion.",
                        "Citation Paper Authors": "Authors:Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "leverages a\nstrong text-to-image diffusion model to find the SVG via\niterative optimization. CLIPasso ",
                    "Citation Text": "Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:\nSemantically-aware object sketching. ACM Transactions on\nGraphics (TOG) , 41(4):1\u201311, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05822",
                        "Citation Paper Title": "Title:CLIPasso: Semantically-Aware Object Sketching",
                        "Citation Paper Abstract": "Abstract:Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "Citation Paper Authors": "Authors:Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "translates pixel images into latent representations, which\n1https://en.wikipedia.org/wiki/Comparison_of_\nraster-to-vector_conversion_software\n2Figure 2. StarVector architecture: Images in the pixel space are encoded into a set of 2D embeddings using CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10638v1": {
            "Paper Title": "HyperPIE: Hyperparameter Information Extraction from Scientific\n  Publications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10617v1": {
            "Paper Title": "Deep dive into language traits of AI-generated Abstracts",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". Our\ncomprehensive analysis aligns with the recent advocacy by Lin et\nal. ",
                    "Citation Text": "Yu-Chen Lin, Si-An Chen, Jie-Jyun Liu, and Chih-Jen Lin. 2023. Linear Clas-\nsifier: An Often-Forgotten Baseline for Text Classification. arXiv preprint\narXiv:2306.07111 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.07111",
                        "Citation Paper Title": "Title:Linear Classifier: An Often-Forgotten Baseline for Text Classification",
                        "Citation Paper Abstract": "Abstract:Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.",
                        "Citation Paper Authors": "Authors:Yu-Chen Lin, Si-An Chen, Jie-Jyun Liu, Chih-Jen Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11554v1": {
            "Paper Title": "Deciphering Compatibility Relationships with Textual Descriptions via\n  Extraction and Explanation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11553v1": {
            "Paper Title": "SeGA: Preference-Aware Self-Contrastive Learning with Prompts for\n  Anomalous User Detection on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10610v1": {
            "Paper Title": "Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question\n  Answering and Summarization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16180v1": {
            "Paper Title": "Investigating salient representations and label Variance in Dimensional\n  Speech Emotion Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.13071v2": {
            "Paper Title": "Machine-Created Universal Language for Cross-lingual Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10579v1": {
            "Paper Title": "DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural\n  Network for Multimodal Dialogue Emotion Recognition",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "performed a spatiotemporal graph neural network\n(STG-Net), which masks graph nodes based on an edge\nweighting strategy. GCN is used to reconstruct contextual\nfeatures to obtain a better data representation. Wang et\nal. ",
                    "Citation Text": "X. Wang, N. Liu, H. Han, and C. Shi, \u201cSelf-supervised hetero-\ngeneous graph neural network with co-contrastive learning,\u201d in\nProceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery &amp; Data Mining . ACM, 2021, p. 1726\u20131736.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.09111",
                        "Citation Paper Title": "Title:Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning",
                        "Citation Paper Abstract": "Abstract:Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-viewcontrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures simultaneously. Then the cross-view contrastive learning, as well as a view mask mechanism, is proposed, which is able to extract the positive and negative embeddings from two views. This enables the two views to collaboratively supervise each other and finally learn high-level node embeddings. Moreover, two extensions of HeCo are designed to generate harder negative samples with high quality, which further boosts the performance of HeCo. Extensive experiments conducted on a variety of real-world networks show the superior performance of the proposed methods over the state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Xiao Wang, Nian Liu, Hui Han, Chuan Shi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.03018v2": {
            "Paper Title": "Zero Resource Code-switched Speech Benchmark Using Speech Utterance\n  Pairs For Multiple Spoken Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.01261v3": {
            "Paper Title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised\n  representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10523v1": {
            "Paper Title": "Paloma: A Benchmark for Evaluating Language Model Fit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12458v1": {
            "Paper Title": "When Parameter-efficient Tuning Meets General-purpose Vision-language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ".) and four\nparameter-efficient tuning methods ( i.e. Head Tuning ",
                    "Citation Text": "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. arXiv preprint arXiv:2106.10199 ,\n2021. 5, 9\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10199",
                        "Citation Paper Title": "Title:BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
                        "Citation Paper Abstract": "Abstract:We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
                        "Citation Paper Authors": "Authors:Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", an advanced iteration of OKVQA. It in-\ncludes 17K training, 1K validation, and 6K test questions,\neach designed to push the boundaries of a model\u2019s reasoning\ncapabilities. (5) TextVQA ",
                    "Citation Text": "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 8309\u20138318, 2019. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08920",
                        "Citation Paper Title": "Title:Towards VQA Models That Can Read",
                        "Citation Paper Abstract": "Abstract:Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new \"TextVQA\" dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.",
                        "Citation Paper Authors": "Authors:Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "presents a unique set of visual questions that demand\nexternal knowledge for accurate responses. The dataset is\npartitioned into 9K training and 5K test samples. (4) A-\nOKVQA ",
                    "Citation Text": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world knowl-\nedge. In European Conference on Computer Vision , 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.01718",
                        "Citation Paper Title": "Title:A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
                        "Citation Paper Abstract": "Abstract:The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models. Project page: this http URL",
                        "Citation Paper Authors": "Authors:Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "features 28K images derivedfrom the open images database, and each image is com-\nplemented by five human-generated captions, emphasizing\ntextual descriptions within the visual content. (3) OKVQA ",
                    "Citation Text": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 3190\u20133199, 2019. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00067",
                        "Citation Paper Title": "Title:OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
                        "Citation Paper Abstract": "Abstract:Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See this http URL to download and browse the dataset.",
                        "Citation Paper Authors": "Authors:Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "comprises 31K images sourced from Flickr, each accom-\npanied by five reference captions provided by human an-\nnotators, enriching the dataset with varied descriptive per-\nspectives. (2) TextCaps ",
                    "Citation Text": "Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image captioning\nwith reading comprehension. ArXiv , abs/2003.12462, 2020.\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12462",
                        "Citation Paper Title": "Title:TextCaps: a Dataset for Image Captioning with Reading Comprehension",
                        "Citation Paper Abstract": "Abstract:Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.",
                        "Citation Paper Authors": "Authors:Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", we initialize Q-Former with the pre-trained weights\nofBERT base ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10479v1": {
            "Paper Title": "A Soft Contrastive Learning-based Prompt Model for Few-shot Sentiment\n  Analysis",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ") for calculating input\nrepresentations. Some recent studies try to apply large foun-\ndation models such as ChatGPT to sentiment analysis with\nzero or few samples given in the prompt ",
                    "Citation Text": "Mostafa M. Amin, E. Cambria, and Bj \u00a8orn Schuller, \u201cWill af-\nfective computing emerge from foundation models and general\nai? a first evaluation on chatgpt,\u201d ArXiv , vol. abs/2303.03186,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.03186",
                        "Citation Paper Title": "Title:Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
                        "Citation Paper Abstract": "Abstract:ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilise three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results, and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where Word2Vec models achieve worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialised training, however, it is not as good as a specialised model for a downstream task.",
                        "Citation Paper Authors": "Authors:Mostafa M. Amin, Erik Cambria, Bj\u00f6rn W. Schuller"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "are used to infer\nthe sentiment. The current SOTA methods make use of pre-\ntrained language models (e.g., BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, \u201cBERT: Pre-training of deep bidirectional trans-\nformers for language understanding,\u201d in Proceedings of NAACL ,\nJune 2019, pp. 4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10466v1": {
            "Paper Title": "RIGHT: Retrieval-augmented Generation for Mainstream Hashtag\n  Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10448v1": {
            "Paper Title": "Resolving Crash Bugs via Large Language Models: An Empirical Study",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": "J. Li, G. Li, Y . Li, and Z. Jin, \u201cStructured chain-of-thought prompting\nfor code generation,\u201d arXiv preprint arXiv:2305.06599 , 2023. ",
                    "Citation Text": "X. Zhou, G. Li, and Z. Liu, \u201cLlm as dba,\u201d arXiv preprint\narXiv:2308.05481 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.05481",
                        "Citation Paper Title": "Title:LLM As DBA",
                        "Citation Paper Abstract": "Abstract:Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at this http URL.",
                        "Citation Paper Authors": "Authors:Xuanhe Zhou, Guoliang Li, Zhiyuan Liu"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y . Choi,\nand H. Hajishirzi, \u201cGenerated knowledge prompting for commonsense\nreasoning,\u201d arXiv preprint arXiv:2110.08387 , 2022. ",
                    "Citation Text": "C. Liu, X. Bao, H. Zhang, N. Zhang, H. Hu, X. Zhang, and\nM. Yan, \u201cImproving chatgpt prompt for code generation,\u201d arXiv preprintarXiv:2305.08360 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.08360",
                        "Citation Paper Title": "Title:Improving ChatGPT Prompt for Code Generation",
                        "Citation Paper Abstract": "Abstract:Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements. Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation. However, the effectiveness of ChatGPT for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt. To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation. We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations. Our results showed that by carefully designing prompts to guide ChatGPT, the generation performance can be improved substantially. We also analyzed the factors that influenced the prompt design and provided insights that could guide future research.",
                        "Citation Paper Authors": "Authors:Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu, Xiaohong Zhang, Meng Yan"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\nK. Narasimhan, \u201cTree of thoughts: Deliberate problem solving with large\nlanguage models,\u201d arXiv preprint arXiv:2305.10601 , 2023. ",
                    "Citation Text": "J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y . Choi,\nand H. Hajishirzi, \u201cGenerated knowledge prompting for commonsense\nreasoning,\u201d arXiv preprint arXiv:2110.08387 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.08387",
                        "Citation Paper Title": "Title:Generated Knowledge Prompting for Commonsense Reasoning",
                        "Citation Paper Abstract": "Abstract:It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, Hannaneh Hajishirzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2306.15222v2": {
            "Paper Title": "Learning to Rank in Generative Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10432v1": {
            "Paper Title": "From Dialogue to Diagram: Task and Relationship Extraction from Natural\n  Language for Accelerated Business Process Prototyping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.04666v3": {
            "Paper Title": "Pre-training LLMs using human-like development data corpus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.10512v2": {
            "Paper Title": "IMAD: IMage-Augmented multi-modal Dialogue",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": "model for experiments as it is one of the best open source models utilising both\nvisual and text modalities and has a handy interface in LA VIS library ",
                    "Citation Text": "Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C. H. Hoi. Lavis: A library for\nlanguage-vision intelligence, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.09019",
                        "Citation Paper Title": "Title:LAVIS: A Library for Language-Vision Intelligence",
                        "Citation Paper Abstract": "Abstract:We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks. The library is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, Steven C.H. Hoi"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", so images will correspond to\nrelevant phrases. The same idea was used in BLIP ",
                    "Citation Text": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.12086",
                        "Citation Paper Title": "Title:BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "scores compared to generated responses that were not\nconditioned on image data.\nAnother study ",
                    "Citation Text": "Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin Choi, and Sung-Hyun Myaeng. Constructing multi-modal\ndialogue dataset by replacing text with semantically relevant images, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.08685",
                        "Citation Paper Title": "Title:Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images",
                        "Citation Paper Abstract": "Abstract:In multi-modal dialogue systems, it is important to allow the use of images as part of a multi-turn conversation. Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images, but such datasets rarely exist. In response, this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. Our method to create such a dataset consists of (1) preparing and pre-processing text dialogue datasets, (2) creating image-mixed dialogues by using a text-to-image replacement technique, and (3) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset. To evaluate the validity of our dataset, we devise a simple retrieval model for dialogue sentence prediction tasks. Automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner. Our dataset and generation code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Nyoungwoo Lee, Suwon Shin, Jaegul Choo, Ho-Jin Choi, Sung-Hyun Myaeng"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", which\nuses image embeddings to align with text embeddings for image-text matching loss ",
                    "Citation Text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03748",
                        "Citation Paper Title": "Title:Representation Learning with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Oriol Vinyals"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10375v1": {
            "Paper Title": "Collect and Connect Data Leaves to Feature Concepts: Interactive Graph\n  Generation Toward Well-being",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10371v1": {
            "Paper Title": "K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via\n  Prompt Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10358v1": {
            "Paper Title": "CONCSS: Contrastive-based Context Comprehension for Dialogue-appropriate\n  Prosody in Conversational Speech Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "to achieve GRU-based\ncontext understanding.\n\u2022M2CTTS : proposed by Xue et al ",
                    "Citation Text": "Jinlong Xue, Yayue Deng, Fengping Wang, Ya Li, Yingming\nGao, Jianhua Tao, Jianqing Sun, and Jiaen Liang, \u201cM2-\nctts: End-to-end multi-scale multi-modal conversational text-\nto-speech synthesis,\u201d in ICASSP 2023-2023 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2023, pp. 1\u20135.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.02269",
                        "Citation Paper Title": "Title:M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:Conversational text-to-speech (TTS) aims to synthesize speech with proper prosody of reply based on the historical conversation. However, it is still a challenge to comprehensively model the conversation, and a majority of conversational TTS systems only focus on extracting global information and omit local prosody features, which contain important fine-grained information like keywords and emphasis. Moreover, it is insufficient to only consider the textual features, and acoustic features also contain various prosody information. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal conversational text-to-speech system, aiming to comprehensively utilize historical conversation and enhance prosodic expression. More specifically, we design a textual context module and an acoustic context module with both coarse-grained and fine-grained modeling. Experimental results demonstrate that our model mixed with fine-grained context information and additionally considering acoustic features achieves better prosody performance and naturalness in CMOS tests.",
                        "Citation Paper Authors": "Authors:Jinlong Xue, Yayue Deng, Fengping Wang, Ya Li, Yingming Gao, Jianhua Tao, Jianqing Sun, Jiaen Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10355v1": {
            "Paper Title": "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.08518v4": {
            "Paper Title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2302.01859v2": {
            "Paper Title": "Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for\n  Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10329v1": {
            "Paper Title": "Perturbation-Invariant Adversarial Training for Neural Ranking Models:\n  Improving the Effectiveness-Robustness Trade-Off",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10321v1": {
            "Paper Title": "LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08726v2": {
            "Paper Title": "Labels Need Prompts Too: Mask Matching for Natural Language\n  Understanding Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08906v2": {
            "Paper Title": "Using eye tracking to investigate what native Chinese speakers notice\n  about linguistic landscape images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.17176v2": {
            "Paper Title": "AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11541v1": {
            "Paper Title": "CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization\n  in Healthcare",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11539v1": {
            "Paper Title": "KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM\n  Does and Doesn't Know",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10259v1": {
            "Paper Title": "CRNNet: Copy Recurrent Neural Network Structure Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10253v1": {
            "Paper Title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10202v1": {
            "Paper Title": "Low-resource classification of mobility functioning information in\n  clinical sentences using large language models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10171v1": {
            "Paper Title": "Pipeline and Dataset Generation for Automated Fact-checking in Almost\n  Any Language",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ".NEIclaims are\nemulated by augmenting the claim facts with extra contexts on top of the provided evi-\ndence. QACG was already used in the context of fact-checking scientific articles ",
                    "Citation Text": "Wright, D., Wadden, D., Lo, K., Kuehl, B., Cohan, A., Augenstein, I., Wang, L.L.:\nGenerating scientific claims for zero-shot scientific fact checking. In: Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 2448\u20132460 (2022)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.12990",
                        "Citation Paper Title": "Title:Generating Scientific Claims for Zero-Shot Scientific Fact Checking",
                        "Citation Paper Abstract": "Abstract:Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its usefulness in zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new supervised method for generating claims supported by the literature, as well as KBIN, a novel method for generating claim negations. Additionally, we adapt an existing unsupervised entity-centric method of claim generation to biomedical claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN, achieve up to 90% performance of fully supervised models trained on manually annotated claims and evidence. A rigorous evaluation study demonstrates significant improvement in generated claim and negation quality over existing baselines",
                        "Citation Paper Authors": "Authors:Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu Wang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ",\nonly several recent publications have covered the topic of multilingual fact-checking.\nX-Fact ",
                    "Citation Text": "Gupta, A., Srikumar, V.: X-fact: A new benchmark dataset for multilingual fact\nchecking. In: Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Nat-\nural Language Processing (Volume 2: Short Papers), pp. 675\u2013682. Association\nfor Computational Linguistics, Online (2021). https://doi.org/10.18653/v1/2021.\nacl-short.86 . https://aclanthology.org/2021.acl-short.86",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09248",
                        "Citation Paper Title": "Title:X-FACT: A New Benchmark Dataset for Multilingual Fact Checking",
                        "Citation Paper Abstract": "Abstract:In this work, we introduce X-FACT: the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40%, suggesting that our dataset is a challenging benchmark for evaluation of multilingual fact-checking models.",
                        "Citation Paper Authors": "Authors:Ashim Gupta, Vivek Srikumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10160v1": {
            "Paper Title": "Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in\n  Chart Captioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10007v1": {
            "Paper Title": "Faithful Persona-based Conversational Dataset Generation with Large\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09966v1": {
            "Paper Title": "Data and Approaches for German Text simplification -- towards an\n  Accessibility-enhanced Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.11844v2": {
            "Paper Title": "How to Use Large Language Models for Text Coding: The Case of Fatherhood\n  Roles in Public Policy Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09932v1": {
            "Paper Title": "RDR: the Recap, Deliberate, and Respond Method for Enhanced Language\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09917v1": {
            "Paper Title": "Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in\n  the US and China",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09907v1": {
            "Paper Title": "Exploring Automatic Text Simplification of German Narrative Documents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.18333v3": {
            "Paper Title": "She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and\n  Sustainable Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09890v1": {
            "Paper Title": "Grammatical information in BERT sentence embeddings as two-dimensional\n  arrays",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11532v1": {
            "Paper Title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided\n  Document Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09211v2": {
            "Paper Title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10126v1": {
            "Paper Title": "Do Text Simplification Systems Preserve Meaning? A Human Evaluation via\n  Reading Comprehension",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09818v1": {
            "Paper Title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09801v1": {
            "Paper Title": "ProCoT: Stimulating Critical Thinking and Writing of Students through\n  Engagement with Large Language Models (LLMs)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.03985v2": {
            "Paper Title": "Dementia Assessment Using Mandarin Speech with an Attention-based Speech\n  Recognition Encoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09736v1": {
            "Paper Title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09718v1": {
            "Paper Title": "Discovering Highly Influential Shortcut Reasoning: An Automated\n  Template-Free Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09670v1": {
            "Paper Title": "Probing Pretrained Language Models with Hierarchy Properties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.01003v2": {
            "Paper Title": "Visual Instruction Tuning with Polite Flamingo",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": ", which requires the model to\ninfer image editing requests ( e.g,Photoshop editing) given\nimage pairs, and 3) Natural Language Visual Reasoning-2\n(NVLR2) ",
                    "Citation Text": "A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y . Artzi, \u201cA cor-\npus for reasoning about natural language grounded in photographs,\u201d\ninProceedings of the 57th Conference of the Association for Com-\nputational Linguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers (A. Korhonen, D. R. Traum, and\nL. M `arquez, eds.), pp. 6418\u20136428, Association for Computational\nLinguistics, 2019. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00491",
                        "Citation Paper Title": "Title:A Corpus for Reasoning About Natural Language Grounded in Photographs",
                        "Citation Paper Abstract": "Abstract:We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
                        "Citation Paper Authors": "Authors:Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "Visual instruction tuning for multi-modal LLM . Re-\nsearch on enabling visual perception for powerful but blind\nLLMs attracted widespread attention recently ",
                    "Citation Text": "S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen,\n\u201cA survey on multimodal large language models,\u201d arXiv preprint\narXiv:2306.13549 , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.13549",
                        "Citation Paper Title": "Title:A Survey on Multimodal Large Language Models",
                        "Citation Paper Abstract": "Abstract:Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.10822v2": {
            "Paper Title": "A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models\n  with Positional Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09625v1": {
            "Paper Title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic\n  Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09601v1": {
            "Paper Title": "Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large\n  Language Models",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". In addition to source code,\nthere are also research efforts on binary code. For example, BinT5\nfinetunes CodeT5 on decompiled code to generate natural language\ndescriptions ",
                    "Citation Text": "Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand Ashok Sawant, Premku-\nmar Devanbu, and Arie van Deursen. Extending source code pre-trained language\nmodels to summarise decompiled binarie. In 2023 IEEE International Conference\non Software Analysis, Evolution and Reengineering (SANER) , pages 260\u2013271. IEEE,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.01701",
                        "Citation Paper Title": "Title:Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries",
                        "Citation Paper Abstract": "Abstract:Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information.\nWhile the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task.\nIn this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model.\nWe first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by generating synthetic datasets and deduplicating the data.\nNext, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully.\nFinally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.",
                        "Citation Paper Authors": "Authors:Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand Ashok Sawant, Premkumar Devanbu, Arie van Deursen"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "pretrains the T5 model with semantics-\nenriched code features, e.g., identifiers, to summarize code. Graph-\nCodeBERT summarizes binary code semantics by modeling the\ndata flow with the BERT model ",
                    "Citation Text": "Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long\nZhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-\ntraining code representations with data flow. arXiv preprint arXiv:2009.08366 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08366",
                        "Citation Paper Title": "Title:GraphCodeBERT: Pre-training Code Representations with Data Flow",
                        "Citation Paper Abstract": "Abstract:Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
                        "Citation Paper Authors": "Authors:Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": ". Recently, the pre-\ndominant focus of this research has been on source code [ 33,35,75]\nAs an example, CodeT5 ",
                    "Citation Text": "Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-\naware unified pre-trained encoder-decoder models for code understanding and\ngeneration. arXiv preprint arXiv:2109.00859 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.00859",
                        "Citation Paper Title": "Title:CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .",
                        "Citation Paper Authors": "Authors:Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": ".\nAutomated Code Summarization. Automated code summariza-\ntion aims at articulating code fragments, typically methods or func-\ntions, using natural language summaries ",
                    "Citation Text": "Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dong-\nmei Zhang, and Hongbin Sun. On the evaluation of neural code summarization.\nInProceedings of the 44th International Conference on Software Engineering , pages\n1597\u20131608, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.07112",
                        "Citation Paper Title": "Title:On the Evaluation of Neural Code Summarization",
                        "Citation Paper Abstract": "Abstract:Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18\\% to +25\\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.",
                        "Citation Paper Authors": "Authors:Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, Hongbin Sun"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "predicts function names by learning\ncalling context and execution behavior. Unlike existing works that\nfocus on BERT-level LLMs, we focus on generative LLMs that ex-\nhibit significantly better generalizability and emergent abilities ",
                    "Citation Text": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\nEmergent abilities of large language models. arXiv preprint arXiv:2206.07682 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.07682",
                        "Citation Paper Title": "Title:Emergent Abilities of Large Language Models",
                        "Citation Paper Abstract": "Abstract:Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
                        "Citation Paper Authors": "Authors:Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.14360v4": {
            "Paper Title": "Is ChatGPT a game changer for geocoding -- a benchmark for geocoding\n  address parsing techniques",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ".\nGiven that address descriptions and formats differ among coun-\ntries ",
                    "Citation Text": "Marouane Yassine, David Beauchemin, Fran\u00e7ois Laviolette, and Luc Lamontagne.\n2021. Leveraging Subword Embeddings for Multinational Address Parsing. In 2020\n6th IEEE Congress on Information Science and Technology (CiSt). IEEE, 353\u2013360.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16152",
                        "Citation Paper Title": "Title:Leveraging Subword Embeddings for Multinational Address Parsing",
                        "Citation Paper Abstract": "Abstract:Address parsing consists of identifying the segments that make up an address such as a street name or a postal code. Because of its importance for tasks like record linkage, address parsing has been approached with many techniques. Neural network methods defined a new state-of-the-art for address parsing. While this approach yielded notable results, previous work has only focused on applying neural networks to achieve address parsing of addresses from one source country. We propose an approach in which we employ subword embeddings and a Recurrent Neural Network architecture to build a single model capable of learning to parse addresses from multiple countries at the same time while taking into account the difference in languages and address formatting systems. We achieved accuracies around 99 % on the countries used for training with no pre-processing nor post-processing needed. We explore the possibility of transferring the address parsing knowledge obtained by training on some countries' addresses to others with no further training in a zero-shot transfer learning setting. We achieve good results for 80 % of the countries (33 out of 41), almost 50 % of which (20 out of 41) is near state-of-the-art performance. In addition, we propose an open-source Python implementation of our trained models.",
                        "Citation Paper Authors": "Authors:Marouane Yassine, David Beauchemin, Fran\u00e7ois Laviolette, Luc Lamontagne"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "has shown better\nparsing performance. In recent years, research has shifted towards\nusing neural networks and LLMs as the foundational framework\nfor building address parsers[ 13,14,27,30,36], given their proven\nsuccess in NER tasks across various domains ",
                    "Citation Text": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,\nand Chris Dyer. 2016. Neural architectures for named entity recognition. arXiv\npreprint arXiv:1603.01360 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01360",
                        "Citation Paper Title": "Title:Neural Architectures for Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.",
                        "Citation Paper Authors": "Authors:Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08274v3": {
            "Paper Title": "High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09545v1": {
            "Paper Title": "GPT-4 Surpassing Human Performance in Linguistic Pragmatics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09542v1": {
            "Paper Title": "Marathon: A Race Through the Realm of Long Context with Large Language\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.07597v2": {
            "Paper Title": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.14132v3": {
            "Paper Title": "CIF-T: A Novel CIF-based Transducer Architecture for Automatic Speech\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09508v1": {
            "Paper Title": "IndicIRSuite: Multilingual Dataset and Neural Information Models for\n  Indian Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.04103v2": {
            "Paper Title": "Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "and Appendix D), so\nwe use GRUs to keep the same settings as the baselines in the\nmain experiments for a fair comparison, but we also conduct\nexperiments with BERT-base-uncased ",
                    "Citation Text": "J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT: pre-training\nof deep bidirectional transformers for language understanding,\u201d in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,2019, Volume 1 (Long and Short Papers) , pp. 4171\u20134186, Association\nfor Computational Linguistics, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", making\nthis kind of generative explanation not trustworthy enough in\nsome high-stakes scenarios. Also, some recent research finds\nthat LLMs are not good at extractive tasks ",
                    "Citation Text": "C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang,\n\u201cIs chatgpt a general-purpose natural language processing task solver?,\u201d\narXiv preprint arXiv:2302.06476 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.06476",
                        "Citation Paper Title": "Title:Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
                        "Citation Paper Abstract": "Abstract:Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.",
                        "Citation Paper Authors": "Authors:Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.08903v2": {
            "Paper Title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.18396v3": {
            "Paper Title": "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly\n  Transformers",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "requires 50 seconds time and 2GB transmission. Two recent studies explore replacing\nthese fundamentally expensive non-linear functions with operators that are more friendly in private\ninference. For instance, Chen et al. ",
                    "Citation Text": "Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou,\nJianxin Li, and Furu Wei. THE-X: Privacy-preserving transformer inference with homomorphic\nencryption. In Findings of the Association for Computational Linguistics: ACL 2022 , pages\n3510\u20133520, Dublin, Ireland, May 2022. Association for Computational Linguistics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.00216",
                        "Citation Paper Title": "Title:THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption",
                        "Citation Paper Abstract": "Abstract:As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce $\\textit{THE-X}$, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. $\\textit{THE-X}$ proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed $\\textit{THE-X}$ can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.",
                        "Citation Paper Authors": "Authors:Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, Furu Wei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08688v2": {
            "Paper Title": "TigerBot: An Open Multilingual Multitask LLM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08282v2": {
            "Paper Title": "Prompting LLMs with content plans to enhance the summarization of\n  scientific articles",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", three variants in size and attention architec-\nture of the LongT5 ",
                    "Citation Text": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\nLongt5: Efficient text-to-text transformer for long sequences. 12 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07916",
                        "Citation Paper Title": "Title:LongT5: Efficient Text-To-Text Transformer for Long Sequences",
                        "Citation Paper Abstract": "Abstract:Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",
                        "Citation Paper Authors": "Authors:Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09411v1": {
            "Paper Title": "OTOv3: Automatic Architecture-Agnostic Neural Network Training and\n  Compression from Structured Pruning to Erasing Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.05741v2": {
            "Paper Title": "Efficiently Adapting Pretrained Language Models To New Languages",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "builds an English-Arabic bilingual LLM, but they train it fr om scratch; while ",
                    "Citation Text": "R. Pires, H. Abonizio, T. S. Almeida, and R. Nogueira, \u201cS abi\u00e1: Portuguese large language\nmodels,\u201d 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.07880",
                        "Citation Paper Title": "Title:Sabi\u00e1: Portuguese Large Language Models",
                        "Citation Paper Abstract": "Abstract:As the capabilities of language models continue to advance, it is conceivable that \"one-size-fits-all\" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\u00e1-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model's knowledge about a domain or culture. Our results indicate that the majority of the benefits stem from the domain-specific knowledge acquired through monolingual pretraining.",
                        "Citation Paper Authors": "Authors:Ramon Pires, Hugo Abonizio, Thales Sales Almeida, Rodrigo Nogueira"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", and similar issues appear when training on a new language [23, 9, 24, 2, 25, 10, 26].\nDifferent training paradigms including instruction-alig n ",
                    "Citation Text": "S. Cahyawijaya, H. Lovenia, T. Yu, W. Chung, and P. Fung, \u201cInstruct-align: Teaching novel\nlanguages with to llms through alignment-based cross-ling ual instruction,\u201d 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.13627",
                        "Citation Paper Title": "Title:InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) that are tuned with instructions have demonstrated remarkable capabilities in various tasks and languages. However, their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability. To address this issue, we propose InstructAlign which uses continual crosslingual instruction tuning to enable LLMs to align new unseen languages with previously learned high-resource languages. Our results demonstrate the effectiveness of InstructAlign in enabling the model to understand low-resource languages with limited parallel data while preventing catastrophic forgetting. Our work contributes to the advancement of language adaptation methods, particularly for adapting instruction-tuned LLMs to underrepresented languages. Our code is released on this https URL",
                        "Citation Paper Authors": "Authors:Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, Pascale Fung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09390v1": {
            "Paper Title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak\n  Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.06330v4": {
            "Paper Title": "Smart Agent-Based Modeling: On the Use of Large Language Models in\n  Computer Simulations",
            "Sentences": [
                {
                    "Sentence ID": 161,
                    "Sentence": ", which uses reasoning instruc-\ntions instead of examples. A popular reasoning paradigm is chain-of-thought ",
                    "Citation Text": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.11903",
                        "Citation Paper Title": "Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Citation Paper Abstract": "Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                        "Citation Paper Authors": "Authors:Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": ". To recall a memory associated with a particular\ntopic, the topic text is transformed into a vector using the text embedding model. A nearest neighbor\nsearch ",
                    "Citation Text": "W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and X. Lin. Approximate nearest neighbor\nsearch on high dimensional data\u2014experiments, analyses, and improvement. IEEE Transactions\non Knowledge and Data Engineering , 32(8):1475\u20131488, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02455",
                        "Citation Paper Title": "Title:Approximate Nearest Neighbor Search on High Dimensional Data --- Experiments, Analyses, and Improvement (v1.0)",
                        "Citation Paper Abstract": "Abstract:Approximate Nearest neighbor search (ANNS) is fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Although many algorithms have been continuously proposed in the literature in the above domains each year, there is no comprehensive evaluation and analysis of their performances.\nIn this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 16 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings.",
                        "Citation Paper Authors": "Authors:Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Wenjie Zhang, Xuemin Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.16218v3": {
            "Paper Title": "Knowledge Editing for Large Language Models: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.13859v2": {
            "Paper Title": "Not all Fake News is Written: A Dataset and Analysis of Misleading Video\n  Headlines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07559v2": {
            "Paper Title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09300v1": {
            "Paper Title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09299v1": {
            "Paper Title": "Weight subcloning: direct initialization of transformers using larger\n  pretrained ones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09241v1": {
            "Paper Title": "TinyGSM: achieving >80% on GSM8k with small language models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09238v1": {
            "Paper Title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "build a foundation model by performing imita-\ntion on YouTube videos. DreamerV3 ",
                    "Citation Text": "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104 , 2023. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.04104",
                        "Citation Paper Title": "Title:Mastering Diverse Domains through World Models",
                        "Citation Paper Abstract": "Abstract:General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.",
                        "Citation Paper Authors": "Authors:Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09230v1": {
            "Paper Title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09207v1": {
            "Paper Title": "WikiMuTe: A web-sourced dataset of semantic descriptions for music audio",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09203v1": {
            "Paper Title": "Measurement in the Age of LLMs: An Application to Ideological Scaling",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ", based only on word counts, that tweets are more effective for\nscaling Democrats than Republicans. We then add to our prompts an encouragement for the LLM\nto first \u201cshow its work\u201d ",
                    "Citation Text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nLanguage Models are Zero-Shot Reasoners. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11916",
                        "Citation Paper Title": "Title:Large Language Models are Zero-Shot Reasoners",
                        "Citation Paper Abstract": "Abstract:Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
                        "Citation Paper Authors": "Authors:Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ", which are based on word counts of Senators\u2019 Tweets and speeches.\nThese results are consistent with the findings in a recent preprint by Wu et al. ",
                    "Citation Text": "Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, and Solomon Messing. Large Language Models\nCan Be Used to Scale the Ideologies of Politicians in a Zero-Shot Learning Setting, April 2023.\nURLhttp://arxiv.org/abs/2303.12057 . arXiv:2303.12057 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.12057",
                        "Citation Paper Title": "Title:Large Language Models Can Be Used to Estimate the Latent Positions of Politicians",
                        "Citation Paper Abstract": "Abstract:Existing approaches to estimating politicians' latent positions along specific dimensions often fail when relevant data is limited. We leverage the embedded knowledge in generative large language models (LLMs) to address this challenge and measure lawmakers' positions along specific political or policy dimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare lawmakers and then scale the resulting graph using the Bradley-Terry model. We estimate novel measures of U.S. senators' positions on liberal-conservative ideology, gun control, and abortion. Our liberal-conservative scale, used to validate LLM-driven scaling, strongly correlates with existing measures and offsets interpretive gaps, suggesting LLMs synthesize relevant data from internet and digitized media rather than memorizing existing measures. Our gun control and abortion measures -- the first of their kind -- differ from the liberal-conservative scale in face-valid ways and predict interest group ratings and legislator votes better than ideology alone. Our findings suggest LLMs hold promise for solving complex social science measurement problems.",
                        "Citation Paper Authors": "Authors:Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ",\n10. LLMs are not robust to minor prompt changes ",
                    "Citation Text": "Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe\nChang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, and Dacheng Tao. Are Large Language\nModels Really Robust to Word-Level Perturbations?, September 2023. URL http://arxiv.org/\nabs/2309.11166 . arXiv:2309.11166 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.11166",
                        "Citation Paper Title": "Title:Are Large Language Models Really Robust to Word-Level Perturbations?",
                        "Citation Paper Abstract": "Abstract:The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. Our extensive empirical experiments demonstrate that TREvaL provides an innovative method for evaluating the robustness of an LLM. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in this https URL.",
                        "Citation Paper Authors": "Authors:Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10097v1": {
            "Paper Title": "Arithmetics-Based Decomposition of Numeral Words -- Arithmetic\n  Conditions give the Unpacking Strategy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.03919v2": {
            "Paper Title": "Data Portraits: Recording Foundation Model Training Data",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". We use the subset of The Stack used to train StarCoder, a 15.5B parameter\nlarge language model for code ",
                    "Citation Text": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,\nThomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko,\nNicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,\nBenjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp\nPatel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,\nWenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel\nRomero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri\nDao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean\nHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be\nwith you!, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.06161",
                        "Citation Paper Title": "Title:StarCoder: may the source be with you!",
                        "Citation Paper Abstract": "Abstract:The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
                        "Citation Paper Authors": "Authors:Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nRecent work on large datasets has investigated deduplication and memorization analysis. Lee et al. ",
                    "Citation Text": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06499",
                        "Citation Paper Title": "Title:Deduplicating Training Data Makes Language Models Better",
                        "Citation Paper Abstract": "Abstract:We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at this https URL.",
                        "Citation Paper Authors": "Authors:Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "studied multiple aspects\nof the C4 corpus and released a searchable in-\ndexed version.2Contemporaneously with this\nwork, Piktus et al. ",
                    "Citation Text": "Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren\u00e7on, G\u00e9rard Dupont, Sasha Luccioni,\nYacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demon-\nstrations) , pages 304\u2013314, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-demo.29. URL https://aclanthology.org/2023.acl-demo.29 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.14035",
                        "Citation Paper Title": "Title:The ROOTS Search Tool: Data Transparency for LLMs",
                        "Citation Paper Abstract": "Abstract:ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces. We describe our implementation and the possible use cases of our tool.",
                        "Citation Paper Authors": "Authors:Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren\u00e7on, G\u00e9rard Dupont, Alexandra Sasha Luccioni, Yacine Jernite, Anna Rogers"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "use Bloom filters to construct language models while others used similar structures to count\nfeatures [ 16,36]. More recent work analyzes the use of a Bloom filter to prevent verbatim copying of\ncommon n-grams from training sets ",
                    "Citation Text": "Daphne Ippolito, Florian Tram\u00e8r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christo-\npher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in language models\ngives a false sense of privacy. arXiv preprint arXiv:2210.17546 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.17546",
                        "Citation Paper Title": "Title:Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy",
                        "Citation Paper Abstract": "Abstract:Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. Many prior works -- and some recently deployed defenses -- focus on \"verbatim memorization\", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \"perfect\" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \"style-transfer\" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.",
                        "Citation Paper Authors": "Authors:Daphne Ippolito, Florian Tram\u00e8r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "earlier proposed a related artifact called Model\nCards for trained models. These suggestions\nhave been taken up within the AI community:\nnew models are released with these artifacts (e.g.\nZhang et al. ",
                    "Citation Text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01068",
                        "Citation Paper Title": "Title:OPT: Open Pre-trained Transformer Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                        "Citation Paper Authors": "Authors:Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "examined memorization and showed that it increased with scale. The initial GPT-3 ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems , 33:1877\u20131901, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "showed that GPT-2 memorizes and can leak sensitive or private information from its training corpus.\nFurther work ",
                    "Citation Text": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram\u00e8r, and Chiyuan Zhang.\nQuantifying memorization across neural language models. ArXiv , abs/2202.07646, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.07646",
                        "Citation Paper Title": "Title:Quantifying Memorization Across Neural Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).\nWe describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "argues that\ncreators should release a Datasheet artifact doc-\numenting the \u201cmotivation, composition, collec-\ntion process\u201d for a dataset. Mitchell et al. ",
                    "Citation Text": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena\nSpitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the\nConference on Fairness, Accountability, and Transparency , FAT* \u201919, page 220\u2013229, New York, NY , USA,\n2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596.\nURL https://doi.org/10.1145/3287560.3287596 .\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.03993",
                        "Citation Paper Title": "Title:Model Cards for Model Reporting",
                        "Citation Paper Abstract": "Abstract:Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
                        "Citation Paper Authors": "Authors:Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.14973v2": {
            "Paper Title": "OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09075v1": {
            "Paper Title": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09067v1": {
            "Paper Title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "showcases its potential to generate large-\nscale interactive environments for training embodied agents.\nPhone2Proc ",
                    "Citation Text": "Matt Deitke, Rose Hendrix, Ali Farhadi, Kiana Ehsani, and\nAniruddha Kembhavi. Phone2proc: Bringing robust robots\ninto our chaotic world. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 9665\u20139675, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04819",
                        "Citation Paper Title": "Title:Phone2Proc: Bringing Robust Robots Into Our Chaotic World",
                        "Citation Paper Abstract": "Abstract:Training embodied agents in simulation has become mainstream for the embodied AI community. However, these agents often struggle when deployed in the physical world due to their inability to generalize to real-world environments. In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan and conditional procedural generation to create a distribution of training scenes that are semantically similar to the target environment. The generated scenes are conditioned on the wall layout and arrangement of large objects from the scan, while also sampling lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materials. Leveraging just a simple RGB camera, training with Phone2Proc shows massive improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav performance across a test suite of over 200 trials in diverse real-world environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's diverse distribution of generated scenes makes agents remarkably robust to changes in the real world, such as human movement, object rearrangement, lighting changes, or clutter.",
                        "Citation Paper Authors": "Authors:Matt Deitke, Rose Hendrix, Luca Weihs, Ali Farhadi, Kiana Ehsani, Aniruddha Kembhavi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09043v1": {
            "Paper Title": "Topic Bias in Emotion Classification",
            "Sentences": []
        }
    }
}