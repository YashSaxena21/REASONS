{
    "Graphics": {
        "http://arxiv.org/abs/2003.09053v6": {
            "Paper Title": "Cross-Shape Attention for Part Segmentation of 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13392v5": {
            "Paper Title": "Deep Learning-Based Human Pose Estimation: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01358v3": {
            "Paper Title": "Segmentation-Driven Feature-Preserving Mesh Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10726v5": {
            "Paper Title": "MINVO Basis: Finding Simplexes with Minimum Volume Enclosing Polynomial\n  Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03207v5": {
            "Paper Title": "NASA: Neural Articulated Shape Approximation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15820v2": {
            "Paper Title": "Photorealism in Driving Simulations: Blending Generative Adversarial\n  Image Synthesis with Rendering",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "enabled the realization ofarXiv:2007.15820v2  [cs.CV]  21 Jul 20222\nphoto-realistic image synthesis ",
                    "Citation Text": "A. Brock, J. Donahue, and K. Simonyan, \u201cLarge scale gan training for\nhigh \ufb01delity natural image synthesis,\u201d in International Conference on\nLearning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11096",
                        "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                        "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2008.05440v4": {
            "Paper Title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06471v3": {
            "Paper Title": "Self-Sampling for Neural Point Cloud Consolidation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13045v4": {
            "Paper Title": "PLAD: Learning to Infer Shape Programs with Pseudo-Labels and\n  Approximate Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08457v7": {
            "Paper Title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake\n  Attribution in Training Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08726v5": {
            "Paper Title": "Responsible Disclosure of Generative Models Using Scalable\n  Fingerprinting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07109v2": {
            "Paper Title": "SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15227v2": {
            "Paper Title": "Federated Visualization: A Privacy-preserving Strategy for Aggregated\n  Visual Query",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02600v3": {
            "Paper Title": "Deep Learning for Free-Hand Sketch: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07409v2": {
            "Paper Title": "Active Scene Understanding via Online Semantic Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06233v4": {
            "Paper Title": "Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15801v2": {
            "Paper Title": "Ray-marching Thurston geometries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04954v2": {
            "Paper Title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09036v3": {
            "Paper Title": "Improved StyleGAN Embedding: Where are the Good Latents?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04728v3": {
            "Paper Title": "Similarity-Based Clustering for Enhancing Image Classification\n  Architectures",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". We also saw the work in\nDeep sparse recti\fer networks ",
                    "Citation Text": "X. Glorot, A. Bordes, Y. Bengio, Deep sparse recti\fer neural networks, in:\nProceedings of the fourteenth international conference on arti\fcial intelli-\ngence and statistics, JMLR Workshop and Conference Proceedings, 2011,\npp. 315{323.\n20",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3485",
                        "Citation Paper Title": "Title:A Semantic Matching Energy Function for Learning with Multi-relational Data",
                        "Citation Paper Abstract": "Abstract:Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",
                        "Citation Paper Authors": "Authors:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio"
                    },
                    "Keywords": [
                        "Image",
                        "cation",
                        "Matrix",
                        "Classi",
                        "similarity",
                        "Similarity",
                        "CNN",
                        ","
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2009.07833v2": {
            "Paper Title": "Layered Neural Rendering for Retiming People in Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12440v2": {
            "Paper Title": "Building 3D Morphable Models from a Single Scan",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12543v3": {
            "Paper Title": "Topology-Aware Surface Reconstruction for Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12772v4": {
            "Paper Title": "How to see the eight Thurston geometries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12948v5": {
            "Paper Title": "Nerfies: Deformable Neural Radiance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03918v4": {
            "Paper Title": "NeRD: Neural Reflectance Decomposition from Image Collections",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nOverall, our image formation is defined as: Lo(x,\u03c9o)\u2248P24\nm=1\u03c1d(\u03c9o,\u0393m,n,b)+\u03c1s(\u03c9o,\u0393m,n,b). Our differen-\ntiable rendering implementation follows Boss et al. ",
                    "Citation Text": "Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A.\nLensch, and Jan Kautz. Two-shot spatially-varying brdf and\nshape estimation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2020. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00403",
                        "Citation Paper Title": "Title:Two-shot Spatially-varying BRDF and Shape Estimation",
                        "Citation Paper Abstract": "Abstract:Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.",
                        "Citation Paper Authors": "Authors:Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, Jan Kautz"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.12247v4": {
            "Paper Title": "Non-Rigid Neural Radiance Fields: Reconstruction and Novel View\n  Synthesis of a Dynamic Scene From Monocular Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09854v3": {
            "Paper Title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a\n  Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02523v5": {
            "Paper Title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04595v3": {
            "Paper Title": "GRF: Learning a General Radiance Field for 3D Representation and\n  Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10687v5": {
            "Paper Title": "HDR Environment Map Estimation for Real-Time Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04929v2": {
            "Paper Title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics\n  Engine for Tensegrity Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14398v2": {
            "Paper Title": "RGBD-Net: Predicting color and depth images for novel views synthesis",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "for novel view synthesis. Each input image is projected onto\nsuccessive virtual planes of the target camera to form a PSV .\nKalantari et al. ",
                    "Citation Text": "Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-\nmamoorthi. Learning-based view synthesis for light \ufb01eld\ncameras. ACM Trans. Graph. , 35(6), Nov. 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.02974",
                        "Citation Paper Title": "Title:Learning-Based View Synthesis for Light Field Cameras",
                        "Citation Paper Abstract": "Abstract:With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase.",
                        "Citation Paper Authors": "Authors:Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2001.04947v3": {
            "Paper Title": "Neural Human Video Rendering by Learning Dynamic Textures and\n  Rendering-to-Video Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05562v2": {
            "Paper Title": "High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07982v2": {
            "Paper Title": "ShapeFlow: Learnable Deformations Among 3D Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08141v2": {
            "Paper Title": "AsyncTaichi: On-the-fly Inter-kernel Optimizations for Imperative and\n  Spatially Sparse Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10796v3": {
            "Paper Title": "Scaling Probe-Based Real-Time Dynamic Global Illumination for Production",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11606v4": {
            "Paper Title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "produce points from 2D images, but\nsimilarly with 3D data as training input. Gadelhan et al. ",
                    "Citation Text": "Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2d views of multiple objects. In 3DV, 2016.\n2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.05872",
                        "Citation Paper Title": "Title:3D Shape Induction from 2D Views of Multiple Objects",
                        "Citation Paper Abstract": "Abstract:In this paper we investigate the problem of inducing a distribution over three-dimensional structures given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach called \"projective generative adversarial networks\" (PrGANs) trains a deep generative model of 3D shapes whose projections match the distributions of the input 2D views. The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained on 3D data for a number of shape categories including chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage is that our model allows us to predict 3D, viewpoint, and generate novel views from an input image in a completely unsupervised manner.",
                        "Citation Paper Authors": "Authors:Matheus Gadelha, Subhransu Maji, Rui Wang"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2010.06217v3": {
            "Paper Title": "TM-NET: Deep Generative Networks for Textured Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08826v5": {
            "Paper Title": "Deep Active Surface Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02190v3": {
            "Paper Title": "pixelNeRF: Neural Radiance Fields from One or Few Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13782v3": {
            "Paper Title": "Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01714v2": {
            "Paper Title": "AutoInt: Automatic Integration for Fast Neural Volume Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11310v3": {
            "Paper Title": "PBNS: Physically Based Neural Simulator for Unsupervised Garment Pose\n  Space Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10357v2": {
            "Paper Title": "Proceduray -- A light-weight engine for procedural primitive ray tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02392v2": {
            "Paper Title": "Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD\n  Construction from Human Design Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05538v5": {
            "Paper Title": "Sound Synthesis, Propagation, and Rendering: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03209v2": {
            "Paper Title": "Mapper Interactive: A Scalable, Extendable, and Interactive Toolbox for\n  the Visual Exploration of High-Dimensional Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12104v2": {
            "Paper Title": "Recurrent Multi-view Alignment Network for Unsupervised Surface\n  Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06434v2": {
            "Paper Title": "Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid\n  Representations",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "use two different encoder-\ndecoder networks to simultaneously predict both an explicit\natlas ",
                    "Citation Text": "Thibault Groueix, Matthew Fisher, Vladimir G Kim,\nBryan C Russell, and Mathieu Aubry. A papier-m \u02c6ach\u00b4e ap-\nproach to learning 3D surface generation. In Proc. IEEE\nConf. on Computer Vision & Pattern Recognition , pages\n216\u2013224, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05384",
                        "Citation Paper Title": "Title:AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation",
                        "Citation Paper Abstract": "Abstract:We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",
                        "Citation Paper Authors": "Authors:Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2007.02072v2": {
            "Paper Title": "Quo Vadis, Skeleton Action Recognition ?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00926v2": {
            "Paper Title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware\n  Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08211v3": {
            "Paper Title": "SSN: Soft Shadow Network for Image Compositing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10118v2": {
            "Paper Title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09029v2": {
            "Paper Title": "Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16011v3": {
            "Paper Title": "Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image\n  Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09159v2": {
            "Paper Title": "DECOR-GAN: 3D Shape Detailization by Conditional Refinement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05551v2": {
            "Paper Title": "DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01158v2": {
            "Paper Title": "Single-Shot Freestyle Dance Reenactment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02363v5": {
            "Paper Title": "Recurrent Transition Networks for Character Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03632v4": {
            "Paper Title": "LCollision: Fast Generation of Collision-Free Human Poses using Learned\n  Non-Penetration Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01068v2": {
            "Paper Title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance\n  Discrimination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09235v2": {
            "Paper Title": "Shape My Face: Registering 3D Face Scans by Surface-to-Surface\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09190v2": {
            "Paper Title": "Landmark Detection and 3D Face Reconstruction for Caricature using a\n  Nonlinear Parametric Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10379v3": {
            "Paper Title": "Neural Scene Graphs for Dynamic Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07791v3": {
            "Paper Title": "Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and\n  Deformable Reconstruction from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07177v3": {
            "Paper Title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03560v3": {
            "Paper Title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00074v2": {
            "Paper Title": "Levitating Rigid Objects with Hidden Rods and Wires",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.12337v3": {
            "Paper Title": "Learning Manifold Patch-Based Representations of Man-Made Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01652v2": {
            "Paper Title": "RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01639v2": {
            "Paper Title": "PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09178v2": {
            "Paper Title": "Gaussian Curvature Filter on 3D Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09808v3": {
            "Paper Title": "On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05204v2": {
            "Paper Title": "Neural Volume Rendering: NeRF And Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13240v2": {
            "Paper Title": "Neural Non-Rigid Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11571v2": {
            "Paper Title": "Neural Sparse Voxel Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02268v3": {
            "Paper Title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\n  Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05019v2": {
            "Paper Title": "SketchZooms: Deep multi-view descriptors for matching line drawings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15176v1": {
            "Paper Title": "Hybrid Function Representation for Heterogeneous Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14901v1": {
            "Paper Title": "Visualization of topology optimization designs with representative\n  subset selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14495v1": {
            "Paper Title": "SASSI -- Super-Pixelated Adaptive Spatio-Spectral Imaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12884v1": {
            "Paper Title": "Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the\n  Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01602v1": {
            "Paper Title": "STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in\n  Motion with Neural Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09290v2": {
            "Paper Title": "Self-Supervised Sketch-to-Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07484v3": {
            "Paper Title": "Pulsar: Efficient Sphere-based Neural Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10565v1": {
            "Paper Title": "No Shadow Left Behind: Removing Objects and their Shadows using\n  Approximate Lighting and Geometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08804v1": {
            "Paper Title": "Temporal Graph Modeling for Skeleton-based Action Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09633v1": {
            "Paper Title": "Visualization and Selection of Dynamic Mode Decomposition Components for\n  Unsteady Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08503v1": {
            "Paper Title": "Object-Centric Neural Scene Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08451v1": {
            "Paper Title": "Geometric Surface Image Prediction for Image Recognition Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08143v1": {
            "Paper Title": "NeuralQAAD: An Efficient Differentiable Framework for High Resolution\n  Point Cloud Compression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10592v2": {
            "Paper Title": "Eyes on the Prize: Improved Biological Surface Registration via Forward\n  Propagation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02546v3": {
            "Paper Title": "GANSpace: Discovering Interpretable GAN Controls",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00653v2": {
            "Paper Title": "Swapping Autoencoder for Deep Image Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12833v2": {
            "Paper Title": "Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11622v2": {
            "Paper Title": "Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06971v6": {
            "Paper Title": "BSP-Net: Generating Compact Meshes via Binary Space Partitioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03927v1": {
            "Paper Title": "NeRV: Neural Reflectance and Visibility Fields for Relighting and View\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10738v4": {
            "Paper Title": "Differentiable Augmentation for Data-Efficient GAN Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08072v2": {
            "Paper Title": "Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view\n  Human Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03939v1": {
            "Paper Title": "Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF\n  Material from Images via Differentiable Path Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03325v1": {
            "Paper Title": "EasyPBR: A Lightweight Physically-Based Renderer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03065v1": {
            "Paper Title": "Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12933v3": {
            "Paper Title": "ClipFlip : Multi-view Clipart Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02459v1": {
            "Paper Title": "Multiscale Mesh Deformation Component Analysis with Attention-based\n  Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12799v2": {
            "Paper Title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13748v2": {
            "Paper Title": "GraphSeam: Supervised Graph Learning Framework for Semantic UV Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01451v1": {
            "Paper Title": "Neural Deformation Graphs for Globally-consistent Non-rigid\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10348v2": {
            "Paper Title": "Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics\n  Hardware",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07689v4": {
            "Paper Title": "Soft Multicopter Control using Neural Dynamics Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15128v1": {
            "Paper Title": "Animating Pictures with Eulerian Motion Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15119v1": {
            "Paper Title": "UniCon: Universal Neural Controller For Physics-based Character Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14535v1": {
            "Paper Title": "Beyond LunAR: An augmented reality UI for deep-space exploration\n  missions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14143v1": {
            "Paper Title": "i3DMM: Deep Implicit 3D Morphable Model of Human Heads",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13417v1": {
            "Paper Title": "Generative Layout Modeling using Constraint Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13202v1": {
            "Paper Title": "t-EVA: Time-Efficient t-SNE Video Annotation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12893v1": {
            "Paper Title": "StyleUV: Diverse and High-fidelity UV Map Generative Model",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "regressed the shape and texture parame-\nters directly from an input image by leveraging the power of\nthe convolutional neural network (CNN). Genova et al. ",
                    "Citation Text": "Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron\nSarna, Daniel Vlasic, and William T Freeman. Unsuper-vised training for 3d morphable model regression. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 8377\u20138386, 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.06098",
                        "Citation Paper Title": "Title:Unsupervised Training for 3D Morphable Model Regression",
                        "Citation Paper Abstract": "Abstract:We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T. Freeman"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2009.00149v2": {
            "Paper Title": "GIF: Generative Interpretable Faces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.10297v5": {
            "Paper Title": "AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07414v2": {
            "Paper Title": "Combinatorial 3D Shape Generation via Sequential Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12490v1": {
            "Paper Title": "DeRF: Decomposed Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "A large literature exists on neural rendering. We refer\nthe reader to a recent survey ",
                    "Citation Text": "Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitz-\nmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-\nBrualla, Tomas Simon, Jason Saragih, Matthias Niessner, et al.\nState of the Art on Neural Rendering. Eurographics 2020\nState of The Art Report , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03805",
                        "Citation Paper Title": "Title:State of the Art on Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.12125v1": {
            "Paper Title": "To Explore What Isn't There -- Glyph-based Visualization for Analysis of\n  Missing Values",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11704v1": {
            "Paper Title": "Evaluating Feedback Strategies for Virtual Human Trainers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10711v2": {
            "Paper Title": "MonoClothCap: Towards Temporally Coherent Clothing Capture from\n  Monocular RGB Video",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". We refer\nreaders to the original papers [52, 29] for details. We also\napply regularization on our estimation, denoted by Eb\nreg,\nwhich consists of a Mixture of Gaussian prior for body pose\nf\u0012igF\ni=1 ",
                    "Citation Text": "F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero,\nand M. J. Black. Keep it smpl: Automatic estimation of 3d\nhuman pose and shape from a single image. In European\nConference on Computer Vision , pages 561\u2013578. Springer,\n2016. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08128",
                        "Citation Paper Title": "Title:Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image",
                        "Citation Paper Abstract": "Abstract:We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.",
                        "Citation Paper Authors": "Authors:Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J. Black"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.10688v1": {
            "Paper Title": "Iterative Text-based Editing of Talking-heads Using Neural Retargeting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10232v1": {
            "Paper Title": "Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08821v4": {
            "Paper Title": "EMU: Efficient Muscle Simulation In Deformation Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09941v2": {
            "Paper Title": "Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02224v1": {
            "Paper Title": "Personality-Driven Gaze Animation with Conditional Generative\n  Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06718v3": {
            "Paper Title": "Line Art Correlation Matching Feature Transfer Network for Automatic\n  Animation Colorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04612v1": {
            "Paper Title": "Fast Fourier Intrinsic Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04910v1": {
            "Paper Title": "Spring-Rod System Identification via Differentiable Physics Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11512v2": {
            "Paper Title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01936v2": {
            "Paper Title": "COALESCE: Component Assembly by Learning to Synthesize Connections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16404v2": {
            "Paper Title": "Unsupervised Monocular Depth Learning in Dynamic Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03630v1": {
            "Paper Title": "Unmasking Communication Partners: A Low-Cost AI Solution for Digitally\n  Removing Head-Mounted Displays in VR-Based Telepresence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03277v1": {
            "Paper Title": "Modular Primitives for High-Performance Differentiable Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03082v1": {
            "Paper Title": "Learning Multiple-Scattering Solutions for Sphere-Tracing of Volumetric\n  Subsurface Effects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08032v4": {
            "Paper Title": "Inferring the Material Properties of Granular Media for Robotic Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02368v2": {
            "Paper Title": "An Empirical-cum-Statistical Approach to Power-Performance\n  Characterization of Concurrent GPU Kernels",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": ", Hughes et al. have characterized the transaction memory workloads. In ",
                    "Citation Text": "Hadi Esmaeilzadeh, Ting Cao, Yang Xi, Stephen M. Blackburn, and Kathryn S. McKinley, \"Looking back on the language and hardware revolu-tions: measured power, performance, and scaling,\" presented at the Proceed-ings of the sixteenth International conference on Architectural support for program-ming languages and operating systems, Newport Becah, CA, USA, March 2011, 319-332",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.07879",
                        "Citation Paper Title": "Title:Conscious AI",
                        "Citation Paper Abstract": "Abstract:Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.",
                        "Citation Paper Authors": "Authors:Hadi Esmaeilzadeh, Reza Vaezi"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.01067v5": {
            "Paper Title": "A Bayesian Inference Framework for Procedural Material Parameter\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08676v1": {
            "Paper Title": "Topology-Based Feature Design and Tracking for Multi-Center Cyclones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14702v1": {
            "Paper Title": "Optimal Textures: Fast and Robust Texture Synthesis and Style Transfer\n  through Optimal Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09040v2": {
            "Paper Title": "Studying Visualization Guidelines According to Grounded Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09852v3": {
            "Paper Title": "Multiview Neural Surface Reconstruction by Disentangling Geometry and\n  Appearance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12930v1": {
            "Paper Title": "A Characterization of 3D Printability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07660v2": {
            "Paper Title": "Convolutional Generation of Textured 3D Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07364v2": {
            "Paper Title": "Residual Force Control for Agile Human Behavior Imitation and Extended\n  Motion Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04115v2": {
            "Paper Title": "DiffGCN: Graph Convolutional Networks via Differential Operators and\n  Algebraic Multigrid Pooling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10557v1": {
            "Paper Title": "Image-Driven Furniture Style for Interactive 3D Scene Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09774v1": {
            "Paper Title": "GAMesh: Guided and Augmented Meshing for Deep Point Networks",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "combine\nvoxel and mesh representation by proposing a differentiable\nmarching cubes algorithm to convert the output of a volume\ndecoder to a mesh and optimized the network using geo-\nmetric losses. Gkioxari et al. ",
                    "Citation Text": "G. Gkioxari, J. Malik, and J. Johnson. Mesh r-cnn. In ICCV ,\n2019. 3, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02739",
                        "Citation Paper Title": "Title:Mesh R-CNN",
                        "Citation Paper Abstract": "Abstract:Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",
                        "Citation Paper Authors": "Authors:Georgia Gkioxari, Jitendra Malik, Justin Johnson"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.07728v3": {
            "Paper Title": "Face Identity Disentanglement via Latent Space Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00305v2": {
            "Paper Title": "Self-supervised Learning of Point Clouds via Orientation Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08888v1": {
            "Paper Title": "Light Stage Super-Resolution: Continuous High-Frequency Relighting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08788v1": {
            "Paper Title": "Discovering Pattern Structure Using Differentiable Compositing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08735v1": {
            "Paper Title": "Real-time High-Quality Rendering of Non-Rotating Black Holes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10294v2": {
            "Paper Title": "Coupling Explicit and Implicit Surface Representations for Generative 3D\n  Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07498v1": {
            "Paper Title": "Bayesian Spatio-Temporal Graph Convolutional Network for Traffic\n  Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03675v1": {
            "Paper Title": "Interactive digital storytelling: bringing cultural heritage in a\n  classroom",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07378v2": {
            "Paper Title": "BOP Challenge 2020 on 6D Object Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02060v3": {
            "Paper Title": "Neural Puppet: Generative Layered Cartoon Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05655v1": {
            "Paper Title": "Intuitive Facial Animation Editing Based On A Generative RNN Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05528v1": {
            "Paper Title": "Fat Pad Cages for Facial Posing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02822v2": {
            "Paper Title": "Scalable Rendering of Variable Density Point Cloud Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04278v1": {
            "Paper Title": "Refinement of Predicted Missing Parts Enhance Point Cloud Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03936v1": {
            "Paper Title": "Cinema Darkroom: A Deferred Rendering Framework for Large-Scale Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01391v3": {
            "Paper Title": "An Infinite, Converging, Sequence of Brocard Porisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11721v3": {
            "Paper Title": "FAKIR: An algorithm for revealing the anatomy and pose of statues from\n  raw point sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02319v1": {
            "Paper Title": "Tensor Fields for Data Extraction from Chart Images: Bar Charts and\n  Scatter Plots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00560v2": {
            "Paper Title": "Dynamic Facial Asset and Rig Generation from a Single Scan",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03169v1": {
            "Paper Title": "Haptic Rendering of Cultural Heritage Objects at Different Scales",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02015v1": {
            "Paper Title": "Combined Hapto-Visual and Auditory Rendering of Cultural Heritage\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01775v1": {
            "Paper Title": "Photon-Driven Neural Path Guiding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01679v1": {
            "Paper Title": "Learning Complete 3D Morphable Face Models from Images and Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05400v2": {
            "Paper Title": "SALD: Sign Agnostic Learning with Derivatives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00450v1": {
            "Paper Title": "X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09267v2": {
            "Paper Title": "Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12395v2": {
            "Paper Title": "SceneGen: Generative Contextual Scene Augmentation using Scene Graph\n  Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11995v1": {
            "Paper Title": "Investigating Cultural Aspects in the Fundamental Diagram using\n  Convolutional Neural Networks and Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14624v1": {
            "Paper Title": "Structured Regularization of Functional Map Computations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14514v1": {
            "Paper Title": "Asynchronous Liquids: Regional Time Stepping for Faster SPH and PCISPH",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10696v2": {
            "Paper Title": "Spectrally Consistent UNet for High Fidelity Image Transformations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13856v1": {
            "Paper Title": "Neural Alignment for Face De-pixelization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13339v1": {
            "Paper Title": "Weakly Supervised Deep Functional Map for Shape Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12967v1": {
            "Paper Title": "Recognition and Synthesis of Object Transport Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05968v2": {
            "Paper Title": "Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12216v1": {
            "Paper Title": "Deep Learning of Individual Aesthetics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.01674v4": {
            "Paper Title": "Semi-Global Weighted Least Squares in Image Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09501v1": {
            "Paper Title": "3D Pseudo Stereo Visualization with Gpgpu Support",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09500v1": {
            "Paper Title": "3D Primitives Gpgpu Generation for Volume Visualization in 3D Graphics\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09485v1": {
            "Paper Title": "PIE: Portrait Image Embedding for Semantic Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08941v1": {
            "Paper Title": "Light Direction and Color Estimation from Single Image with Deep\n  Regression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08692v1": {
            "Paper Title": "DeepRemaster: Temporal Source-Reference Attention Networks for\n  Comprehensive Video Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07647v1": {
            "Paper Title": "Related by Similarity II: Poncelet 3-Periodics in the Homothetic Pair\n  and the Brocard Porism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11632v3": {
            "Paper Title": "Wavelet-based Heat Kernel Derivatives: Towards Informative Localized\n  Shape Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07047v1": {
            "Paper Title": "Old Photo Restoration via Deep Latent Space Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05284v1": {
            "Paper Title": "Attribute-conditioned Layout GAN for Automatic Graphic Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04927v1": {
            "Paper Title": "Sketch2CAD: Sequential CAD Modeling by Sketching in Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01363v2": {
            "Paper Title": "Illustrations of non-Euclidean geometry in virtual reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02532v2": {
            "Paper Title": "MapTree: Recovering Multiple Solutions in the Space of Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04601v1": {
            "Paper Title": "Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and\n  Seamless Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04592v1": {
            "Paper Title": "Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03483v2": {
            "Paper Title": "Skeletonization via Local Separators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04177v1": {
            "Paper Title": "MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "can be regarded as\nan extension of Gene Generative Adversarial Network (Gene-\nGAN) ",
                    "Citation Text": "S. Zhou, T. Xiao, Y . Yang, D. Feng, Q. He, and W. He, \u201cGene-\ngan: Learning object trans\ufb01guration and attribute subspace from\nunpaired data,\u201d arXiv preprint arXiv:1705.04932 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.11886",
                        "Citation Paper Title": "Title:DeepViT: Towards Deeper Vision Transformer",
                        "Citation Paper Abstract": "Abstract:Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, Jiashi Feng"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2009.03784v1": {
            "Paper Title": "Improving Engagement of Animated Visualization with Visual Foreshadowing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09940v1": {
            "Paper Title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03254v1": {
            "Paper Title": "Interactive Visualization of Terascale Data in the Browser: Fact or\n  Fiction?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03044v1": {
            "Paper Title": "Nonlinear Spectral Geometry Processing via the TV Transform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02969v1": {
            "Paper Title": "Palettailor: Discriminable Colorization for Categorical Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02702v1": {
            "Paper Title": "The Mixture Graph-A Data Structure for Compressing, Rendering, and\n  Querying Segmentation Histograms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02494v1": {
            "Paper Title": "A curvature and density-based generative representation of shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01524v2": {
            "Paper Title": "Neural Control Variates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02005v1": {
            "Paper Title": "Staged Animation Strategies for Online Dynamic Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01891v1": {
            "Paper Title": "Improving the Usability of Virtual Reality Neuron Tracing with\n  Topological Elements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01512v1": {
            "Paper Title": "TopoMap: A 0-dimensional Homology Preserving Projection of\n  High-Dimensional Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01456v1": {
            "Paper Title": "DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation\n  Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02256v1": {
            "Paper Title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray\n  Scattering Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00931v1": {
            "Paper Title": "A Study of Opacity Ranges for Transparent Overlays in 3D Landscapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00905v1": {
            "Paper Title": "Neural Crossbreed: Neural Based Image Metamorphosis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01959v1": {
            "Paper Title": "Topological Analysis of Magnetic Reconnection in Kinetic Plasma\n  Simulations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12591v4": {
            "Paper Title": "Conflict and Cooperation: AI Research and Development in terms of the\n  Economy of Conventions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00083v1": {
            "Paper Title": "Localized Topological Simplification of Scalar Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00485v3": {
            "Paper Title": "SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries\n  of 3D Shapes from Single-View RGB-D Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13150v1": {
            "Paper Title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in\n  Virtual Screening",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11657v2": {
            "Paper Title": "Test Scene Design for Physically Based Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04323v2": {
            "Paper Title": "ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11989v1": {
            "Paper Title": "GraphFederator: Federated Visual Analysis for Multi-party Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11256v1": {
            "Paper Title": "Differentiating a Tensor Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05976v2": {
            "Paper Title": "HOTVis: Higher-Order Time-Aware Visualisation of Dynamic Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10599v1": {
            "Paper Title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10247v1": {
            "Paper Title": "Monocular Reconstruction of Neural Face Reflectance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10174v1": {
            "Paper Title": "Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12098v3": {
            "Paper Title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09688v1": {
            "Paper Title": "Toward Quantifying Ambiguities in Artistic Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09655v1": {
            "Paper Title": "DeepLandscape: Adversarial Modeling of Landscape Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08823v2": {
            "Paper Title": "DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose\n  Estimation via a Smooth Silhouette Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09092v1": {
            "Paper Title": "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08999v1": {
            "Paper Title": "Object Properties Inferring from and Transfer for Human Interaction\n  Motions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08171v1": {
            "Paper Title": "Learning to Generate Diverse Dance Motions with Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15272v2": {
            "Paper Title": "ConceptExplorer: Visual Analysis of Concept Driftsin Multi-source\n  Time-series Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01971v3": {
            "Paper Title": "Structure-Aware Human-Action Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08424v1": {
            "Paper Title": "AutoSimulate: (Quickly) Learning Synthetic Data Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03824v2": {
            "Paper Title": "Neural Reflectance Fields for Appearance Acquisition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06678v1": {
            "Paper Title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones\n  Leveraging an Explainable Reinforcement Learning Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05567v2": {
            "Paper Title": "Procedural Urban Forestry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06134v1": {
            "Paper Title": "Interactive volume illumination of slice-based ray casting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05872v1": {
            "Paper Title": "Motion Similarity Modeling -- A State of the Art Report",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09533v2": {
            "Paper Title": "Real-Time Visualization in Non-Isotropic Geometries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05157v1": {
            "Paper Title": "Towards Geometry Guided Neural Relighting with Flash Photography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04852v1": {
            "Paper Title": "GeLaTO: Generative Latent Textured Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04367v1": {
            "Paper Title": "Deep Detail Enhancement for Any Garment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01970v4": {
            "Paper Title": "Predictive Generalized Graph Fourier Transform for Attribute Compression\n  of Dynamic Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03875v1": {
            "Paper Title": "RocNet: Recursive Octree Network for Efficient 3D Deep Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03834v1": {
            "Paper Title": "Dual In-painting Model for Unsupervised Gaze Correction and Animation in\n  the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02152v2": {
            "Paper Title": "Augmented Semantic Signatures of Airborne LiDAR Point Clouds for\n  Comparison",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11544v2": {
            "Paper Title": "Image2StyleGAN++: How to Edit the Embedded Images?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02912v1": {
            "Paper Title": "Predicting Visual Importance Across Graphic Design Types",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02796v1": {
            "Paper Title": "Learning to Factorize and Relight a City",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02396v1": {
            "Paper Title": "Learning Illumination from Diverse Portraits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13834v2": {
            "Paper Title": "Label-Efficient Learning on Point Clouds using Approximate Convex\n  Decompositions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01815v1": {
            "Paper Title": "Deep Multi Depth Panoramas for View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10290v2": {
            "Paper Title": "DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00409v2": {
            "Paper Title": "P-Cloth: Interactive Complex Cloth Simulation on Multi-GPU Systems using\n  Dynamic Matrix Assembly and Pipelined Implicit Integrators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08934v2": {
            "Paper Title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00823v1": {
            "Paper Title": "Rethinking Image Deraining via Rain Streaks and Vapors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00214v2": {
            "Paper Title": "BCNet: Learning Body and Cloth Shape from A Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00579v1": {
            "Paper Title": "Modeling of Personalized Anatomy using Plastic Strains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13378v2": {
            "Paper Title": "A Benchmarking Framework for Interactive 3D Applications in the Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01541v1": {
            "Paper Title": "Optimized Processing of Localized Collisions in Projective Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00387v2": {
            "Paper Title": "Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual\n  Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01228v3": {
            "Paper Title": "Deformation-Aware 3D Model Embedding and Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12057v2": {
            "Paper Title": "Differentiable Rendering: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "take a different approach.\nThey project the voxels from the world space to the screen\nspace (using camera parameters) and perform a bilinear\nsampling similar to Spatial Transformer ",
                    "Citation Text": "M. Jaderberg, K. Simonyan, A. Zisserman, and k. kavukcuoglu,\n\u201cSpatial Transformer Networks,\u201d in NeurIPS , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02025",
                        "Citation Paper Title": "Title:Spatial Transformer Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
                        "Citation Paper Authors": "Authors:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2007.15646v1": {
            "Paper Title": "Rewriting a Deep Generative Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02480v2": {
            "Paper Title": "Deep Radiance Caching: Convolutional Autoencoders Deeper in Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15446v1": {
            "Paper Title": "Visual Analysis of Multi-Parameter Distributions across Ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15242v1": {
            "Paper Title": "Understanding the Stability of Deep Control Policies for Biped\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05566v2": {
            "Paper Title": "Neural Voice Puppetry: Audio-driven Facial Reenactment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14394v1": {
            "Paper Title": "Signed Distance Fields Dynamic Diffuse Global Illumination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13988v1": {
            "Paper Title": "Monocular Real-Time Volumetric Performance Capture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13344v1": {
            "Paper Title": "Self-Prediction for Joint Instance and Semantic Segmentation of Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09170v3": {
            "Paper Title": "The Vector Heat Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11250v1": {
            "Paper Title": "FASTSWARM: A Data-driven FrAmework for Real-time Flying InSecT SWARM\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10918v1": {
            "Paper Title": "DecoSurf: Recursive Geodesic Patterns on Triangle Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10430v1": {
            "Paper Title": "A Survey of Algorithms for Geodesic Paths and Distances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09892v1": {
            "Paper Title": "Deep Reflectance Volumes: Relightable Reconstructions from Multi-View\n  Photometric Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11038v2": {
            "Paper Title": "Deformable Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09740v1": {
            "Paper Title": "Octahedral Frames for Feature-Aligned Cross-Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03450v3": {
            "Paper Title": "Learning to Accelerate Decomposition for Multi-Directional 3D Printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09077v1": {
            "Paper Title": "Generating Person Images with Appearance-aware Pose Stylizer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08547v1": {
            "Paper Title": "Talking-head Generation with Rhythmic Head Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08501v1": {
            "Paper Title": "Accelerating 3D Deep Learning with PyTorch3D",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10510v2": {
            "Paper Title": "Few-shot Compositional Font Generation with Dual Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08624v2": {
            "Paper Title": "PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree\n  Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07243v1": {
            "Paper Title": "Transposer: Universal Texture Synthesis Using Feature Maps as Transposed\n  Convolution Filter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06237v1": {
            "Paper Title": "LSQT: Low-Stretch Quasi-Trees for Bundling and Layout",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05661v1": {
            "Paper Title": "Deep Patch-based Human Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02578v1": {
            "Paper Title": "Learning Graph-Convolutional Representations for Point Cloud Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00559v2": {
            "Paper Title": "RigNet: Neural Rigging for Articulated Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02168v1": {
            "Paper Title": "Scalable Differentiable Physics for Learning and Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12642v2": {
            "Paper Title": "Deep 3D Capture: Geometry and Reflectance from Sparse Multi-View Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01629v1": {
            "Paper Title": "A Discrete Probabilistic Approach to Dense Flow Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00987v1": {
            "Paper Title": "ADD: Analytically Differentiable Dynamics for Multi-Body Systems with\n  Frictional Contact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00977v1": {
            "Paper Title": "PerceptionGAN: Real-world Image Construction from Provided Text through\n  Perceptual Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00324v1": {
            "Paper Title": "On Designing GPU Algorithms with Applications to Mesh Refinement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16112v2": {
            "Paper Title": "GramGAN: Deep 3D Texture Synthesis From 2D Exemplars",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15432v1": {
            "Paper Title": "Automatic Recommendation of Strategies for Minimizing Discomfort in\n  Virtual Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15059v1": {
            "Paper Title": "Computing Light Transport Gradients using the Adjoint Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14539v2": {
            "Paper Title": "Anderson Acceleration for Nonconvex ADMM Based on Douglas-Rachford\n  Splitting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09819v3": {
            "Paper Title": "An Evolutional Algorithm for Automatic 2D Layer Segmentation in\n  Laser-aided Additive Manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14726v1": {
            "Paper Title": "Augmenting Image Warping-Based Remote Volume Rendering with Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10637v3": {
            "Paper Title": "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12661v1": {
            "Paper Title": "SN-Engine, a Scale-free Geometric Modelling Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12075v1": {
            "Paper Title": "MotioNet: 3D Human Motion Reconstruction from Monocular Video with\n  Skeleton Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04439v3": {
            "Paper Title": "Folding-based compression of point cloud attributes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11840v1": {
            "Paper Title": "Quanta Burst Photography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11620v1": {
            "Paper Title": "Technical Note: Generating Realistic Fighting Scenes by Game Tree",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11348v1": {
            "Paper Title": "Ray-VR: Ray Tracing Virtual Reality in Falcor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02711v2": {
            "Paper Title": "A Morphable Face Albedo Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10509v1": {
            "Paper Title": "Structure and Design of HoloGen",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09662v1": {
            "Paper Title": "MetaSDF: Meta-learning Signed Distance Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00960v2": {
            "Paper Title": "Going Deeper with Lean Point Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08819v1": {
            "Paper Title": "A study of the effect of the illumination model on the generation of\n  synthetic training datasets",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ", renderings of 3D\nchair images are combined with natural image backgrounds\nto train FlowNet. In ",
                    "Citation Text": "Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon\nStent, and Roberto Cipolla. Understanding real world indoor\nscenes with synthetic data. In Proc. CVPR , pages 4077\u20134085, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.07041",
                        "Citation Paper Title": "Title:SceneNet: Understanding Real World Indoor Scenes With Synthetic Data",
                        "Citation Paper Abstract": "Abstract:Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.",
                        "Citation Paper Authors": "Authors:Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.07859v1": {
            "Paper Title": "Repulsive Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07818v1": {
            "Paper Title": "Alternating ConvLSTM: Learning Force Propagation with Alternate State\n  Updates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00121v2": {
            "Paper Title": "StyleRig: Rigging StyleGAN for 3D Control over Portrait Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06126v2": {
            "Paper Title": "Local Deep Implicit Functions for 3D Shape",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00416v3": {
            "Paper Title": "LatentFusion: End-to-End Differentiable Reconstruction and Rendering for\n  Unseen Object Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.13225v2": {
            "Paper Title": "DIST: Rendering Deep Implicit Signed Distance Function with\n  Differentiable Sphere Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06071v1": {
            "Paper Title": "Affective Movement Generation using Laban Effort and Shape and Hidden\n  Markov Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05921v1": {
            "Paper Title": "Computational Design and Evaluation Methods for Empowering Non-Experts\n  in Digital Fabrication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05743v1": {
            "Paper Title": "Towards 3D Dance Motion Synthesis and Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04096v1": {
            "Paper Title": "Robust Learning Through Cross-Task Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03762v1": {
            "Paper Title": "Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape\n  and Scene Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03743v1": {
            "Paper Title": "Simple Primary Colour Editing for Consumer Product Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01047v2": {
            "Paper Title": "Deep Generation of Face Images from Sketches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00190v1": {
            "Paper Title": "OPAL-Net: A Generative Model for Part-based Object Layout Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03575v5": {
            "Paper Title": "Bilinear Graph Neural Network with Neighbor Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00114v1": {
            "Paper Title": "Design and Implementation of a Virtual 3D Educational Environment to\n  improve Deaf Education",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00084v1": {
            "Paper Title": "Clustering-informed Cinematic Astrophysical Data Visualization with\n  Application to the Moon-forming Terrestrial Synestia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14695v1": {
            "Paper Title": "Non-Rigid Volume to Surface Registration using a Data-Driven\n  Biomechanical Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08891v2": {
            "Paper Title": "Generative Tweening: Long-term Inbetweening of 3D Human Motions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13532v1": {
            "Paper Title": "4D Visualization of Dynamic Events from Unconstrained Multi-View Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05280v2": {
            "Paper Title": "City-GAN: Learning architectural styles using a custom Conditional GAN\n  architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12662v1": {
            "Paper Title": "A Deep Learning based Fast Signed Distance Map Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12518v1": {
            "Paper Title": "Survey: Machine Learning in Production Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11621v1": {
            "Paper Title": "ManifoldPlus: A Robust and Scalable Watertight Manifold Surface\n  Generation Method for Triangle Soups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11617v1": {
            "Paper Title": "MeshODE: A Robust and Scalable Framework for Mesh Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07547v2": {
            "Paper Title": "Online path sampling control with progressive spatio-temporal filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13615v3": {
            "Paper Title": "Learning to Dress 3D People in Generative Clothing",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "Yes Yes Yes No Yes No No\nClothing Wang et al. ",
                    "Citation Text": "Tuanfeng Y Wang, Duygu Ceylan, Jovan Popovi \u00b4c, and Niloy J Mitra.\nLearning a shared shape space for multimodal garment design. In\nACM SIGGRAPH ASIA , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.11335",
                        "Citation Paper Title": "Title:Learning a Shared Shape Space for Multimodal Garment Design",
                        "Citation Paper Abstract": "Abstract:Designing real and virtual garments is becoming extremely demanding with rapidly changing fashion trends and increasing need for synthesizing realistic dressed digital humans for various applications. This necessitates creating simple and effective workflows to facilitate authoring sewing patterns customized to garment and target body shapes to achieve desired looks. Traditional workflow involves a trial-and-error procedure wherein a mannequin is draped to judge the resultant folds and the sewing pattern iteratively adjusted until the desired look is achieved. This requires time and experience. Instead, we present a data-driven approach wherein the user directly indicates desired fold patterns simply by sketching while our system estimates corresponding garment and body shape parameters at interactive rates. The recovered parameters can then be further edited and the updated draped garment previewed. Technically, we achieve this via a novel shared shape space that allows the user to seamlessly specify desired characteristics across multimodal input {\\em without} requiring to run garment simulation at design time. We evaluate our approach qualitatively via a user study and quantitatively against test datasets, and demonstrate how our system can generate a rich quality of on-body garments targeted for a range of body shapes while achieving desired fold characteristics.",
                        "Citation Paper Authors": "Authors:Tuanfeng Y. Wang, Duygu Ceylan, Jovan Popovic, Niloy J. Mitra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1907.03107v3": {
            "Paper Title": "CoAug-MR: An MR-based Interactive Office Workstation Design System via\n  Augmented Multi-Person Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10663v1": {
            "Paper Title": "Wish You Were Here: Context-Aware Human Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03387v2": {
            "Paper Title": "Learning Structural Graph Layouts and 3D Shapes for Long Span Bridges 3D\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08925v2": {
            "Paper Title": "Portrait Shadow Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09704v1": {
            "Paper Title": "Contextual Residual Aggregation for Ultra High-Resolution Image\n  Inpainting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08468v1": {
            "Paper Title": "An error reduced and uniform parameter approximation in fitting of\n  B-spline curves to data points",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07504v2": {
            "Paper Title": "Real-time Image Smoothing via Iterative Least Squares",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08000v1": {
            "Paper Title": "Deep Lighting Environment Map Estimation from Spherical Panoramas",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07865v1": {
            "Paper Title": "Attribute2Font: Creating Fonts You Want From Attributes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07702v1": {
            "Paper Title": "Generative Adversarial Networks for photo to Hayao Miyazaki style\n  cartoons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06998v1": {
            "Paper Title": "Plane-Activated Mapped Microstructure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06671v1": {
            "Paper Title": "Optimally Fast Soft Shadows on Curved Terrain with Dynamic Programming\n  and Maximum Mipmaps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08367v2": {
            "Paper Title": "Lighthouse: Predicting Lighting Volumes for Spatially-Coherent\n  Illumination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06469v1": {
            "Paper Title": "Representing Whole Slide Cancer Image Features with Hilbert Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05386v1": {
            "Paper Title": "Design and visualization of Riemannian metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01878v1": {
            "Paper Title": "Illumination-Invariant Image from 4-Channel Images: The Effect of\n  Near-Infrared Data in Shadow Removal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01819v1": {
            "Paper Title": "Neural Subdivision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14107v1": {
            "Paper Title": "Informative Scene Decomposition for Crowd Analysis, Comparison and\n  Simulation Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13896v1": {
            "Paper Title": "Organic Narrative Charts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14381v1": {
            "Paper Title": "Visualization of Unsteady Flow Using Heat Kernel Signatures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13859v1": {
            "Paper Title": "A First Principles Approach for Data-Efficient System Identification of\n  Spring-Rod Systems via Differentiable Physics Engines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13497v1": {
            "Paper Title": "A framework for adaptive width control of dense contour-parallel\n  toolpaths in fused deposition modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03640v2": {
            "Paper Title": "Unsupervised multi-modal Styled Content Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.01026v2": {
            "Paper Title": "Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12069v1": {
            "Paper Title": "Deep Photon Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10949v3": {
            "Paper Title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11364v1": {
            "Paper Title": "Single-View View Synthesis with Multiplane Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08873v1": {
            "Paper Title": "Knot Morphing Algorithm for Quantum `Fragile Topology'",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13921v3": {
            "Paper Title": "Neural View-Interpolation for Sparse Light Field Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09484v1": {
            "Paper Title": "Bringing Old Photos Back to Life",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09136v1": {
            "Paper Title": "Robust and efficient tool path generation for poor-quality triangular\n  mesh surface machining",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03355v2": {
            "Paper Title": "Learning Physics-guided Face Relighting under Directional Light",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08475v1": {
            "Paper Title": "A Simple, General, and GPU Friendly Method for Computing Dual Mesh and\n  Iso-Surfaces of Adaptive Mesh Refinement (AMR) Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00302v4": {
            "Paper Title": "READ: Recursive Autoencoders for Document Layout Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01815v2": {
            "Paper Title": "3D Morphable Face Models -- Past, Present and Future",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06848v1": {
            "Paper Title": "Intuitive, Interactive Beard and Hair Synthesis with Generative Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11758v3": {
            "Paper Title": "MixNMatch: Multifactor Disentanglement and Encoding for Conditional\n  Image Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05736v4": {
            "Paper Title": "CvxNet: Learnable Convex Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05679v1": {
            "Paper Title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05571v1": {
            "Paper Title": "Cross-domain Correspondence Learning for Exemplar-based Image\n  Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04572v2": {
            "Paper Title": "ARCH: Animatable Reconstruction of Clothed Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09098v2": {
            "Paper Title": "Inconsistent Surface Registration via Optimization of Mapping\n  Distortions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04242v1": {
            "Paper Title": "Deep Manifold Prior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03805v1": {
            "Paper Title": "State of the Art on Neural Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03590v1": {
            "Paper Title": "Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03028v1": {
            "Paper Title": "Learning Generative Models of Shape Handles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05980v1": {
            "Paper Title": "NiLBS: Neural Inverse Linear Blend Skinning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02867v1": {
            "Paper Title": "Rethinking Spatially-Adaptive Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04591v2": {
            "Paper Title": "Neural Voxel Renderer: Learning an Accurate and Controllable Rendering\n  Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11702v1": {
            "Paper Title": "Multimodal Medical Volume Colorization from 2D Style",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02460v1": {
            "Paper Title": "Robust 3D Self-portraits in Seconds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10333v3": {
            "Paper Title": "Neural Contours: Learning to Draw Lines from 3D Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09457v2": {
            "Paper Title": "TopoLines: Topological Smoothing for Line Charts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01661v1": {
            "Paper Title": "Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00663v1": {
            "Paper Title": "Synchronizing Probability Measures on Rotations via Optimal Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00452v1": {
            "Paper Title": "PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution\n  3D Human Digitization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11922v2": {
            "Paper Title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09204v4": {
            "Paper Title": "DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13845v1": {
            "Paper Title": "AvatarMe: Realistically Renderable 3D Facial Reconstruction\n  \"in-the-wild\"",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13326v1": {
            "Paper Title": "PointGMM: a Neural GMM Network for Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04302v2": {
            "Paper Title": "DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11025v1": {
            "Paper Title": "Automatic Modelling of Human Musculoskeletal Ligaments -- Framework\n  Overview and Model Quality Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09820v1": {
            "Paper Title": "Rig-space Neural Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08921v2": {
            "Paper Title": "Deep Parametric Shape Predictions using Distance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06395v2": {
            "Paper Title": "Neural Cages for Detail-Preserving 3D Deformations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08124v1": {
            "Paper Title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation from\n  Single-View Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04583v2": {
            "Paper Title": "TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape\n  and Garment Style",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04187v2": {
            "Paper Title": "Style-compatible Object Recommendation for Multi-room Indoor Scene\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06659v1": {
            "Paper Title": "Interactive Neural Style Transfer with Artists",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06520v1": {
            "Paper Title": "Symmetry Detection of Occluded Point Cloud Using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05863v1": {
            "Paper Title": "Towards Photo-Realistic Virtual Try-On by Adaptively\n  Generating$\\leftrightarrow$Preserving Image Content",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08723v1": {
            "Paper Title": "Latent Space Subdivision: Stable and Controllable Time Predictions for\n  Fluid Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03551v1": {
            "Paper Title": "STD-Net: Structure-preserving and Topology-adaptive Deformation Network\n  for 3D Reconstruction from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10137v2": {
            "Paper Title": "Audio-driven Talking Face Video Generation with Learning-based\n  Personalized Head Pose",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00410v1": {
            "Paper Title": "PF-Net: Point Fractal Network for 3D Point Cloud Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12623v1": {
            "Paper Title": "MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12354v4": {
            "Paper Title": "Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\n  Mesh Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10880v1": {
            "Paper Title": "PolyGen: An Autoregressive Generative Model of 3D Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10945v1": {
            "Paper Title": "Image Stylization: From Predefined to Personalized",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08897v1": {
            "Paper Title": "STW and SPIHT Wavelet compression using MATLAB wavelet Tool for Color\n  Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07481v1": {
            "Paper Title": "Quantitative Evaluation of Time-Dependent Multidimensional Projection\n  Techniques",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07002v1": {
            "Paper Title": "Jittering Samples using a kd-Tree Stratification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06597v1": {
            "Paper Title": "Analytic Marching: An Analytic Meshing Solution from Deep Implicit\n  Surface Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05509v1": {
            "Paper Title": "Replacing Mobile Camera ISP with a Single Deep Learning Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05721v1": {
            "Paper Title": "A New Exocentric Metaphor for Complex Path Following to Control a UAV\n  Using Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05409v1": {
            "Paper Title": "A User-centered Approach for Optimizing Information Visualizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09267v3": {
            "Paper Title": "Semantic Hierarchy Emerges in Deep Generative Representations for Scene\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03891v1": {
            "Paper Title": "SplitStreams: A Visual Metaphor for Evolving Hierarchies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10672v2": {
            "Paper Title": "GAC-GAN: A General Method for Appearance-Controllable Human Video Motion\n  Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02792v1": {
            "Paper Title": "AnimePose: Multi-person 3D pose estimation and animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00328v2": {
            "Paper Title": "Fast 3D Indoor Scene Synthesis with Discrete and Exact Layout Pattern\n  Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10589v2": {
            "Paper Title": "Front2Back: Single View 3D Shape Reconstruction via Front to Back\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11131v1": {
            "Paper Title": "Developing an Augmented Reality Tourism App through User-Centred Design\n  (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09792v1": {
            "Paper Title": "Running on Raygun",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10585v1": {
            "Paper Title": "An Automated Approach for the Discovery of Interoperability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06896v2": {
            "Paper Title": "Fast quasi-conformal regional flattening of the left atrium",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05201v1": {
            "Paper Title": "Everybody's Talkin': Let Me Talk as You Want",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02620v1": {
            "Paper Title": "Digesting the Elephant -- Experiences with Interactive Production\n  Quality Path Tracing of the Moana Island Scene",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13243v1": {
            "Paper Title": "Learning to Infer User Interface Attributes from Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12786v1": {
            "Paper Title": "Adding Custom Intersectors to the C++ Ray Tracing Template Library\n  Visionaray",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11932v1": {
            "Paper Title": "Skeleton Extraction from 3D Point Clouds by Decomposing the Object into\n  Parts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00986v1": {
            "Paper Title": "Inverse Rendering Techniques for Physically Grounded Image Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00521v1": {
            "Paper Title": "Lightform: Procedural Effects for Projected AR",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11568v1": {
            "Paper Title": "Blind Recovery of Spatially Varying Reflectance from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11567v1": {
            "Paper Title": "ConstructAide: Analyzing and Visualizing Construction Sites through\n  Photographs and Building Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12297v1": {
            "Paper Title": "Automatic Scene Inference for 3D Object Compositing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11565v1": {
            "Paper Title": "Rendering Synthetic Objects into Legacy Photographs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10352v3": {
            "Paper Title": "EFANet: Exchangeable Feature Alignment Network for Arbitrary Style\n  Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10379v2": {
            "Paper Title": "Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using\n  Parametric Level Set Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09596v1": {
            "Paper Title": "Comparing Hierarchical Data Structures for Sparse Volume Rendering with\n  Empty Space Skipping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09580v1": {
            "Paper Title": "MVF Designer: Design and Visualization of Morse Vector Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04769v2": {
            "Paper Title": "Computational Parquetry: Fabricated Style Transfer with Wood Pixels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06341v1": {
            "Paper Title": "Uncertainty Visualization of 2D Morse Complex Ensembles Using\n  Statistical Summary Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05099v1": {
            "Paper Title": "RoboCoDraw: Robotic Avatar Drawing with GAN-based Style Transfer and\n  Time-efficient Path Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01248v2": {
            "Paper Title": "Multiple Approaches to Frame Field Correction for CAD Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03416v1": {
            "Paper Title": "On the Optical Accuracy of the Salvator Mundi",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03310v1": {
            "Paper Title": "Geometric Capsule Autoencoders for 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06001v2": {
            "Paper Title": "Efficient Animation of Sparse Voxel Octrees for Real-Time Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01942v1": {
            "Paper Title": "Accelerating Surface Tension Calculation in SPH via Particle\n  Classification & Monte Carlo Integration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05172v3": {
            "Paper Title": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human\n  Digitization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12512v2": {
            "Paper Title": "Smooth Shells: Multi-Scale Shape Registration with Functional Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00321v1": {
            "Paper Title": "A SVBRDF Modeling Pipeline using Pixel Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.13032v1": {
            "Paper Title": "Safe Walking In VR using Augmented Virtuality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00739v1": {
            "Paper Title": "Continuous Histograms for Anisotropy of 2D Symmetric Piece-wise Linear\n  Tensor Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12327v1": {
            "Paper Title": "Inattentional Blindness for Redirected Walking Using Dynamic Foveated\n  Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12070v1": {
            "Paper Title": "Vectorizing Quantum Turbulence Vortex-Core Lines for Real-Time\n  Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11999v1": {
            "Paper Title": "Recovering Facial Reflectance and Geometry from Multi-view Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09642v4": {
            "Paper Title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11098v1": {
            "Paper Title": "StructEdit: Learning Structural Shape Variations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10274v1": {
            "Paper Title": "Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body\n  Robotics using NVIDIA CUDA",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08591v1": {
            "Paper Title": "Learning Stylized Character Expressions from Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13875v2": {
            "Paper Title": "Expressive Inverse Kinematics Solving in Real-time for Virtual and\n  Robotic Interactive Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06906v1": {
            "Paper Title": "Applying Rational Envelope curves for skinning purposes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06877v1": {
            "Paper Title": "Exploring Configurations for Multi-user Communication in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09707v3": {
            "Paper Title": "Collision Detection for Agents in Multi-Agent Pathfinding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.04015v2": {
            "Paper Title": "NormalNet: Learning-based Normal Filtering for Mesh Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06144v1": {
            "Paper Title": "A Penetration Metric for Deforming Tetrahedra using Object Norm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05204v1": {
            "Paper Title": "Locking-free Simulation of Isometric Thin Plates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04694v4": {
            "Paper Title": "Channel Decomposition into Painting Actions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04446v1": {
            "Paper Title": "Enhancing User Experience in Virtual Reality with Radial Basis Function\n  Interpolation Based Stereoscopic Camera Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.06034v2": {
            "Paper Title": "Eccentricity Effects on Blur and Depth Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07291v2": {
            "Paper Title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00767v1": {
            "Paper Title": "Learning to Infer Implicit Surfaces without 3D Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00189v1": {
            "Paper Title": "Learning-based Real-time Detection of Intrinsic Reflectional Symmetry",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ", a funda-\nmental problem of spectral convolution is its dependency on\nthe basis, making it dif\ufb01cult to be generalized to different\ndomains. To mitigate this, Yi et al. ",
                    "Citation Text": "Li Yi, Hao Su, Xingwen Guo, and Leonidas J Guibas. Sync-\nSpecCNN: Synchronized spectral CNN for 3d shape seg-\nmentation. In CVPR , pages 6584\u20136592, 2017.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00606",
                        "Citation Paper Title": "Title:SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.",
                        "Citation Paper Authors": "Authors:Li Yi, Hao Su, Xingwen Guo, Leonidas Guibas"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1910.13688v1": {
            "Paper Title": "Dual Illumination Estimation for Robust Exposure Correction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12056v2": {
            "Paper Title": "ETNet: Error Transition Network for Arbitrary Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.00863v3": {
            "Paper Title": "Deep Appearance Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12713v1": {
            "Paper Title": "Few-shot Video-to-Video Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "GANs. The proposed few-shot vid2vid model is based on GANs ",
                    "Citation Text": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio.\nGenerative adversarial networks. In Advances in Neural Information Processing Systems (NIPS) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1907.08884v2": {
            "Paper Title": "Human Extraction and Scene Transition utilizing Mask R-CNN",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01911v1": {
            "Paper Title": "BlenderProc",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11626v1": {
            "Paper Title": "Seeing What a GAN Cannot Generate",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10836v1": {
            "Paper Title": "Gloss, Color and Topography Scanning for Reproducing a Painting's\n  Appearance using 3D printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08044v2": {
            "Paper Title": "Non-Line-of-Sight Reconstruction using Efficient Transient Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00902v2": {
            "Paper Title": "Effects of Illumination on the Categorization of Shiny Materials",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09399v1": {
            "Paper Title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09166v1": {
            "Paper Title": "Dynamic Upsampling of Smoke through Dictionary-based Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08865v1": {
            "Paper Title": "Deck.gl: Large-scale Web-based Visual Analytics Made Easy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08685v1": {
            "Paper Title": "Real-Time Lip Sync for Live 2D Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08470v1": {
            "Paper Title": "Illumination-Based Data Augmentation for Robust Background Subtraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.06368v3": {
            "Paper Title": "Embedded Spectral Descriptors: Learning the point-wise correspondence\n  metric via Siamese neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08537v1": {
            "Paper Title": "Normal Estimation for 3D Point Clouds via Local Plane Constraint and\n  Multi-scale Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07615v2": {
            "Paper Title": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08398v1": {
            "Paper Title": "Statistical Parameter Selection for Clustering Persistence Diagrams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07192v1": {
            "Paper Title": "Animating Landscape: Self-Supervised Learning of Decoupled Motion and\n  Appearance for Single-Image Video Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02533v2": {
            "Paper Title": "C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06610v1": {
            "Paper Title": "Analyzing symmetry and symmetry breaking by computational aesthetic\n  measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05148v1": {
            "Paper Title": "Single Image BRDF Parameter Estimation with a Conditional Adversarial\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04942v1": {
            "Paper Title": "Point cloud ridge-valley feature enhancement based on position and\n  normal guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06082v2": {
            "Paper Title": "Local Fourier Slice Photography",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02055v1": {
            "Paper Title": "Neural Turtle Graphics for Modeling City Road Layouts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05253v1": {
            "Paper Title": "Adversarial Colorization Of Icons Based On Structure And Color\n  Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01546v1": {
            "Paper Title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking\n  Interface for Study and Reflection in VR Learning Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01304v1": {
            "Paper Title": "Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by\n  Exploiting Ray Locality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13501v1": {
            "Paper Title": "DSRGAN: Explicitly Learning Disentangled Representation of Underlying\n  Structure and Rendering for Image Generation without Tuple Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03299v2": {
            "Paper Title": "PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding\n  Module for Classification and Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13200v1": {
            "Paper Title": "Shape Analysis via Functional Map Construction and Bases Pursuit",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03438v2": {
            "Paper Title": "Metric Curvatures and their Applications 2: Metric Ricci Curvature and\n  Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12583v1": {
            "Paper Title": "Color continuity along the journey from ideas to objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08233v2": {
            "Paper Title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11842v2": {
            "Paper Title": "Self-Imitation Learning of Locomotion Movements through Termination\n  Curriculum",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09351v1": {
            "Paper Title": "An Experimental Comparison of Map-like Visualisations and Treemaps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11620v1": {
            "Paper Title": "Manufacturability Oriented Model Correction and Build Direction\n  Optimization for Additive Manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07732v2": {
            "Paper Title": "KeystoneDepth: Visualizing History in 3D",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02822v5": {
            "Paper Title": "Learning Implicit Fields for Generative Shape Modeling",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "learned an embedding space of\n3D voxel shapes for 3D shape inference from images and\nshape generation. Wu et al. ",
                    "Citation Text": "J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.\nLearning a probabilistic latent space of object shapes via 3d\ngenerative-adversarial modeling. In Advances in Neural In-\nformation Processing Systems (NIPS) , pages 82\u201390, 2016. 1,\n2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.07584",
                        "Citation Paper Title": "Title:Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
                        "Citation Paper Abstract": "Abstract:We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
                        "Citation Paper Authors": "Authors:Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1909.11622v1": {
            "Paper Title": "Photorealistic Material Editing Through Direct Image Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07865v4": {
            "Paper Title": "ZoomOut: Spectral Upsampling for Efficient Shape Correspondence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05483v1": {
            "Paper Title": "3D Ken Burns Effect from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05295v1": {
            "Paper Title": "Measures in Visualization Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09874v3": {
            "Paper Title": "Perceptual deep depth super-resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03669v1": {
            "Paper Title": "DensePoint: Learning Densely Contextual Representation for Efficient\n  Point Cloud Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01723v2": {
            "Paper Title": "Few-Shot Unsupervised Image-to-Image Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02807v1": {
            "Paper Title": "Real-time Deformation with Coupled Cages and Skeletons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02641v1": {
            "Paper Title": "Deep Iterative Frame Interpolation for Full-frame Video Stabilization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02165v1": {
            "Paper Title": "Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.02266v2": {
            "Paper Title": "ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range\n  Expansion from Low Dynamic Range Content",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01456v1": {
            "Paper Title": "Topologically-Guided Color Image Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00573v1": {
            "Paper Title": "Next Event Backtracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00551v1": {
            "Paper Title": "Implicit Progressive-Iterative Approximation for Curve and Surface\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00470v1": {
            "Paper Title": "Accelerating ADMM for Efficient Simulation and Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07441v4": {
            "Paper Title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.08861v3": {
            "Paper Title": "Animating Arbitrary Objects via Deep Motion Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02446v2": {
            "Paper Title": "A Robust Billboard-based Free-viewpoint Video Synthesizing Algorithm for\n  Sports Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.11505v1": {
            "Paper Title": "EventCap: Monocular 3D Capture of High-Speed Human Motions using an\n  Event Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12373v2": {
            "Paper Title": "Diverse Image Synthesis from Semantic Layouts via Conditional IMLE",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03842v2": {
            "Paper Title": "Barriers towards no-reference metrics application to compressed video\n  quality analysis: on the example of no-reference metric NIQE",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10754v2": {
            "Paper Title": "OperatorNet: Recovering 3D Shapes From Difference Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07371v2": {
            "Paper Title": "Everybody Dance Now",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10335v1": {
            "Paper Title": "Physics-Based Rendering for Improving Robustness to Rain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00606v4": {
            "Paper Title": "General Support-Effective Decomposition for Multi-Directional 3D\n  Printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09530v1": {
            "Paper Title": "A Flexible Neural Renderer for Material Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08893v1": {
            "Paper Title": "You Can't Publish Replication Studies (and How to Anyways)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08627v2": {
            "Paper Title": "BrainPainter: A software for the visualisation of brain structures,\n  biomarkers and associated pathological processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08597v1": {
            "Paper Title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08506v1": {
            "Paper Title": "Predicting Animation Skeletons for 3D Articulated Models via Volumetric\n  Nets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08505v1": {
            "Paper Title": "ColorNet -- Estimating Colorfulness in Natural Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.03794v3": {
            "Paper Title": "Unsupervised Deep Learning for Structured Shape Matching",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "containing 100 human\nshapes in 1-1 correspondence and the remeshed versions of\nSCAPE ",
                    "Citation Text": "Dragomir Anguelov, Praveen Srinivasan, Daphne\nKoller, Sebastian Thrun, Jim Rodgers, and James\nDavis. SCAPE: Shape Completion and Animation of\nPeople. In ACM Transactions on Graphics (TOG) ,\nvolume 24, pages 408\u2013416. ACM, 2005. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.4129v1",
                        "Citation Paper Title": "Title:Recovering Articulated Object Models from 3D Range Data",
                        "Citation Paper Abstract": "Abstract:We address the problem of unsupervised learning of complex articulated object models from 3D range data. We describe an algorithm whose input is a set of meshes corresponding to different configurations of an articulated object. The algorithm automatically recovers a decomposition of the object into approximately rigid parts, the location of the parts in the different object instances, and the articulated object skeleton linking the parts. Our algorithm first registers allthe meshes using an unsupervised non-rigid technique described in a companion paper. It then segments the meshes using a graphical model that captures the spatial contiguity of parts. The segmentation is done using the EM algorithm, iterating between finding a decomposition of the object into rigid parts, and finding the location of the parts in the object instances. Although the graphical model is densely connected, the object decomposition step can be performed optimally and efficiently, allowing us to identify a large number of object parts while avoiding local maxima. We demonstrate the algorithm on real world datasets, recovering a 15-part articulated model of a human puppet from just 7 different puppet configurations, as well as a 4 part model of a fiexing arm where significant non-rigid deformation was present.",
                        "Citation Paper Authors": "Authors:Dragomir Anguelov, Daphne Koller, Hoi-Cheung Pang, Praveen Srinivasan, Sebastian Thrun"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1908.08185v1": {
            "Paper Title": "Pro-Cam SSfM: Projector-Camera System for Structure and Spectral\n  Reflectance from Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08151v1": {
            "Paper Title": "Multi-level Graph Drawing using Infomap Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07117v1": {
            "Paper Title": "360-Degree Textures of People in Clothing from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04337v2": {
            "Paper Title": "VV-Net: Voxel VAE Net with Group Convolutions for Point Cloud\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07015v1": {
            "Paper Title": "The Topological Complexity of Spaces of Digital Jordan Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06922v1": {
            "Paper Title": "Thumbnails for Data Stories: A Survey of Current Practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06154v1": {
            "Paper Title": "Extending editing capabilities of subdivision schemes by refinement of\n  point-normal pairs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08180v3": {
            "Paper Title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03837v2": {
            "Paper Title": "LumiPath -- Towards Real-time Physically-based Rendering on Embedded\n  Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05932v1": {
            "Paper Title": "FSGAN: Subject Agnostic Face Swapping and Reenactment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11228v2": {
            "Paper Title": "BAE-NET: Branched Autoencoder for Shape Co-Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11738v2": {
            "Paper Title": "DVP: Data Visualization Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04338v1": {
            "Paper Title": "Convolutional Humanoid Animation via Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07716v2": {
            "Paper Title": "Conditional Parallel Coordinates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.02875v3": {
            "Paper Title": "Learning to Infer and Execute 3D Shape Programs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02681v1": {
            "Paper Title": "Rendering Point Clouds with Compute Shaders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02507v1": {
            "Paper Title": "Mesh Variational Autoencoders with Edge Contraction Pooling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02111v1": {
            "Paper Title": "Point Cloud Super Resolution with Adversarial Residual Graph Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01938v1": {
            "Paper Title": "Heterogeneous porous scaffold generation in trivariate B-spline solid\n  with triply periodic minimal surface in the parametric domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01906v1": {
            "Paper Title": "Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes\n  Using Hardware Accelerated Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01809v1": {
            "Paper Title": "Geometric Sample Reweighting for Monte Carlo Integration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00629v1": {
            "Paper Title": "Color Crafting: Automating the Construction of Designer Quality Color\n  Ramps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00576v1": {
            "Paper Title": "Evaluating Ordering Strategies of Star Glyph Axes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00575v1": {
            "Paper Title": "StructureNet: Hierarchical Graph Networks for 3D Shape Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00398v1": {
            "Paper Title": "Extract and Merge: Merging extracted humans from different images\n  utilizing Mask R-CNN",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00073v1": {
            "Paper Title": "Biased Average Position Estimates in Line and Bar Graphs:\n  Underestimation, Overestimation, and Perceptual Pull",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11308v1": {
            "Paper Title": "SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10402v1": {
            "Paper Title": "Data-Driven Physical Face Inversion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10163v1": {
            "Paper Title": "A system for efficient 3D printed stop-motion face animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09375v1": {
            "Paper Title": "DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D\n  Lung Models from Single-View Projections by Deep Deformation Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05783v1": {
            "Paper Title": "Improving the Projection of Global Structures in Data through Spanning\n  Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05552v1": {
            "Paper Title": "Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded\n  Labors of Brick Kilns in South Asia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03953v1": {
            "Paper Title": "Efficient Cloth Simulation using Miniature Cloth and Upscaling Deep\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09787v2": {
            "Paper Title": "ZomeFab: Cost-effective Hybrid Fabrication with Zometools",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03118v1": {
            "Paper Title": "Fast Universal Style Transfer for Artistic and Photorealistic Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02102v1": {
            "Paper Title": "EVA: Generating Emotional Behavior of Virtual Agents using Expressive\n  Features of Gait and Gaze",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00161v2": {
            "Paper Title": "Temporally Coherent Full 3D Mesh Human Pose Recovery from Monocular\n  Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00523v1": {
            "Paper Title": "Geodesic Centroidal Voronoi Tessellations: Theories, Algorithms and\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00377v1": {
            "Paper Title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement\n  Characteristics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04756v2": {
            "Paper Title": "Deep-learning the Latent Space of Light Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01060v3": {
            "Paper Title": "PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11557v1": {
            "Paper Title": "Flexible SVBRDF Capture with a Multi-Image Deep Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11478v1": {
            "Paper Title": "A Convolutional Decoder for Point Clouds using Adaptive Instance\n  Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11633v1": {
            "Paper Title": "ORRB -- OpenAI Remote Rendering Backend",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10886v1": {
            "Paper Title": "Joint Multi-frame Detection and Segmentation for Multi-cell Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10669v1": {
            "Paper Title": "Structural Design Using Laplacian Shells",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.10754v2": {
            "Paper Title": "Loopy Cuts: Surface-Field Aware Block Decomposition for Hex-Meshing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11745v2": {
            "Paper Title": "Learning to Synthesize Motion Blur",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07968v1": {
            "Paper Title": "Camouflage Design of Analysis Based on HSV Color Statistics and K-means\n  Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07870v1": {
            "Paper Title": "Analytical Derivatives for Differentiable Renderer: 3D Pose Estimation\n  by Silhouette Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07645v1": {
            "Paper Title": "3D Geometric salient patterns analysis on 3D meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07316v1": {
            "Paper Title": "DeepView: View Synthesis with Learned Gradient Descent",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03366v2": {
            "Paper Title": "Scan-flood Fill(SCAFF): an Efficient Automatic Precise Region Filling\n  Algorithm for Complicated Regions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06113v1": {
            "Paper Title": "A Survey on Deep Learning Architectures for Image-based Depth\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "Re\ufb01nement XL1\n1\u000e 2:67 1:40 1:61 2:88 1:51 1:74 2:67 1:40 1:61 2:88 1:51 1:74 0:5 GPU @ 2.5 Ghz\n(C/C++)\nEdgeStereo ",
                    "Citation Text": "X. Song, X. Zhao, H. Hu, and L. Fang, \u201cEdgeStereo: A Context\nIntegrated Residual Pyramid Network for Stereo Matching,\u201d\nACCV , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05196",
                        "Citation Paper Title": "Title:EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching",
                        "Citation Paper Abstract": "Abstract:Recent convolutional neural networks, especially end-to-end disparity estimation models, achieve remarkable performance on stereo matching task. However, existed methods, even with the complicated cascade structure, may fail in the regions of non-textures, boundaries and tiny details. Focus on these problems, we propose a multi-task network EdgeStereo that is composed of a backbone disparity network and an edge sub-network. Given a binocular image pair, our model enables end-to-end prediction of both disparity map and edge map. Basically, we design a context pyramid to encode multi-scale context information in disparity branch, followed by a compact residual pyramid for cascaded refinement. To further preserve subtle details, our EdgeStereo model integrates edge cues by feature embedding and edge-aware smoothness loss regularization. Comparative results demonstrates that stereo matching and edge detection can help each other in the unified model. Furthermore, our method achieves state-of-art performance on both KITTI Stereo and Scene Flow benchmarks, which proves the effectiveness of our design.",
                        "Citation Paper Authors": "Authors:Xiao Song, Xu Zhao, Hanwen Hu, Liangji Fang"
                    },
                    "Keywords": []
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Raw disparity XL5\n1\u000e 4:31 1:71 2:14 4:62 1:86 2:32 4:31 1:71 2:14 4:62 1:86 2:32 0:41 Nvidia GTX Titan\nXp\nZhong et al. ",
                    "Citation Text": "Y. Zhong, Y. Dai, and H. Li, \u201cSelf-supervised learning for\nstereo matching with self-improving ability,\u201d arXiv preprint\narXiv:1709.00930 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.00930",
                        "Citation Paper Title": "Title:Self-Supervised Learning for Stereo Matching with Self-Improving Ability",
                        "Citation Paper Abstract": "Abstract:Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.",
                        "Citation Paper Authors": "Authors:Yiran Zhong, Yuchao Dai, Hongdong Li"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1906.05710v1": {
            "Paper Title": "RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed\n  Joints and Precision-Cut Rods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05260v1": {
            "Paper Title": "VIPER: Volume Invariant Position-based Elastic Rods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04777v1": {
            "Paper Title": "Estimating Homogeneous Data-driven BRDF Parameters from a Reflectance\n  Map under Known Natural Lighting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04576v1": {
            "Paper Title": "Multi-Resolution Rendering for Computationally Expensive Lighting\n  Effects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05075v1": {
            "Paper Title": "Blue-noise sampling for human retinal cone spatial distribution modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04910v1": {
            "Paper Title": "Inferring 3D Shapes from Image Collections using Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03841v1": {
            "Paper Title": "Synthesizing 3D Shapes from Silhouette Image Collections using\n  Multi-projection Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00240v3": {
            "Paper Title": "Fast and Accurate Reconstruction of Pan-Tilt RGB-D Scans via Axis Bound\n  Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03027v1": {
            "Paper Title": "CrossFill: Foam Structures with Graded Density for Continuous Material\n  Extrusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05921v1": {
            "Paper Title": "Symmetric Algorithmic Components for Shape Analysis with Diffeomorphisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02470v1": {
            "Paper Title": "StyleNAS: An Empirical Study of Neural Architecture Search to Uncover\n  Surprisingly Fast End-to-End Universal Style Transfer Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02426v1": {
            "Paper Title": "Salient Building Outline Enhancement and Extraction Using Iterative L0\n  Smoothing and Line Enhancing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01524v1": {
            "Paper Title": "Text-based Editing of Talking-head Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02898v3": {
            "Paper Title": "Nutty-based Robot Animation -- Principles and Practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00544v1": {
            "Paper Title": "3D Magic Mirror: Automatic Video to 3D Caricature Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00124v1": {
            "Paper Title": "Learning Patterns in Sample Distributions for Monte Carlo Variance\n  Reduction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13187v1": {
            "Paper Title": "Use of convexity in contour detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12228v2": {
            "Paper Title": "Differentiable Visual Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01627v1": {
            "Paper Title": "Benchmark of Polygon Quality Metrics for Polytopal Element Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10822v1": {
            "Paper Title": "EgoFace: Egocentric Face Performance Capture and Videorealistic\n  Reenactment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10793v1": {
            "Paper Title": "Unsupervised Intuitive Physics from Past Experiences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07601v3": {
            "Paper Title": "Relation-Shape Convolutional Neural Network for Point Cloud Analysis",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "2k,nor 81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nSyncCNN ",
                    "Citation Text": "L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:\nSynchronized spectral CNN for 3D shape segmentation. In\nCVPR , pages 6584\u20136592, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00606",
                        "Citation Paper Title": "Title:SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.",
                        "Citation Paper Authors": "Authors:Li Yi, Hao Su, Xingwen Guo, Leonidas Guibas"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1905.10444v1": {
            "Paper Title": "Overt visual attention on rendered 3D objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06326v3": {
            "Paper Title": "Synthetic Defocus and Look-Ahead Autofocus for Casual Videography",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05411v2": {
            "Paper Title": "Measuring and simulating latency in interactive remote rendering systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07515v1": {
            "Paper Title": "Learning Perspective Undistortion of Portraits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07432v1": {
            "Paper Title": "Evaluation of 4D Light Field Compression Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13149v1": {
            "Paper Title": "The Art of Food: Meal Image Synthesis from Ingredients",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05161v1": {
            "Paper Title": "Spectral Coarsening of Geometric Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09193v5": {
            "Paper Title": "GRAINS: Generative Recursive Autoencoders for INdoor Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02586v1": {
            "Paper Title": "Picturing Bivariate Separable-Features for Univariate Vector Magnitudes\n  in Large-Magnitude-Range Quantum Physics Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00889v1": {
            "Paper Title": "Local Light Field Fusion: Practical View Synthesis with Prescriptive\n  Sampling Guidelines",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.03705v1": {
            "Paper Title": "A note on 'A fully parallel 3D thinning algorithm and its applications'",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06216v2": {
            "Paper Title": "ABC: A Big CAD Model Dataset For Geometric Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12356v1": {
            "Paper Title": "Deferred Neural Rendering: Image Synthesis using Neural Textures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12294v1": {
            "Paper Title": "Synthetic Data Generation and Adaption for Object Detection in Smart\n  Vending Machines",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", a lot of CNN-based\ndetectors have been suggested, including two different kinds\nof approaches. One regards the detection task as a regression\nor classi\ufb01cation problem, make a \ufb01xed number of predictions\non grid (one stage) ",
                    "Citation Text": "J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look\nonce: Uni\ufb01ed, real-time object detection,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2016, pp. 779\u2013\n788.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02640",
                        "Citation Paper Title": "Title:You Only Look Once: Unified, Real-Time Object Detection",
                        "Citation Paper Abstract": "Abstract:We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\nOur unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
                        "Citation Paper Authors": "Authors:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1904.12240v1": {
            "Paper Title": "Structurally optimized shells",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11621v1": {
            "Paper Title": "Meta-Sim: Learning to Generate Synthetic Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11084v1": {
            "Paper Title": "How much do you perceive this? An analysis on perceptions of geometric\n  features, personalities and emotions in virtual humans (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10213v1": {
            "Paper Title": "Surface2Volume: Surface Segmentation Conforming Assemblable Volumetric\n  Partition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08225v1": {
            "Paper Title": "Rendering of Complex Heterogenous Scenes using Progressive Blue Surfels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06903v1": {
            "Paper Title": "Learning Deformable Kernels for Image and Video Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06447v1": {
            "Paper Title": "Learning Shape Templates with Structured Implicit Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05814v1": {
            "Paper Title": "Probabilistic Permutation Synchronization using the Riemannian Structure\n  of the Birkhoff Polytope",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05448v1": {
            "Paper Title": "Predicting Future Pedestrian Motion in Video Sequences using Crowd\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05440v1": {
            "Paper Title": "Generating Animations from Screenplays",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05373v1": {
            "Paper Title": "Pixel-Adaptive Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05124v1": {
            "Paper Title": "Predicting Novel Views Using Generative Adversarial Query Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04998v1": {
            "Paper Title": "Depth from Videos in the Wild: Unsupervised Monocular Depth Learning\n  from Unknown Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11155v2": {
            "Paper Title": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained\n  Object Generation and Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04290v1": {
            "Paper Title": "Neural Rerendering in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03620v1": {
            "Paper Title": "Teaching GANs to Sketch in Vector Format",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "is given by:\n(8) r\u0012J(\u0012) =TX\nt=1EY1:t\u00001\u0018^G\u0012\u0014X\nyt2Yr\u0012^G\u0012(ytjY1:t\u00001)\u0001Q^G\u0012\nD\u001e(Y1:t\u00001;yt)\u0015\nUsing likelihood ratios ",
                    "Citation Text": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy\ngradient. In Thirty-First AAAI Conference on Arti\ufb01cial Intelligence , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.05473",
                        "Citation Paper Title": "Title:SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
                        "Citation Paper Abstract": "Abstract:As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.",
                        "Citation Paper Authors": "Authors:Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1811.10943v2": {
            "Paper Title": "Deep Geometric Prior for Surface Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "). The closest problems these\ntypes of networks solve is shape completion and point cloud\nupsampling.\nShape completion is considered, e.g., in ",
                    "Citation Text": "A. Dai, C. R. Qi, and M. Nie\u00dfner. Shape completion\nusing 3d-encoder-predictor cnns and shape synthesis.\nInProc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR) , volume 3, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00101",
                        "Citation Paper Title": "Title:Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis",
                        "Citation Paper Abstract": "Abstract:We introduce a data-driven approach to complete partial 3D shapes through a combination of volumetric deep neural networks and 3D shape synthesis. From a partially-scanned input shape, our method first infers a low-resolution -- but complete -- output. To this end, we introduce a 3D-Encoder-Predictor Network (3D-EPN) which is composed of 3D convolutional layers. The network is trained to predict and fill in missing data, and operates on an implicit surface representation that encodes both known and unknown space. This allows us to predict global structure in unknown areas at high accuracy. We then correlate these intermediary results with 3D geometry from a shape database at test time. In a final pass, we propose a patch-based 3D shape synthesis method that imposes the 3D geometry from these retrieved shapes as constraints on the coarsely-completed mesh. This synthesis process enables us to reconstruct fine-scale detail and generate high-resolution output while respecting the global mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms state-of-the-art completion method, the main contribution in our work lies in the combination of a data-driven shape predictor and analytic 3D shape synthesis. In our results, we show extensive evaluations on a newly-introduced shape completion benchmark for both real-world and synthetic data.",
                        "Citation Paper Authors": "Authors:Angela Dai, Charles Ruizhongtai Qi, Matthias Nie\u00dfner"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1904.02756v1": {
            "Paper Title": "Blind Visual Motif Removal from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02348v1": {
            "Paper Title": "Orthogonal Voronoi Diagram and Treemap",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09396v2": {
            "Paper Title": "HMLFC: Hierarchical Motion-Compensated Light Field Compression for\n  Interactive Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02526v1": {
            "Paper Title": "Constrained Generative Adversarial Networks for Interactive Image\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.10024v2": {
            "Paper Title": "Cross-Domain Image Manipulation by Demonstration",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ", great\nstrides have been made in controlled neural image manipu-\nlation. For example, the work of Thies et al. ",
                    "Citation Text": "J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and\nM. NieBner. Face2Face: Real-time face capture and reenact-\nment of RGB videos. IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 2387\u20132395, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.14808",
                        "Citation Paper Title": "Title:Face2Face: Real-time Face Capture and Reenactment of RGB Videos",
                        "Citation Paper Abstract": "Abstract:We present Face2Face, a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.",
                        "Citation Paper Authors": "Authors:Justus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1904.01428v1": {
            "Paper Title": "Non-Rigid Point Set Registration Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01175v1": {
            "Paper Title": "DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00275v1": {
            "Paper Title": "Prediction Model for Semitransparent Watercolor Pigment Mixtures Using\n  Deep Learning with a Dataset of Transmittance and Reflectance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00824v1": {
            "Paper Title": "Training Object Detectors on Synthetic Images Containing Reflecting\n  Materials",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00722v1": {
            "Paper Title": "Learning Soft Tissue Behavior of Organs for Surgical Navigation with\n  Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11286v3": {
            "Paper Title": "Patch-based Progressive 3D Point Set Upsampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08642v1": {
            "Paper Title": "Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08356v1": {
            "Paper Title": "Machine Learning for Data-Driven Movement Generation: a Review of the\n  State of the Art",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08004v1": {
            "Paper Title": "ReviewerNet: Visualizing Citation and Authorship Relations for Finding\n  Reviewers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06763v1": {
            "Paper Title": "Smart, Deep Copy-Paste",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06490v1": {
            "Paper Title": "colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03911v2": {
            "Paper Title": "Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D\n  Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04820v2": {
            "Paper Title": "Petascale Cloud Supercomputing for Terapixel Visualization of a Digital\n  Twin",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06657v1": {
            "Paper Title": "Effects of Self-Avatar and Gaze on Avoidance Movement Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.10123v3": {
            "Paper Title": "Latent-space Physics: Towards Learning the Temporal Evolution of Fluid\n  Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00899v1": {
            "Paper Title": "Robust corner and tangent point detection for strokes with deep learning\n  approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00695v1": {
            "Paper Title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated\n  Temporal Fully-Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08755v1": {
            "Paper Title": "Parallel Rendering and Large Data Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08228v1": {
            "Paper Title": "A Comprehensive Theory and Variational Framework for Anti-aliasing\n  Sampling Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.07854v4": {
            "Paper Title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.09076v3": {
            "Paper Title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach\n  to Visual Blending",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.02651v2": {
            "Paper Title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\n  Differentiable Renderer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.05672v1": {
            "Paper Title": "Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution\n  via LSTM Modelling on Epipolar Plane Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04805v1": {
            "Paper Title": "Task-based Augmented Contour Trees with Fibonacci Heaps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04285v1": {
            "Paper Title": "Puppet Dubbing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11220v2": {
            "Paper Title": "Content-Preserving Image Stitching with Regular Boundary Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02906v1": {
            "Paper Title": "X3D in Urban Planning - Savannah in 3D",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02806v1": {
            "Paper Title": "Automated pebble mosaic stylization of images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.05395v1": {
            "Paper Title": "Realistic Image Generation using Region-phrase Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.00801v1": {
            "Paper Title": "A Robust Volume Conserving Method for Character-Water Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.07449v3": {
            "Paper Title": "CNNs based Viewpoint Estimation for Volume Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.01192v1": {
            "Paper Title": "Advances in the Treatment of Trimmed CAD Models due to Isogeometric\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.10939v3": {
            "Paper Title": "Automatic Generation of Constrained Furniture Layouts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.08397v1": {
            "Paper Title": "Periodic-corrected data driven coupling of blood flow and vessel wall\n  for virtual surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.07165v1": {
            "Paper Title": "Generation High resolution 3D model from natural language by Generative\n  Adversarial Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.06931v1": {
            "Paper Title": "A Monte Carlo Framework for Rendering Speckle Statistics in Scattering\n  Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06658v1": {
            "Paper Title": "Surface Compression Using Dynamic Color Palettes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.06034v1": {
            "Paper Title": "High-speed Video from Asynchronous Camera Array",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06047v3": {
            "Paper Title": "AlSub: Fully Parallel and Modular Subdivision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04686v1": {
            "Paper Title": "Image Synthesis and Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04161v1": {
            "Paper Title": "Joint Stabilization and Direction of 360\u00b0Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.03968v1": {
            "Paper Title": "A Fully Bayesian Infinite Generative Model for Dynamic Texture\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10940v2": {
            "Paper Title": "Functional Maps Representation on Product Manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.02508v1": {
            "Paper Title": "An Application of Manifold Learning in Global Shape Descriptors",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ", several existing discretization methods, including variants of linear\nFEM [32, 33] and heat kernel weighting proposed in ",
                    "Citation Text": "Mikhail Belkin, Jian Sun, and Yusu Wang. Discrete laplace operator on\nmeshed surfaces. In Proceedings of the twenty-fourth annual symposium on\nComputational geometry , pages 278{287. ACM, 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1211.6727",
                        "Citation Paper Title": "Title:Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries",
                        "Citation Paper Abstract": "Abstract:Recently, much of the existing work in manifold learning has been done under the assumption that the data is sampled from a manifold without boundaries and singularities or that the functions of interest are evaluated away from such points. At the same time, it can be argued that singularities and boundaries are an important aspect of the geometry of realistic data.\nIn this paper we consider the behavior of graph Laplacians at points at or near boundaries and two main types of other singularities: intersections, where different manifolds come together and sharp \"edges\", where a manifold sharply changes direction. We show that the behavior of graph Laplacian near these singularities is quite different from that in the interior of the manifolds. In fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of Fourier series, can be observed in the behavior of graph Laplacian near such points. Unlike in the interior of the domain, where graph Laplacian converges to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a first-order differential operator, which exhibits different scaling behavior as a function of the kernel width. One important implication is that while points near the singularities occupy only a small part of the total volume, the difference in scaling results in a disproportionately large contribution to the total behavior. Another significant finding is that while the scaling behavior of the operator is the same near different types of singularities, they are very distinct at a more refined level of analysis.\nWe believe that a comprehensive understanding of these structures in addition to the standard case of a smooth manifold can take us a long way toward better methods for analysis of complex non-linear data and can lead to significant progress in algorithm design.",
                        "Citation Paper Authors": "Authors:Mikhail Belkin, Qichao Que, Yusu Wang, Xueyuan Zhou"
                    },
                    "Keywords": [
                        "descriptor",
                        "retrieval",
                        "shape",
                        ";",
                        "Scale-invariant",
                        "Shape"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1901.02037v1": {
            "Paper Title": "Modeling Data-Driven Dominance Traits for Virtual Characters using Gait\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00238v2": {
            "Paper Title": "Singularity Structure Simplification of Hexahedral Mesh via Weighted\n  Ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05870v4": {
            "Paper Title": "Lifted Wasserstein Matcher for Fast and Robust Topology Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10650v1": {
            "Paper Title": "Sampling Using Neural Networks for colorizing the grayscale images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.10111v1": {
            "Paper Title": "A Survey on Non-rigid 3D Shape Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08718v2": {
            "Paper Title": "Wide Activation for Efficient and Accurate Image Super-Resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09351v4": {
            "Paper Title": "3D-Aware Scene Manipulation via Inverse Graphics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07273v1": {
            "Paper Title": "cellPACKexplorer: Interactive Model Building for Volumetric Data of\n  Complex Cells",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04944v1": {
            "Paper Title": "Computational Fluid Dynamics on 3D Point Set Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03235v2": {
            "Paper Title": "Fashion is Taking Shape: Understanding Clothing Preference Based on Body\n  Shape From Online Sources",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04303v1": {
            "Paper Title": "Analytic heuristics for a fast DSC-MRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04233v1": {
            "Paper Title": "Antara: An Interactive 3D Volume Rendering and Visualization Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10597v2": {
            "Paper Title": "GAN Dissection: Visualizing and Understanding Generative Adversarial\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02725v1": {
            "Paper Title": "Visual Object Networks: Image Generation with Disentangled 3D\n  Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02246v1": {
            "Paper Title": "Photo Wake-Up: 3D Character Animation from a Single Photo",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01598v1": {
            "Paper Title": "Monocular Total Capture: Posing Face, Body, and Hands in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01525v1": {
            "Paper Title": "A Face-to-Face Neural Conversation Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01387v1": {
            "Paper Title": "Estimating 6D Pose From Localizing Designated Surface Keypoints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.06783v2": {
            "Paper Title": "Low Rank Matrix Approximation for Geometry Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06601v2": {
            "Paper Title": "Video-to-Video Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "Generative Adversarial Networks (GANs). We build our model on GANs ",
                    "Citation Text": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio.\nGenerative adversarial networks. In Advances in Neural Information Processing Systems (NIPS) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1812.01677v1": {
            "Paper Title": "A Pixel-Based Framework for Data-Driven Clothing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12670v1": {
            "Paper Title": "Instance-level Facial Attributes Transfer with Geometry-Aware Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12599v1": {
            "Paper Title": "Gregory Solid Construction for Polyhedral Volume Parameterization by\n  Sparse Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12597v1": {
            "Paper Title": "Constructing Trivariate B-splines with Positive Jacobian by Pillow\n  Operation and Geometric Iterative Fitting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12463v1": {
            "Paper Title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.04867v3": {
            "Paper Title": "Single-image Tomography: 3D Volumes from 2D Cranial X-Rays",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11482v1": {
            "Paper Title": "Image Reconstruction with Predictive Filter Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10175v1": {
            "Paper Title": "Multilevel active registration for kinect human body scans: from low\n  quality to high quality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10121v1": {
            "Paper Title": "Foreground Clustering for Joint Segmentation and Localization in Videos\n  and Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10036v1": {
            "Paper Title": "Procedural Crowd Generation for Semantically Augmented Virtual Cities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08170v1": {
            "Paper Title": "Sketch-R2CNN: An Attentive Network for Vector Sketch Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07390v1": {
            "Paper Title": "A Study on 3D Surface Graph Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.08004v1": {
            "Paper Title": "Photorealistic Facial Synthesis in the Dimensional Affect Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07023v1": {
            "Paper Title": "An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03151v1": {
            "Paper Title": "DragonPaint: Rule based bootstrapping for small data with an application\n  to cartoon coloring",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02626v1": {
            "Paper Title": "Printable Aggregate Elements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02517v1": {
            "Paper Title": "NeuralDrop: DNN-based Simulation of Small-Scale Liquid Flows on Solids",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07791v5": {
            "Paper Title": "PointCNN: Convolution On $\\mathcal{X}$-Transformed Points",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00548v1": {
            "Paper Title": "Enhancing the Structural Performance of Additively Manufactured Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08497v2": {
            "Paper Title": "ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06358v2": {
            "Paper Title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11536v1": {
            "Paper Title": "Automatic Graphics Program Generation using Attention-Based Hierarchical\n  Decoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.04305v2": {
            "Paper Title": "Light Transport Simulation via Generalized Multiple Importance Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10933v1": {
            "Paper Title": "Practical Shape Analysis and Segmentation Methods for Point Cloud Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.11586v4": {
            "Paper Title": "Toward Multimodal Image-to-Image Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.07882v1": {
            "Paper Title": "Measuring the Effects of Scalar and Spherical Colormaps on Ensembles of\n  DMRI Tubes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.10417v2": {
            "Paper Title": "Divergence-Free Shape Interpolation and Correspondence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.03599v2": {
            "Paper Title": "SFV: Reinforcement Learning of Physical Skills from Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.04703v1": {
            "Paper Title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse\n  Inertial Measurements in Real Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02042v1": {
            "Paper Title": "Learning Bidirectional LSTM Networks for Synthesizing 3D Mesh Animation\n  Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04455v3": {
            "Paper Title": "Continuous and Orientation-preserving Correspondences via Functional\n  Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.01575v1": {
            "Paper Title": "Deep Fundamental Matrix Estimation without Correspondences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.01406v1": {
            "Paper Title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02303v1": {
            "Paper Title": "Multi-directional Geodesic Neural Networks via Equivariant Convolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02560v2": {
            "Paper Title": "A Deeper Look at 3D Shape Classifiers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00107v1": {
            "Paper Title": "Superimposition-guided Facial Reconstruction from Skull",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00028v1": {
            "Paper Title": "Data-Driven Modeling of Group Entitativity in Virtual Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10402v1": {
            "Paper Title": "3D Face Synthesis Driven by Personality Impression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06664v2": {
            "Paper Title": "A Simple Approach to Intrinsic Correspondence Learning on Unstructured\n  3D Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09270v1": {
            "Paper Title": "Next Generation of Star Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05398v1": {
            "Paper Title": "SCORES: Shape Composition with Recursive Substructure Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04765v1": {
            "Paper Title": "Video to Fully Automatic 3D Hair Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03144v1": {
            "Paper Title": "Texturing and Deforming Meshes with Casual Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02057v1": {
            "Paper Title": "Surface Light Field Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01890v1": {
            "Paper Title": "Full-body High-resolution Anime Generation with Progressive\n  Structure-conditional Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.01922v2": {
            "Paper Title": "Vectorization of Line Drawings via PolyVector Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01720v1": {
            "Paper Title": "Extending Mandelbox Fractals with Shape Inversions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01334v1": {
            "Paper Title": "Distributed-Memory Forest-of-Octrees Raycasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05999v2": {
            "Paper Title": "Mumford-Shah Mesh Processing using the Ambrosio-Tortorelli Functional",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.10654v1": {
            "Paper Title": "Gibson Env: Real-World Perception for Embodied Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.10083v1": {
            "Paper Title": "Differential and integral invariants under Mobius transformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01471v1": {
            "Paper Title": "Chest X-ray Inpainting with Deep Generative Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07840v1": {
            "Paper Title": "Learning to Importance Sample in Primary Sample Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07778v1": {
            "Paper Title": "StretchDenoise: Parametric Curve Reconstruction with Guarantees by\n  Separating Connectivity from Residual Uncertainty of Samples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06847v1": {
            "Paper Title": "Deep Video-Based Performance Cloning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06715v1": {
            "Paper Title": "Image-based remapping of spatially-varying material appearance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03823v2": {
            "Paper Title": "Learning Discriminative 3D Shape Representations by View Discerning\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.11585v2": {
            "Paper Title": "High-Resolution Image Synthesis and Semantic Manipulation with\n  Conditional GANs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05174v1": {
            "Paper Title": "Recycle-GAN: Unsupervised Video Retargeting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04931v1": {
            "Paper Title": "Neural Material: Learning Elastic Constitutive Material and Damping\n  Models from Sparse Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04545v1": {
            "Paper Title": "MT-VAE: Learning Motion Transformations to Generate Multimodal Human\n  Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.04121v1": {
            "Paper Title": "Image Inpainting Based on a Novel Criminisi Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03338v1": {
            "Paper Title": "DeepMag: Source Specific Motion Magnification Using Gradient Ascent",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.01427v1": {
            "Paper Title": "Structure-Aware Shape Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.00328v1": {
            "Paper Title": "There is more to PCG than Meets the Eye: NPC AI, Dynamic Camera, PVS and\n  Lightmaps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.02684v3": {
            "Paper Title": "Learning-based Video Motion Magnification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11847v1": {
            "Paper Title": "Fast Sketch Segmentation and Labeling with Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11212v1": {
            "Paper Title": "Persistence Atlas for Critical Point Variability in Ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11079v1": {
            "Paper Title": "ReenactGAN: Learning to Reenact Faces via Boundary Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09594v2": {
            "Paper Title": "Tracking Emerges by Colorizing Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.10517v1": {
            "Paper Title": "FARM: Functional Automatic Registration Method for 3D Human Bodies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09064v1": {
            "Paper Title": "CaricatureShop: Personalized and Photorealistic Caricature Sketching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08937v3": {
            "Paper Title": "Deep High Dynamic Range Imaging with Large Foreground Motions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08486v1": {
            "Paper Title": "Conformal Mesh Parameterization Using Discrete Calabi Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08474v1": {
            "Paper Title": "Robust Edge-Preserved Surface Mesh Polycube Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08304v1": {
            "Paper Title": "Deep Learning Parametrization for B-Spline Curve Approximation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.08323v3": {
            "Paper Title": "Pigmento: Pigment-Based Image Analysis and Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.07459v3": {
            "Paper Title": "A Complementary Tracking Model with Multiple Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.07006v2": {
            "Paper Title": "Large Margin Structured Convolution Operator for Thermal Infrared Object\n  Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06551v1": {
            "Paper Title": "A Deep Learning Driven Active Framework for Segmentation of Large 3D\n  Shape Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.02906v3": {
            "Paper Title": "Multi-Agent Diverse Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.11335v2": {
            "Paper Title": "Learning a Shared Shape Space for Multimodal Garment Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03520v2": {
            "Paper Title": "Multiresolution Tree Networks for 3D Point Cloud Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.04184v1": {
            "Paper Title": "Indy: a virtual reality multi-player game for navigation skills training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.07467v2": {
            "Paper Title": "HairNet: Single-View Hair Reconstruction using Convolutional Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03249v1": {
            "Paper Title": "StyleBlit: Fast Example-Based Stylization with Local Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02921v1": {
            "Paper Title": "Inferring Quality in Point Cloud-based 3D Printed Objects using\n  Topological Data Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02222v1": {
            "Paper Title": "Digital Geometry, a Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03350v1": {
            "Paper Title": "Detecting Socio-Economic Impact of Cultural Investment Through\n  Geo-Social Network Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.01519v1": {
            "Paper Title": "Learning Fuzzy Set Representations of Partial Shapes on Dual Embedding\n  Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00866v1": {
            "Paper Title": "Solid Geometry Processing on Deconstructed Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00687v1": {
            "Paper Title": "Simplifying Urban Data Fusion with BigSUR",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00410v1": {
            "Paper Title": "$P_N$-Method for Multiple Scattering in Participating Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.04456v2": {
            "Paper Title": "Liquid Splash Modeling with Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09070v1": {
            "Paper Title": "Generative Models for Pose Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08485v1": {
            "Paper Title": "Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction\n  from Binary Mask Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08460v1": {
            "Paper Title": "Homology-Preserving Dimensionality Reduction via Manifold Landmarking\n  and Tearing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08126v1": {
            "Paper Title": "Topological Data Analysis Made Easy with the Topology ToolKit",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01225v3": {
            "Paper Title": "Palette-based image decomposition, harmonization, and color transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.07729v1": {
            "Paper Title": "Void Space Surfaces to Convey Depth in Vessel Visualizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.10819v2": {
            "Paper Title": "Surface Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06710v1": {
            "Paper Title": "End-to-end Sampling Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06613v1": {
            "Paper Title": "Coupled Fluid Density and Motion from Single Views",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05385v1": {
            "Paper Title": "Perceptual Rasterization for Head-mounted Display Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03967v2": {
            "Paper Title": "Latent Space Representation for Shape Analysis and Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.04001v2": {
            "Paper Title": "Weighted simplicial complex reconstruction from mobile laser scanning\n  using sensor topology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03053v1": {
            "Paper Title": "The Saturated Subpaths Decomposition in Z 2 : a short note on\n  generalized Tangential Cover",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.02918v1": {
            "Paper Title": "Color Sails: Discrete-Continuous Palettes for Deep Color Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.08972v2": {
            "Paper Title": "FaceShop: Deep Sketch-based Face Image Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11714v1": {
            "Paper Title": "Deep Video Portraits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09817v1": {
            "Paper Title": "Stereo Magnification: Learning View Synthesis using Multiplane Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09562v1": {
            "Paper Title": "Progressive Transient Photon Beams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09488v1": {
            "Paper Title": "VisemeNet: Audio-Driven Animator-Centric Speech Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.09834v2": {
            "Paper Title": "Deep Illumination: Approximating Dynamic Global Illumination with\n  Generative Adversarial Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.08893v1": {
            "Paper Title": "On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.08500v1": {
            "Paper Title": "Improved Shortest Path Maps with GPU Shaders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.06021v1": {
            "Paper Title": "Topological Eulerian Synthesis of Slow Motion Periodic Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.03482v2": {
            "Paper Title": "Full 3D Reconstruction of Transparent Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04487v1": {
            "Paper Title": "Non-Stationary Texture Synthesis by Adversarial Expansion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.08275v4": {
            "Paper Title": "SPLATNet: Sparse Lattice Networks for Point Cloud Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.04486v6": {
            "Paper Title": "Can Computers Create Art?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01934v1": {
            "Paper Title": "Learning to See in the Dark",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.06673v3": {
            "Paper Title": "Tags2Parts: Discovering Semantic Regions from Shape Tags",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.10992v1": {
            "Paper Title": "Semi-parametric Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.09404v1": {
            "Paper Title": "Probabilistic Plant Modeling via Multi-View Image-to-Image Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.07070v2": {
            "Paper Title": "Steklov Spectral Geometry for Extrinsic Shape Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.09152v1": {
            "Paper Title": "Layered Fields for Natural Tessellations on Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.06996v1": {
            "Paper Title": "Metamorphs: Bistable Planar Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.06092v1": {
            "Paper Title": "Normal Image Manipulation for Bas-relief Generation with Hybrid Styles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.02823v3": {
            "Paper Title": "TextureGAN: Controlling Deep Image Synthesis with Texture Patches",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "use\nadversarial training to discriminate between real and fake\ntextures based on a feature patch from the VGG network.\nInstead of operating on feature space, Jetchev et al. ",
                    "Citation Text": "N. Jetchev, U. Bergmann, and R. V ollgraf. Texture synthesis\nwith spatial generative adversarial networks. arXiv preprint\narXiv:1611.08207 , 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08207",
                        "Citation Paper Title": "Title:Texture Synthesis with Spatial Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs.\nOur method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.",
                        "Citation Paper Authors": "Authors:Nikolay Jetchev, Urs Bergmann, Roland Vollgraf"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1804.05261v1": {
            "Paper Title": "Physics-driven Fire Modeling from Multi-view Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.04371v1": {
            "Paper Title": "Image Correction via Deep Reciprocating HDR Transformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.03924v2": {
            "Paper Title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.07487v2": {
            "Paper Title": "Sensor-topology based simplicial complex reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.06542v2": {
            "Paper Title": "Convolutional Point-set Representation: A Convolutional Bridge Between a\n  Densely Annotated Image and 3D Face Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.08163v2": {
            "Paper Title": "DVQA: Understanding Data Visualizations via Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04307v3": {
            "Paper Title": "Variational Autoencoders for Deforming 3D Mesh Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.10048v1": {
            "Paper Title": "Scalable closed-form trajectories for periodic and non-periodic\n  human-like walking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.06761v2": {
            "Paper Title": "PU-Net: Point Cloud Upsampling Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.08495v1": {
            "Paper Title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07892v2": {
            "Paper Title": "Generative Image Inpainting with Contextual Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.07835v1": {
            "Paper Title": "Joint 3D Face Reconstruction and Dense Alignment with Position Map\n  Regression Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.01170v2": {
            "Paper Title": "Direct interactive visualization of locally refined spline volumes for\n  scalar and vector fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.05788v1": {
            "Paper Title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.07490v2": {
            "Paper Title": "ViTac: Feature Sharing between Vision and Tactile Sensing for Cloth\n  Texture Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.05592v2": {
            "Paper Title": "Robust Structure-based Shape Correspondence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00940v1": {
            "Paper Title": "Protecting JPEG Images Against Adversarial Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00430v1": {
            "Paper Title": "Interactive Sound Rendering on Mobile Devices using Ray-Parameterized\n  Reverberation Filters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.02136v2": {
            "Paper Title": "MonoPerfCap: Human Performance Capture from Monocular Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.08022v1": {
            "Paper Title": "Equalizer 2.0 - Convergence of a Parallel Rendering Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.07710v1": {
            "Paper Title": "Medical Volume Reconstruction Techniques",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.06852v1": {
            "Paper Title": "Be-Educated: Multimedia Learning through 3D Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.03168v1": {
            "Paper Title": "Hierarchical Cloth Simulation using Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.02731v1": {
            "Paper Title": "Topologically Controlled Lossy Compression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.03678v2": {
            "Paper Title": "Self-Supervised Intrinsic Image Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.10434v1": {
            "Paper Title": "Robust 3D Human Motion Reconstruction Via Dynamic Template Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.10040v1": {
            "Paper Title": "Animation-by-Demonstration Computer Puppetry Authoring Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.09358v1": {
            "Paper Title": "Smooth, Efficient, and Interruptible Zooming and Panning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08126v2": {
            "Paper Title": "Cascaded 3D Full-body Pose Regression from Single Depth Image at 100 FPS",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.08863v1": {
            "Paper Title": "3D Scanning: A Comprehensive Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.02453v1": {
            "Paper Title": "Reversible Harmonic Maps between Discrete Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00968v1": {
            "Paper Title": "Joint convolutional neural pyramid for depth map super-resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.06654v1": {
            "Paper Title": "Graphic Narrative with Interactive Stylization Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04304v2": {
            "Paper Title": "Mesh-based Autoencoders for Localized Deformation Component Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.03686v2": {
            "Paper Title": "A practical guide and software for analysing pairwise comparison\n  experiments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.05644v1": {
            "Paper Title": "graphTPP: A multivariate based method for interactive graph layout and\n  analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.03931v1": {
            "Paper Title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.03380v1": {
            "Paper Title": "A Deep Recurrent Framework for Cleaning Motion Capture Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.02494v1": {
            "Paper Title": "Adversarial Examples that Fool Detectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.00750v1": {
            "Paper Title": "Haptic Assembly and Prototyping: An Expository Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01021v2": {
            "Paper Title": "Where and Who? Automatic Semantic-Aware Person Composition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.00238v1": {
            "Paper Title": "Shape Complementarity Analysis for Objects of Arbitrary Shape",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.11470v1": {
            "Paper Title": "Constraint Bubbles: Adding Efficient Zero-Density Bubbles to\n  Incompressible Free Surface Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.11123v1": {
            "Paper Title": "A Novel Image-centric Approach Towards Direct Volume Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.10824v1": {
            "Paper Title": "Compression for Smooth Shape Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.09377v1": {
            "Paper Title": "Visual Subpopulation Discovery and Validation in Cohort Study Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08279v1": {
            "Paper Title": "Visual Analytics of Group Differences in Tensor Fields: Application to\n  Clinical DTI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.05341v2": {
            "Paper Title": "Robust and High Fidelity Mesh Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.07793v1": {
            "Paper Title": "Optimized Visibility Functions for Revectorization-Based Shadow Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.00090v3": {
            "Paper Title": "Learning to Predict Indoor Illumination from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.08313v2": {
            "Paper Title": "Joint Material and Illumination Estimation from Photo Sets in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.04419v1": {
            "Paper Title": "Overlaying Quantitative Measurement on Networks: An Evaluation of Three\n  Positioning and Nine Visual Marker Techniques",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.04194v1": {
            "Paper Title": "Full-Body Locomotion Reconstruction of Virtual Characters Using a Single\n  IMU",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.04038v2": {
            "Paper Title": "Denoising a Point Cloud for Surface Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.03065v1": {
            "Paper Title": "An Application of Mosaic Diagrams to the Visualization of Set\n  Relationships",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.00967v1": {
            "Paper Title": "Dynamic Influence Networks for Rule-based Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.02596v2": {
            "Paper Title": "Localized Manifold Harmonics for Spectral Shape Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03337v2": {
            "Paper Title": "Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.06815v1": {
            "Paper Title": "Photo-Guided Exploration of Volume Data Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.04034v1": {
            "Paper Title": "Image retargeting via Beltrami representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02862v1": {
            "Paper Title": "Exploration of Heterogeneous Data Using Robust Similarity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02754v1": {
            "Paper Title": "Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity\n  Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02173v1": {
            "Paper Title": "Clustrophile: A Tool for Visual Clustering Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.01052v1": {
            "Paper Title": "Simulating Structure-from-Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.06683v2": {
            "Paper Title": "Visual Detection of Structural Changes in Time-Varying Graphs Using\n  Persistent Homology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.06375v3": {
            "Paper Title": "3D Shape Reconstruction from Sketches via Multi-view Convolutional\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.10214v1": {
            "Paper Title": "Photometric Stabilization for Fast-forward Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06759v4": {
            "Paper Title": "Comicolorization: Semi-Automatic Manga Colorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.09701v1": {
            "Paper Title": "Functional Characterization of Deformation Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.07599v1": {
            "Paper Title": "High-Resolution Shape Completion Using Deep Neural Networks for Global\n  Structure and Local Geometry Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.07581v1": {
            "Paper Title": "Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D\n  Generative Adversarial Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.03140v2": {
            "Paper Title": "Restoration of Atmospheric Turbulence-distorted Images via RPCA and\n  Quasiconformal Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.05056v1": {
            "Paper Title": "Learning Compact Geometric Features",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "describe an approach that matches\npoints on human body scans and operates on ensembles\nof depth images. Boscani et al. ",
                    "Citation Text": "D. Boscaini, J. Masci, E. Rodol `a, and M. M. Bronstein.\nLearning shape correspondence with anisotropic convolu-\ntional neural networks. In NIPS , 2016. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06437",
                        "Citation Paper Title": "Title:Learning shape correspondence with anisotropic convolutional neural networks",
                        "Citation Paper Abstract": "Abstract:Establishing correspondence between shapes is a fundamental problem in geometry processing, arising in a wide variety of applications. The problem is especially difficult in the setting of non-isometric deformations, as well as in the presence of topological noise and missing parts, mainly due to the limited capability to model such deformations axiomatically. Several recent works showed that invariance to complex shape transformations can be learned from examples. In this paper, we introduce an intrinsic convolutional neural network architecture based on anisotropic diffusion kernels, which we term Anisotropic Convolutional Neural Network (ACNN). In our construction, we generalize convolutions to non-Euclidean domains by constructing a set of oriented anisotropic diffusion kernels, creating in this way a local intrinsic polar representation of the data (`patch'), which is then correlated with a filter. Several cascades of such filters, linear, and non-linear operators are stacked to form a deep neural network whose parameters are learned by minimizing a task-specific cost. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.",
                        "Citation Paper Authors": "Authors:Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Michael M. Bronstein"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1704.07528v2": {
            "Paper Title": "Automatic Content-aware Projection for 360\u00b0 Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.08188v2": {
            "Paper Title": "Active Animations of Reduced Deformable Models with Environment\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.01221v2": {
            "Paper Title": "MLSEB: Edge Bundling using Moving Least Squares Approximation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.01638v1": {
            "Paper Title": "360 Panorama Cloning on Sphere",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.01295v1": {
            "Paper Title": "SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using\n  Multi-Task Hierarchical Deep Networks",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "consists of a complex multi-stage\nhybrid CNN-RNN architecture evaluated on only two animal categories ( horse andcow). The\napproach of Xia et al. ",
                    "Citation Text": "F. Xia, P. Wang, L.-C. Chen, and A. L. Yuille. Zoom better to see clearer: Human and object\nparsing with hierarchical auto-zoom net. In Proceedings of 14th European Conference in\nComputer Vision: Part V , pages 648\u2013663, 2016. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06881",
                        "Citation Paper Title": "Title:Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net",
                        "Citation Paper Abstract": "Abstract:Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two \"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively \"zoom\" (resize) predicted image regions into their proper scales to refine the parsing.\nWe conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. For humans, our approach significantly outperforms the state-of-the-arts by 5% mIOU and is especially better at segmenting small instances and small parts. We obtain similar improvements for parsing cows and horses over alternative methods. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that, for example, we do not need to waste computational resources scaling the entire image.",
                        "Citation Paper Authors": "Authors:Fangting Xia, Peng Wang, Liang-Chieh Chen, Alan L. Yuille"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1709.01250v1": {
            "Paper Title": "Sparse Data Driven Mesh Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.04496v2": {
            "Paper Title": "Learning Local Shape Descriptors from Part Correspondences With\n  Multi-view Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.00643v1": {
            "Paper Title": "Fast Image Processing with Fully-Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.07559v1": {
            "Paper Title": "Efficient barycentric point sampling on meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.05349v1": {
            "Paper Title": "PixelNN: Example-based Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03748v1": {
            "Paper Title": "Calipso: Physics-based Image and Video Editing through CAD Model Proxies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03686v1": {
            "Paper Title": "Visualizing Time-Varying Particle Flows with Diffusion Geometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.04672v1": {
            "Paper Title": "DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction\n  from a Single Image",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ", which contains a large quantity of\nmanually created and cleaned 3D CAD models. Speci\ufb01cally, we select 5 representative categories to\nstudy on: chair, car, airplane, bench and sofa, following Gwak et al. ",
                    "Citation Text": "JunYoung Gwak, Christopher B Choy, Manmohan Chandraker, Animesh Garg, and Silvio Savarese.\n\u201cWeakly Supervised Generative Adversarial Networks for 3D Reconstruction\u201d. In: arXiv preprint (2017)\n(cit. on pp. 1, 6).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10904",
                        "Citation Paper Title": "Title:Weakly supervised 3D Reconstruction with Adversarial Constraint",
                        "Citation Paper Abstract": "Abstract:Supervised 3D reconstruction has witnessed a significant progress through the use of deep neural networks. However, this increase in performance requires large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D supervision as an alternative for expensive 3D CAD annotation. Specifically, we use foreground masks as weak supervision through a raytrace pooling layer that enables perspective projection and backpropagation. Additionally, since the 3D reconstruction from masks is an ill posed problem, we propose to constrain the 3D reconstruction to the manifold of unlabeled realistic 3D shapes that match mask observations. We demonstrate that learning a log-barrier solution to this constrained optimization problem resembles the GAN objective, enabling the use of existing tools for training GANs. We evaluate and analyze the manifold constrained reconstruction on various datasets for single and multi-view reconstruction of both synthetic and real images.",
                        "Citation Paper Authors": "Authors:JunYoung Gwak, Christopher B. Choy, Animesh Garg, Manmohan Chandraker, Silvio Savarese"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1708.03292v1": {
            "Paper Title": "Learning to Synthesize a 4D RGBD Light Field from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.02970v1": {
            "Paper Title": "Personalized Cinemagraphs using Semantic Understanding and Collaborative\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.02731v1": {
            "Paper Title": "Weakly- and Self-Supervised Learning for Content-Aware Deep Image\n  Retargeting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.07391v1": {
            "Paper Title": "A Novel Stretch Energy Minimization Algorithm for Equiareal\n  Parameterizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.02463v2": {
            "Paper Title": "Video Frame Synthesis using Deep Voxel Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00636v1": {
            "Paper Title": "Generation of High Dynamic Range Illumination from a Single Image for\n  the Enhancement of Undesirably Illuminated Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00224v1": {
            "Paper Title": "Fast Preprocessing for Robust Face Sketch Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00223v1": {
            "Paper Title": "Learning to Hallucinate Face Images via Component Generation and\n  Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.09713v1": {
            "Paper Title": "Numerical analysis of shell-based geometric image inpainting algorithms\n  and their semi-implicit extension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.09629v1": {
            "Paper Title": "Kernel Projection of Latent Structures Regression for Facial Animation\n  Retargeting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.09432v1": {
            "Paper Title": "Generation of concept-representative symbols",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.09405v1": {
            "Paper Title": "Photographic Image Synthesis with Cascaded Refinement Networks",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "learn a model of scene dynamics and use it\nto synthesize video sequences from single images. Xue et\nal. ",
                    "Citation Text": "T. Xue, J. Wu, K. L. Bouman, and B. Freeman. Visual dy-\nnamics: Probabilistic future frame synthesis via cross convo-\nlutional networks. In NIPS , 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02586",
                        "Citation Paper Title": "Title:Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves low- and high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations.",
                        "Citation Paper Authors": "Authors:Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman"
                    },
                    "Keywords": {}
                }
            ]
        },
        "http://arxiv.org/abs/1708.06695v1": {
            "Paper Title": "A two-level approach to implicit surface modeling with compactly\n  supported radial basis functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.08360v1": {
            "Paper Title": "Discrete Geodesic Nets for Modeling Developable Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.06267v1": {
            "Paper Title": "Shape Generation using Spatially Partitioned Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.04805v1": {
            "Paper Title": "A Streamline Selection Technique Overlaying with Isosurfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.04348v1": {
            "Paper Title": "Natural Boundary Conditions for Smoothing in Geometry Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.05138v2": {
            "Paper Title": "Visualization of Feature Separation in Advected Scalar Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.09577v1": {
            "Paper Title": "Analysis and Modeling of 3D Indoor Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06918v1": {
            "Paper Title": "cGAN-based Manga Colorization Using a Single Training Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.03950v1": {
            "Paper Title": "Procedural Wang Tile Algorithm for Stochastic Wall Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.03024v2": {
            "Paper Title": "A Physically Plausible Model for Rendering Highly Scattering Fluorescent\n  Participating Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.04077v1": {
            "Paper Title": "Interactive Shape Perturbation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01558v1": {
            "Paper Title": "QuickCSG: Fast Arbitrary Boolean Combinations of N Solids",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01208v1": {
            "Paper Title": "Approximate Program Smoothing Using Mean-Variance Statistics, with\n  Application to Procedural Shader Bandlimiting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.00648v1": {
            "Paper Title": "Examplar-Based Face Colorization Using Image Morphing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.08657v2": {
            "Paper Title": "Accelerating Discrete Wavelet Transforms on Parallel Architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07640v1": {
            "Paper Title": "Dynamics Based 3D Skeletal Hand Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07108v1": {
            "Paper Title": "Snapshot Difference Imaging using Time-of-Flight Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.06086v1": {
            "Paper Title": "Scratch iridescence: Wave-optical rendering of diffractive surface\n  structure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.05893v1": {
            "Paper Title": "Computed Axial Lithography (CAL): Toward Single Step 3D Printing of\n  Arbitrary Geometries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.05508v1": {
            "Paper Title": "Automated Body Structure Extraction from Arbitrary 3D Mesh",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.05016v1": {
            "Paper Title": "A Correspondence Relaxation Approach for 3D Shape Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.04932v1": {
            "Paper Title": "GeneGAN: Learning Object Transfiguration and Attribute Subspace from\n  Unpaired Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.00274v2": {
            "Paper Title": "Topologically Robust 3D Shape Matching via Gradual Deflation and\n  Inflation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.06250v1": {
            "Paper Title": "Shape Classification using Spectral Graph Wavelets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.02999v1": {
            "Paper Title": "Real-Time User-Guided Image Colorization with Learned Deep Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.02422v1": {
            "Paper Title": "On Discrete Conformal Seamless Similarity Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.01759v1": {
            "Paper Title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360\u00b0\n  Sports Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.01263v1": {
            "Paper Title": "The Iray Light Transport Simulation and Rendering System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.01156v1": {
            "Paper Title": "Shading Annotations in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.00949v1": {
            "Paper Title": "Scalable Surface Reconstruction from Point Clouds with Extreme Scale and\n  Density Diversity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.06835v1": {
            "Paper Title": "Reversible Jump Metropolis Light Transport using Inverse Mappings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.04610v1": {
            "Paper Title": "A learning-based approach for automatic image and video colorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.02724v1": {
            "Paper Title": "CanvoX: High-resolution VR Painting in Large Volumetric Canvas",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.10405v1": {
            "Paper Title": "Autocomplete 3D Sculpting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.09964v1": {
            "Paper Title": "Image Restoration using Autoencoding Priors",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". This motivates using the\nmagnitude of the autoencoder error as our prior.\nOur work has an interesting connection to the plug-and-\nplay priors introduced by Venkatakrishnan et al. ",
                    "Citation Text": "S. V . Venkatakrishnan, C. A. Bouman, and B. Wohlberg.\nPlug-and-play priors for model based reconstruction. In\nGlobalSIP , pages 945\u2013948. IEEE, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10630",
                        "Citation Paper Title": "Title:Model-based Reconstruction for Single Particle Cryo-Electron Microscopy",
                        "Citation Paper Abstract": "Abstract:Single particle cryo-electron microscopy is a vital tool for 3D characterization of protein structures. A typical workflow involves acquiring projection images of a collection of randomly oriented particles, picking and classifying individual particle projections by orientation, and finally using the individual particle projections to reconstruct a 3D map of the electron density profile. The reconstruction is challenging because of the low signal-to-noise ratio of the data, the unknown orientation of the particles, and the sparsity of data especially when dealing with flexible proteins where there may not be sufficient data corresponding to each class to obtain an accurate reconstruction using standard algorithms. In this paper we present a model-based image reconstruction technique that uses a regularized cost function to reconstruct the 3D density map by assuming known orientations for the particles. Our method casts the reconstruction as minimizing a cost function involving a novel forward model term that accounts for the contrast transfer function of the microscope, the orientation of the particles and the center of rotation offsets. We combine the forward model term with a regularizer that enforces desirable properties in the volume to be reconstructed. Using simulated data, we demonstrate how our method can significantly improve upon the typically used approach.",
                        "Citation Paper Authors": "Authors:S. V. Venkatakrishnan, Puneet Juneja, Hugh O'Neill"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1704.06192v1": {
            "Paper Title": "The Design, Implementation, and Deployment of a System to Transparently\n  Compress Hundreds of Petabytes of Image Files for a File-Storage Service",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.08014v2": {
            "Paper Title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse\n  IMUs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.07255v2": {
            "Paper Title": "ZM-Net: Real-time Zero-shot Image Manipulation Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.07729v1": {
            "Paper Title": "Graffinity: Visualizing Connectivity In Large Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.06003v1": {
            "Paper Title": "Color Orchestra: Ordering Color Palettes for Interpolation and\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.05700v1": {
            "Paper Title": "Autocomplete Textures for 3D Printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.00061v1": {
            "Paper Title": "SceneSuggest: Context-driven 3D Scene Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.00050v1": {
            "Paper Title": "SceneSeer: 3D Scene Design with Natural Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.08680v1": {
            "Paper Title": "A Data-driven Approach for Furniture and Indoor Scene Colorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.02635v1": {
            "Paper Title": "A Computational Model of a Single-Photon Avalanche Diode Sensor for\n  Transient Imaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.06228v1": {
            "Paper Title": "Learning to Generate Posters of Scientific Papers by Probabilistic\n  Graphical Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.04852v1": {
            "Paper Title": "Visualization and Analysis of Large-Scale, Tree-Based, Adaptive Mesh\n  Refinement Simulations with Arbitrary Rectilinear Geometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.03246v1": {
            "Paper Title": "Towards Developing an Easy-To-Use Scripting Environment for Animating\n  Virtual Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.01537v1": {
            "Paper Title": "Conceptual and algorithmic development of Pseudo 3D Graphics and Video\n  Content Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.00737v1": {
            "Paper Title": "HoNVis: Visualizing and Exploring Higher-Order Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.08893v2": {
            "Paper Title": "Stable and Controllable Neural Texture Synthesis and Style Transfer\n  Using Histogram Losses",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.00182v1": {
            "Paper Title": "Inkjet printing-based volumetric display projecting multiple full-colour\n  2D patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.07110v1": {
            "Paper Title": "By chance is not enough: Preserving relative density through non uniform\n  sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.05754v1": {
            "Paper Title": "User-guided free-form asset modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.03754v2": {
            "Paper Title": "LayerBuilder: Layer Decomposition for Interactive Image and Video Color\n  Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.04303v1": {
            "Paper Title": "Poisson Vector Graphics (PVG) and Its Closed-Form Solver",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.03981v1": {
            "Paper Title": "A feasibility study on SSVEP-based interaction with motivating and\n  immersive virtual and augmented reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.03230v1": {
            "Paper Title": "Surface Reconstruction with Data-driven Exemplar Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.02357v1": {
            "Paper Title": "Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware\n  Semantic Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09772v3": {
            "Paper Title": "Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "uses networks to predict 2D landmarks and face orienta-\ntion at the same time. HopeNet ",
                    "Citation Text": "Nataniel Ruiz, Eunji Chong, and James M Rehg. Fine-\ngrained head pose estimation without keypoints. In CVPR\nWorkshops , pages 2074\u20132083, 2018. 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00925",
                        "Citation Paper Title": "Title:Fine-Grained Head Pose Estimation Without Keypoints",
                        "Citation Paper Abstract": "Abstract:Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.",
                        "Citation Paper Authors": "Authors:Nataniel Ruiz, Eunji Chong, James M. Rehg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.12682v5": {
            "Paper Title": "Axomorphic Perspective Projection Model for Immersive Imagery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16284v3": {
            "Paper Title": "FIRe: Fast Inverse Rendering using Directional and Signed Distance\n  Functions",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\n3.3. Training\nWe train a model for each class of the ShapeNet\ndataset ",
                    "Citation Text": "Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\nlis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and\nFisher Yu. Shapenet: An information-rich 3d model reposi-\ntory, 2015. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", have proven to accelerate volumetric ren-\ndering. However, they only model single objects and they\nstill need to perform local sampling for volumetric render-\ning. We note CPDDF ",
                    "Citation Text": "Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven\nDickinson, and Allan D. Jepson. Representing 3d shapes\nwith probabilistic directed distance fields. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 19343\u201319354, June 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05300",
                        "Citation Paper Title": "Title:Representing 3D Shapes with Probabilistic Directed Distance Fields",
                        "Citation Paper Abstract": "Abstract:Differentiable rendering is an essential operation in modern vision, allowing inverse graphics approaches to 3D understanding to be utilized in modern machine learning frameworks. Explicit shape representations (voxels, point clouds, or meshes), while relatively easily rendered, often suffer from limited geometric fidelity or topological constraints. On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability. In this work, we endeavour to address both shortcomings with a novel shape representation that allows fast differentiable rendering within an implicit architecture. Building on implicit distance representations, we define Directed Distance Fields (DDFs), which map an oriented point (position and direction) to surface visibility and depth. Such a field can render a depth map with a single forward pass per pixel, enable differential surface geometry extraction (e.g., surface normals and curvatures) via network derivatives, be easily composed, and permit extraction of classical unsigned distance fields. Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field. Finally, we apply our method to fitting single shapes, unpaired 3D-aware generative image modelling, and single-image 3D reconstruction tasks, showcasing strong performance with simple architectural components via the versatility of our representation.",
                        "Citation Paper Authors": "Authors:Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, Allan Jepson"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", or, alternatively, a region along the ray instead\nof distance ",
                    "Citation Text": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum , 40(4), 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03231",
                        "Citation Paper Title": "Title:DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks",
                        "Citation Paper Abstract": "Abstract:The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",
                        "Citation Paper Authors": "Authors:Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "learns the SDF\nvalues using an autodecoder architecture conditioned on\na learned latent code set to generate different 3D shapes.\nOccupancyNet ",
                    "Citation Text": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03828",
                        "Citation Paper Title": "Title:Occupancy Networks: Learning 3D Reconstruction in Function Space",
                        "Citation Paper Abstract": "Abstract:With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13220v2": {
            "Paper Title": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": ", or rasterizes 3D data into a voxel grid\nin order to apply conventional 3D convolutions [2, 50]. To\novercome fixed structures and to induce permutation invari-\nance, PointFlow ",
                    "Citation Text": "Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. PointFlow: 3d point cloud\ngeneration with continuous normalizing flows. In ICCV ,\n2019. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.12320",
                        "Citation Paper Title": "Title:PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows",
                        "Citation Paper Abstract": "Abstract:As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, Bharath Hariharan"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "introduces a two-\nstage training pipeline consisting of a triplane V AE and a\nseparately trained latent diffusion model in triplane space.\nSomewhat similarly, ",
                    "Citation Text": "J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field genera-\ntion using triplane diffusion. In CVPR , pages 20875\u201320886,\n2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.16677",
                        "Citation Paper Title": "Title:3D Neural Field Generation using Triplane Diffusion",
                        "Citation Paper Abstract": "Abstract:Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.",
                        "Citation Paper Authors": "Authors:J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "to turn those into smooth meshes. To circumvent\nthat extra surface extractor, 3DGen ",
                    "Citation Text": "Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O \u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371 , 2023. 1, 3, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.05371",
                        "Citation Paper Title": "Title:3DGen: Triplane Latent Diffusion for Textured Mesh Generation",
                        "Citation Paper Abstract": "Abstract:Latent diffusion models for image generation have crossed a quality threshold which enabled them to achieve mass adoption. Recently, a series of works have made advancements towards replicating this success in the 3D domain, introducing techniques such as point cloud VAE, triplane representation, neural implicit surfaces and differentiable rendering based training. We take another step along this direction, combining these developments in a two-step pipeline consisting of 1) a triplane VAE which can learn latent representations of textured meshes and 2) a conditional diffusion model which generates the triplane features. For the first time this architecture allows conditional and unconditional generation of high quality textured or untextured 3D meshes across multiple diverse categories in a few seconds on a single GPU. It outperforms previous work substantially on image-conditioned and unconditional generation on mesh quality as well as texture generation. Furthermore, we demonstrate the scalability of our model to large datasets for increased quality and diversity. We will release our code and trained models.",
                        "Citation Paper Authors": "Authors:Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, Barlas O\u011fuz"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "introduce an implicit field\ndecoder to learn a signed distance function. The network\nis trained in adversarial fashion to predict the distance from\nthe surface when presented a point coordinate and its asso-\nciated feature encoding. In ",
                    "Citation Text": "Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun\nHao, Serge Belongie, Noah Snavely, and Bharath Hariha-\nran. Learning gradient fields for shape generation. In ECCV ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06520",
                        "Citation Paper Title": "Title:Learning Gradient Fields for Shape Generation",
                        "Citation Paper Abstract": "Abstract:In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, Bharath Hariharan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.09997v2": {
            "Paper Title": "Beyond ExaBricks: GPU Volume Path Tracing of AMR Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08289v2": {
            "Paper Title": "Continuously Controllable Facial Expression Editing in Talking Face\n  Videos",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "which maps the real image tinto the W+space\nby regression. The encoder first extracts multi-resolutional\nfeature maps from tusing a standard feature pyramid over\na ResNet backbone ",
                    "Citation Text": "J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive\nangular margin loss for deep face recognition,\u201d in CVPR , 2019, pp.\n4690\u20134699.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07698",
                        "Citation Paper Title": "Title:ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.08693v4": {
            "Paper Title": "Elastic Shape Analysis of Tree-like 3D Objects using Extended SRVF\n  Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05231v3": {
            "Paper Title": "NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "), our proposed method is over 3x faster than V ox-\nurf and achieves better geometry quality when compared\nwith V oxurf\u2019s results reported in their paper, as shown in\nthe Suppl. document. Neuralangelo ",
                    "Citation Text": "Zhaoshuo Li, Thomas M \u00a8uller, Alex Evans, Russell H Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-fidelity neural surface reconstruction. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.03092",
                        "Citation Paper Title": "Title:Neuralangelo: High-Fidelity Neural Surface Reconstruction",
                        "Citation Paper Abstract": "Abstract:Neural surface reconstruction has been shown to be powerful for recovering dense 3D surfaces via image-based neural rendering. However, current methods struggle to recover detailed structures of real-world scenes. To address the issue, we present Neuralangelo, which combines the representation power of multi-resolution 3D hash grids with neural surface rendering. Two key ingredients enable our approach: (1) numerical gradients for computing higher-order derivatives as a smoothing operation and (2) coarse-to-fine optimization on the hash grids controlling different levels of details. Even without auxiliary inputs such as depth, Neuralangelo can effectively recover dense 3D surface structures from multi-view images with fidelity significantly surpassing previous methods, enabling detailed large-scale scene reconstruction from RGB video captures.",
                        "Citation Paper Authors": "Authors:Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "presents a grid based\nmethod for efficiently reconstructing radiance fields frame\nby frame. ",
                    "Citation Text": "Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n13524\u201313534, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.08614",
                        "Citation Paper Title": "Title:Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time",
                        "Citation Paper Abstract": "Abstract:Implicit neural representations such as Neural Radiance Field (NeRF) have focused mainly on modeling static objects captured under multi-view settings where real-time rendering can be achieved with smart data structures, e.g., PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO) technique to tackle efficient neural modeling and real-time rendering of dynamic scenes captured under the free-view video (FVV) setting. The key idea in our FPO is a novel combination of generalized NeRF, PlenOctree representation, volumetric fusion and Fourier transform. To accelerate FPO construction, we present a novel coarse-to-fine fusion scheme that leverages the generalizable NeRF technique to generate the tree via spatial blending. To tackle dynamic scenes, we tailor the implicit network to model the Fourier coefficients of timevarying density and color attributes. Finally, we construct the FPO and train the Fourier coefficients directly on the leaves of a union PlenOctree structure of the dynamic sequence. We show that the resulting FPO enables compact memory overload to handle dynamic objects and supports efficient fine-tuning. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF and achieves over an order of magnitude acceleration over SOTA while preserving high visual quality for the free-viewpoint rendering of unseen dynamic scenes.",
                        "Citation Paper Authors": "Authors:Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "proposes a static-to-dynamic learning paradigm\nfor fast dynamic scene learning. ",
                    "Citation Text": "Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping\nTan. Streaming radiance fields for 3d video synthesis. arXiv\npreprint arXiv:2210.14831 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.14831",
                        "Citation Paper Title": "Title:Streaming Radiance Fields for 3D Video Synthesis",
                        "Citation Paper Abstract": "Abstract:We present an explicit-grid based method for efficiently reconstructing streaming radiance fields for novel view synthesis of real world dynamic scenes. Instead of training a single model that combines all the frames, we formulate the dynamic modeling problem with an incremental learning paradigm in which per-frame model difference is trained to complement the adaption of a base model on the current frame. By exploiting the simple yet effective tuning strategy with narrow bands, the proposed method realizes a feasible framework for handling video sequences on-the-fly with high training efficiency. The storage overhead induced by using explicit grid representations can be significantly reduced through the use of model difference based compression. We also introduce an efficient strategy to further accelerate model optimization for each frame. Experiments on challenging video sequences demonstrate that our approach is capable of achieving a training speed of 15 seconds per-frame with competitive rendering quality, which attains $1000 \\times$ speedup over the state-of-the-art implicit methods. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, Ping Tan"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "represents a 4D scene with a time-aware voxel fea-\nture. ",
                    "Citation Text": "Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Devrf: Fast deformable\nvoxel radiance fields for dynamic scenes. arXiv preprint\narXiv:2205.15723 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.15723",
                        "Citation Paper Title": "Title:DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:Modeling dynamic scenes is important for many applications such as virtual reality and telepresence. Despite achieving unprecedented fidelity for novel view synthesis in dynamic scenes, existing methods based on Neural Radiance Fields (NeRF) suffer from slow convergence (i.e., model training time measured in days). In this paper, we present DeVRF, a novel representation to accelerate learning dynamic radiance fields. The core of DeVRF is to model both the 3D canonical space and 4D deformation field of a dynamic, non-rigid scene with explicit and discrete voxel-based representations. However, it is quite challenging to train such a representation which has a large number of model parameters, often resulting in overfitting issues. To overcome this challenge, we devise a novel static-to-dynamic learning paradigm together with a new data capture setup that is convenient to deploy in practice. This paradigm unlocks efficient learning of deformable radiance fields via utilizing the 3D volumetric canonical space learnt from multi-view static images to ease the learning of 4D voxel deformation field with only few-view dynamic sequences. To further improve the efficiency of our DeVRF and its synthesized novel view's quality, we conduct thorough explorations and identify a set of strategies. We evaluate DeVRF on both synthetic and real-world dynamic scenes with different types of deformation. Experiments demonstrate that DeVRF achieves two orders of magnitude speedup (100x faster) with on-par high-fidelity results compared to the previous state-of-the-art approaches. The code and dataset will be released in this https URL.",
                        "Citation Paper Authors": "Authors:Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie, Mike Zheng Shou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01368v2": {
            "Paper Title": "Fast Non-Rigid Radiance Fields from Monocularized Data",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ",introducing an efficient temporal extension that leverages hash\nencoding for non-rigid reconstruction.\nFast Non-Rigid Radiance Fields. Very recently, other non-\nrigid NeRF methods adapt explicit scene representations to\nimprove the training speed in various scenarios. Guo et al. ",
                    "Citation Text": "X. Guo, G. Chen, Y . Dai, X. Ye, J. Sun, X. Tan, and E. Ding, \u201cNeural\ndeformable voxel grid for fast optimization of dynamic view synthesis,\u201d\ninACCV , 2023, p. 450\u2013468.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.07698",
                        "Citation Paper Title": "Title:Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis",
                        "Citation Paper Abstract": "Abstract:Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel view synthesis (NVS) for its superior performance. In this paper, we propose to synthesize dynamic scenes. Extending the methods for static scenes to dynamic scenes is not straightforward as both the scene geometry and appearance change over time, especially under monocular setup. Also, the existing dynamic NeRF methods generally require a lengthy per-scene training procedure, where multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In this paper, built on top of the recent advances in voxel-grid optimization, we propose a fast deformable radiance field method to handle dynamic scenes. Our method consists of two modules. The first module adopts a deformation grid to store 3D dynamic features, and a light-weight MLP for decoding the deformation that maps a 3D point in the observation space to the canonical space using the interpolated features. The second module contains a density and a color grid to model the geometry and density of the scene. The occlusion is explicitly modeled to further improve the rendering quality. Experimental results show that our method achieves comparable performance to D-NeRF using only 20 minutes for training, which is more than 70x faster than D-NeRF, clearly demonstrating the efficiency of our proposed method.",
                        "Citation Paper Authors": "Authors:Xiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Jiadai Sun, Xiao Tan, Errui Ding"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". Recently, multiple works introduced explicit\nscene representations such as discrete grids ",
                    "Citation Text": "C. Sun, M. Sun, and H.-T. Chen, \u201cDirect voxel grid optimization: Super-\nfast convergence for radiance fields reconstruction,\u201d in CVPR , 2022, pp.\n5459\u20135469.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11215",
                        "Citation Paper Title": "Title:Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction",
                        "Citation Paper Abstract": "Abstract:We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene.",
                        "Citation Paper Authors": "Authors:Cheng Sun, Min Sun, Hwann-Tzong Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.16314v2": {
            "Paper Title": "Approximating Intersections and Differences Between Linear Statistical\n  Shape Models Using Markov Chain Monte Carlo",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02341v2": {
            "Paper Title": "Complex Locomotion Skill Learning via Differentiable Physics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12550v2": {
            "Paper Title": "Training and Tuning Generative Neural Radiance Fields for\n  Attribute-Conditional 3D-Aware Face Generation",
            "Sentences": [
                {
                    "Sentence ID": 89,
                    "Sentence": "to evaluate identity preservation across differ-\nent views. Specifically, we use ArcFace ",
                    "Citation Text": "J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive\nangular margin loss for deep face recognition,\u201d in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\n2019, pp. 4690\u20134699.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07698",
                        "Citation Paper Title": "Title:ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "as our baseline for comparing face se-\nmantic disentanglement with multiple-view generation re-\nsults. We also compare with the 3DMM-guided model,\nDiscoFaceGAN ",
                    "Citation Text": "Y. Deng, J. Yang, D. Chen, F. Wen, and X. Tong, \u201cDisentangled\nand controllable face image generation via 3d imitative-contrastive\nlearning,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 5154\u20135163.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11660",
                        "Citation Paper Title": "Title:Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity of non-existing people, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.",
                        "Citation Paper Authors": "Authors:Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", we train our\nmodel using sampled images and their corresponding la-\ntent codes. We use 40,000 images for training StyleSDF\nand 40,000 for EG3d. We employ off-the-shelf attribute-\nclassifiers ",
                    "Citation Text": "T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,\n\u201cAnalyzing and improving the image quality of stylegan,\u201d in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , 2020, pp. 8110\u20138119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ".\nGOAE is the state-of-the-art 3D-aware image inversion and\nediting method.\nEvaluation Metrics. Five metrics are used for evaluation:\nFID (Fr \u00b4echet Inception Distance) score ",
                    "Citation Text": "M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochre-\niter, \u201cGans trained by a two time-scale update rule converge to a\nlocal nash equilibrium,\u201d in NeurIPS , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": "is a very similar model to our TT-\nGNeRF, but it does not release the code. DragGAN ",
                    "Citation Text": "X. Pan, A. Tewari, T. Leimk \u00a8uhler, L. Liu, A. Meka, and C. Theobalt,\n\u201cDrag your gan: Interactive point-based manipulation on the\ngenerative image manifold,\u201d in ACM SIGGRAPH 2023 Conference\nProceedings , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10973",
                        "Citation Paper Title": "Title:Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
                        "Citation Paper Abstract": "Abstract:Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.",
                        "Citation Paper Authors": "Authors:Xingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra Meka, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "to compare multiple-view\ngeneration. However, LiftedGAN cannot control individual\nattributes. LENeRF ",
                    "Citation Text": "J. Hyung, S. Hwang, D. Kim, H. Lee, and J. Choo, \u201cLocal 3d\nediting via 3d distillation of clip knowledge,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2023, pp. 12 674\u201312 684.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.12570",
                        "Citation Paper Title": "Title:Local 3D Editing via 3D Distillation of CLIP Knowledge",
                        "Citation Paper Abstract": "Abstract:3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as 2D semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome these problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.",
                        "Citation Paper Authors": "Authors:Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.11762v2": {
            "Paper Title": "MFA-DVR: Direct Volume Rendering of MFA Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00613v3": {
            "Paper Title": "NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and\n  Animation",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": ".\nIn Tab 1, we show the reconstruction related metrics MSE,\nSSIM, PSNR and LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "builds a volumetric represen-\ntation that can generate high-quality real time renderings\nthat look realistic even for challenging materials like hair\nand clothing. Several follow up works extend it to model\nbody dynamics ",
                    "Citation Text": "Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito,\nChenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo, Zhe\nCao, Fabian Prada, Jason Saragih, et al. Drivable volumet-\nric avatars using texel-aligned features. In ACM SIGGRAPH\n2022 Conference Proceedings , pages 1\u20139, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.09774",
                        "Citation Paper Title": "Title:Drivable Volumetric Avatars using Texel-Aligned Features",
                        "Citation Paper Abstract": "Abstract:Photorealistic telepresence requires both high-fidelity body modeling and faithful driving to enable dynamically synthesized appearance that is indistinguishable from reality. In this work, we propose an end-to-end framework that addresses two core challenges in modeling and driving full-body avatars of real people. One challenge is driving an avatar while staying faithful to details and dynamics that cannot be captured by a global low-dimensional parameterization such as body pose. Our approach supports driving of clothed avatars with wrinkles and motion that a real driving performer exhibits beyond the training corpus. Unlike existing global state representations or non-parametric screen-space approaches, we introduce texel-aligned features -- a localised representation which can leverage both the structural prior of a skeleton-based parametric model and observed sparse image signals at the same time. Another challenge is modeling a temporally coherent clothed avatar, which typically requires precise surface tracking. To circumvent this, we propose a novel volumetric avatar representation by extending mixtures of volumetric primitives to articulated objects. By explicitly incorporating articulation, our approach naturally generalizes to unseen poses. We also introduce a localized viewpoint conditioning, which leads to a large improvement in generalization of view-dependent appearance. The proposed volumetric representation does not require high-quality mesh tracking as a prerequisite and brings significant quality improvements compared to mesh-based counterparts. In our experiments, we carefully examine our design choices and demonstrate the efficacy of our approach, outperforming the state-of-the-art methods on challenging driving scenarios.",
                        "Citation Paper Authors": "Authors:Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Tomas Simon, Chenglei Wu, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "utilize pixel aligned information to\nextend NeRF\u2019s drivability and generalization over sequence\ndata. Nerfies ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 5865\u20135874, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "for modeling 3D scenes from\nmultiple images, there are many works that build avatars\nwith NeRF [7, 8, 10, 14, 16, 17, 27, 33, 34, 36, 55]. PV A ",
                    "Citation Text": "Amit Raj, Michael Zollhofer, Tomas Simon, Jason Saragih,\nShunsuke Saito, James Hays, and Stephen Lombardi. Pixel-\naligned volumetric avatars. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 11733\u201311742, June 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.02697",
                        "Citation Paper Title": "Title:PVA: Pixel-aligned Volumetric Avatars",
                        "Citation Paper Abstract": "Abstract:Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.",
                        "Citation Paper Authors": "Authors:Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "treats hair rendering\nas an image translation problem and generates realistic ren-\ndering of hair conditioned on 2D hair masks and strokes.\nSimilarly, Chai et al. ",
                    "Citation Text": "Menglei Chai, Jian Ren, and Sergey Tulyakov. Neural hair\nrendering. In European Conference on Computer Vision ,\npages 371\u2013388. Springer, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13297",
                        "Citation Paper Title": "Title:Neural Hair Rendering",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a generic neural-based hair rendering pipeline that can synthesize photo-realistic images from virtual 3D hair models. Unlike existing supervised translation methods that require model-level similarity to preserve consistent structure representation for both real images and fake renderings, our method adopts an unsupervised solution to work on arbitrary hair models. The key component of our method is a shared latent space to encode appearance-invariant structure information of both domains, which generates realistic renderings conditioned by extra appearance inputs. This is achieved by domain-specific pre-disentangled structure representation, partially shared domain encoder layers and a structure discriminator. We also propose a simple yet effective temporal conditioning method to enforce consistency for video sequence generation. We demonstrate the superiority of our method by testing it on a large number of portraits and comparing it with alternative baselines and state-of-the-art unsupervised image translation methods.",
                        "Citation Paper Authors": "Authors:Menglei Chai, Jian Ren, Sergey Tulyakov"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "uses deep\nneural networks for adaptive binding between normal hair\nand guide hair. Olszewski et al. ",
                    "Citation Text": "Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria,\nZhili Chen, Weikai Chen, and Hao Li. Intuitive, interactive\nbeard and hair synthesis with generative models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 7446\u20137456, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06848",
                        "Citation Paper Title": "Title:Intuitive, Interactive Beard and Hair Synthesis with Generative Models",
                        "Citation Paper Abstract": "Abstract:We present an interactive approach to synthesizing realistic variations in facial hair in images, ranging from subtle edits to existing hair to the addition of complex and challenging hair in images of clean-shaven subjects. To circumvent the tedious and computationally expensive tasks of modeling, rendering and compositing the 3D geometry of the target hairstyle using the traditional graphics pipeline, we employ a neural network pipeline that synthesizes realistic and detailed images of facial hair directly in the target image in under one second. The synthesis is controlled by simple and sparse guide strokes from the user defining the general structural and color properties of the target hairstyle. We qualitatively and quantitatively evaluate our chosen method compared to several alternative approaches. We show compelling interactive editing results with a prototype user interface that allows novice users to progressively refine the generated image to match their desired hairstyle, and demonstrate that our approach also allows for flexible and high-fidelity scalp hair synthesis.",
                        "Citation Paper Authors": "Authors:Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen, Weikai Chen, Hao Li"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "perform hair strand track-\ning by extracting hair strand tracklets from spatio-temporal\nslices of a video volume. Liang et al. ",
                    "Citation Text": "Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen,\nLinda G Shapiro, and Ira Kemelmacher-Shlizerman. Video\nto fully automatic 3d hair model. ACM Transactions on\nGraphics (TOG) , 37(6):1\u201314, 2018. 2Figure 16. Animation on Bald Sequence.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.04765",
                        "Citation Paper Title": "Title:Video to Fully Automatic 3D Hair Model",
                        "Citation Paper Abstract": "Abstract:Imagine taking a selfie video with your mobile phone and getting as output a 3D model of your head (face and 3D hair strands) that can be later used in VR, AR, and any other domain. State of the art hair reconstruction methods allow either a single photo (thus compromising 3D quality) or multiple views, but they require manual user interaction (manual hair segmentation and capture of fixed camera views that span full 360 degree). In this paper, we describe a system that can completely automatically create a reconstruction from any video (even a selfie video), and we don't require specific views, since taking your -90 degree, 90 degree, and full back views is not feasible in a selfie capture.\nIn the core of our system, in addition to the automatization components, hair strands are estimated and deformed in 3D (rather than 2D as in state of the art) thus enabling superior results. We provide qualitative, quantitative, and Mechanical Turk human studies that support the proposed system, and show results on a diverse variety of videos (8 different celebrity videos, 9 selfie mobile videos, spanning age, gender, hair length, type, and styling).",
                        "Citation Paper Authors": "Authors:Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen, Linda G. Shapiro, Ira Kemelmacher-Shlizerman"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "extends this approach to reconstruct both geom-\netry and appearance with OLAT images. Rosu et al. ",
                    "Citation Text": "Radu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chen-\nglei Wu, Sven Behnke, and Giljoo Nam. Neural strands:\nLearning hair geometry and appearance from multi-view im-\nages. ECCV , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.14067",
                        "Citation Paper Title": "Title:Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images",
                        "Citation Paper Abstract": "Abstract:We present Neural Strands, a novel learning framework for modeling accurate hair geometry and appearance from multi-view image inputs. The learned hair model can be rendered in real-time from any viewpoint with high-fidelity view-dependent effects. Our model achieves intuitive shape and style control unlike volumetric counterparts. To enable these properties, we propose a novel hair representation based on a neural scalp texture that encodes the geometry and appearance of individual strands at each texel location. Furthermore, we introduce a novel neural rendering framework based on rasterization of the learned hair strands. Our neural rendering is strand-accurate and anti-aliased, making the rendering view-consistent and photorealistic. Combining appearance with a multi-view geometric prior, we enable, for the first time, the joint learning of appearance and explicit hair geometry from a multi-view setup. We demonstrate the efficacy of our approach in terms of fidelity and efficiency for various hairstyles.",
                        "Citation Paper Authors": "Authors:Radu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chenglei Wu, Sven Behnke, Giljoo Nam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.13643v4": {
            "Paper Title": "Differentiable solver for time-dependent deformation problems with\n  contact",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "No Material, Initial Only planes or SDF, no self-collisions Dynamic-Only Adjoint\nGradSim ",
                    "Citation Text": "Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti,\nLinda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin\nXie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and\nSanja Fidler. 2021. gradSim: Differentiable simulation for system identification\nand visuomotor control. International Conference on Learning Representations\n(ICLR) (2021). https://openreview.net/forum?id=c_E8kFWfhp0",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.02646",
                        "Citation Paper Title": "Title:gradSim: Differentiable simulation for system identification and visuomotor control",
                        "Citation Paper Abstract": "Abstract:We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph -- spanning from the dynamics and through the rendering process -- enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
                        "Citation Paper Authors": "Authors:Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". While the original formulation is introduced in\na discrete form, it can be derived with minimal changes as a linear\nfinite-element discretization of a continuum formulation ",
                    "Citation Text": "Minchen Li, Zachary Ferguson, Teseo Schneider, Timothy Langlois, Denis Zorin,\nDaniele Panozzo, Chenfanfu Jiang, and Danny M. Kaufman. 2023. Convergent\nIncremental Potential Contact. arXiv:2307.15908 [math.NA]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.15908",
                        "Citation Paper Title": "Title:Convergent Incremental Potential Contact",
                        "Citation Paper Abstract": "Abstract:Recent advances in the simulation of frictionally contacting elastodynamics with the Incremental Potential Contact (IPC) model have enabled inversion and intersection-free simulation via the application of mollified barriers, filtered line-search, and optimization-based solvers for time integration. In its current formulation the IPC model is constructed via a discrete constraint model, replacing non-interpenetration constraints with barrier potentials on an already spatially discretized domain. However, while effective, this purely discrete formulation prohibits convergence under refinement. To enable a convergent IPC model we reformulate IPC potentials in the continuous setting and provide a first, convergent discretization thereof. We demonstrate and analyze the convergence behavior of this new model and discretization on a range of elastostatic and dynamic contact problems, and evaluate its accuracy on both analytical benchmarks and application-driven examples.",
                        "Citation Paper Authors": "Authors:Minchen Li, Zachary Ferguson, Teseo Schneider, Timothy Langlois, Denis Zorin, Daniele Panozzo, Chenfanfu Jiang, Danny M. Kaufman"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "No Material Only planes or SDF, no friction Dynamic-Only Code transformation/autodiff\nNeuralSim ",
                    "Citation Text": "Eric Heiden, David Millard, Erwin Coumans, Yizhou Sheng, and Gaurav S\nSukhatme. 2020. NeuralSim: Augmenting Differentiable Simulators with Neural\nNetworks. arXiv preprint arXiv:2011.04217 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.04217",
                        "Citation Paper Title": "Title:NeuralSim: Augmenting Differentiable Simulators with Neural Networks",
                        "Citation Paper Abstract": "Abstract:Differentiable simulators provide an avenue for closing the sim-to-real gap by enabling the use of efficient, gradient-based optimization algorithms to find the simulation parameters that best fit the observed sensor readings. Nonetheless, these analytical models can only predict the dynamical behavior of systems for which they have been designed. In this work, we study the augmentation of a novel differentiable rigid-body physics engine via neural networks that is able to learn nonlinear relationships between dynamic quantities and can thus learn effects not accounted for in traditional simulators.Such augmentations require less data to train and generalize better compared to entirely data-driven models. Through extensive experiments, we demonstrate the ability of our hybrid simulator to learn complex dynamics involving frictional contacts from real data, as well as match known models of viscous friction, and present an approach for automatically discovering useful augmentations. We show that, besides benefiting dynamics modeling, inserting neural networks can accelerate model-based control architectures. We observe a ten-fold speed-up when replacing the QP solver inside a model-predictive gait controller for quadruped robots with a neural network, allowing us to significantly improve control delays as we demonstrate in real-hardware experiments.\nWe publish code, additional results and videos from our experiments on our project webpage at this https URL.",
                        "Citation Paper Authors": "Authors:Eric Heiden, David Millard, Erwin Coumans, Yizhou Sheng, Gaurav S. Sukhatme"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "No Material, Initial Only planes, no self-collisions Dynamic-Only Code transformation\nDiSECt ",
                    "Citation Text": "Eric Heiden, Miles Macklin, Yashraj S Narang, Dieter Fox, Animesh Garg, and\nFabio Ramos. 2021. DiSECt: A Differentiable Simulation Engine for Autonomous\nRobotic Cutting. In Proceedings of Robotics: Science and Systems . Virtual. https:\n//doi.org/10.15607/RSS.2021.XVII.067",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.12244",
                        "Citation Paper Title": "Title:DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting",
                        "Citation Paper Abstract": "Abstract:Robotic cutting of soft materials is critical for applications such as food processing, household automation, and surgical manipulation. As in other areas of robotics, simulators can facilitate controller verification, policy learning, and dataset generation. Moreover, differentiable simulators can enable gradient-based optimization, which is invaluable for calibrating simulation parameters and optimizing controllers. In this work, we present DiSECt: the first differentiable simulator for cutting soft materials. The simulator augments the finite element method (FEM) with a continuous contact model based on signed distance fields (SDF), as well as a continuous damage model that inserts springs on opposite sides of the cutting plane and allows them to weaken until zero stiffness, enabling crack formation. Through various experiments, we evaluate the performance of the simulator. We first show that the simulator can be calibrated to match resultant forces and deformation fields from a state-of-the-art commercial solver and real-world cutting datasets, with generality across cutting velocities and object instances. We then show that Bayesian inference can be performed efficiently by leveraging the differentiability of the simulator, estimating posteriors over hundreds of parameters in a fraction of the time of derivative-free methods. Finally, we illustrate that control parameters in the simulation can be optimized to minimize cutting forces via lateral slicing motions.\nWe publish videos and additional results on our project website at this https URL.",
                        "Citation Paper Authors": "Authors:Eric Heiden, Miles Macklin, Yashraj Narang, Dieter Fox, Animesh Garg, Fabio Ramos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.02469v4": {
            "Paper Title": "One-shot Implicit Animatable Avatars with Model-based Priors",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": "vi-\nsual encoders pre-trained on diverse image-text pairs data\nare suitable for this task. In Figure 3, we carry out a similar\nevaluation of CLIP-NeRF ",
                    "Citation Text": "Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipula-\ntion of neural radiance fields. In Computer Vision and Pat-\ntern Recognition (CVPR) , pages 3835\u20133844, 2022. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05139",
                        "Citation Paper Title": "Title:CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manipulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled conditional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation field to the positional encoding and deferring color conditioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reflect the targeted editing. The mappers are trained with a CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.06261v4": {
            "Paper Title": "Real2Sim2Real Transfer for Control of Cable-driven Robots via a\n  Differentiable Physics Engine",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "has been limited to only\nsim2sim transfer and never demonstrated on a real tensegrity\nrobot.\nPrior work on tensegrity locomotion ",
                    "Citation Text": "M. Zhang, X. Geng, J. Bruce, K. Caluwaerts, M. Vespignani, V . Sun-\nSpiral, P. Abbeel, and S. Levine, \u201cDeep reinforcement learning for\ntensegrity robot locomotion,\u201d in ICRA . IEEE, 2017, pp. 634\u2013641.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09049",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning for Tensegrity Robot Locomotion",
                        "Citation Paper Abstract": "Abstract:Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. Our code and other supplementary materials are available from this http URL",
                        "Citation Paper Authors": "Authors:Marvin Zhang, Xinyang Geng, Jonathan Bruce, Ken Caluwaerts, Massimo Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01082v2": {
            "Paper Title": "FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D Face\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 75,
                    "Sentence": "4.96 0.155 61.7 5.58 0.174 56.1 7.55 0.205 40.9 26.03 0.275 27.0 85.3\nPRNet ",
                    "Citation Text": "Y. Feng, F. Wu, X. Shao, Y. Wang, and X. Zhou, \u201cJoint 3d face re-\nconstruction and dense alignment with position map regression\nnetwork,\u201d in ECCV , 2018, pp. 534\u2013551. 3, 12, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07835",
                        "Citation Paper Title": "Title:Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network",
                        "Citation Paper Abstract": "Abstract:We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment. To achieve this, we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space, then train a simple Convolutional Neural Network to regress it from a single 2D image. We also integrate a weight mask into the loss function during training to improve the performance of the network. Our method does not rely on any prior face model, and can reconstruct full facial geometry along with semantic meaning. Meanwhile, our network is very light-weighted and spends only 9.8ms to process an image, which is extremely faster than previous works. Experiments on multiple challenging datasets show that our method surpasses other state-of-the-art methods on both reconstruction and alignment tasks by a large margin.",
                        "Citation Paper Authors": "Authors:Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "2.25 0.084 69.1 2.50 0.088 68.2 3.38 0.101 65.1 6.31 0.176 49.0 87.0\nFaceScape(Opti.) ",
                    "Citation Text": "H. Yang, H. Zhu, Y. Wang, M. Huang, Q. Shen, R. Yang, and\nX. Cao, \u201cFacescape: a large-scale high quality 3d face dataset and\ndetailed riggable 3d face prediction,\u201d in CVPR , 2020, pp. 601\u2013610.\n12, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.13989",
                        "Citation Paper Title": "Title:FaceScape: a Large-scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a large-scale detailed 3D face dataset, FaceScape, and propose a novel algorithm that is able to predict elaborate riggable 3D face models from a single image input. FaceScape dataset provides 18,760 textured 3D faces, captured from 938 subjects and each with 20 specific expressions. The 3D models contain the pore-level facial geometry that is also processed to be topologically uniformed. These fine 3D facial models can be represented as a 3D morphable model for rough shapes and displacement maps for detailed geometry. Taking advantage of the large-scale and high-accuracy dataset, a novel algorithm is further proposed to learn the expression-specific dynamic details using a deep neural network. The learned relationship serves as the foundation of our 3D face prediction system from a single image input. Different than the previous methods, our predicted 3D models are riggable with highly detailed geometry under different expressions. The unprecedented dataset and code will be released to public for research purpose.",
                        "Citation Paper Authors": "Authors:Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, Xun Cao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.02048v4": {
            "Paper Title": "Efficient Spatially Sparse Inference for Conditional GANs and Diffusion\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 108,
                    "Sentence": ", we explicitly\nspecify the usage permission of our engine with proper\nlicenses. Additionally, we run a forensics detector ",
                    "Citation Text": "S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, \u201cCnn-\ngenerated images are surprisingly easy to spot... for now,\u201d in\nCVPR , 2020. 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11035",
                        "Citation Paper Title": "Title:CNN-generated images are surprisingly easy to spot... for now",
                        "Citation Paper Abstract": "Abstract:In this work we ask whether it is possible to create a \"universal\" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis. Code and pre-trained networks are available at this https URL .",
                        "Citation Paper Authors": "Authors:Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": ", we use the\nstandard metrics Peak Signal Noise Ratio (PSNR, higher is\nbetter), LPIPS (lower is better) ",
                    "Citation Text": "R. Zhang, P . Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe\nunreasonable effectiveness of deep features as a perceptual metric,\u201d\ninCVPR , 2018. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 101,
                    "Sentence": "\u2020to evaluate the\nimage quality. For Cityscapes, we additionally adopt a\nsemantic segmentation metric to evaluate the generated\nimages. Specifically, we run DRN-D-105 ",
                    "Citation Text": "F. Yu, V . Koltun, and T. Funkhouser, \u201cDilated residual networks,\u201d\ninCVPR , 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.09914",
                        "Citation Paper Title": "Title:Dilated Residual Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.",
                        "Citation Paper Authors": "Authors:Fisher Yu, Vladlen Koltun, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "to segment the\nimages in the validation set. For each segmented object,\nwe use its segmentation mask to inpaint the image by\nCoModGAN ",
                    "Citation Text": "S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, and\nY. Xu, \u201cLarge scale image completion via co-modulated generative\nadversarial networks,\u201d in ICLR , 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10428",
                        "Citation Paper Title": "Title:Large Scale Image Completion via Co-Modulated Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": ". To automatically generate a stroke\nediting benchmark, we first use Detic ",
                    "Citation Text": "X. Zhou, R. Girdhar, A. Joulin, P . Kr\u00e4henb\u00fchl, and I. Misra,\n\u201cDetecting twenty-thousand classes using image-level supervision,\u201d\ninarXiv preprint arXiv:2201.02605 , 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02605",
                        "Citation Paper Title": "Title:Detecting Twenty-thousand Classes using Image-level Supervision",
                        "Citation Paper Abstract": "Abstract:Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, Ishan Misra"
                    }
                },
                {
                    "Sentence ID": 94,
                    "Sentence": "is a diffusion probabilistic model that models the\ndata distribution through an iterative denoising process. It\nadopts a U-Net ",
                    "Citation Text": "O. Ronneberger, P . Fischer, and T. Brox, \u201cU-net: Convolutional\nnetworks for biomedical image segmentation,\u201d in Medical Image\nComputing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October 5-9, 2015, Pro-\nceedings, Part III 18 . Springer, 2015, pp. 234\u2013241. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "and extendIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 4\nthe idea to different tasks ",
                    "Citation Text": "L. Wang, X. Dong, Y. Wang, X. Ying, Z. Lin, W. An, and\nY. Guo, \u201cExploring sparsity in image super-resolution for efficient\ninference,\u201d in CVPR , 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09603",
                        "Citation Paper Title": "Title:Exploring Sparsity in Image Super-Resolution for Efficient Inference",
                        "Citation Paper Abstract": "Abstract:Current CNN-based super-resolution (SR) methods process all locations equally with computational resources being uniformly assigned in space. However, since missing details in low-resolution (LR) images mainly exist in regions of edges and textures, less computational resources are required for those flat regions. Therefore, existing CNN-based methods involve redundant computation in flat regions, which increases their computational cost and limits their applications on mobile devices. In this paper, we explore the sparsity in image SR to improve inference efficiency of SR networks. Specifically, we develop a Sparse Mask SR (SMSR) network to learn sparse masks to prune redundant computation. Within our SMSR, spatial masks learn to identify \"important\" regions while channel masks learn to mark redundant channels in those \"unimportant\" regions. Consequently, redundant computation can be accurately localized and skipped while maintaining comparable performance. It is demonstrated that our SMSR achieves state-of-the-art performance with 41%/33%/27% FLOPs being reduced for x2/3/4 SR. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Longguang Wang, Xiaoyu Dong, Yingqian Wang, Xinyi Ying, Zaiping Lin, Wei An, Yulan Guo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.04521v6": {
            "Paper Title": "An App for the Discovery of Properties of Poncelet Triangles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00647v3": {
            "Paper Title": "IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable\n  Novel View Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 81,
                    "Sentence": "is another state-of-the-art un-\nsupervised learning method, and we use their pre-trained\nmodels. We do not choose IRISformer ",
                    "Citation Text": "Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli,\nand Manmohan Chandraker. IRISformer: Dense Vision\nTransformers for Single-Image Inverse Rendering in In-\ndoor Scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 2822\u2013\n2831, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.08423",
                        "Citation Paper Title": "Title:IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:Indoor scenes exhibit significant appearance variations due to myriad interactions between arbitrarily diverse object shapes, spatially-changing materials, and complex lighting. Shadows, highlights, and inter-reflections caused by visible and invisible light sources require reasoning about long-range interactions for inverse rendering, which seeks to recover the components of image formation, namely, shape, material, and lighting. In this work, our intuition is that the long-range attention learned by transformer architectures is ideally suited to solve longstanding challenges in single-image inverse rendering. We demonstrate with a specific instantiation of a dense vision transformer, IRISformer, that excels at both single-task and multi-task reasoning required for inverse rendering. Specifically, we propose a transformer architecture to simultaneously estimate depths, normals, spatially-varying albedo, roughness and lighting from a single image of an indoor scene. Our extensive evaluations on benchmark datasets demonstrate state-of-the-art results on each of the above tasks, enabling applications like object insertion and material editing in a single unconstrained real image, with greater photorealism than prior works. Code and data are publicly released at this https URL.",
                        "Citation Paper Authors": "Authors:Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, Manmohan Chandraker"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "and learning-based [34, 38] intrinsic decomposition meth-\nods, and neural rendering methods [76, 74, 77] com-\nbined with inverse rendering on synthetic object dataset in\nSec. 4.2. Then we only compare qualitative results on syn-\nthetic scenes (e.g. Replica ",
                    "Citation Text": "Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik\nWijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal,\nCarl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,\nBrian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang\nZou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler\nGillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva,\nDhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael\nGoesele, Steven Lovegrove, and Richard Newcombe. The\nReplica Dataset: A Digital Replica of Indoor Spaces. arXiv\npreprint arXiv:1906.05797 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05797",
                        "Citation Paper Title": "Title:The Replica Dataset: A Digital Replica of Indoor Spaces",
                        "Citation Paper Abstract": "Abstract:We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.",
                        "Citation Paper Authors": "Authors:Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04636v3": {
            "Paper Title": "Ego-Body Pose Estimation via Ego-Head Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "is a large-scale motion capture dataset with\nabout 45 hours of diverse motions. We split training and\ntesting data following HuMoR ",
                    "Citation Text": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J Guibas. HuMoR: 3D human\nmotion model for robust pose estimation. In ICCV , 2021. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04668",
                        "Citation Paper Title": "Title:HuMoR: 3D Human Motion Model for Robust Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos.",
                        "Citation Paper Authors": "Authors:Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ", we place the first pose in a random location\nwith the feet in contact with the floor. Then we calculate\npenetration loss following Wang et al. ",
                    "Citation Text": "Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao-\nlong Wang. Synthesizing long-term 3D human motion and\ninteraction in 3D scenes. In CVPR , 2021. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.05522",
                        "Citation Paper Title": "Title:Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes",
                        "Citation Paper Abstract": "Abstract:Synthesizing 3D human motion plays an important role in many graphics applications as well as understanding human activity. While many efforts have been made on generating realistic and natural human motion, most approaches neglect the importance of modeling human-scene interactions and affordance. On the other hand, affordance reasoning (e.g., standing on the floor or sitting on the chair) has mainly been studied with static human pose and gestures, and it has rarely been addressed with human motion. In this paper, we propose to bridge human motion synthesis and scene affordance reasoning. We present a hierarchical generative framework to synthesize long-term 3D human motion conditioning on the 3D scene structure. Building on this framework, we further enforce multiple geometry constraints between the human mesh and scene point clouds via optimization to improve realistic synthesis. Our experiments show significant improvements over previous approaches on generating natural and physically plausible human motion in a scene.",
                        "Citation Paper Authors": "Authors:Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, Xiaolong Wang"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", we deploy\na diffusion model to generate full-body poses conditioned\non head poses. We use the formulation proposed in the\ndenoising diffusion probabilistic model (DDPM) ",
                    "Citation Text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS , 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11239",
                        "Citation Paper Title": "Title:Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Ajay Jain, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", head translation difference, and\nhead rotation difference computed by O\u22121\nt\u22121Otwhere Ot\ndenotes the head rotation matrix at time step t. We adopt a\ntransformer-based architecture ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "follows the same sensor setting and deploys a transformer-\nbased model to leverage IMU sequential information effec-\ntively. Fewer sensors are investigated in LoBSTr ",
                    "Citation Text": "Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lobstr:\nReal-time lower-body pose prediction from sparse upper-body\ntracking signals. Computer Graphics Forum , 40(2):265\u2013275,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01500",
                        "Citation Paper Title": "Title:LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body Tracking Signals",
                        "Citation Paper Abstract": "Abstract:With the popularization of game and VR/AR devices, there is a growing need for capturing human motion with a sparse set of tracking data. In this paper, we introduce a deep neural-network (DNN) based method for real-time prediction of the lower-body pose only from the tracking signals of the upper-body joints. Specifically, our Gated Recurrent Unit (GRU)-based recurrent architecture predicts the lower-body pose and feet contact probability from past sequence of tracking signals of the head, hands and pelvis. A major feature of our method is that the input signal is represented with the velocity of tracking signals. We show that the velocity representation better models the correlation between the upper-body and lower-body motions and increase the robustness against the diverse scales and proportions of the user body than position-orientation representations. In addition, to remove foot-skating and floating artifacts, our network predicts feet contact state, which is used to post-process the lower-body pose with inverse kinematics to preserve the contact. Our network is lightweight so as to run in real-time applications. We show the effectiveness of our method through several quantitative evaluations against other architectures and input representations, with respect to wild tracking data obtained from commercial VR devices.",
                        "Citation Paper Authors": "Authors:Dongseok Yang, Doyeon Kim, Sung-Hee Lee"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "further\nincludes a PD controller on top of the kinematic estimator to\nintroduce physics constraints during reconstruction. TIP ",
                    "Citation Text": "Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won,\nAlexander W Winkler, and C Karen Liu. Transformer Inertial\nPoser: Attention-based real-time human motion reconstruc-\ntion from sparse imus. In SIGGRAPH ASIA , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.15720",
                        "Citation Paper Title": "Title:Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation",
                        "Citation Paper Abstract": "Abstract:Real-time human motion reconstruction from a sparse set of (e.g. six) wearable IMUs provides a non-intrusive and economic approach to motion capture. Without the ability to acquire position information directly from IMUs, recent works took data-driven approaches that utilize large human motion datasets to tackle this under-determined problem. Still, challenges remain such as temporal consistency, drifting of global and joint motions, and diverse coverage of motion types on various terrains. We propose a novel method to simultaneously estimate full-body motion and generate plausible visited terrain from only six IMU sensors in real-time. Our method incorporates 1. a conditional Transformer decoder model giving consistent predictions by explicitly reasoning prediction history, 2. a simple yet general learning target named \"stationary body points\" (SBPs) which can be stably predicted by the Transformer model and utilized by analytical routines to correct joint and global drifting, and 3. an algorithm to generate regularized terrain height maps from noisy SBP predictions which can in turn correct noisy global motion estimation. We evaluate our framework extensively on synthesized and real IMU data, and with real-time live demos, and show superior performance over strong baseline methods.",
                        "Citation Paper Authors": "Authors:Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, C. Karen Liu"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "proposes a real-time pipeline to predict full body motions\nfrom 6 IMU sensors, including head, torso, left/right arms,and left/right lower legs. Follow-up work PIP ",
                    "Citation Text": "Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,\nVladislav Golyanik, Christian Theobalt, and Feng Xu. Physi-\ncal inertial poser (PIP): Physics-aware real-time human mo-\ntion tracking from sparse inertial sensors. In CVPR , 2022.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.08528",
                        "Citation Paper Title": "Title:Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors",
                        "Citation Paper Abstract": "Abstract:Motion capture from sparse inertial sensors has shown great potential compared to image-based approaches since occlusions do not lead to a reduced tracking quality and the recording space is not restricted to be within the viewing frustum of the camera. However, capturing the motion and global position only from a sparse set of inertial sensors is inherently ambiguous and challenging. In consequence, recent state-of-the-art methods can barely handle very long period motions, and unrealistic artifacts are common due to the unawareness of physical constraints. To this end, we present the first method which combines a neural kinematics estimator and a physics-aware motion optimizer to track body motions with only 6 inertial sensors. The kinematics module first regresses the motion status as a reference, and then the physics module refines the motion to satisfy the physical constraints. Experiments demonstrate a clear improvement over the state of the art in terms of capture accuracy, temporal stability, and physical correctness.",
                        "Citation Paper Authors": "Authors:Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, Feng Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.02501v4": {
            "Paper Title": "SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance\n  Fields",
            "Sentences": [
                {
                    "Sentence ID": 75,
                    "Sentence": ".\nSimilar to us, all train with images and poses. We also\ncompare against state-of-the-art 3D-aware GAN, namely\nSynSin ",
                    "Citation Text": "Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. SynSin: End-to-end view synthesis from a single\nimage. In CVPR , 2020. 5, 6, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.08804",
                        "Citation Paper Title": "Title:SynSin: End-to-end View Synthesis from a Single Image",
                        "Citation Paper Abstract": "Abstract:Single image view synthesis allows for the generation of new views of a scene given a single input image. This is challenging, as it requires comprehensively understanding the 3D scene from a single image. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Unlike prior work, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.",
                        "Citation Paper Authors": "Authors:Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", inherently limits reconstruction\nto the visible surface. Differentiable renderers are also pop-\nular, trained with views and poses [51, 65, 15]. To alleviate\nthe need of color rendering, some optimize silhouettes ",
                    "Citation Text": "Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias\nZwicker. Drwr: A differentiable renderer without render-\ning for unsupervised 3d structure learning from silhouette\nimages. In ICML , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06127",
                        "Citation Paper Title": "Title:DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D Structure Learning from Silhouette Images",
                        "Citation Paper Abstract": "Abstract:Differentiable renderers have been used successfully for unsupervised 3D structure learning from 2D images because they can bridge the gap between 3D and 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise losses between rendered images of 3D reconstructions and ground truth images from corresponding viewpoints. Hence they require interpolation of the recovered 3D structure at each pixel, visibility handling, and optionally evaluating a shading model. In contrast, here we propose a Differentiable Renderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a simple but effective loss that evaluates how well the projections of reconstructed 3D point clouds cover the ground truth object silhouette. Specifically, DRWR employs a smooth silhouette loss to pull the projection of each individual 3D point inside the object silhouette, and a structure-aware repulsion loss to push each pair of projections that fall inside the silhouette far away from each other. Although we omit surface interpolation, visibility handling, and shading, our results demonstrate that DRWR achieves state-of-the-art accuracies under widely used benchmarks, outperforming previous methods both qualitatively and quantitatively. In addition, our training times are significantly lower due to the simplicity of DRWR.",
                        "Citation Paper Authors": "Authors:Zhizhong Han, Chao Chen, Yu-Shen Liu, Matthias Zwicker"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ".\nWhen semantic and geometry are estimated jointly it is re-\nferred as semantic scene completion (SSC), recently sur-\nveyed in ",
                    "Citation Text": "Luis Roldao, Raoul De Charette, and Anne Verroust-\nBlondet. 3d semantic scene completion: a survey. IJCV ,\n2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07466",
                        "Citation Paper Title": "Title:3D Semantic Scene Completion: a Survey",
                        "Citation Paper Abstract": "Abstract:Semantic Scene Completion (SSC) aims to jointly estimate the complete geometry and semantics of a scene, assuming partial sparse input. In the last years following the multiplication of large-scale 3D datasets, SSC has gained significant momentum in the research community because it holds unresolved challenges. Specifically, SSC lies in the ambiguous completion of large unobserved areas and the weak supervision signal of the ground truth. This led to a substantially increasing number of papers on the matter. This survey aims to identify, compare and analyze the techniques providing a critical analysis of the SSC literature on both methods and datasets. Throughout the paper, we provide an in-depth analysis of the existing works covering all choices made by the authors while highlighting the remaining avenues of research. SSC performance of the SoA on the most popular datasets is also evaluated and analyzed.",
                        "Citation Paper Authors": "Authors:Luis Roldao, Raoul de Charette, Anne Verroust-Blondet"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "synthesizes novel depths\nand views building on Multiplane Images, while very re-\ncently ",
                    "Citation Text": "Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel\nCremers. Behind the scenes: Density fields for single view\nreconstruction. In CVPR , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.07668",
                        "Citation Paper Title": "Title:Behind the Scenes: Density Fields for Single View Reconstruction",
                        "Citation Paper Abstract": "Abstract:Inferring a meaningful geometric scene representation from a single image is a fundamental problem in computer vision. Approaches based on traditional depth map prediction can only reason about areas that are visible in the image. Currently, neural radiance fields (NeRFs) can capture true 3D including color, but are too complex to be generated from a single image. As an alternative, we propose to predict implicit density fields. A density field maps every location in the frustum of the input image to volumetric density. By directly sampling color from the available views instead of storing color in the density field, our scene representation becomes significantly less complex compared to NeRFs, and a neural network can predict it in a single forward pass. The prediction network is trained through self-supervision from only video data. Our formulation allows volume rendering to perform both depth prediction and novel view synthesis. Through experiments, we show that our method is able to predict meaningful geometry for regions that are occluded in the input image. Additionally, we demonstrate the potential of our approach on three datasets for depth prediction and novel-view synthesis.",
                        "Citation Paper Authors": "Authors:Felix Wimbauer, Nan Yang, Christian Rupprecht, Daniel Cremers"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "handle objects\non cluttered scenes or large-scale scenes being synthetic as\nin SEE3D ",
                    "Citation Text": "Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov,\nRares Ambrus, Adrien Gaidon, William T Freeman, Fredo\nDurand, Joshua B Tenenbaum, and Vincent Sitzmann. See-\ning 3d objects in a single image via self-supervised static-\ndynamic disentanglement. arXiv preprint arXiv:2207.11232 ,\n2022. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.11232",
                        "Citation Paper Title": "Title:Neural Groundplans: Persistent Neural Scene Representations from a Single Image",
                        "Citation Paper Abstract": "Abstract:We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird's-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance-level segmentation, 3D bounding box prediction, and scene editing. This highlights the value of neural groundplans as a backbone for efficient 3D scene understanding models.",
                        "Citation Paper Authors": "Authors:Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares Ambrus, Adrien Gaidon, William T. Freeman, Fredo Durand, Joshua B. Tenenbaum, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ", or category-centric/agnostic\nview synthesis [56, 39]. In the latter, objects are usually\non a plain background though CO3D ",
                    "Citation Text": "Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3D: Large-scale learning and evaluation of\nreal-life 3D category reconstruction. In ICCV , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.00512",
                        "Citation Paper Title": "Title:Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction",
                        "Citation Paper Abstract": "Abstract:Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale \"in-the-wild\" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at this https URL .",
                        "Citation Paper Authors": "Authors:Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "which learn a representation generalizable to un-\nseen input images. The almost entire single-view literature\nhowever focuses on objects which hold specific challenges\nsuch as shape and appearance disentanglement [30, 58], ex-\nploiting symmetry priors ",
                    "Citation Text": "Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian,\nand Guosheng Lin. Symmnerf: Learning to explore symme-\ntry prior for single-view view synthesis. In ACCV , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14819",
                        "Citation Paper Title": "Title:SymmNeRF: Learning to Explore Symmetry Prior for Single-View View Synthesis",
                        "Citation Paper Abstract": "Abstract:We study the problem of novel view synthesis of objects from a single image. Existing methods have demonstrated the potential in single-view view synthesis. However, they still fail to recover the fine appearance details, especially in self-occluded areas. This is because a single view only provides limited information. We observe that manmade objects usually exhibit symmetric appearances, which introduce additional prior knowledge. Motivated by this, we investigate the potential performance gains of explicitly embedding symmetry into the scene representation. In this paper, we propose SymmNeRF, a neural radiance field (NeRF) based framework that combines local and global conditioning under the introduction of symmetry priors. In particular, SymmNeRF takes the pixel-aligned image features and the corresponding symmetric features as extra inputs to the NeRF, whose parameters are generated by a hypernetwork. As the parameters are conditioned on the image-encoded latent codes, SymmNeRF is thus scene-independent and can generalize to new scenes. Experiments on synthetic and real-world datasets show that SymmNeRF synthesizes novel views with more details regardless of the pose transformation, and demonstrates good generalization when applied to unseen objects. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian, Guosheng Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01735v4": {
            "Paper Title": "Neural Fourier Filter Bank",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "have been introduced. These recent methods, however, do\nnot, at the very least explicitly, consider how grid resolu-\ntions and frequency interact.\nThus, some works try to combine both directions. For\nexample, SAPE ",
                    "Citation Text": "Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung,\nand Daniel Cohen-Or. SAPE: Spatially-Adaptive Progres-\nsive Encoding for Neural Optimization. Adv. Neural Inform.\nProcess. Syst. , 2021. 2, 3, 4, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.09125",
                        "Citation Paper Title": "Title:SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization",
                        "Citation Paper Abstract": "Abstract:Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",
                        "Citation Paper Authors": "Authors:Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "encode the input feature vectors into a high-dimension la-\ntent space through a sequence of periodic functions. Tan-\nciket al. ",
                    "Citation Text": "Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier Fea-\ntures Let Networks Learn High Frequency Functions in Low\nDimensional Domains. Adv. Neural Inform. Process. Syst. ,\n2020. 1, 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10739",
                        "Citation Paper Title": "Title:Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
                        "Citation Paper Abstract": "Abstract:We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "directly\nlearn trainable coefficients of the hierarchical Haar wavelet\ntransform, reporting impressive compression results. Con-\ncurrently, Rho et al. ",
                    "Citation Text": "Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan\nLee, Jong Hwan Ko, and Eunbyung Park. Masked wavelet\nrepresentation for compact neural radiance fields. 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.09069",
                        "Citation Paper Title": "Title:Masked Wavelet Representation for Compact Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural radiance fields (NeRF) have demonstrated the potential of coordinate-based neural representation (neural fields or implicit neural representation) in neural rendering. However, using a multi-layer perceptron (MLP) to represent a 3D scene or object requires enormous computational resources and time. There have been recent studies on how to reduce these computational inefficiencies by using additional data structures, such as grids or trees. Despite the promising performance, the explicit data structure necessitates a substantial amount of memory. In this work, we present a method to reduce the size without compromising the advantages of having additional data structures. In detail, we propose using the wavelet transform on grid-based neural fields. Grid-based neural fields are for fast convergence, and the wavelet transform, whose efficiency has been demonstrated in high-performance standard codecs, is to improve the parameter efficiency of grids. Furthermore, in order to achieve a higher sparsity of grid coefficients while maintaining reconstruction quality, we present a novel trainable masking approach. Experimental results demonstrate that non-spatial grid coefficients, such as wavelet coefficients, are capable of attaining a higher level of sparsity than spatial grid coefficients, resulting in a more compact representation. With our proposed mask and compression pipeline, we achieved state-of-the-art performance within a memory budget of 2 MB. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, Eunbyung Park"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "propose a transformation\nthat resembles an adaptive variation of Haar wavelets to fa-\ncilitate 3D point cloud compression. Isik et al. ",
                    "Citation Text": "Berivan Isik, Philip Chou, Sung Jin Hwang, Nicholas John-\nston, and George Toderici. LV AC: Learned volumetric at-\ntribute compression for point clouds using coordinate based\nnetworks. Frontiers in Signal Processing , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.08988",
                        "Citation Paper Title": "Title:LVAC: Learned Volumetric Attribute Compression for Point Clouds using Coordinate Based Networks",
                        "Citation Paper Abstract": "Abstract:We consider the attributes of a point cloud as samples of a vector-valued volumetric function at discrete positions. To compress the attributes given the positions, we compress the parameters of the volumetric function. We model the volumetric function by tiling space into blocks, and representing the function over each block by shifts of a coordinate-based, or implicit, neural network. Inputs to the network include both spatial coordinates and a latent vector per block. We represent the latent vectors using coefficients of the region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based point cloud codec G-PCC. The coefficients, which are highly compressible, are rate-distortion optimized by back-propagation through a rate-distortion Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT by 2--4 dB. This is the first work to compress volumetric functions represented by local coordinate-based neural networks. As such, we expect it to be applicable beyond point clouds, for example to compression of high-resolution neural radiance fields.",
                        "Citation Paper Authors": "Authors:Berivan Isik, Philip A. Chou, Sung Jin Hwang, Nick Johnston, George Toderici"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "introduce wavelet\nscattering transform to create geometric invariants and de-\nformation stability. Phung et al. ",
                    "Citation Text": "Hao Phung, Quan Dao, and Anh Tran. Wavelet diffu-\nsion models are fast and scalable image generators. arXiv\npreprint , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.16152",
                        "Citation Paper Title": "Title:Wavelet Diffusion Models are fast and scalable Image Generators",
                        "Citation Paper Abstract": "Abstract:Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models' running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion scheme. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Hao Phung, Quan Dao, Anh Tran"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "reproduce wavelets through linearly combining\nactivation functions. Gauthier et al. ",
                    "Citation Text": "Shanel Gauthier, Benjamin Th \u00b4erien, Laurent Alsene-\nRacicot, Muawiz Chaudhary, Irina Rish, Eugene Belilovsky,\nMichael Eickenberg, and Guy Wolf. Parametric scattering\nnetworks. In IEEE Conf. Comput. Vis. Pattern Recog. , 2022.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.09539",
                        "Citation Paper Title": "Title:Parametric Scattering Networks",
                        "Citation Paper Abstract": "Abstract:The wavelet scattering transform creates geometric invariants and deformation stability. In multiple signal domains, it has been shown to yield more discriminative representations compared to other non-learned representations and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. In this work, we investigate whether this standard wavelet filterbank construction is optimal. Focusing on Morlet wavelets, we propose to learn the scales, orientations, and aspect ratios of the filters to produce problem-specific parameterizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains in small-sample classification settings over the standard scattering transform. Moreover, our empirical results suggest that traditional filterbank constructions may not always be necessary for scattering transforms to extract effective representations.",
                        "Citation Paper Authors": "Authors:Shanel Gauthier, Benjamin Th\u00e9rien, Laurent Als\u00e8ne-Racicot, Muawiz Chaudhary, Irina Rish, Eugene Belilovsky, Michael Eickenberg, Guy Wolf"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ",\nand for image generation [21,32,42,56,56]. Recently, Liang\net al. ",
                    "Citation Text": "Senwei Liang, Liyao Lyu, Chunmei Wang, and Haizhao\nYang. Reproducing activation function for deep learning.\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.04844",
                        "Citation Paper Title": "Title:Reproducing Activation Function for Deep Learning",
                        "Citation Paper Abstract": "Abstract:We propose reproducing activation functions (RAFs) to improve deep learning accuracy for various applications ranging from computer vision to scientific computing. The idea is to employ several basic functions and their learnable linear combination to construct neuron-wise data-driven activation functions for each neuron. Armed with RAFs, neural networks (NNs) can reproduce traditional approximation tools and, therefore, approximate target functions with a smaller number of parameters than traditional NNs. In NN training, RAFs can generate neural tangent kernels (NTKs) with a better condition number than traditional activation functions lessening the spectral bias of deep learning. As demonstrated by extensive numerical tests, the proposed RAFs can facilitate the convergence of deep learning optimization for a solution with higher accuracy than existing deep learning solvers for audio/image/video reconstruction, PDEs, and eigenvalue problems. With RAFs, the errors of audio/video reconstruction, PDEs, and eigenvalue problems are decreased by over 14%, 73%, 99%, respectively, compared with baseline, while the performance of image reconstruction increases by 58%.",
                        "Citation Paper Authors": "Authors:Senwei Liang, Liyao Lyu, Chunmei Wang, Haizhao Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.09636v4": {
            "Paper Title": "Neural Implicit Surface Evolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02500v3": {
            "Paper Title": "PhysDiff: Physics-Guided Human Motion Diffusion Model",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "12.81 0.950 13.077 13.912 1.383 28.371\nPhysDiff w/ MDM (Ours) 13.27 0.956 0.874 0.201 0.389 1.463\nTable 3. Action-to-motion results on UESTC ",
                    "Citation Text": "Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao\nShen, and Wei-Shi Zheng. A large-scale rgb-d database for\narbitrary-view human action recognition. In Proceedings\nof the 26th ACM international Conference on Multimedia ,\npages 1510\u20131518, 2018. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10681",
                        "Citation Paper Title": "Title:A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition",
                        "Citation Paper Abstract": "Abstract:Current researches of action recognition mainly focus on single-view and multi-view recognition, which can hardly satisfies the requirements of human-robot interaction (HRI) applications to recognize actions from arbitrary views. The lack of datasets also sets up barriers. To provide data for arbitrary-view action recognition, we newly collect a large-scale RGB-D action dataset for arbitrary-view action analysis, including RGB videos, depth and skeleton sequences. The dataset includes action samples captured in 8 fixed viewpoints and varying-view sequences which covers the entire 360 degree view angles. In total, 118 persons are invited to act 40 action categories, and 25,600 video samples are collected. Our dataset involves more participants, more viewpoints and a large number of samples. More importantly, it is the first dataset containing the entire 360 degree varying-view sequences. The dataset provides sufficient data for multi-view, cross-view and arbitrary-view action analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to tackle the problem of arbitrary-view action recognition. Experiment results show that the VS-CNN achieves superior performance.",
                        "Citation Paper Authors": "Authors:Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, Wei-Shi Zheng"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ".\nAfter the denoising diffusion model has been trained,\none can apply it to solve the SDE / ODE in Eq. (1). A par-\nticular approach, DDIM ",
                    "Citation Text": "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations , 2021. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02502",
                        "Citation Paper Title": "Title:Denoising Diffusion Implicit Models",
                        "Citation Paper Abstract": "Abstract:Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",
                        "Citation Paper Authors": "Authors:Jiaming Song, Chenlin Meng, Stefano Ermon"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "Denoising Diffusion Models. Score-based denoising diffu-\nsion models [75, 25, 77, 76] have achieved great successes\nin various applications such as image generation [68, 63,\n67, 74, 81], text-to-speech synthesis ",
                    "Citation Text": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. DiffWave: A versatile diffusion model for\naudio synthesis. In International Conference on Learning\nRepresentations , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09761",
                        "Citation Paper Title": "Title:DiffWave: A Versatile Diffusion Model for Audio Synthesis",
                        "Citation Paper Abstract": "Abstract:In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.",
                        "Citation Paper Authors": "Authors:Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.04214v2": {
            "Paper Title": "VM-NeRF: Tackling Sparsity in NeRF with View Morphing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06300v4": {
            "Paper Title": "Time of Impact Dataset for Continuous Collision Detection and a Scalable\n  Conservative Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01248v2": {
            "Paper Title": "Differentiable Rendering for Synthetic Aperture Radar Imagery",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ".\nRasterization is computationally-efficient, but not nat-\nurally differentiable. However, smoothed variants have\nemerged ",
                    "Citation Text": "H. Kato, Y . Ushiku, and T. Harada, \u201cNeural 3d mesh renderer,\u201d\ninThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07566",
                        "Citation Paper Title": "Title:Neural 3D Mesh Renderer",
                        "Citation Paper Abstract": "Abstract:For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.",
                        "Citation Paper Authors": "Authors:Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "describes these collectively as \u201c2D\nNeural Rendering\u201d methods, and distinguishes them from\nan emerging paradigm it calls \u201c3D Neural Rendering.\u201d\nMethods of this new paradigm, such as ",
                    "Citation Text": "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-\nmamoorthi, and R. Ng, \u201cNerf: Representing scenes as neural\nradiance fields for view synthesis,\u201d in Computer Vision \u2013 ECCV\n2020 , A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.\nCham: Springer International Publishing, 2020, pp. 405\u2013421.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.03923v2": {
            "Paper Title": "Unsupervised Discovery and Composition of Object Light Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14330v4": {
            "Paper Title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09477v4": {
            "Paper Title": "UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a\n  Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11620v3": {
            "Paper Title": "Interactive Volume Visualization via Multi-Resolution Hash Encoding\n  based Neural Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11243v2": {
            "Paper Title": "Multiface: A Dataset for Neural Face Rendering",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ": by integrating both\nvertically and horizontally on the generated warping grid to\navoid flipping of relative pixel positions.\nResidual Connection. We insert residual layers ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. CoRR ,\nabs/1512.03385, 2015. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "are the first\nsuccessful approach to building photorealistic, high qualityavatars but suffer from limitations in their expressiveness\nand controllability. To overcome these issues, in ",
                    "Citation Text": "Hang Chu, Shugao Ma, Fernando De la Torre, Sanja Fi-\ndler, and Yaser Sheikh. Expressive telepresence via modular\ncodec avatars. In European Conference on Computer Vision ,\npages 330\u2013345. Springer, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.11789",
                        "Citation Paper Title": "Title:Expressive Telepresence via Modular Codec Avatars",
                        "Citation Paper Abstract": "Abstract:VR telepresence consists of interacting with another human in a virtual space represented by an avatar. Today most avatars are cartoon-like, but soon the technology will allow video-realistic ones. This paper aims in this direction and presents Modular Codec Avatars (MCA), a method to generate hyper-realistic faces driven by the cameras in the VR headset. MCA extends traditional Codec Avatars (CA) by replacing the holistic models with a learned modular representation. It is important to note that traditional person-specific CAs are learned from few training samples, and typically lack robustness as well as limited expressiveness when transferring facial expressions. MCAs solve these issues by learning a modulated adaptive blending of different facial components as well as an exemplar-based latent alignment. We demonstrate that MCA achieves improved expressiveness and robustness w.r.t to CA in a variety of real-world datasets and practical scenarios. Finally, we showcase new applications in VR telepresence enabled by the proposed model.",
                        "Citation Paper Authors": "Authors:Hang Chu, Shugao Ma, Fernando De la Torre, Sanja Fidler, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "We briefly review two prior efforts in human face data\ncollection, HUMBI ",
                    "Citation Text": "Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth\nVenkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park.\nHumbi: A large multiview dataset of human body expres-\nsions, 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00281",
                        "Citation Paper Title": "Title:HUMBI: A Large Multiview Dataset of Human Body Expressions",
                        "Citation Paper Abstract": "Abstract:This paper presents a new large multiview dataset called HUMBI for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and physical condition. With the multiview image streams, we reconstruct high fidelity body expressions using 3D mesh models, which allows representing view-specific appearance using their canonical atlas. We demonstrate that HUMBI is highly effective in learning and reconstructing a complete human model and is complementary to the existing datasets of human body expressions with limited views and subjects such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",
                        "Citation Paper Authors": "Authors:Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, Hyun Soo Park"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.00519v2": {
            "Paper Title": "Learning Neural Implicit Representations with Surface Signal\n  Parameterizations",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". We further achieve a\nhigher compression rate by pruning our model using the\nlottery ticket method ",
                    "Citation Text": "Frankle, J, Carbin, M. The lottery ticket hypothesis: Finding\nsparse, trainable neural networks. In: Proc. Int. Conf. Learn.\nRepresent. 2019,.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.03635",
                        "Citation Paper Title": "Title:The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
                        "Citation Paper Abstract": "Abstract:Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
                        "Citation Paper Authors": "Authors:Jonathan Frankle, Michael Carbin"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "conditions their model on lighting and view-\npoint information, allowing the model to represent prop-\nerties such as specular reflection and shadows. Neu-\nral radiance fields (NeRFs) ",
                    "Citation Text": "Mildenhall, B, Srinivasan, PP, Tancik, M, Barron, JT, Ra-\nmamoorthi, R, Ng, R. NeRF: Representing scenes as neural\nradiance fields for view synthesis. In: Proc. Eur. Conf. Comput.\nVis. 2020, p. 405\u2013421.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "\u2014 or those that minimize distortion metrics of the orig-\ninal mesh, e.g., angular [22, 23, 24], distance [25, 26],\narea [24, 27], and boundary distortions ",
                    "Citation Text": "Sawhney, R, Crane, K. Boundary first flattening. ACM Trans\nGraph 2018;37(1):5:1\u20135:14.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.06873",
                        "Citation Paper Title": "Title:Boundary First Flattening",
                        "Citation Paper Abstract": "Abstract:A conformal flattening maps a curved surface to the plane without distorting angles---such maps have become a fundamental building block for problems in geometry processing, numerical simulation, and computational design. Yet existing methods provide little direct control over the shape of the flattened domain, or else demand expensive nonlinear optimization. Boundary first flattening (BFF) is a linear method for conformal parameterization which is faster than traditional linear methods, yet provides control and quality comparable to sophisticated nonlinear schemes. The key insight is that the boundary data for many conformal mapping problems can be efficiently constructed via the Cherrier formula together with a pair of Poincare-Steklov operators; once the boundary is known, the map can be easily extended over the rest of the domain. Since computation demands only a single factorization of the real Laplace matrix, the amortized cost is about 50x less than any previously published technique for boundary-controlled conformal flattening. As a result, BFF opens the door to real-time editing or fast optimization of high-resolution maps, with direct control over boundary length or angle. We show how this method can be used to construct maps with sharp corners, cone singularities, minimal area distortion, and uniformization over the unit disk; we also demonstrate for the first time how a surface can be conformally flattened directly onto any given target shape.",
                        "Citation Paper Authors": "Authors:Rohan Sawhney, Keenan Crane"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "uses a separate\nnetwork \u2014 atop of a volumetric NeRF representation\n\u2014 in an attempt to disentangle texture content into\na 2D projective space parameterized by a cube map.\nAUV-Net ",
                    "Citation Text": "Chen, Z, Yin, K, Fidler, S. AUV-Net: Learning aligned UV\nmaps for texture transfer and synthesis. In: Proc. IEEE Conf.\nComput. Vis. Pattern Recog. 2022, p. 1455\u20131464.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03105",
                        "Citation Paper Title": "Title:AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of texture representation for 3D shapes for the challenging and underexplored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the traditional way of representing textures with images and linking them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding semantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applications including texture transfer, texture synthesis, and textured single view 3D reconstruction. We conduct experiments on multiple datasets to demonstrate the effectiveness of our method. Project page: this https URL.",
                        "Citation Paper Authors": "Authors:Zhiqin Chen, Kangxue Yin, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "overfit neural\nnetworks to single shapes as weight-encoded represen-\ntations of 3D surfaces, which can be interpreted as an\nefficient and lossy compression of 3D shapes. Takikawa\net al. ",
                    "Citation Text": "Takikawa, T, Litalien, J, Yin, K, Kreis, K, Loop, C,\nNowrouzezahrai, D, et al. Neural geometric level of detail:\nReal-time rendering with implicit 3D shapes. In: Proc. IEEE\nConf. Comput. Vis. Pattern Recog. 2021, p. 11353\u201311362.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.10994",
                        "Citation Paper Title": "Title:Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
                        "Citation Paper Abstract": "Abstract:Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
                        "Citation Paper Authors": "Authors:Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04488v2": {
            "Paper Title": "Multi-Concept Customization of Text-to-Image Diffusion",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ", (2) Text-alignment of the generated images with\ngiven prompts, using text-image similarity in CLIP feature\nspace ",
                    "Citation Text": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation metric\nfor image captioning. In EMNLP , 2021. 5\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08718",
                        "Citation Paper Title": "Title:CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
                        "Citation Paper Abstract": "Abstract:Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality.\nIn this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
                        "Citation Paper Authors": "Authors:Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "dataset with corresponding captions that\nhave a high similarity with the target text prompt, above\nthreshold 0.85in CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n11Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nInInternational Conference on Machine Learning (ICML) ,\n2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", such that running an encoder-\ndecoder can recover an input image. They then train a diffu-\nsion model ",
                    "Citation Text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Conference on Neural Informa-\ntion Processing Systems (NeurIPS) , 2020. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11239",
                        "Citation Paper Title": "Title:Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Ajay Jain, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "as our\nbackbone model, which is built on the Latent Diffusion\nModel (LDM) ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2022.\n1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04420v2": {
            "Paper Title": "Generating Holistic 3D Human Motion from Speech",
            "Sentences": [
                {
                    "Sentence ID": 69,
                    "Sentence": "directly regresses SMPL-X parameters\nusing moderators that estimate the confidence of part-specific\nfeatures. These features are fused and fed to independent\nregressors. PyMAF-X ",
                    "Citation Text": "Hongwen Zhang, Yating Tian, Yuxiang Zhang,\nMengcheng Li, Liang An, Zhenan Sun, and Yebin Liu.\nPymaf-x: Towards well-aligned full-body model re-\ngression from monocular images. arXiv , 2022. 2, 4,\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.06400",
                        "Citation Paper Title": "Title:PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images",
                        "Citation Paper Abstract": "Abstract:We present PyMAF-X, a regression-based approach to recovering parametric full-body models from monocular images. This task is very challenging since minor parametric deviation may lead to noticeable misalignment between the estimated mesh and the input image. Moreover, when integrating part-specific estimations into the full-body model, existing solutions tend to either degrade the alignment or produce unnatural wrist poses. To address these issues, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for well-aligned human mesh recovery and extend it as PyMAF-X for the recovery of expressive full-body models. The core idea of PyMAF is to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status. Specifically, given the currently predicted parameters, mesh-aligned evidence will be extracted from finer-resolution features accordingly and fed back for parameter rectification. To enhance the alignment perception, an auxiliary dense supervision is employed to provide mesh-image correspondence guidance while spatial alignment attention is introduced to enable the awareness of the global contexts for our network. When extending PyMAF for full-body mesh recovery, an adaptive integration strategy is proposed in PyMAF-X to produce natural wrist poses while maintaining the well-aligned performance of the part-specific estimations. The efficacy of our approach is validated on several benchmark datasets for body, hand, face, and full-body mesh recovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment and achieve new state-of-the-art results. The project page with code and video results can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, Yebin Liu"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ".\n\u2022RS: Score on the realism of the generated body and\nhand motions. Following [ 4,62], we trained a binary\nclassifier to discriminate real samples from fake ones\nand the prediction represents the realistic score.\n\u2022Variation : As used in ",
                    "Citation Text": "Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor\nDarrell, Angjoo Kanazawa, and Shiry Ginosar. Learn-\ning to listen: Modeling non-deterministic dyadic facial\nmotion. In Computer Vision and Pattern Recognition\n(CVPR) , pages 20395\u201320405, 2022. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.08451",
                        "Citation Paper Title": "Title:Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion",
                        "Citation Paper Abstract": "Abstract:We present a framework for modeling interactional communication in dyadic conversations: given multimodal inputs of a speaker, we autoregressively output multiple possibilities of corresponding listener motion. We combine the motion and speech audio of the speaker using a motion-audio cross attention transformer. Furthermore, we enable non-deterministic prediction by learning a discrete latent representation of realistic listener motion with a novel motion-encoding VQ-VAE. Our method organically captures the multimodal and non-deterministic nature of nonverbal dyadic interactions. Moreover, it produces realistic 3D listener facial motion synchronous with the speaker (see video). We demonstrate that our method outperforms baselines qualitatively and quantitatively via a rich suite of experiments. To facilitate this line of research, we introduce a novel and large in-the-wild dataset of dyadic conversations. Code, data, and videos available at this https URL.",
                        "Citation Paper Authors": "Authors:Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, Shiry Ginosar"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ". Specifically, the encoder\nconsists of an audio feature extractor and a transformer en-\ncoder ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nAdvances in neural information processing systems , 30,\n2017. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "\u2718 \u2718 3D keypoint \u2718 \u2718 4h MoCap\nYoon et.al [66, 67] \u2718 \u2718 3D keypoint \u2718 \u2714 52h p-GT\nSpeech2Gesture ",
                    "Citation Text": "S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and\nJ. Malik. Learning individual styles of conversational\ngesture. In Computer Vision and Pattern Recognition\n(CVPR) . IEEE, June 2019. 1, 2, 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04160",
                        "Citation Paper Title": "Title:Learning Individual Styles of Conversational Gesture",
                        "Citation Paper Abstract": "Abstract:Human speech is often accompanied by hand and arm gestures. Given audio speech input, we generate plausible gestures to go along with the sound. Specifically, we perform cross-modal translation from \"in-the-wild'' monologue speech of a single speaker to their hand and arm motion. We train on unlabeled videos for which we only have noisy pseudo ground truth from an automatic pose detection system. Our proposed model significantly outperforms baseline methods in a quantitative comparison. To support research toward obtaining a computational understanding of the relationship between gesture and speech, we release a large video dataset of person-specific gestures. The project website with video, code and data can be found at this http URL .",
                        "Citation Paper Authors": "Authors:Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, Jitendra Malik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11202v3": {
            "Paper Title": "FLNeRF: 3D Facial Landmarks Estimation in Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "which includes the original Cartesian coordinates:\n\u03b3(p) = (p,sin(20\u03c0p),cos(20\u03c0p), ...,sin(2L\u22121\u03c0p),cos(2L\u22121\u03c0p)).\n(3)\nWe set L= 4, and \u03b3(\u00b7)is applied to individual coordinates.\nWe adopt the V oxResNet ",
                    "Citation Text": "Hao Chen, Qi Dou, Lequan Yu, and Pheng-Ann Heng.\nV oxresnet: Deep voxelwise residual networks for volumetric\nbrain segmentation. arXiv preprint arXiv:1608.05895 , 2016.\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.05895",
                        "Citation Paper Title": "Title:VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation",
                        "Citation Paper Abstract": "Abstract:Recently deep residual learning with residual units for training very deep neural networks advanced the state-of-the-art performance on 2D image recognition tasks, e.g., object detection and segmentation. However, how to fully leverage contextual representations for recognition tasks from volumetric data has not been well studied, especially in the field of medical image computing, where a majority of image modalities are in volumetric format. In this paper we explore the deep residual learning on the task of volumetric brain segmentation. There are at least two main contributions in our work. First, we propose a deep voxelwise residual network, referred as VoxResNet, which borrows the spirit of deep residual learning in 2D image recognition tasks, and is extended into a 3D variant for handling volumetric data. Second, an auto-context version of VoxResNet is proposed by seamlessly integrating the low-level image appearance features, implicit shape information and high-level context together for further improving the volumetric segmentation performance. Extensive experiments on the challenging benchmark of brain segmentation from magnetic resonance (MR) images corroborated the efficacy of our proposed method in dealing with volumetric data. We believe this work unravels the potential of 3D deep learning to advance the recognition performance on volumetric image segmentation.",
                        "Citation Paper Authors": "Authors:Hao Chen, Qi Dou, Lequan Yu, Pheng-Ann Heng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13226v3": {
            "Paper Title": "ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field",
            "Sentences": [
                {
                    "Sentence ID": 90,
                    "Sentence": "or calculating ray intersections with ex-\nplicit geometry, such as voxel grids [ 48,85,61], octrees ",
                    "Citation Text": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren\nNg, and Angjoo Kanazawa. Plenoctrees for real-time\nrendering of neural radiance fields. In ICCV , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14024",
                        "Citation Paper Title": "Title:PlenOctrees for Real-time Rendering of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "collects cli-\nmate images dataset and performs image editing with Cycle-\nGAN ",
                    "Citation Text": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and\nAlexei A Efros. Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks. In\nICCV , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10593",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                        "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "Climate simulation: The importance of making climate\nsimulations accessible is well known. ",
                    "Citation Text": "Victor Schmidt, Alexandra Luccioni, S Karthik\nMukkavilli, Narmada Balasooriya, Kris Sankaran,\nJennifer Chayes, and Yoshua Bengio. Visualizing\nthe consequences of climate change using cycle-\nconsistent adversarial networks. ICLR , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03709",
                        "Citation Paper Title": "Title:Visualizing the Consequences of Climate Change Using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We present a project that aims to generate images that depict accurate, vivid, and personalized outcomes of climate change using Cycle-Consistent Adversarial Networks (CycleGANs). By training our CycleGAN model on street-view images of houses before and after extreme weather events (e.g. floods, forest fires, etc.), we learn a mapping that can then be applied to images of locations that have not yet experienced these events. This visual transformation is paired with climate model predictions to assess likelihood and type of climate-related events in the long term (50 years) in order to bring the future closer in the viewers mind. The eventual goal of our project is to enable individuals to make more informed choices about their climate future by creating a more visceral understanding of the effects of climate change, while maintaining scientific credibility by drawing on climate model projections.",
                        "Citation Paper Authors": "Authors:Victor Schmidt, Alexandra Luccioni, S. Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer Chayes, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.00124v2": {
            "Paper Title": "Implicit Neural Spatial Representations for Time-dependent PDEs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00277v5": {
            "Paper Title": "MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient\n  Neural Field Rendering on Mobile Architectures",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ") consti-\ntutes an exciting avenue for future works.\nThe explicit mesh representation provided by MobileN-\neRF gives us direct editing control over the NeRF model\nwithout any complex architectural change (e.g. Control-\nNerf ",
                    "Citation Text": "Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey\nTulyakov, and Gerard Pons-Moll. Control-nerf: Editable fea-\nture volumes for scene rendering and manipulation. arXiv\npreprint arXiv:2204.10850 , 2022. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.10850",
                        "Citation Paper Title": "Title:Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation",
                        "Citation Paper Abstract": "Abstract:We present a novel method for performing flexible, 3D-aware image content manipulation while enabling high-quality novel view synthesis. While NeRF-based approaches are effective for novel view synthesis, such models memorize the radiance for every point in a scene within a neural network. Since these models are scene-specific and lack a 3D scene representation, classical editing such as shape manipulation, or combining scenes is not possible. Hence, editing and combining NeRF-based scenes has not been demonstrated. With the aim of obtaining interpretable and controllable scene representations, our model couples learnt scene-specific feature volumes with a scene agnostic neural rendering network. With this hybrid representation, we decouple neural rendering from scene-specific geometry and appearance. We can generalize to novel scenes by optimizing only the scene-specific 3D feature representation, while keeping the parameters of the rendering network fixed. The rendering function learnt during the initial training stage can thus be easily applied to new scenes, making our approach more flexible. More importantly, since the feature volumes are independent of the rendering model, we can manipulate and combine scenes by editing their corresponding feature volumes. The edited volume can then be plugged into the rendering model to synthesize high-quality novel views. We demonstrate various scene manipulations, including mixing scenes, deforming objects and inserting objects into scenes, while still producing photo-realistic results.",
                        "Citation Paper Authors": "Authors:Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". Recently, significant speed-ups have been\nachieved by decoding features fetched from a 3D embed-\nding with a small neural network. This embedding can ei-\nther be a dense voxel grid [23, 43], a sparse voxel grid ",
                    "Citation Text": "Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR , 2022. 2,\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05131",
                        "Citation Paper Title": "Title:Plenoxels: Radiance Fields without Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.",
                        "Citation Paper Authors": "Authors:Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "uses a\ndense voxel grid and represents view-dependence with a\nglobal spherical basis function. PlenOctrees ",
                    "Citation Text": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In ICCV , 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14024",
                        "Citation Paper Title": "Title:PlenOctrees for Real-time Rendering of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nThese 3D embeddings can also be used without a trained\ndecoder, for example by directly storing diffuse colors ",
                    "Citation Text": "Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nSIGGRAPH , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07751",
                        "Citation Paper Title": "Title:Neural Volumes: Learning Dynamic Renderable Volumes from Images",
                        "Citation Paper Abstract": "Abstract:Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",
                        "Citation Paper Authors": "Authors:Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "or by subdividing\nthe scene and modeling each sub-region with a smaller neu-\nral network ",
                    "Citation Text": "Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,\nKwang Moo Yi, and Andrea Tagliasacchi. DeRF: Decom-\nposed radiance fields. CVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12490",
                        "Citation Paper Title": "Title:DeRF: Decomposed Radiance Fields",
                        "Citation Paper Abstract": "Abstract:With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).",
                        "Citation Paper Authors": "Authors:Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "differentiably renders a tetrahedral grid with occupancy and\ncolor at each vertex, and then compositing the interpolated\nvalues at all intersected faces along a ray. NVDiffRec ",
                    "Citation Text": "Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M \u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In CVPR , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12503",
                        "Citation Paper Title": "Title:Extracting Triangular 3D Models, Materials, and Lighting From Images",
                        "Citation Paper Abstract": "Abstract:We present an efficient method for joint optimization of topology, materials and lighting from multi-view image observations. Unlike recent multi-view reconstruction approaches, which typically produce entangled 3D representations encoded in neural networks, we output triangle meshes with spatially-varying materials and environment lighting that can be deployed in any traditional graphics engine unmodified. We leverage recent work in differentiable rendering, coordinate-based networks to compactly represent volumetric texturing, alongside differentiable marching tetrahedrons to enable gradient-based optimization directly on the surface mesh. Finally, we introduce a differentiable formulation of the split sum approximation of environment lighting to efficiently recover all-frequency lighting. Experiments show our extracted models used in advanced scene editing, material decomposition, and high quality view interpolation, all running at interactive rates in triangle-based renderers (rasterizers and path tracers). Project website: this https URL .",
                        "Citation Paper Authors": "Authors:Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00fcller, Sanja Fidler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04048v3": {
            "Paper Title": "Executing your Commands via Motion Diffusion in Latent Space",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "on the 2D image la-\ntentzI, we build a transformer-based denoising model with\nlong skip connections ",
                    "Citation Text": "Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are\nworth words: a vit backbone for score-based diffusion mod-\nels.arXiv preprint arXiv:2209.12152 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.12152",
                        "Citation Paper Title": "Title:All are Worth Words: A ViT Backbone for Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets.",
                        "Citation Paper Authors": "Authors:Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": ". Inspired by their works, most recent\nmethods [69, 75, 28] leverage diffusion models for human\nmotion synthesis. MotionDiffuse ",
                    "Citation Text": "Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\nfuse: Text-driven human motion generation with diffusion\nmodel. arXiv preprint arXiv:2208.15001 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.15001",
                        "Citation Paper Title": "Title:MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model",
                        "Citation Paper Abstract": "Abstract:Human motion modeling is important for many modern graphics applications, which typically require professional skills. In order to remove the skill barriers for laymen, recent motion generation methods can directly generate human motions conditioned on natural languages. However, it remains challenging to achieve diverse and fine-grained motion generation with various text inputs. To address this problem, we propose MotionDiffuse, the first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods. 1) Probabilistic Mapping. Instead of a deterministic language-motion mapping, MotionDiffuse generates motions through a series of denoising steps in which variations are injected. 2) Realistic Synthesis. MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences. 3) Multi-Level Manipulation. MotionDiffuse responds to fine-grained instructions on body parts, and arbitrary-length motion synthesis with time-varied text prompts. Our experiments show MotionDiffuse outperforms existing SoTA methods by convincing margins on text-driven motion generation and action-conditioned motion generation. A qualitative analysis further demonstrates MotionDiffuse's controllability for comprehensive motion generation. Homepage: this https URL",
                        "Citation Paper Authors": "Authors:Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, Ziwei Liu"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "achieve signi\ufb01cant success in the image syn-\nthesis domain, such as Imagen ",
                    "Citation Text": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al. Photorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11487",
                        "Citation Paper Title": "Title:Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                        "Citation Paper Abstract": "Abstract:We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                        "Citation Paper Authors": "Authors:Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08422v2": {
            "Paper Title": "Real-time motion amplification on mobile devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04847v3": {
            "Paper Title": "NerfAcc: A General NeRF Acceleration Toolbox",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "( \u223cdays) 24.32 22.64 29.15 26.38 27.80 28.87 24.34 26.21\nMip-NeRF 360 ",
                    "Citation Text": "Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Un-\nbounded anti-aliased neural radiance \ufb01elds. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 5470\u20135479, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12077",
                        "Citation Paper Title": "Title:Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03293v2": {
            "Paper Title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ". We adopt a text-conditioned\nscore estimator. In detail, to fit the target conditions into\nthe 3D denoising autoencoder, we utilize the cross-attention\nmechanism as proposed in ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR , pages 10674\u2013\n10685, 2022. 2, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "decoder, while these approaches neither employ\nthe more flexible implicit SDFs as data representation nor\ntake into account the constraints for local shape structures.\nMore similar to our work, AutoSDF ",
                    "Citation Text": "Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-\nham Tulsiani. Autosdf: Shape priors for 3D completion, re-\nconstruction and generation. In CVPR , pages 306\u2013315, 2022.\n1, 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09516",
                        "Citation Paper Title": "Title:AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation",
                        "Citation Paper Abstract": "Abstract:Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the effectiveness of the proposed method using both quantitative and qualitative evaluation and show that the proposed method outperforms the specialized state-of-the-art methods trained for individual tasks. The project page with code and video visualizations can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, Shubham Tulsiani"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "sampler to\nreduce the original DDPM sampling steps from T= 1000\nto50. For the conditioning mechanism, we adopt a freeze\nCLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML , pages\n8748\u20138763, 2021. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nT2S gathered data in the form of shape-text pairs based on\ntwo object classes (chairs, tables) in ShapeNet ",
                    "Citation Text": "Angel X. Chang, Thomas A. Funkhouser, Leonidas J.\nGuibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio\nSavarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong\nXiao, Li Yi, and Fisher Yu. ShapeNet: An information-rich\n3D model repository. abs/1512.03012, 2015. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "structure to learn indepen-\ndent patch-focused information. We also introduce the spa-\ntial Transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , pages\n5998\u20136008, 2017. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "treated points\nin point clouds as particles in a thermodynamic system with\na heat bath. LION ",
                    "Citation Text": "Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis. LION: latent\npoint diffusion models for 3D shape generation. 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.06978",
                        "Citation Paper Title": "Title:LION: Latent Point Diffusion Models for 3D Shape Generation",
                        "Citation Paper Abstract": "Abstract:Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text- and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction. Project page and code: this https URL.",
                        "Citation Paper Authors": "Authors:Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", the diffusion model demon-\nstrates its superiority in both training stability and genera-\ntive diversity ",
                    "Citation Text": "Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\n2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.04747",
                        "Citation Paper Title": "Title:Diffusion Models in Vision: A Survey",
                        "Citation Paper Abstract": "Abstract:Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.",
                        "Citation Paper Authors": "Authors:Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.17256v2": {
            "Paper Title": "CLIPascene: Scene Sketching with Different Types and Levels of\n  Abstraction",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "CLIP model is able to capture the global context required\nfor generating a coherent sketch of a whole scene, includ-\ning both foreground and background. This also follows\nthe observation of Raghu et al. ",
                    "Citation Text": "Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks? Advances\nin Neural Information Processing Systems , 34:12116\u201312128,\n2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08810",
                        "Citation Paper Title": "Title:Do Vision Transformers See Like Convolutional Neural Networks?",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.",
                        "Citation Paper Authors": "Authors:Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, Alexey Dosovitskiy"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", in which, strokes are initialized in salient regions\nbased on a relevancy map extracted automatically ",
                    "Citation Text": "Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-\nmodel explainability for interpreting bi-modal and encoder-\ndecoder transformers. 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 387\u2013396, 2021.\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15679",
                        "Citation Paper Title": "Title:Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
                        "Citation Paper Abstract": "Abstract:Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",
                        "Citation Paper Authors": "Authors:Hila Chefer, Shir Gur, Lior Wolf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13219v2": {
            "Paper Title": "Automating Rigid Origami Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09395v2": {
            "Paper Title": "From river flow to spatial flow: flow map via river flow directions\n  assignment algorithm",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "presented a priority -flood algorithm that processe d \ndepressions from the edge grid cells to the interior cells. \nThese strategies help reduce the computation complexity. \nBarnes et al. ",
                    "Citation Text": "R. Barnes, C. Lehman  and D.  Mulla. Priority -flood: an optimal de-\npression -filling and watershe d-labeling algorithm for digital ele-\nvation models. Computers & Geosciences . 62, 117 \u2013127, 2014 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04463",
                        "Citation Paper Title": "Title:Priority-Flood: An Optimal Depression-Filling and Watershed-Labeling Algorithm for Digital Elevation Models",
                        "Citation Paper Abstract": "Abstract:Depressions (or pits) are low areas within a digital elevation model that are surrounded by higher terrain, with no outlet to lower areas. Filling them so they are level, as fluid would fill them if the terrain were impermeable, is often necessary in preprocessing DEMs. The depression-filling algorithm presented here---called Priority-Flood---unifies and improves on the work of a number of previous authors who have published similar algorithms. The algorithm operates by flooding DEMs inwards from their edges using a priority queue to determine the next cell to be flooded. The resultant DEM has no depressions or digital dams: every cell is guaranteed to drain. The algorithm is optimal for both integer and floating-point data, working in O(n) and O(n lg n) time, respectively. It is shown that by using a plain queue to fill depressions once they have been found, an O(m lg m) time-complexity can be achieved, where m does not exceed the number of cells n. This is the lowest time complexity of any known floating-point depression-filling algorithm. In testing, this improved variation of the algorithm performed up to 37% faster than the original. Additionally, a parallel version of an older, but widely-used depression-filling algorithm required six parallel processors to achieve a run-time on par with what the newer algorithm's improved variation took on a single processor. The Priority-Flood Algorithm is simple to understand and implement: the included pseudocode is only 20 lines and the included C++ reference implementation is under a hundred lines. The algorithm can work on irregular meshes as well as 4-, 6-, 8-, and n-connected grids. It can also be adapted to label watersheds and determine flow directions through either incremental elevation changes or depression carving. In the case of incremental elevation changes, the algorithm includes safety checks not present in prior works.",
                        "Citation Paper Authors": "Authors:Richard Barnes, Clarence Lehman, David Mulla"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.07718v3": {
            "Paper Title": "HUMAP: Hierarchical Uniform Manifold Approximation and Projection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09898v3": {
            "Paper Title": "Text2Light: Zero-Shot Text-Driven HDR Panorama Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.15511v2": {
            "Paper Title": "Sphere-Guided Training of Neural Implicit Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13634v2": {
            "Paper Title": "Vitruvio: 3D Building Meshes via Single Perspective Sketches",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ". The possibility of en-\nhancing human capabilities has inspired this research, and\nwe hope this research direction could facilitate the design of\nmore sustainable buildings. Finally, while employing deep\ngenerative models such as diffusion models ",
                    "Citation Text": "Gimin Nam, Mariem Khli\ufb01, Andrew Rodriguez, Alberto\nTono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural\nimplicit 3d shape generation with latent diffusion models.\narXiv pre-print , 2022. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.00842",
                        "Citation Paper Title": "Title:3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Diffusion models have shown great promise for image generation, beating GANs in terms of generation diversity, with comparable image quality. However, their application to 3D shapes has been limited to point or voxel representations that can in practice not accurately represent a 3D surface. We propose a diffusion model for neural implicit representations of 3D shapes that operates in the latent space of an auto-decoder. This allows us to generate diverse and high quality 3D surfaces. We additionally show that we can condition our model on images or text to enable image-to-3D generation and text-to-3D generation using CLIP embeddings. Furthermore, adding noise to the latent codes of existing shapes allows us to explore shape variations.",
                        "Citation Paper Authors": "Authors:Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, Paul Guerrero"
                    }
                },
                {
                    "Sentence ID": 126,
                    "Sentence": ", or a series of procedures\n[40,55,56,69,70,87], these simpli\ufb01ed CAD interfaces , pro-\nvide complete control of the 3D geometry, at the expense of\nthe artistic style. Thus, the models developed have been\nview, and style-dependent ",
                    "Citation Text": "Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, and Yi-\nZhe Song. Deep sketch-based modeling: Tips and tricks.\n2020 International Conference on 3D Vision (3DV) , 2020.\n2, 6, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.06133",
                        "Citation Paper Title": "Title:Deep Sketch-Based Modeling: Tips and Tricks",
                        "Citation Paper Abstract": "Abstract:Deep image-based modeling received lots of attention in recent years, yet the parallel problem of sketch-based modeling has only been briefly studied, often as a potential application. In this work, for the first time, we identify the main differences between sketch and image inputs: (i) style variance, (ii) imprecise perspective, and (iii) sparsity. We discuss why each of these differences can pose a challenge, and even make a certain class of image-based methods inapplicable. We study alternative solutions to address each of the difference. By doing so, we drive out a few important insights: (i) sparsity commonly results in an incorrect prediction of foreground versus background, (ii) diversity of human styles, if not taken into account, can lead to very poor generalization properties, and finally (iii) unless a dedicated sketching interface is used, one can not expect sketches to match a perspective of a fixed viewpoint. Finally, we compare a set of representative deep single-image modeling solutions and show how their performance can be improved to tackle sketch input by taking into consideration the identified critical differences.",
                        "Citation Paper Authors": "Authors:Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, Yi-Zhe Song"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ",\nwhich performs TSDF-fusion on random depth renderings\nof the object to create watertight versions of the meshes.\nWe centered and re-scaled all meshes for voxelizations from\n3DR2-N2 ",
                    "Citation Text": "Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A uni\ufb01ed approach\nfor single and multi-view 3d object reconstruction. Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , 2016. 3, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.00449",
                        "Citation Paper Title": "Title:3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction",
                        "Citation Paper Abstract": "Abstract:Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).",
                        "Citation Paper Authors": "Authors:Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "method\nthat better aligns with our desiderata, as previously de-scribed. Deep generative models, Variational Auto-Encoder\n(V AE) [33, 52, 82, 125] , Auto-Encoder (AE), Generative\nAdversarial Network (GAN) [16, 114], Flow-based ",
                    "Citation Text": "Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer,\nand Vittorio Ferrari. C-\ufb02ow: Conditional generative \ufb02ow\nmodels for images and 3d point clouds. Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , abs/1912.07009, 2019. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.07009",
                        "Citation Paper Title": "Title:C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:Flow-based generative models have highly desirable properties like exact log-likelihood evaluation and exact latent-variable inference, however they are still in their infancy and have not received as much attention as alternative generative models. In this paper, we introduce C-Flow, a novel conditioning scheme that brings normalizing flows to an entirely new scenario with great possibilities for multi-modal data modeling. C-Flow is based on a parallel sequence of invertible mappings in which a source flow guides the target flow at every step, enabling fine-grained control over the generation process. We also devise a new strategy to model unordered 3D point clouds that, in combination with the conditioning scheme, makes it possible to address 3D reconstruction from a single image and its inverse problem of rendering an image given a point cloud. We demonstrate our conditioning method to be very adaptable, being also applicable to image manipulation, style transfer and multi-modal image-to-image mapping in a diversity of domains, including RGB images, segmentation maps, and edge masks.",
                        "Citation Paper Authors": "Authors:Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, Vittorio Ferrari"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": ", constructive solid geome-\ntries (CSG) [55, 56] coons patch-based ",
                    "Citation Text": "Dmitriy Smirnov, Mikhail Bessmeltsev, and Justin\nSolomon. Learning manifold patch-based representations\nof man-made shapes. International Conference on Learn-\ning Representations (ICLR) , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.12337",
                        "Citation Paper Title": "Title:Learning Manifold Patch-Based Representations of Man-Made Shapes",
                        "Citation Paper Abstract": "Abstract:Choosing the right representation for geometry is crucial for making 3D models compatible with existing applications. Focusing on piecewise-smooth man-made shapes, we propose a new representation that is usable in conventional CAD modeling pipelines and can also be learned by deep neural networks. We demonstrate its benefits by applying it to the task of sketch-based modeling. Given a raster image, our system infers a set of parametric surfaces that realize the input in 3D. To capture piecewise smooth geometry, we learn a special shape representation: a deformable parametric template composed of Coons patches. Naively training such a system, however, is hampered by non-manifold artifacts in the parametric shapes and by a lack of data. To address this, we introduce loss functions that bias the network to output non-self-intersecting shapes and implement them as part of a fully self-supervised system, automatically generating both shape templates and synthetic training data. We develop a testbed for sketch-based modeling, demonstrate shape interpolation, and provide comparison to related work.",
                        "Citation Paper Authors": "Authors:Dmitriy Smirnov, Mikhail Bessmeltsev, Justin Solomon"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12503v5": {
            "Paper Title": "Extracting Triangular 3D Models, Materials, and Lighting From Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11232v2": {
            "Paper Title": "Neural Groundplans: Persistent Neural Scene Representations from a\n  Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08398v3": {
            "Paper Title": "SPIDR: SDF-based Neural Point Fields for Illumination and Deformation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". Since our environment light\nmodel represents an HDR image, we adopt an exponential\nactivation function ",
                    "Citation Text": "Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\nPratul P Srinivasan, and Jonathan T Barron. Nerf in the dark:\nHigh dynamic range view synthesis from noisy raw images.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 16190\u201316199, 2022.\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.13679",
                        "Citation Paper Title": "Title:NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ") to predict the corre-\nsponding light intensity E(^!i). This parameterization en-\nables SPIDR to learn more lighting details while maintain-\ning spatial continuity ",
                    "Citation Text": "Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. Advances in Neural Information Processing\nSystems , 33:7537\u20137547, 2020. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10739",
                        "Citation Paper Title": "Title:Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
                        "Citation Paper Abstract": "Abstract:We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "Neural rendering and scene representation. Neural\nrendering is a class of reconstruction and rendering ap-\nproaches that use deep networks to learn complex map-\npings from captured images to novel images ",
                    "Citation Text": "Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,\nStephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-\nBrualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner,\net al. State of the art on neural rendering. In Computer\nGraphics Forum , volume 39, pages 701\u2013727. Wiley Online\nLibrary, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03805",
                        "Citation Paper Title": "Title:State of the Art on Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.12761v3": {
            "Paper Title": "BANMo: Building Animatable 3D Neural Models from Many Casual Videos",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": ".\nSpecifically, we train a separate network PoseNet , which\nis applied to every test video frame. Similar to DenseRaC ",
                    "Citation Text": "Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Denserac: Joint\n3d pose and shape estimation by dense render-and-compare.\nInICCV , 2019. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.00116",
                        "Citation Paper Title": "Title:DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare",
                        "Citation Paper Abstract": "Abstract:We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various humanrelated tasks.",
                        "Citation Paper Authors": "Authors:Yuanlu Xu, Song-Chun Zhu, Tony Tung"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", and we learn sep-\narate weight matrices Ai\u2208{1...,M}for each video. The\nframe index tis normalized by the maximum video length\nmaxM\ni=1|Ti|. Using the temporal Fourier basis stabilizes the\noptimization and produces more smooth deformations.Skinning weights. Similar to SCANimate ",
                    "Citation Text": "Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.\nBlack. SCANimate: Weakly supervised learning of skinned\nclothed avatar networks. In CVPR , 2021. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03313",
                        "Citation Paper Title": "Title:SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks",
                        "Citation Paper Abstract": "Abstract:We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.17260v2": {
            "Paper Title": "SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ". While numerous 3D-GAN methods were proposed,\nwe could not find any other suitable baseline that demon-\nstrated the modeling of apartment-scale scenes, such as\nthe ones from the Replica dataset. Concurrently to our\nwork, GAUDI ",
                    "Citation Text": "Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin\nDehghan, and Josh Susskind. GAUDI: A neural architect for\nimmersive 3D scene generation. arXiv , 2022. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.13751",
                        "Citation Paper Title": "Title:GAUDI: A Neural Architect for Immersive 3D Scene Generation",
                        "Citation Paper Abstract": "Abstract:We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera. We tackle this challenging problem with a scalable yet powerful approach, where we first optimize a latent representation that disentangles radiance fields and camera poses. This latent representation is then used to learn a generative model that enables both unconditional and conditional generation of 3D scenes. Our model generalizes previous works that focus on single objects by removing the assumption that the camera pose distribution can be shared across samples. We show that GAUDI obtains state-of-the-art performance in the unconditional generative setting across multiple datasets and allows for conditional generation of 3D scenes given conditioning variables like sparse image observations or text that describes the scene.",
                        "Citation Paper Authors": "Authors:Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, Josh Susskind"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", we find it useful to adopt two regularizationlosses: R1 gradient penalty ",
                    "Citation Text": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\nWhich training methods for GANs do actually converge? In\nInternational conference on machine learning , pages 3481\u2013\n3490. PMLR, 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.04406",
                        "Citation Paper Title": "Title:Which Training Methods for GANs do actually Converge?",
                        "Citation Paper Abstract": "Abstract:Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Andreas Geiger, Sebastian Nowozin"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ", 3D textures [17, 39], or 3D shapes [18, 52].\nHowever, none of these works applied 3D generative mod-\nels from single-scene images. Concurrently to our work, ",
                    "Citation Text": "Yujie Wang, Xuelin Chen, and Baoquan Chen. SinGRA V:\nLearning a generative radiance volume from a single natural\nscene. arXiv preprint arXiv:2210.01202 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.01202",
                        "Citation Paper Title": "Title:SinGRAV: Learning a Generative Radiance Volume from a Single Natural Scene",
                        "Citation Paper Abstract": "Abstract:We present a 3D generative model for general natural scenes. Lacking necessary volumes of 3D data characterizing the target scene, we propose to learn from a single scene. Our key insight is that a natural scene often contains multiple constituents whose geometry, texture, and spatial arrangements follow some clear patterns, but still exhibit rich variations over different regions within the same scene. This suggests localizing the learning of a generative model on substantial local regions. Hence, we exploit a multi-scale convolutional network, which possesses the spatial locality bias in nature, to learn from the statistics of local regions at multiple scales within a single scene. In contrast to existing methods, our learning setup bypasses the need to collect data from many homogeneous 3D scenes for learning common features. We coin our method SinGRAV, for learning a Generative RAdiance Volume from a Single natural scene. We demonstrate the ability of SinGRAV in generating plausible and diverse variations from a single scene, the merits of SinGRAV over state-of-the-art generative neural scene methods, as well as the versatility of SinGRAV by its use in a variety of applications, spanning 3D scene editing, composition, and animation. Code and data will be released to facilitate further research.",
                        "Citation Paper Authors": "Authors:Yujie Wang, Xuelin Chen, Baoquan Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.12886v3": {
            "Paper Title": "OReX: Object Reconstruction from Planar Cross-sections Using Neural\n  Fields",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ",\nwithout additional information such as normals; see a re-\ncent survey for more approaches ",
                    "Citation Text": "Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\nkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in\nvisual computing and beyond. In Computer Graphics Forum ,\nvolume 41, pages 641\u2013676. Wiley Online Library, 2022. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Other\nneural fields have been employed as well in this context ",
                    "Citation Text": "Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias\nZwicker. Neural-pull: Learning signed distance function\nfrom point clouds by learning to pull space onto surface. In\nInternational Conference on Machine Learning , pages 7246\u2013\n7257. PMLR, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13495",
                        "Citation Paper Title": "Title:Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces",
                        "Citation Paper Abstract": "Abstract:Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D geometry processing. Several recent state-of-the-art methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce \\textit{Neural-Pull}, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest points on the surface using the predicted signed distance values and the gradient at the query locations, both of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Baorui Ma, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.13489v2": {
            "Paper Title": "Measuring Perceptual Color Differences of Smartphone Photographs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11417v2": {
            "Paper Title": "DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular\n  Automata",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "use a\npre-trained optical \ufb02ow network for extracting the motion\nfeatures, which allows them to disentangle the appearance\nand motion aspects of video textures. Xie et al. ",
                    "Citation Text": "Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthe-\nsizing dynamic patterns by spatial-temporal generative con-\nvnet. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 7093\u20137101, 2017. 1,\n2, 3, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.00972",
                        "Citation Paper Title": "Title:Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet",
                        "Citation Paper Abstract": "Abstract:Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an \"analysis by synthesis\" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.",
                        "Citation Paper Authors": "Authors:Jianwen Xie, Song-Chun Zhu, Ying Nian Wu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "extend Gatys\u2019s idea to dynamic tex-\nture synthesis, and use a cross-frame Gram matrix of VGG\nfeatures to capture the temporal characteristics of dynamic\ntextures. Tesfaldet et al. ",
                    "Citation Text": "Matthew Tesfaldet, Marcus A Brubaker, and Konstantinos G\nDerpanis. Two-stream convolutional networks for dynamic\ntexture synthesis. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 6703\u2013\n6712, 2018. 1, 2, 3, 5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06982",
                        "Citation Paper Title": "Title:Two-Stream Convolutional Networks for Dynamic Texture Synthesis",
                        "Citation Paper Abstract": "Abstract:We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.",
                        "Citation Paper Authors": "Authors:Matthew Tesfaldet, Marcus A. Brubaker, Konstantinos G. Derpanis"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "capture the perceptual qualities of texture im-\nages. Funke et al. ",
                    "Citation Text": "Christina M Funke, Leon A Gatys, Alexander S Ecker,\nand Matthias Bethge. Synthesising dynamic textures\nusing convolutional neural networks. arXiv preprint\narXiv:1702.07006 , 2017. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.07006",
                        "Citation Paper Title": "Title:Synthesising Dynamic Textures using Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Here we present a parametric model for dynamic textures. The model is based on spatiotemporal summary statistics computed from the feature representations of a Convolutional Neural Network (CNN) trained on object recognition. We demonstrate how the model can be used to synthesise new samples of dynamic textures and to predict motion in simple movies.",
                        "Citation Paper Authors": "Authors:Christina M. Funke, Leon A. Gatys, Alexander S. Ecker, Matthias Bethge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.17263v2": {
            "Paper Title": "Plateau-reduced Differentiable Path Tracing",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ", while Eq. 9 can be interpreted as reparametrization\ngradient [15, 36]. Suh et al. ",
                    "Citation Text": "Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ\nTedrake. Do differentiable simulators give better policy gra-\ndients? In International Conference on Machine Learning ,\npages 20668\u201320696. PMLR, 2022. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.00817",
                        "Citation Paper Title": "Title:Do Differentiable Simulators Give Better Policy Gradients?",
                        "Citation Paper Abstract": "Abstract:Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an $\\alpha$-order gradient estimator, with $\\alpha \\in [0,1]$, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the $\\alpha$-order estimator on some numerical examples.",
                        "Citation Paper Authors": "Authors:H.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". Finally, some work focuses on differentiable\nrendering of special primitives, such as points [11, 48],\nspheres ",
                    "Citation Text": "Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc\nPollefeys, and Zhaopeng Cui. Dist: Rendering deep implicit\nsigned distance function with differentiable sphere tracing.\nInProc. CVPR , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.13225",
                        "Citation Paper Title": "Title:DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing",
                        "Citation Paper Abstract": "Abstract:We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.",
                        "Citation Paper Authors": "Authors:Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "approximate the spatial gradients dur-\ning the backward pass by \ufb01nite differences. Early, Rhodin\net al. ",
                    "Citation Text": "Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-\nPeter Seidel, and Christian Theobalt. A versatile scene model\nwith differentiable visibility applied to generative pose esti-\nmation. In Proc. ICCV , 2015. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.03725",
                        "Citation Paper Title": "Title:A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Generative reconstruction methods compute the 3D configuration (such as pose and/or geometry) of a shape by optimizing the overlap of the projected 3D shape model with images. Proper handling of occlusions is a big challenge, since the visibility function that indicates if a surface point is seen from a camera can often not be formulated in closed form, and is in general discrete and non-differentiable at occlusion boundaries. We present a new scene representation that enables an analytically differentiable closed-form formulation of surface visibility. In contrast to previous methods, this yields smooth, analytically differentiable, and efficient to optimize pose similarity energies with rigorous occlusion handling, fewer local minima, and experimentally verified improved convergence of numerical optimization. The underlying idea is a new image formation model that represents opaque objects by a translucent medium with a smooth Gaussian density distribution which turns visibility into a smooth phenomenon. We demonstrate the advantages of our versatile scene model in several generative pose estimation problems, namely marker-less multi-object pose estimation, marker-less human motion capture with few cameras, and image-based 3D geometry estimation.",
                        "Citation Paper Authors": "Authors:Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.03192v2": {
            "Paper Title": "Integration-free Learning of Flow Maps",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". Each\ngrid stores 8-dimensional feature vectors at its nodes, and\nthus the resulting concatenated feature is 32-dimensional.\nWe employ 2 and 1-layer MLPs for f\u0017andf\u001c, respectively,\nalong with activation \u001b\u001cchosen to be a Swish activation ",
                    "Citation Text": "S. Hayou, A. Doucet, and J. Rousseau, \u201cOn the selection of\ninitialization and activation function for deep neural networks,\u201d\narXiv preprint arXiv:1805.08266 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08266",
                        "Citation Paper Title": "Title:On the Selection of Initialization and Activation Function for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:The weight initialization and the activation function of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of information of the input during forward propagation and the exponential vanishing/exploding of gradients during back-propagation. Understanding the theoretical properties of untrained random networks is key to identifying which deep networks may be trained successfully as recently demonstrated by Schoenholz et al. (2017) who showed that for deep feedforward neural networks only a specific choice of hyperparameters known as the `edge of chaos' can lead to good performance. We complete this analysis by providing quantitative results showing that, for a class of ReLU-like activation functions, the information propagates indeed deeper for an initialization at the edge of chaos. By further extending this analysis, we identify a class of activation functions that improve the information propagation over ReLU-like functions. This class includes the Swish activation, $\\phi_{swish}(x) = x \\cdot \\text{sigmoid}(x)$, used in Hendrycks & Gimpel (2016), Elfwing et al. (2017) and Ramachandran et al. (2017). This provides a theoretical grounding for the excellent empirical performance of $\\phi_{swish}$ observed in these contributions. We complement those previous results by illustrating the benefit of using a random initialization on the edge of chaos in this context.",
                        "Citation Paper Authors": "Authors:Soufiane Hayou, Arnaud Doucet, Judith Rousseau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10275v2": {
            "Paper Title": "ARO-Net: Learning Implicit Fields from Anchored Radial Observations",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Other works [10, 11, 25], like ARO-Net,\nalso utilize query-speci\ufb01c shape features.\nImplicit \ufb01eld from query-speci\ufb01c features. For occu-\npancy prediction at a query point p, ConvONet ",
                    "Citation Text": "Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger. Convolutional occupancy\nnetworks. In Eur. Conf. Comput. Vis. , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.04618",
                        "Citation Paper Title": "Title:Convolutional Occupancy Networks",
                        "Citation Paper Abstract": "Abstract:Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",
                        "Citation Paper Authors": "Authors:Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". ARO-Net receives\n2,048 points as input when training, and can handle arbi-\ntrary point numbers when testing. We tested 3 sets of point\nnumbers n=f512;1024;2048g.\nDataset. Our experiments are conducted on the \u201cBig CAD\ndataset\", ABC ",
                    "Citation Text": "Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis\nWilliams, Alexey Artemov, Evgeny Burnaev, Marc Alexa,\nDenis Zorin, and Daniele Panozzo. Abc: A big cad model\ndataset for geometric deep learning. In IEEE Conf. Comput.\nVis. Pattern Recog. , pages 9601\u20139611, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.06216",
                        "Citation Paper Title": "Title:ABC: A Big CAD Model Dataset For Geometric Deep Learning",
                        "Citation Paper Abstract": "Abstract:We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.",
                        "Citation Paper Authors": "Authors:Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "extracts\nconvolutional features at pby interpolating over a feature\n\ufb01eld de\ufb01ned for an input shape. IF-Net ",
                    "Citation Text": "Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.\nImplicit functions in feature space for 3D shape reconstruction\nand completion. In IEEE Conf. Comput. Vis. Pattern Recog. ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01456",
                        "Citation Paper Title": "Title:Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion",
                        "Citation Paper Abstract": "Abstract:While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",
                        "Citation Paper Authors": "Authors:Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "from training\nshapes, where the learned priors are parameterized in dif-\nferent ways, e.g., voxel-level latent codes in Deep Local\nShape ",
                    "Citation Text": "Rohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt,\nJulian Straub, Steven Lovegrove, and Richard Newcombe.\nDeep local shapes: Learning local sdf priors for detailed 3D\nreconstruction. In Eur. Conf. Comput. Vis. , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10983",
                        "Citation Paper Title": "Title:Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction",
                        "Citation Paper Abstract": "Abstract:Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.",
                        "Citation Paper Authors": "Authors:Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.12285v2": {
            "Paper Title": "Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for\n  Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "Already numerous work has focused on improving NeRF\nsince its original inception ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. Communications of the ACM , 65(1):99\u2013106, 2021. 1,\n2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "uses a\ncontracted space representation to extend the mip-NeRF pa-\nrameterization to 360\u00b0 unbounded scenes, since they found\nthat the approximation given in mip-NeRF degrades for\nelongated frustums which arise in the background. Addi-\ntionally, and similar to DONeRF ",
                    "Citation Text": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum , 40(4), 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03231",
                        "Citation Paper Title": "Title:DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks",
                        "Citation Paper Abstract": "Abstract:The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",
                        "Citation Paper Authors": "Authors:Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", such as decreasing\nthe training time [5, 6, 9, 13, 37], increasing the synthesis\nspeed [11,31,38], reducing the number of input images ",
                    "Citation Text": "Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance \ufb01elds for view syn-\nthesis from sparse inputs. In Proceedings of the IEEE Conf.\nComput. Vis. Pattern Recog. , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.00724",
                        "Citation Paper Title": "Title:RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.10440v2": {
            "Paper Title": "Magic3D: High-Resolution Text-to-3D Content Creation",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ", which allows one to care-\nfully weigh the strength of the text conditioning (see Sec. 6).\nDreamFusion relies on large classifier-free guidance weights\nto obtain results with better quality.\nDreamFusion adopts a variant of Mip-NeRF 360 ",
                    "Citation Text": "Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5470\u20135479, 2022. 4, 5, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12077",
                        "Citation Paper Title": "Title:Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "showcased impressive ca-\npability in text-to-3D synthesis by utilizing a powerful pre-\ntrained text-to-image diffusion model ",
                    "Citation Text": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. arXiv preprint arXiv:2205.11487 ,\n2022. 1, 3, 4, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11487",
                        "Citation Paper Title": "Title:Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                        "Citation Paper Abstract": "Abstract:We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                        "Citation Paper Authors": "Authors:Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "in this section.\n4.1. Coarse-to-fine Diffusion Priors\nMagic3D uses two different diffusion priors in a coarse-\nto-fine fashion to generate high-resolution geometry and\ntextures. In the first stage, we use the base diffusion model\ndescribed in eDiff-I ",
                    "Citation Text": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, et al. ediff-i: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324 , 2022. 1, 3, 4, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.01324",
                        "Citation Paper Title": "Title:eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
                        "Citation Paper Abstract": "Abstract:Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at this https URL",
                        "Citation Paper Authors": "Authors:Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "mitigate the training data issue by relying on a\npre-trained image-text model ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nInInternational Conference on Machine Learning , pages\n8748\u20138763. PMLR, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "synthe-\nsizes objects by learning a normalizing flow model to sample\nshape embeddings from textual input. However, it requires\n3D assets in voxel representations during training, making it\nchallenging to scale with data. DreamField ",
                    "Citation Text": "Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01455",
                        "Citation Paper Title": "Title:Zero-Shot Text-Guided Object Generation with Dream Fields",
                        "Citation Paper Abstract": "Abstract:We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve fidelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.",
                        "Citation Paper Authors": "Authors:Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "representations. Most of these approaches rely\non training data in the form of 3D assets, which are hard to\nacquire at scale. Inspired by the success of neural volume\nrendering ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view synthe-\nsis. In ECCV , 2020. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.06344v3": {
            "Paper Title": "DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15664v2": {
            "Paper Title": "State of the Art in Dense Monocular Non-Rigid 3D Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.14306v2": {
            "Paper Title": "RUST: Latent Neural Scene Representations from Unposed Imagery",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "and\neven volumetric models, especially when given few input\nimages ",
                    "Citation Text": "Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nNeRF: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In CVPR , 2022. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.00724",
                        "Citation Paper Title": "Title:RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", even special-ized NeRF variants for sparse input view settings ",
                    "Citation Text": "Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF\non a Diet: Semantically Consistent Few-Shot View Synthesis.\nInICCV , 2021. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00677",
                        "Citation Paper Title": "Title:Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360\u00b0 scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",
                        "Citation Paper Authors": "Authors:Ajay Jain, Matthew Tancik, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Neural rendering. The field of neural rendering is vast\nand the introduction of Neural Radiance Fields (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan\nBarron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing\nScenes as Neural Radiance Fields for View Synthesis. In\nECCV , 2020. 1, 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "which adds per-image latent representations\nto compute a global scene representation that is used by a\nrecurrent latent variable model for novel view synthesis. A\nrecent extension ",
                    "Citation Text": "Dan Rosenbaum, Frederic Besse, Fabio Viola,\nDanilo Jimenez Rezende, and S M Ali Eslami. Learning\nmodels for visual 3d localization with implicit mapping.\nNeurIPS workshop on Bayesian Deep Learning , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03149",
                        "Citation Paper Title": "Title:Learning models for visual 3D localization with implicit mapping",
                        "Citation Paper Abstract": "Abstract:We consider learning based methods for visual localization that do not require the construction of explicit maps in the form of point clouds or voxels. The goal is to learn an implicit representation of the environment at a higher, more abstract level. We propose to use a generative approach based on Generative Query Networks (GQNs, Eslami et al. 2018), asking the following questions: 1) Can GQN capture more complex scenes than those it was originally demonstrated on? 2) Can GQN be used for localization in those scenes? To study this approach we consider procedurally generated Minecraft worlds, for which we can generate images of complex 3D scenes along with camera pose coordinates. We first show that GQNs, enhanced with a novel attention mechanism can capture the structure of 3D scenes in Minecraft, as evidenced by their samples. We then apply the models to the localization problem, comparing the results to a discriminative baseline, and comparing the ways each approach captures the task uncertainty.",
                        "Citation Paper Authors": "Authors:Dan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "extends this to settings where the\nprior distribution is unknown.\nAll methods above require a comparably large number of\nimages of the same scene, since NeRF tends to fail with few\nobservations even with perfect pose ",
                    "Citation Text": "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural Radiance Fields from One or Few Images.\nInCVPR , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02190",
                        "Citation Paper Title": "Title:pixelNeRF: Neural Radiance Fields from One or Few Images",
                        "Citation Paper Abstract": "Abstract:We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "jointly\noptimizes the NeRF MLP along with the camera poses for\nthe images. BARF ",
                    "Citation Text": "Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\nLucey. BARF: Bundle-Adjusting Neural Radiance Fields. In\nICCV , 2021. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06405",
                        "Citation Paper Title": "Title:BARF: Bundle-Adjusting Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\u00efvely applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.",
                        "Citation Paper Authors": "Authors:Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "uses pre-trained NeRFs to\nestimate the camera pose of novel views of the same scene.\nHowever, posed imagery is required for each scene in order\nto obtain the original NeRF models. NeRF -- ",
                    "Citation Text": "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\ntor Adrian Prisacariu. NeRF --: Neural radiance fields without\nknown camera parameters. arXiv preprint arXiv:2102.07064 ,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07064",
                        "Citation Paper Title": "Title:NeRF--: Neural Radiance Fields Without Known Camera Parameters",
                        "Citation Paper Abstract": "Abstract:Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (NeRF) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose NeRF$--$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with NeRF training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.",
                        "Citation Paper Authors": "Authors:Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.10951v3": {
            "Paper Title": "Paint by Word",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "to jointly generate natural\ntext and image tokens using a VQ-V AE encoding ",
                    "Citation Text": "Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-\ning diverse high-\ufb01delity images with VQ-V AE-2. In NeurIPS ,\n2019. 1, 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00446",
                        "Citation Paper Title": "Title:Generating Diverse High-Fidelity Images with VQ-VAE-2",
                        "Citation Paper Abstract": "Abstract:We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
                        "Citation Paper Authors": "Authors:Ali Razavi, Aaron van den Oord, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "; out-\nputs are sampled to maximize semantic consistency via a\nstate-of-the-art image-text matching model CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\narXiv preprint arXiv:2103.00020 , 2021. 1, 2, 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "model\ngenerates images from text autoregressively using a trans-\nformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Neural Information\nProcessing Systems , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "can allow a generator to\nexploit logical structure explicitly. Recent work stands apart\nby eschewing the use of GANs: the DALL-E ",
                    "Citation Text": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. arXiv preprint\narXiv:2102.12092 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12092",
                        "Citation Paper Title": "Title:Zero-Shot Text-to-Image Generation",
                        "Citation Paper Abstract": "Abstract:Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "; this idea can be re-\n\ufb01ned by viewing the sampling as an energy-based modeling\nprocedure ",
                    "Citation Text": "Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovit-\nskiy, and Jason Yosinski. Plug & play generative networks:\nConditional iterative generation of images in latent space. In\nCVPR , pages 4467\u20134477, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00005",
                        "Citation Paper Title": "Title:Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space",
                        "Citation Paper Abstract": "Abstract:Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models \"Plug and Play Generative Networks\". PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable \"condition\" network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization, which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.",
                        "Citation Paper Authors": "Authors:Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.07422v2": {
            "Paper Title": "ECON: Explicit Clothed humans Optimized via Normal integration",
            "Sentences": [
                {
                    "Sentence ID": 82,
                    "Sentence": "use SMPL to unpose the pixel-aligned query\npoints from a posed space to a canonical space. To further\ngeneralize to unseen poses on in-the-wild photos, ICON ",
                    "Citation Text": "Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and\nMichael J. Black. ICON: Implicit Clothed humans Obtained\nfrom Normals. In Computer Vision and Pattern Recognition\n(CVPR) , 2022. 2, 3, 4, 6, 9, 10, 13, 14, 15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.09127",
                        "Citation Paper Title": "Title:ICON: Implicit Clothed humans Obtained from Normals",
                        "Citation Paper Abstract": "Abstract:Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.",
                        "Citation Paper Authors": "Authors:Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": ", and MonoPort [47, 48] for fast implicit sur-\nface query, and PyTorch3D ",
                    "Citation Text": "Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari. Accelerating 3D deep learning with PyTorch3D.\narXiv:2007.08501 , 2020. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08501",
                        "Citation Paper Title": "Title:Accelerating 3D Deep Learning with PyTorch3D",
                        "Citation Paper Abstract": "Abstract:Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.",
                        "Citation Paper Authors": "Authors:Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "completes the\n3D shape from a deficient 3D input, such as an incomplete\n3D human shape or a low-resolution voxel grid. Inspired by\nLiet al. ",
                    "Citation Text": "Lei Li, Zhizheng Liu, Weining Ren, Liudi Yang, Fangjin-\nhua Wang, Marc Pollefeys, and Songyou Peng. 3D\ntextured shape recovery with learned geometric priors.\narXiv:2209.03254 , 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.03254",
                        "Citation Paper Title": "Title:3D Textured Shape Recovery with Learned Geometric Priors",
                        "Citation Paper Abstract": "Abstract:3D textured shape recovery from partial scans is crucial for many real-world applications. Existing approaches have demonstrated the efficacy of implicit function representation, but they suffer from partial inputs with severe occlusions and varying object types, which greatly hinders their application value in the real world. This technical report presents our approach to address these limitations by incorporating learned geometric priors. To this end, we generate a SMPL model from learned pose prediction and fuse it into the partial input to add prior knowledge of human bodies. We also propose a novel completeness-aware bounding box adaptation for handling different levels of scales and partialness of partial scans.",
                        "Citation Paper Authors": "Authors:Lei Li, Zhizheng Liu, Weining Ren, Liudi Yang, Fangjinhua Wang, Marc Pollefeys, Songyou Peng"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "condition the pixel-aligned features on a posed and voxelized\nSMPL mesh. JIFF Introduces a 3DMM face prior to improve\nthe realism of the facial region. ARCH ",
                    "Citation Text": "Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and\nTony Tung. ARCH: Animatable reconstruction of clothed hu-\nmans. In Computer Vision and Pattern Recognition (CVPR) ,\n2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04572",
                        "Citation Paper Title": "Title:ARCH: Animatable Reconstruction of Clothed Humans",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",
                        "Citation Paper Authors": "Authors:Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.12782v2": {
            "Paper Title": "Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": ", both of which are from officially released\ncodes and re-trained on InterHand2.6M dataset. SelfRecon\nuses the surface-based rendering ",
                    "Citation Text": "Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance. In NeurIPS , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09852",
                        "Citation Paper Title": "Title:Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance",
                        "Citation Paper Abstract": "Abstract:In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",
                        "Citation Paper Authors": "Authors:Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "designed a lighting estimation module, and then\nperformed face relighting. NeRF-OSR ",
                    "Citation Text": "Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie\nLiu, Vladislav Golyanik, and Christian Theobalt. NeRF for\noutdoor scene relighting. In ECCV , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05140",
                        "Citation Paper Title": "Title:NeRF for Outdoor Scene Relighting",
                        "Citation Paper Abstract": "Abstract:Photorealistic editing of outdoor scenes from photographs requires a profound understanding of the image formation process and an accurate estimation of the scene geometry, reflectance and illumination. A delicate manipulation of the lighting can then be performed while keeping the scene albedo and geometry unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene relighting based on neural radiance fields. In contrast to the prior art, our technique allows simultaneous editing of both scene illumination and camera viewpoint using only a collection of outdoor photos shot in uncontrolled settings. Moreover, it enables direct control over the scene illumination, as defined through a spherical harmonics model. For evaluation, we collect a new benchmark dataset of several outdoor sites photographed from multiple viewpoints and at different times. For each time, a 360 degree environment map is captured together with a colour-calibration chequerboard to allow accurate numerical evaluations on real data against ground truth. Comparisons against SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at higher quality and with realistic self-shadowing reproduction. Our method and the dataset are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "disentangled albedo and illumina-\ntion with an inverse 3DMM model to achieve a considerably\nimproved quality of face rendering. S2HAND ",
                    "Citation Text": "Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying\nZhang, Xuefei Zhe, Ruizhi Chen, and Junsong Yuan. Model-\nbased 3D hand reconstruction via self-supervised learning.\nInCVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.11703",
                        "Citation Paper Title": "Title:Model-based 3D Hand Reconstruction via Self-Supervised Learning",
                        "Citation Paper Abstract": "Abstract:Reconstructing a 3D hand from a single-view RGB image is challenging due to various hand configurations and depth ambiguity. To reliably reconstruct a 3D hand from a monocular image, most state-of-the-art methods heavily rely on 3D annotations at the training stage, but obtaining 3D annotations is expensive. To alleviate reliance on labeled training data, we propose S2HAND, a self-supervised 3D hand reconstruction network that can jointly estimate pose, shape, texture, and the camera viewpoint. Specifically, we obtain geometric cues from the input image through easily accessible 2D detected keypoints. To learn an accurate hand reconstruction model from these noisy geometric cues, we utilize the consistency between 2D and 3D representations and propose a set of novel losses to rationalize outputs of the neural network. For the first time, we demonstrate the feasibility of training an accurate 3D hand reconstruction network without relying on manual annotations. Our experiments show that the proposed method achieves comparable performance with recent fully-supervised methods while using fewer supervision data.",
                        "Citation Paper Authors": "Authors:Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying Zhang, Xuefei Zhe, Ruizhi Chen, Junsong Yuan"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". Meanwhile, there has been a surge of interest in\nhuman inverse rendering, the purpose of which is to extract\nintrinsic components ( i.e., geometry, material, and illumina-\ntion) from RGB data [37,56\u201358,77]. For example, GAN2X ",
                    "Citation Text": "Xingang Pan, Ayush Tewari, Lingjie Liu, and Christian\nTheobalt. GAN2X: Non-lambertian inverse rendering of im-\nage gans. In 3DV, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.09244",
                        "Citation Paper Title": "Title:GAN2X: Non-Lambertian Inverse Rendering of Image GANs",
                        "Citation Paper Abstract": "Abstract:2D images are observations of the 3D physical world depicted with the geometry, material, and illumination components. Recovering these underlying intrinsic components from 2D images, also known as inverse rendering, usually requires a supervised setting with paired images collected from multiple viewpoints and lighting conditions, which is resource-demanding. In this work, we present GAN2X, a new method for unsupervised inverse rendering that only uses unpaired images for training. Unlike previous Shape-from-GAN approaches that mainly focus on 3D shapes, we take the first attempt to also recover non-Lambertian material properties by exploiting the pseudo paired data generated by a GAN. To achieve precise inverse rendering, we devise a specularity-aware neural surface representation that continuously models the geometry and material properties. A shading-based refinement technique is adopted to further distill information in the target image and recover more fine details. Experiments demonstrate that GAN2X can accurately decompose 2D images to 3D shape, albedo, and specular properties for different object categories, and achieves the state-of-the-art performance for unsupervised single-view 3D face reconstruction. We also show its applications in downstream tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",
                        "Citation Paper Authors": "Authors:Xingang Pan, Ayush Tewari, Lingjie Liu, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "also put color features on mesh\nvertices, and achieved an editable radiance field. Further-\nmore, mesh-guided local volume ",
                    "Citation Text": "Stephen Lombardi, Tomas Simon, Gabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM TOG , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01954",
                        "Citation Paper Title": "Title:Mixture of Volumetric Primitives for Efficient Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a deconvolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.",
                        "Citation Paper Authors": "Authors:Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, Jason Saragih"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "attached latent codes to\nmesh vertices, which can diffuse into space with sparse con-\nvolution ",
                    "Citation Text": "Benjamin Graham, Martin Engelcke, and Laurens Van\nDer Maaten. 3D semantic segmentation with submanifold\nsparse convolutional networks. In CVPR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10275",
                        "Citation Paper Title": "Title:3D Semantic Segmentation with Submanifold Sparse Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard \"dense\" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.",
                        "Citation Paper Authors": "Authors:Benjamin Graham, Martin Engelcke, Laurens van der Maaten"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "can pro-\nduce a hand mesh with 778 vertices and 1,538 faces. This\nmesh template is too coarse so its representation capacity\nis largely limited. Gyeongsik et al. ",
                    "Citation Text": "Gyeongsik Moon, Takaaki Shiratori, and Kyoung Mu\nLee. DeepHandMesh: A weakly-supervised deep encoder-\ndecoder framework for high-fidelity hand mesh modeling. In\nECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.08213",
                        "Citation Paper Title": "Title:DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling",
                        "Citation Paper Abstract": "Abstract:Human hands play a central role in interacting with other people and objects. For realistic replication of such hand motions, high-fidelity hand meshes have to be reconstructed. In this study, we firstly propose DeepHandMesh, a weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling. We design our system to be trained in an end-to-end and weakly-supervised manner; therefore, it does not require groundtruth meshes. Instead, it relies on weaker supervisions such as 3D joint coordinates and multi-view depth maps, which are easier to get than groundtruth meshes and do not dependent on the mesh topology. Although the proposed DeepHandMesh is trained in a weakly-supervised way, it provides significantly more realistic hand mesh than previous fully-supervised hand models. Our newly introduced penetration avoidance loss further improves results by replicating physical interaction between hand parts. Finally, we demonstrate that our system can also be applied successfully to the 3D hand mesh estimation from general images. Our hand model, dataset, and codes are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Gyeongsik Moon, Takaaki Shiratori, Kyoung Mu Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09069v2": {
            "Paper Title": "Masked Wavelet Representation for Compact Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "proposed using hash-\nbased multi-resolution grids. Using hash functions, each\ngrid maps input coordinates to corresponding feature vec-\ntors. Similarly, VQAD ",
                    "Citation Text": "Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00a8uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields. In ACM SIGGRAPH 2022 Con-\nference Proceedings , SIGGRAPH \u201922, New York, NY , USA,\n2022. Association for Computing Machinery. 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.07707",
                        "Citation Paper Title": "Title:Variable Bitrate Neural Fields",
                        "Citation Paper Abstract": "Abstract:Neural approximations of scalar and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100x and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M\u00fcller, Morgan McGuire, Alec Jacobson, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Nevertheless, the overall size is\nlarger than MLP-only methods. PREF ",
                    "Citation Text": "Binbin Huang, Xinhao Yan, Anpei Chen, Shenghua Gao, and\nJingyi Yu. Pref: Phasorial embedding fields for compact neu-\nral representations. arXiv preprint arXiv:2205.13524 , 2022.\n2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.13524",
                        "Citation Paper Title": "Title:PREF: Phasorial Embedding Fields for Compact Neural Representations",
                        "Citation Paper Abstract": "Abstract:We present an efficient frequency-based neural representation termed PREF: a shallow MLP augmented with a phasor volume that covers significant border spectra than previous Fourier feature mapping or Positional Encoding. At the core is our compact 3D phasor volume where frequencies distribute uniformly along a 2D plane and dilate along a 1D axis. To this end, we develop a tailored and efficient Fourier transform that combines both Fast Fourier transform and local interpolation to accelerate na\u00efve Fourier mapping. We also introduce a Parsvel regularizer that stables frequency-based learning. In these ways, Our PREF reduces the costly MLP in the frequency-based representation, thereby significantly closing the efficiency gap between it and other hybrid representations, and improving its interpretability. Comprehensive experiments demonstrate that our PREF is able to capture high-frequency details while remaining compact and robust, including 2D image generalization, 3D signed distance function regression and 5D neural radiance field reconstruction.",
                        "Citation Paper Authors": "Authors:Binbin Huang, Xinhao Yan, Anpei Chen, Shenghua Gao, Jingyi Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13203v3": {
            "Paper Title": "Inversion-Based Style Transfer with Diffusion Models",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "develop a text-conditional image\ngenerator based on the diffusion models and the inverted\nCLIP. The above methods are dif\ufb01cult to generate new in-\nstances of a given example while maintaining \ufb01delity. Gal\net al. ",
                    "Citation Text": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint\narXiv:2208.01618 , 2022. 2, 3, 4, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.01618",
                        "Citation Paper Title": "Title:An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
                        "Citation Paper Abstract": "Abstract:Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new \"words\" in the embedding space of a frozen text-to-image model. These \"words\" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.\nOur code, data and new words will be available at: this https URL",
                        "Citation Paper Authors": "Authors:Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "learn spatial attention score from both\nshallow and deep features by an adaptive attention nor-\nmalization module (AdaAttN). An et al. ",
                    "Citation Text": "Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and\nJiebo Luo. ArtFlow: Unbiased image style transfer via re-\nversible neural \ufb02ows. In IEEE/CVF Conferences on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 862\u2013\n871, 2021. 1, 2, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.16877",
                        "Citation Paper Title": "Title:ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows",
                        "Citation Paper Abstract": "Abstract:Universal style transfer retains styles from reference images in content images. While existing methods have achieved state-of-the-art style transfer performance, they are not aware of the content leak phenomenon that the image content may corrupt after several rounds of stylization process. In this paper, we propose ArtFlow to prevent content leak during universal style transfer. ArtFlow consists of reversible neural flows and an unbiased feature transfer module. It supports both forward and backward inferences and operates in a projection-transfer-reversion scheme. The forward inference projects input images into deep features, while the backward inference remaps deep features back to input images in a lossless and unbiased way. Extensive experiments demonstrate that ArtFlow achieves comparable performance to state-of-the-art style transfer methods while avoiding content leak.",
                        "Citation Paper Authors": "Authors:Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, Jiebo Luo"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "apply internal-\nexternal scheme to learn feature statistics (mean and stan-\ndard deviation) as style priors (IEST). Zhang et al. ",
                    "Citation Text": "Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang,\nChongyang Ma, Tong-Yee Lee, and Changsheng Xu. Do-\nmain enhanced arbitrary image style transfer via contrastive\nlearning. In ACM SIGGRAPH 2022 Conference Proceed-\nings, pages 12:1\u201312:8, New York, NY , USA, 2022. Associa-\ntion for Computing Machinery. 1, 2, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09542",
                        "Citation Paper Title": "Title:Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:In this work, we tackle the challenging problem of arbitrary image style transfer using a novel style feature representation learning method. A suitable style representation, as a key component in image stylization tasks, is essential to achieve satisfactory results. Existing deep neural network based approaches achieve reasonable results with the guidance from second-order statistics such as Gram matrix of content features. However, they do not leverage sufficient style information, which results in artifacts such as local distortions and style inconsistency. To address these issues, we propose to learn style representation directly from image features instead of their second-order statistics, by analyzing the similarities and differences between multiple styles and considering the style distribution. Specifically, we present Contrastive Arbitrary Style Transfer (CAST), which is a new style representation learning and style transfer method via contrastive learning. Our framework consists of three key components, i.e., a multi-layer style projector for style code encoding, a domain enhancement module for effective learning of style distribution, and a generative network for image style transfer. We conduct qualitative and quantitative evaluations comprehensively to demonstrate that our approach achieves significantly better results compared to those obtained via state-of-the-art methods. Code and models are available at this https URL",
                        "Citation Paper Authors": "Authors:Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Changsheng Xu"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "to generate artis-\ntic images of various styles from text prompts. Rombach et\nal. ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n10684\u201310695, 2022. 2, 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "propose a\ndiffusion-based artistic image generation approach by uti-\nlizing multimodal prompts as guidance to control the clas-\nsi\ufb01er free diffusion model. Hertz et al. ",
                    "Citation Text": "Amir Hertz, Ron Mokady, Jay Tenenbaum, K\ufb01r Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626 , 2022. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.01626",
                        "Citation Paper Title": "Title:Prompt-to-Prompt Image Editing with Cross Attention Control",
                        "Citation Paper Abstract": "Abstract:Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.",
                        "Citation Paper Authors": "Authors:Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "sampling process in closed\nform to obtain a latent noise map that will produce a given\nreal image. Ramesh ",
                    "Citation Text": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125 ,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.06125",
                        "Citation Paper Title": "Title:Hierarchical Text-Conditional Image Generation with CLIP Latents",
                        "Citation Paper Abstract": "Abstract:Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "jointly optimizes text de-\nscription and style image for artistic image generation. Liu\net al. ",
                    "Citation Text": "Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, and Vicky Kalo-\ngeiton. Name your style: An arbitrary artist-aware image\nstyle transfer. arXiv preprint arXiv:2202.13562 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.13562",
                        "Citation Paper Title": "Title:Name Your Style: An Arbitrary Artist-aware Image Style Transfer",
                        "Citation Paper Abstract": "Abstract:Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a text-driven image style transfer (TxST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient attention module that explores cross-attentions to fuse style and content features. Finally, we achieve an arbitrary artist-aware image style transfer to learn and transfer specific artistic characters such as Picasso, oil painting, or a rough sketch. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods on both image and textual styles. Moreover, it can mimic the styles of one or many artists to achieve attractive results, thus highlighting a promising direction in image style transfer.",
                        "Citation Paper Authors": "Authors:Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, Vicky Kalogeiton"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "trans-\nfers an input image to a desired style with a text descrip-\ntion by using CLIP loss and PatchCLIP loss. StyleGAN-\nNADA ",
                    "Citation Text": "Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano,\nGal Chechik, and Daniel Cohen-Or. StyleGAN-NADA:\nCLIP-guided domain adaptation of image generators. ACM\nTransactions on Graphics , 41(4):141:1\u2013141:13, 2022. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00946",
                        "Citation Paper Title": "Title:StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators",
                        "Citation Paper Abstract": "Abstract:Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",
                        "Citation Paper Authors": "Authors:Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "to maximize similarity between\nthe textual description and generated drawing. VQGAN-\nCLIP ",
                    "Citation Text": "Katherine Crowson, Stella Biderman, Daniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. VQGAN-CLIP: Open domain image genera-\ntion and editing with natural language guidance. In Euro-\npean Conference on Computer Vision (ECCV) , pages 88\u2013\n105. Springer, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.08583",
                        "Citation Paper Title": "Title:VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance",
                        "Citation Paper Abstract": "Abstract:Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.",
                        "Citation Paper Authors": "Authors:Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, Edward Raff"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11674v2": {
            "Paper Title": "Shape, Pose, and Appearance from a Single Image via Bootstrapped\n  Radiance Field Inversion",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "as a starting point for the backbone\nof our generator. It consists of a mapping network that maps\na prior z\u0018N (0;I)to a latent code w2W , the latter of\nwhich is plugged into a StyleGAN2 generator ",
                    "Citation Text": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\ning the image quality of stylegan. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8110\u20138119, 2020. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "learns this\ntask using NeRFs, although it suffers from the high compu-\ntational cost of MLP-based NeRFs. [5,19,40,42,49,50,62]\nincorporate both 2D and 3D components as a trade-off be-\ntween 3D consistency and ef\ufb01ciency. Finally, ",
                    "Citation Text": "Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. In Advances In Neural\nInformation Processing Systems , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.11163",
                        "Citation Paper Title": "Title:GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images",
                        "Citation Paper Abstract": "Abstract:As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.",
                        "Citation Paper Authors": "Authors:Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01160v2": {
            "Paper Title": "High-Res Facial Appearance Capture from Polarized Smartphone Images",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ". The method\nfits a parametric face mask to the observed images and is\nable to handle self-shadowing, but complex lighting envi-\nronments can have an impact on separation of lighting and\nmaterial. Wang et al. ",
                    "Citation Text": "Yifan Wang, Aleksander Holynski, Xiuming Zhang, and Xu-\naner Cecilia Zhang. Sunstage: Portrait reconstruction and\nrelighting using the sun as a light stage. arXiv preprint\narXiv:2204.03648 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03648",
                        "Citation Paper Title": "Title:SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage",
                        "Citation Paper Abstract": "Abstract:A light stage uses a series of calibrated cameras and lights to capture a subject's facial appearance under varying illumination and viewpoint. This captured information is crucial for facial reconstruction and relighting. Unfortunately, light stages are often inaccessible: they are expensive and require significant technical expertise for construction and operation. In this paper, we present SunStage: a lightweight alternative to a light stage that captures comparable data using only a smartphone camera and the sun. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as guidance in joint reconstruction of facial geometry, reflectance, camera pose, and lighting parameters. Despite the in-the-wild un-calibrated setting, our approach is able to reconstruct detailed facial appearance and geometry, enabling compelling effects such as relighting, novel view synthesis, and reflectance editing. Results and interactive demos are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yifan Wang, Aleksander Holynski, Xiuming Zhang, Xuaner Zhang"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "to produce unbiased gradients for shape es-\ntimation on an explicit mesh. With the same co-located\nsetup, Zhang et al. ",
                    "Citation Text": "Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron:\nInverse rendering by optimizing neural sdfs and materials\nfrom photometric images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5565\u20135574, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02232",
                        "Citation Paper Title": "Title:IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images",
                        "Citation Paper Abstract": "Abstract:We propose a neural inverse rendering pipeline called IRON that operates on photometric images and outputs high-quality 3D content in the format of triangle meshes and material textures readily deployable in existing graphics pipelines. Our method adopts neural representations for geometry as signed distance fields (SDFs) and materials during optimization to enjoy their flexibility and compactness, and features a hybrid optimization scheme for neural SDFs: first, optimize using a volumetric radiance field approach to recover correct topology, then optimize further using edgeaware physics-based surface rendering for geometry refinement and disentanglement of materials and lighting. In the second stage, we also draw inspiration from mesh-based differentiable rendering, and design a novel edge sampling algorithm for neural SDFs to further improve performance. We show that our IRON achieves significantly better inverse rendering quality compared to prior works. Our project page is here: this https URL",
                        "Citation Paper Authors": "Authors:Kai Zhang, Fujun Luan, Zhengqi Li, Noah Snavely"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "use polarization to estimate the shape and SVBRDF of an\nobject with normal, diffuse, specular, roughness and depth\nmaps from a single view. Dave et al. ",
                    "Citation Text": "Akshat Dave, Yongyi Zhao, and Ashok Veeraraghavan. Pan-\ndora: Polarization-aided neural decomposition of radiance.\narXiv preprint arXiv:2203.13458 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13458",
                        "Citation Paper Title": "Title:PANDORA: Polarization-Aided Neural Decomposition Of Radiance",
                        "Citation Paper Abstract": "Abstract:Reconstructing an object's geometry and appearance from multiple images, also known as inverse rendering, is a fundamental problem in computer graphics and vision. Inverse rendering is inherently ill-posed because the captured image is an intricate function of unknown lighting conditions, material properties and scene geometry. Recent progress in representing scene properties as coordinate-based neural networks have facilitated neural inverse rendering resulting in impressive geometry reconstruction and novel-view synthesis. Our key insight is that polarization is a useful cue for neural inverse rendering as polarization strongly depends on surface normals and is distinct for diffuse and specular reflectance. With the advent of commodity, on-chip, polarization sensors, capturing polarization has become practical. Thus, we propose PANDORA, a polarimetric inverse rendering approach based on implicit neural representations. From multi-view polarization images of an object, PANDORA jointly extracts the object's 3D geometry, separates the outgoing radiance into diffuse and specular and estimates the illumination incident on the object. We show that PANDORA outperforms state-of-the-art radiance decomposition techniques. PANDORA outputs clean surface reconstructions free from texture artefacts, models strong specularities accurately and estimates illumination under practical unstructured scenarios.",
                        "Citation Paper Authors": "Authors:Akshat Dave, Yongyi Zhao, Ashok Veeraraghavan"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "recover SVBRDF\nmaps of planar objects with near-field display illumination,\nexploiting Brewster angle properties. Deschaintre et al. ",
                    "Citation Text": "Valentin Deschaintre, Yiming Lin, and Abhijeet Ghosh.\nDeep polarization imaging for 3d shape and svbrdf acquisi-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , June 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.02875",
                        "Citation Paper Title": "Title:Deep Polarization Imaging for 3D shape and SVBRDF Acquisition",
                        "Citation Paper Abstract": "Abstract:We present a novel method for efficient acquisition of shape and spatially varying reflectance of 3D objects using polarization cues. Unlike previous works that have exploited polarization to estimate material or object appearance under certain constraints (known shape or multiview acquisition), we lift such restrictions by coupling polarization imaging with deep learning to achieve high quality estimate of 3D object shape (surface normals and depth) and SVBRDF using single-view polarization imaging under frontal flash illumination. In addition to acquired polarization images, we provide our deep network with strong novel cues related to shape and reflectance, in the form of a normalized Stokes map and an estimate of diffuse color. We additionally describe modifications to network architecture and training loss which provide further qualitative improvements. We demonstrate our approach to achieve superior results compared to recent works employing deep learning in conjunction with flash illumination.",
                        "Citation Paper Authors": "Authors:Valentin Deschaintre, Yiming Lin, Abhijeet Ghosh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12242v2": {
            "Paper Title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for\n  Subject-Driven Generation",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "extended this work\nto GAN \ufb01netuning on faces to train a personalized prior,\nwhich requires around 100 images and are limited to the\nface domain. Casanova et al. ",
                    "Citation Text": "Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\nDrozdzal, and Adriana Romero Soriano. Instance-\nconditioned gan. Advances in Neural Information Process-\ning Systems , 34:27517\u201327529, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05070",
                        "Citation Paper Title": "Title:Instance-Conditioned GAN",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) can generate near photo realistic images in narrow domains such as human faces. Yet, modeling complex distributions of datasets such as ImageNet and COCO-Stuff remains challenging in unconditional settings. In this paper, we take inspiration from kernel density estimation techniques and introduce a non-parametric approach to modeling distributions of complex datasets. We partition the data manifold into a mixture of overlapping neighborhoods described by a datapoint and its nearest neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN), which learns the distribution around each datapoint. Experimental results on ImageNet and COCO-Stuff show that IC-GAN significantly improves over unconditional models and unsupervised data partitioning baselines. Moreover, we show that IC-GAN can effortlessly transfer to datasets not seen during training by simply changing the conditioning instances, and still generate realistic images. Finally, we extend IC-GAN to the class-conditional case and show semantically controllable generation and competitive quantitative results on ImageNet; while improving over BigGAN on ImageNet-LT. Code and trained models to reproduce the reported results are available at this https URL.",
                        "Citation Paper Authors": "Authors:Arantxa Casanova, Marl\u00e8ne Careil, Jakob Verbeek, Michal Drozdzal, Adriana Romero-Soriano"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "allows for\nreal image editing by \ufb01netuning the model with an inverted\nlatent code anchor, and Nitzan et al. ",
                    "Citation Text": "Yotam Nitzan, K\ufb01r Aberman, Qiurui He, Orly Liba, Michal\nYarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and\nDaniel Cohen-Or. Mystyle: A personalized generative prior.\narXiv preprint arXiv:2203.17272 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.17272",
                        "Citation Paper Title": "Title:MyStyle: A Personalized Generative Prior",
                        "Citation Paper Abstract": "Abstract:We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.",
                        "Citation Paper Authors": "Authors:Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-or"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "allows for local and global editing\nwithout an input mask. These methods fall short of identity-\npreserving novel sample generation of a subject.\nIn the context of GANs, Pivotal Tuning ",
                    "Citation Text": "Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Transactions on Graphics (TOG) , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05744",
                        "Citation Paper Title": "Title:Pivotal Tuning for Latent-based Editing of Real Images",
                        "Citation Paper Abstract": "Abstract:Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". While most works that require only\ntext are limited to global editing [14, 33], Bar-Tal et al. ",
                    "Citation Text": "Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. arXiv preprint arXiv:2204.02491 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02491",
                        "Citation Paper Title": "Title:Text2LIVE: Text-Driven Layered Image and Video Editing",
                        "Citation Paper Abstract": "Abstract:We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.",
                        "Citation Paper Authors": "Authors:Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, Tali Dekel"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", yielding realistic manip-\nulations using text [2, 7, 21, 43, 48, 71]. These methodswork well on structured scenarios (e.g. human face edit-\ning) and can struggle over diverse datasets where sub-\njects are varied. Crowson et al. ",
                    "Citation Text": "Katherine Crowson, Stella Biderman, Daniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation\nand editing with natural language guidance. arXiv preprint\narXiv:2204.08583 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.08583",
                        "Citation Paper Title": "Title:VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance",
                        "Citation Paper Abstract": "Abstract:Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.",
                        "Citation Paper Authors": "Authors:Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, Edward Raff"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.07227v2": {
            "Paper Title": "DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.09988v2": {
            "Paper Title": "Crowdsourcing Autonomous Traffic Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02607v3": {
            "Paper Title": "CROM: Continuous Reduced-Order Modeling of PDEs Using Implicit Neural\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07590v2": {
            "Paper Title": "Stroke-based Rendering and Planning for Robotic Performance of Artistic\n  Drawing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10992v3": {
            "Paper Title": "NIFT: Neural Interaction Field and Template for Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08377v2": {
            "Paper Title": "PointAvatar: Deformable Point-based Head Avatars from Videos",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". We update the SDF using\ncurrent point locations at each training step.\nCanonical albedo. We use an MLP to map the point loca-\ntionsxcto the albedo colors a2R3, similar to ",
                    "Citation Text": "Pol Caselles, Eduard Ramon, Jaime Garcia, Xavier Giro-\ni Nieto, Francesc Moreno-Noguer, and Gil Triginer. Sira:\nRelightable avatars from a single image. arXiv preprint\narXiv:2209.03027 , 2022. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.03027",
                        "Citation Paper Title": "Title:SIRA: Relightable Avatars from a Single Image",
                        "Citation Paper Abstract": "Abstract:Recovering the geometry of a human head from a single image, while factorizing the materials and illumination is a severely ill-posed problem that requires prior information to be solved. Methods based on 3D Morphable Models (3DMM), and their combination with differentiable renderers, have shown promising results. However, the expressiveness of 3DMMs is limited, and they typically yield over-smoothed and identity-agnostic 3D shapes limited to the face region. Highly accurate full head reconstructions have recently been obtained with neural fields that parameterize the geometry using multilayer perceptrons. The versatility of these representations has also proved effective for disentangling geometry, materials and lighting. However, these methods require several tens of input images. In this paper, we introduce SIRA, a method which, from a single image, reconstructs human head avatars with high fidelity geometry and factorized lights and surface materials. Our key ingredients are two data-driven statistical models based on neural fields that resolve the ambiguities of single-view 3D surface reconstruction and appearance factorization. Experiments show that SIRA obtains state of the art results in 3D head reconstruction while at the same time it successfully disentangles the global illumination, and the diffuse and specular albedos. Furthermore, our reconstructions are amenable to physically-based appearance editing and head model relighting.",
                        "Citation Paper Authors": "Authors:Pol Caselles, Eduard Ramon, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer, Gil Triginer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.07085v3": {
            "Paper Title": "Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.07555v3": {
            "Paper Title": "IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10988v3": {
            "Paper Title": "Attention-Aware Anime Line Drawing Colorization",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "presented the\nColorization Transformer that entirely relies on self-attention\nfor image colorization. Since few research work focuses on\nline drawing colorization using attention-based method, we\nmainly compare our method with Lee et al. ",
                    "Citation Text": "Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk\nChang, and Jaegul Choo, \u201cReference-based sketch image colorization\nusing augmented-self reference and dense semantic correspondence,\u201d\ninProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2020, pp. 5801\u20135810.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.05207",
                        "Citation Paper Title": "Title:Reference-Based Sketch Image Colorization using Augmented-Self Reference and Dense Semantic Correspondence",
                        "Citation Paper Abstract": "Abstract:This paper tackles the automatic colorization task of a sketch image given an already-colored reference image. Colorizing a sketch image is in high demand in comics, animation, and other content creation applications, but it suffers from information scarcity of a sketch image. To address this, a reference image can render the colorization process in a reliable and user-driven manner. However, it is difficult to prepare for a training data set that has a sufficient amount of semantically meaningful pairs of images as well as the ground truth for a colored image reflecting a given reference (e.g., coloring a sketch of an originally blue car given a reference green car). To tackle this challenge, we propose to utilize the identical image with geometric distortion as a virtual reference, which makes it possible to secure the ground truth for a colored output image. Furthermore, it naturally provides the ground truth for dense semantic correspondence, which we utilize in our internal attention mechanism for color transfer from reference to sketch input. We demonstrate the effectiveness of our approach in various types of sketch image colorization via quantitative as well as qualitative evaluation against existing methods.",
                        "Citation Paper Authors": "Authors:Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk Chang, Jaegul Choo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13944v2": {
            "Paper Title": "SolidGen: An Autoregressive Model for Direct B-rep Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03410v4": {
            "Paper Title": "Fast and Robust Non-Rigid Registration Using Accelerated\n  Majorization-Minimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15947v2": {
            "Paper Title": "NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed\n  Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "predicts the unsigned dis-\ntance \ufb01eld for 3D shapes from point clouds. NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In European Conference on Computer Vision , pages\n405\u2013421. Springer, 2020. 1, 2, 4, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "use convolu-\ntional neural nets to compose static and dynamic parts of\nthe event and then adopt U-Net to render images from inter-\nmediate results composited from depth-based re-projected\nimages. Neural V olumes (NV) ",
                    "Citation Text": "Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: learning dynamic renderable volumes from images.\nACM Transactions on Graphics , 38(4):1\u201314, 2019. 3, 7, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07751",
                        "Citation Paper Title": "Title:Neural Volumes: Learning Dynamic Renderable Volumes from Images",
                        "Citation Paper Abstract": "Abstract:Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",
                        "Citation Paper Authors": "Authors:Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". Directly modeling the 4D \ufb01eld\nby introducing an extra time dimension into the original ra-\ndiance \ufb01eld is adopted in NSFF ",
                    "Citation Text": "Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene \ufb02ow \ufb01elds for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2021. 2, 3, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13084",
                        "Citation Paper Title": "Title:Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",
                        "Citation Paper Authors": "Authors:Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "con-\nsider input images from different view, time or illumina-\ntion conditions in structured captures. DyNeRF ",
                    "Citation Text": "Tianye Li, Mira Slavcheva, Michael Zollh \u00a8ofer, Simon Green,\nChristoph Lassner, Changil Kim, Tanner Schmidt, Steven\nLovegrove, Michael Goesele, Richard Newcombe, and\nZhaoyang Lv. Neural 3d video synthesis from multi-view\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 5521\u2013\n5531, June 2022. 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.02597",
                        "Citation Paper Title": "Title:Neural 3D Video Synthesis from Multi-view Video",
                        "Citation Paper Abstract": "Abstract:We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance field that represents scene dynamics using a set of compact latent codes. We are able to significantly boost the training speed and perceptual quality of the generated imagery by a novel hierarchical training scheme in combination with ray importance sampling. Our learned representation is highly compact and able to represent a 10 second 30 FPS multiview video recording by 18 cameras with a model size of only 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the state of the art. Project website: this https URL.",
                        "Citation Paper Authors": "Authors:Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, Zhaoyang Lv"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "proposes a hash en-\ncoding of the saved feature grids and solves hashing col-\nlision by multi-scale encoding and small MLP decoding.\nTensoRF ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance \ufb01elds. In Proceedings\nof the European Conference on Computer Vision , 2022. 1, 2,\n3, 4, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09517",
                        "Citation Paper Title": "Title:TensoRF: Tensorial Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "uses two feature voxels\n2to represent occupancy and appearance. The feature vec-\ntors queried from the voxels are decoded by small MLPs.\nPlenoxels ",
                    "Citation Text": "Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance \ufb01elds without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2022. 1, 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05131",
                        "Citation Paper Title": "Title:Plenoxels: Radiance Fields without Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.",
                        "Citation Paper Authors": "Authors:Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". The trajectory of points is modeled by a neural\n\ufb01eld in DCT-NeRF ",
                    "Citation Text": "Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio\nGallo. Neural trajectory \ufb01elds for dynamic novel view syn-\nthesis. arXiv preprint arXiv:2105.05994 , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.05994",
                        "Citation Paper Title": "Title:Neural Trajectory Fields for Dynamic Novel View Synthesis",
                        "Citation Paper Abstract": "Abstract:Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.",
                        "Citation Paper Authors": "Authors:Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "assign\nobservation frames with a set of compact latent codes and\nthen use time-conditioned neural radiance \ufb01elds to repre-\nsent dynamic scenes. Fourier PlenOctrees ",
                    "Citation Text": "Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance \ufb01eld ren-\ndering in real-time. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n13524\u201313534, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.08614",
                        "Citation Paper Title": "Title:Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time",
                        "Citation Paper Abstract": "Abstract:Implicit neural representations such as Neural Radiance Field (NeRF) have focused mainly on modeling static objects captured under multi-view settings where real-time rendering can be achieved with smart data structures, e.g., PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO) technique to tackle efficient neural modeling and real-time rendering of dynamic scenes captured under the free-view video (FVV) setting. The key idea in our FPO is a novel combination of generalized NeRF, PlenOctree representation, volumetric fusion and Fourier transform. To accelerate FPO construction, we present a novel coarse-to-fine fusion scheme that leverages the generalizable NeRF technique to generate the tree via spatial blending. To tackle dynamic scenes, we tailor the implicit network to model the Fourier coefficients of timevarying density and color attributes. Finally, we construct the FPO and train the Fourier coefficients directly on the leaves of a union PlenOctree structure of the dynamic sequence. We show that the resulting FPO enables compact memory overload to handle dynamic objects and supports efficient fine-tuning. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF and achieves over an order of magnitude acceleration over SOTA while preserving high visual quality for the free-viewpoint rendering of unseen dynamic scenes.",
                        "Citation Paper Authors": "Authors:Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "proposes multi-sphere\nimage based Layered Meshes. The capturing setting is a\nlow-cost hemispherical array with 46 synchronized cameras\nand Layered Meshes are validated to be ef\ufb01cient and can\nwell-handle non-Lambertian surfaces with view-dependent\nor semi-transparent effects. Bansal et al. ",
                    "Citation Text": "Aayush Bansal, Minh V o, Yaser Sheikh, Deva Ramanan, and\nSrinivasa Narasimhan. 4d visualization of dynamic events\nfrom unconstrained multi-view videos. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5366\u20135375, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13532",
                        "Citation Paper Title": "Title:4D Visualization of Dynamic Events from Unconstrained Multi-View Videos",
                        "Citation Paper Abstract": "Abstract:We present a data-driven approach for 4D space-time visualization of dynamic events from videos captured by hand-held multiple cameras. Key to our approach is the use of self-supervised neural networks specific to the scene to compose static and dynamic aspects of an event. Though captured from discrete viewpoints, this model enables us to move around the space-time of the event continuously. This model allows us to create virtual cameras that facilitate: (1) freezing the time and exploring views; (2) freezing a view and moving through time; and (3) simultaneously changing both time and view. We can also edit the videos and reveal occluded objects for a given view if it is visible in any of the other views. We validate our approach on challenging in-the-wild events captured using up to 15 mobile cameras.",
                        "Citation Paper Authors": "Authors:Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "decompose scenes into\nsemantic scene graphs. Objects are decomposed by mo-\ntion in NeuralDiff ",
                    "Citation Text": "Vadim Tschernezki, Diane Larlus, and Andrea Vedaldi. Neu-\nraldiff: Segmenting 3d objects that move in egocentric\nvideos. In International Conference on 3D Vision (3DV) ,\npages 910\u2013919. IEEE, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.09936",
                        "Citation Paper Title": "Title:NeuralDiff: Segmenting 3D objects that move in egocentric videos",
                        "Citation Paper Abstract": "Abstract:Given a raw video sequence taken from a freely-moving camera, we study the problem of decomposing the observed 3D scene into a static background and a dynamic foreground containing the objects that move in the video sequence. This task is reminiscent of the classic background subtraction problem, but is significantly harder because all parts of the scene, static and dynamic, generate a large apparent motion due to the camera large viewpoint change. In particular, we consider egocentric videos and further separate the dynamic component into objects and the actor that observes and moves them. We achieve this factorization by reconstructing the video via a triple-stream neural rendering network that explains the different motions based on corresponding inductive biases. We demonstrate that our method can successfully separate the different types of motion, outperforming recent neural rendering baselines at this task, and can accurately segment moving objects. We do so by assessing the method empirically on challenging videos from the EPIC-KITCHENS dataset which we augment with appropriate annotations to create a new benchmark for the task of dynamic object segmentation on unconstrained video sequences, for complex 3D environments.",
                        "Citation Paper Authors": "Authors:Vadim Tschernezki, Diane Larlus, Andrea Vedaldi"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "semantically decompose the scene with\npre-trained models. Ost et al. ",
                    "Citation Text": "Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and\nFelix Heide. Neural scene graphs for dynamic scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 2856\u20132865, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.10379",
                        "Citation Paper Title": "Title:Neural Scene Graphs for Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",
                        "Citation Paper Authors": "Authors:Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "spatially decomposes the scene and uses small networks for\neach area for ef\ufb01ciency. Kobayashi et al. ",
                    "Citation Text": "Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature \ufb01eld dis-\ntillation. In arXiv , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.15585",
                        "Citation Paper Title": "Title:Decomposing NeRF for Editing via Feature Field Distillation",
                        "Citation Paper Abstract": "Abstract:Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.",
                        "Citation Paper Authors": "Authors:Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "2.1. Neural Fields\nNeural \ufb01elds are neural networks that take in the coordi-\nnates and output the properties of that point ",
                    "Citation Text": "Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\nkin, Vincent Sitzmann, and Srinath Sridhar. Neural \ufb01elds in\nvisual computing and beyond. In Computer Graphics Forum ,\nvolume 41, pages 641\u2013676. Wiley Online Library, 2022. 1,\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.13681v3": {
            "Paper Title": "BSDF Importance Baking: A Lightweight Neural Solution to Importance\n  Sampling General Parametric BSDFs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00792v3": {
            "Paper Title": "SparseFusion: Distilling View-conditioned Diffusion for 3D\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nIn this work, we leverage this class of models for (proba-\nbilistic) novel view synthesis while using geometry-aware\nfeatures as conditioning. Inspired by the impressive re-\nsults in DreamFusion ",
                    "Citation Text": "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv ,\n2022. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14988",
                        "Citation Paper Title": "Title:DreamFusion: Text-to-3D using 2D Diffusion",
                        "Citation Paper Abstract": "Abstract:Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",
                        "Citation Paper Authors": "Authors:Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposes a 2D diffusion approach for\nimage-conditioned novel view synthesis, but does not in-\nfer a 3D representation like our approach. Closer to our\nwork, Deng et al. ",
                    "Citation Text": "Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen\nYan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov,\net al. Nerdi: Single-view nerf synthesis with language-\nguided diffusion as general image priors. arXiv preprint\narXiv:2212.03267 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.03267",
                        "Citation Paper Title": "Title:NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors",
                        "Citation Paper Abstract": "Abstract:2D-to-3D reconstruction is an ill-posed problem, yet humans are good at solving this problem due to their prior knowledge of the 3D world developed over years. Driven by this observation, we propose NeRDi, a single-view NeRF synthesis framework with general image priors from 2D diffusion models. Formulating single-view reconstruction as an image-conditioned 3D generation problem, we optimize the NeRF representations by minimizing a diffusion loss on its arbitrary view renderings with a pretrained image diffusion model under the input-view constraint. We leverage off-the-shelf vision-language models and introduce a two-section language guidance as conditioning inputs to the diffusion model. This is essentially helpful for improving multiview content coherence as it narrows down the general image prior conditioned on the semantic and visual features of the single-view input image. Additionally, we introduce a geometric loss based on estimated depth maps to regularize the underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset show that our method can synthesize novel views with higher quality even compared to existing methods trained on this dataset. We also demonstrate our generalizability in zero-shot NeRF synthesis for in-the-wild images.",
                        "Citation Paper Authors": "Authors:Congyue Deng, Chiyu \"Max'' Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.15401v2": {
            "Paper Title": "VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for\n  Analysis-by-Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12390v5": {
            "Paper Title": "Model reduction for the material point method via an implicit neural\n  representation of the deformation map",
            "Sentences": [
                {
                    "Sentence ID": 130,
                    "Sentence": "Wi\u0119ckowski, Z., Youn, S.K., Yeon, J.H., 1999. A particle-in-cell solution to the silo discharging problem. International\njournal for numerical methods in engineering 45, 1203\u20131225. ",
                    "Citation Text": "Wiewel, S., Becher, M., Thuerey, N., 2019. Latent space physics: Towards learning the temporal evolution of \ufb02uid\n\ufb02ow, in: Computer graphics forum, Wiley Online Library. pp. 71\u201382.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.10123",
                        "Citation Paper Title": "Title:Latent-space Physics: Towards Learning the Temporal Evolution of Fluid Flow",
                        "Citation Paper Abstract": "Abstract:We propose a method for the data-driven inference of temporal evolutions of physical functions with deep learning. More specifically, we target fluid flows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approach to predict the changes of pressure fields over time. The central challenge in this context is the high dimensionality of Eulerian space-time data sets. We demonstrate for the first time that dense 3D+time functions of physics system can be predicted within the latent spaces of neural networks, and we arrive at a neural-network based simulation algorithm with significant practical speed-ups. We highlight the capabilities of our method with a series of complex liquid simulations, and with a set of single-phase buoyancy simulations. With a set of trained networks, our method is more than two orders of magnitudes faster than a traditional pressure solver. Additionally, we present and discuss a series of detailed evaluations for the different components of our algorithm.",
                        "Citation Paper Authors": "Authors:Steffen Wiewel, Moritz Becher, Nils Thuerey"
                    }
                },
                {
                    "Sentence ID": 128,
                    "Sentence": "Wang, X., Qiu, Y., Slattery, S.R., Fang, Y., Li, M., Zhu, S.C., Zhu, Y., Tang, M., Manocha, D., Jiang, C., 2020b. A\nmassively parallel and scalable multi-GPU material point method. ACM Transactions on Graphics (TOG) 39, 30\u20131. ",
                    "Citation Text": "Wang, Z., Akhtar, I., Borggaard, J., Iliescu, T., 2012. Proper orthogonal decomposition closure models for turbulent\n\ufb02ows: a numerical comparison. Computer Methods in Applied Mechanics and Engineering 237, 10\u201326.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1106.3585",
                        "Citation Paper Title": "Title:Proper Orthogonal Decomposition Closure Models For Turbulent Flows: A Numerical Comparison",
                        "Citation Paper Abstract": "Abstract:This paper puts forth two new closure models for the proper orthogonal decomposition reduced-order modeling of structurally dominated turbulent flows: the dynamic subgrid-scale model and the variational multiscale model. These models, which are considered state-of-the-art in large eddy simulation, together with the mixing length and the Smagorinsky closure models, are tested in the numerical simulation of a 3D turbulent flow around a circular cylinder at Re = 1,000. Two criteria are used in judging the performance of the proper orthogonal decomposition reduced-order models: the kinetic energy spectrum and the time evolution of the POD coefficients. All the numerical results are benchmarked against a direct numerical simulation. Based on these numerical results, we conclude that the dynamic subgrid-scale and the variational multiscale models perform best.",
                        "Citation Paper Authors": "Authors:Zhu Wang, Imran Akhtar, Jeff Borggaard, Traian Iliescu"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": "Takeishi, N., Kawahara, Y., Yairi, T., 2017. LearningKoopmaninvariantsubspacesfordynamicmodedecomposition,\nin: Advances in Neural Information Processing Systems, pp. 1130\u20131140. ",
                    "Citation Text": "Takikawa, T., Litalien, J., Yin, K., Kreis, K., Loop, C., Nowrouzezahrai, D., Jacobson, A., McGuire, M., Fidler, S.,\n2021. Neural geometric level of detail: Real-time rendering with implicit 3d shapes, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 11358\u201311367.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.10994",
                        "Citation Paper Title": "Title:Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
                        "Citation Paper Abstract": "Abstract:Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
                        "Citation Paper Authors": "Authors:Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "Schulz, A., Xu, J., Zhu, B., Zheng, C., Grinspun, E., Matusik, W., 2017. Interactive design space exploration and\noptimization for CAD models. ACM Transactions on Graphics (TOG) 36, 1\u201314. ",
                    "Citation Text": "Shen, S., Yin, Y., Shao, T., Wang, H., Jiang, C., Lan, L., Zhou, K., 2021. High-order di\ufb00erentiable autoencoder for\nnonlinear model reduction. arXiv preprint arXiv:2102.11026 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.11026",
                        "Citation Paper Title": "Title:High-order Differentiable Autoencoder for Nonlinear Model Reduction",
                        "Citation Paper Abstract": "Abstract:This paper provides a new avenue for exploiting deep neural networks to improve physics-based simulation. Specifically, we integrate the classic Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot be established without evaluating the second-order derivatives of the deep autoencoder network. This is beyond the capability of off-the-shelf automatic differentiation packages and algorithms, which mainly focus on the gradient evaluation. Solving the nonlinear force equilibrium is even more challenging if the standard Newton's method is to be used. This is because we need to compute a third-order derivative of the network to obtain the variational Hessian. We attack those difficulties by exploiting complex-step finite difference, coupled with reverse automatic differentiation. This strategy allows us to enjoy the convenience and accuracy of complex-step finite difference and in the meantime, to deploy complex-value perturbations as collectively as possible to save excessive network passes. With a GPU-based implementation, we are able to wield deep autoencoders (e.g., $10+$ layers) with a relatively high-dimension latent space in real-time. Along this pipeline, we also design a sampling network and a weighting network to enable \\emph{weight-varying} Cubature integration in order to incorporate nonlinearity in the model reduction. We believe this work will inspire and benefit future research efforts in nonlinearly reduced physical simulation problems.",
                        "Citation Paper Authors": "Authors:Siyuan Shen, Yang Yin, Tianjia Shao, He Wang, Chenfanfu Jiang, Lei Lan, Kun Zhou"
                    }
                },
                {
                    "Sentence ID": 108,
                    "Sentence": "Sadeghirad, A., Brannon, R.M., Burghardt, J., 2011. A convected particle domain interpolation technique to extend\napplicability of the material point method for problems involving massive deformations. International Journal for\nnumerical methods in Engineering 86, 1435\u20131456. ",
                    "Citation Text": "Sanchez-Gonzalez, A., Godwin, J., Pfa\ufb00, T., Ying, R., Leskovec, J., Battaglia, P., 2020. Learning to simulate complex\nphysics with graph networks, in: International Conference on Machine Learning, PMLR. pp. 8459\u20138468.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.09405",
                        "Citation Paper Title": "Title:Learning to Simulate Complex Physics with Graph Networks",
                        "Citation Paper Abstract": "Abstract:Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term \"Graph Network-based Simulators\" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.",
                        "Citation Paper Authors": "Authors:Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia"
                    }
                },
                {
                    "Sentence ID": 99,
                    "Sentence": "and our work are the \ufb01rst time implicit\nneural representations have been used for model reduction for any physical system. Alternatively, our\nlow-dimensionalapproximationcanalsobeviewedasanextensionofthephysics-informedneuralnetwork\n4(PINN) ",
                    "Citation Text": "Raissi, M., Perdikaris, P., Karniadakis, G.E., 2019. Physics-informed neural networks: A deep learning framework\nfor solving forward and inverse problems involving nonlinear partial di\ufb00erential equations. Journal of Computational\nPhysics 378, 686\u2013707.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10561",
                        "Citation Paper Title": "Title:Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations",
                        "Citation Paper Abstract": "Abstract:We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.",
                        "Citation Paper Authors": "Authors:Maziar Raissi, Paris Perdikaris, George Em Karniadakis"
                    }
                },
                {
                    "Sentence ID": 93,
                    "Sentence": "Pan, S., Brunton, S.L., Kutz, J.N., 2023. Neural implicit \ufb02ow: a mesh-agnostic dimensionality reduction paradigm\nof spatio-temporal data. Journal of Machine Learning Research 24, 1\u201360. URL: http://jmlr.org/papers/v24/\n22-0365.html . ",
                    "Citation Text": "Parish, E.J., Carlberg, K.T., 2020. Time-series machine-learning error models for approximate solutions to parame-\nterized dynamical systems. Computer Methods in Applied Mechanics and Engineering 365, 112990.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11822",
                        "Citation Paper Title": "Title:Time-series machine-learning error models for approximate solutions to parameterized dynamical systems",
                        "Citation Paper Abstract": "Abstract:This work proposes a machine-learning framework for modeling the error incurred by approximate solutions to parameterized dynamical systems. In particular, we extend the machine-learning error models (MLEM) framework proposed in Ref. 15 to dynamical systems. The proposed Time-Series Machine-Learning Error Modeling (T-MLEM) method constructs a regression model that maps features--which comprise error indicators that are derived from standard a posteriori error-quantification techniques--to a random variable for the approximate-solution error at each time instance. The proposed framework considers a wide range of candidate features, regression methods, and additive noise models. We consider primarily recursive regression techniques developed for time-series modeling, including both classical time-series models (e.g., autoregressive models) and recurrent neural networks (RNNs), but also analyze standard non-recursive regression techniques (e.g., feed-forward neural networks) for comparative purposes. Numerical experiments conducted on multiple benchmark problems illustrate that the long short-term memory (LSTM) neural network, which is a type of RNN, outperforms other methods and yields substantial improvements in error predictions over traditional approaches.",
                        "Citation Paper Authors": "Authors:Eric J. Parish, Kevin T. Carlberg"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "Nocedal, J., Wright, S., 2006. Numerical optimization. Springer Science & Business Media. ",
                    "Citation Text": "Otto, S.E., Rowley, C.W., 2019. Linearly recurrent autoencoder networks for learning dynamics. SIAM Journal on\nApplied Dynamical Systems 18, 558\u2013593.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01378",
                        "Citation Paper Title": "Title:Linearly-Recurrent Autoencoder Networks for Learning Dynamics",
                        "Citation Paper Abstract": "Abstract:This paper describes a method for learning low-dimensional approximations of nonlinear dynamical systems, based on neural-network approximations of the underlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) provides a useful data-driven approximation of the Koopman operator for analyzing dynamical systems. This paper addresses a fundamental problem associated with EDMD: a trade-off between representational capacity of the dictionary and over-fitting due to insufficient data. A new neural network architecture combining an autoencoder with linear recurrent dynamics in the encoded state is used to learn a low-dimensional and highly informative Koopman-invariant subspace of observables. A method is also presented for balanced model reduction of over-specified EDMD systems in feature space. Nonlinear reconstruction using partially linear multi-kernel regression aims to improve reconstruction accuracy from the low-dimensional state when the data has complex but intrinsically low-dimensional structure. The techniques demonstrate the ability to identify Koopman eigenfunctions of the unforced Duffing equation, create accurate low-dimensional models of an unstable cylinder wake flow, and make short-time predictions of the chaotic Kuramoto-Sivashinsky equation.",
                        "Citation Paper Authors": "Authors:Samuel E. Otto, Clarence W. Rowley"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "Morton, J., Jameson, A., Kochenderfer, M.J., Witherden, F., 2018. Deep dynamical modeling and control of unsteady\n\ufb02uid \ufb02ows, in: Advances in Neural Information Processing Systems, pp. 9258\u20139268. ",
                    "Citation Text": "M\u00fcller, T., Evans, A., Schied, C., Keller, A., 2022. Instant neural graphics primitives with a multiresolution hash\nencoding. arXiv preprint arXiv:2201.05989 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.05989",
                        "Citation Paper Title": "Title:Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
                        "Citation Paper Abstract": "Abstract:Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.",
                        "Citation Paper Authors": "Authors:Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller"
                    }
                },
                {
                    "Sentence ID": 85,
                    "Sentence": "Moore, B., 1981. Principal component analysis in linear systems: Controllability, observability, and model reduction.\nIEEE Transactions on Automatic Control 26, 17\u201332. URL: https://www.scopus.com/inward/record.uri?eid=2-s2.\n0-0019533482&doi=10.1109%2fTAC.1981.1102568&partnerID=40&md5=23f83f786523f08268214845f6cb25c8 , doi: 10.\n1109/TAC.1981.1102568 . cited By 3526. ",
                    "Citation Text": "Morton, J., Jameson, A., Kochenderfer, M.J., Witherden, F., 2018. Deep dynamical modeling and control of unsteady\n\ufb02uid \ufb02ows, in: Advances in Neural Information Processing Systems, pp. 9258\u20139268.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07472",
                        "Citation Paper Title": "Title:Deep Dynamical Modeling and Control of Unsteady Fluid Flows",
                        "Citation Paper Abstract": "Abstract:The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.",
                        "Citation Paper Authors": "Authors:Jeremy Morton, Freddie D. Witherden, Antony Jameson, Mykel J. Kochenderfer"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "Maulik, R., Mohan, A., Lusch, B., Madireddy, S., Balaprakash, P., Livescu, D., 2020. Time-series learning of\nlatent-space dynamics for reduced-order model closure. Physica D: Nonlinear Phenomena 405, 132368. ",
                    "Citation Text": "Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A., 2019. Occupancy networks: Learning 3d\nreconstruction in function space, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 4460\u20134470.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03828",
                        "Citation Paper Title": "Title:Occupancy Networks: Learning 3D Reconstruction in Function Space",
                        "Citation Paper Abstract": "Abstract:With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Maulik, R., Lusch, B., Balaprakash, P., 2021. Reduced-ordermodelingofadvection-dominatedsystemswithrecurrent\nneural networks and convolutional autoencoders. Physics of Fluids 33, 037106. ",
                    "Citation Text": "Maulik, R., Mohan, A., Lusch, B., Madireddy, S., Balaprakash, P., Livescu, D., 2020. Time-series learning of\nlatent-space dynamics for reduced-order model closure. Physica D: Nonlinear Phenomena 405, 132368.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07815",
                        "Citation Paper Title": "Title:Time-series learning of latent-space dynamics for reduced-order model closure",
                        "Citation Paper Abstract": "Abstract:We study the performance of long short-term memory networks (LSTMs) and neural ordinary differential equations (NODEs) in learning latent-space representations of dynamical equations for an advection-dominated problem given by the viscous Burgers equation. Our formulation is devised in a non-intrusive manner with an equation-free evolution of dynamics in a reduced space with the latter being obtained through a proper orthogonal decomposition. In addition, we leverage the sequential nature of learning for both LSTMs and NODEs to demonstrate their capability for closure in systems which are not completely resolved in the reduced space. We assess our hypothesis for two advection-dominated problems given by the viscous Burgers equation. It is observed that both LSTMs and NODEs are able to reproduce the effects of the absent scales for our test cases more effectively than intrusive dynamics evolution through a Galerkin projection. This result empirically suggests that time-series learning techniques implicitly leverage a memory kernel for coarse-grained system closure as is suggested through the Mori-Zwanzig formalism.",
                        "Citation Paper Authors": "Authors:Romit Maulik, Arvind Mohan, Bethany Lusch, Sandeep Madireddy, Prasanna Balaprakash, Daniel Livescu"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "Mast, C.M., Arduino, P., Miller, G.R., Mackenzie-Helnwein, P., 2014. Avalanche and landslide simulation using the\nmaterial point method: \ufb02ow dynamics and force interaction with structures. Computational Geosciences 18, 817\u2013830. ",
                    "Citation Text": "Maulik, R., Lusch, B., Balaprakash, P., 2021. Reduced-ordermodelingofadvection-dominatedsystemswithrecurrent\nneural networks and convolutional autoencoders. Physics of Fluids 33, 037106.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.00470",
                        "Citation Paper Title": "Title:Reduced-order modeling of advection-dominated systems with recurrent neural networks and convolutional autoencoders",
                        "Citation Paper Abstract": "Abstract:A common strategy for the dimensionality reduction of nonlinear partial differential equations relies on the use of the proper orthogonal decomposition (POD) to identify a reduced subspace and the Galerkin projection for evolving dynamics in this reduced space. However, advection-dominated PDEs are represented poorly by this methodology since the process of truncation discards important interactions between higher-order modes during time evolution. In this study, we demonstrate that an encoding using convolutional autoencoders (CAEs) followed by a reduced-space time evolution by recurrent neural networks overcomes this limitation effectively. We demonstrate that a truncated system of only two latent-space dimensions can reproduce a sharp advecting shock profile for the viscous Burgers equation with very low viscosities, and a six-dimensional latent space can recreate the evolution of the inviscid shallow water equations. Additionally, the proposed framework is extended to a parametric reduced-order model by directly embedding parametric information into the latent space to detect trends in system evolution. Our results show that these advection-dominated systems are more amenable to low-dimensional encoding and time evolution by a CAE and recurrent neural network combination than the POD Galerkin technique.",
                        "Citation Paper Authors": "Authors:Romit Maulik, Bethany Lusch, Prasanna Balaprakash"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "Mainini, L., Willcox, K., 2015. Surrogate modeling approach to support real-time structural assessment and decision\nmaking. AIAA Journal 53, 1612\u20131626. ",
                    "Citation Text": "Martel, J.N., Lindell, D.B., Lin, C.Z., Chan, E.R., Monteiro, M., Wetzstein, G., 2021. ACORN: Adaptive coordinate\nnetworks for neural scene representation. arXiv preprint arXiv:2105.02788 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.02788",
                        "Citation Paper Title": "Title:ACORN: Adaptive Coordinate Networks for Neural Scene Representation",
                        "Citation Paper Abstract": "Abstract:Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",
                        "Citation Paper Authors": "Authors:Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C., 2020. Neural sparse voxel \ufb01elds. Advances in Neural\nInformation Processing Systems 33, 15651\u201315663. ",
                    "Citation Text": "Lusch, B., Kutz, J.N., Brunton, S.L., 2018. Deep learning for universal linear embeddings of nonlinear dynamics.\nNature communications 9, 4950.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09707",
                        "Citation Paper Title": "Title:Deep learning for universal linear embeddings of nonlinear dynamics",
                        "Citation Paper Abstract": "Abstract:Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.",
                        "Citation Paper Authors": "Authors:Bethany Lusch, J. Nathan Kutz, Steven L. Brunton"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "Lieu, T., Farhat, C., Lesoinne, M., 2006. Reduced-order \ufb02uid/structure modeling of a complete aircraft con\ufb01guration.\nComputer methods in applied mechanics and engineering 195, 5730\u20135742. ",
                    "Citation Text": "Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C., 2020. Neural sparse voxel \ufb01elds. Advances in Neural\nInformation Processing Systems 33, 15651\u201315663.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "Li, X., McWilliams, J., Li, M., Sung, C., Jiang, C., 2020. Soft hybrid aerial vehicle via bistable mechanism. arXiv\npreprint arXiv:2011.00426 . ",
                    "Citation Text": "Li, Y., Li, X., Li, M., Zhu, Y., Zhu, B., Jiang, C., 2021a. Lagrangian\u2013Eulerian multidensity topology optimization\nwith the material point method. International Journal for Numerical Methods in Engineering 122, 3400\u20133424.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01215",
                        "Citation Paper Title": "Title:Lagrangian-Eulerian Multi-Density Topology Optimization with the Material Point Method",
                        "Citation Paper Abstract": "Abstract:In this paper, a hybrid Lagrangian-Eulerian topology optimization (LETO) method is proposed to solve the elastic force equilibrium with the Material Point Method (MPM). LETO transfers density information from freely movable Lagrangian carrier particles to a fixed set of Eulerian quadrature points. This transfer is based on a smooth radial kernel involved in the compliance objective to avoid the artificial checkerboard pattern. The quadrature points act as MPM particles embedded in a lower-resolution grid and enable a sub-cell multi-density resolution of intricate structures with a reduced computational cost. A quadrature-level connectivity graph-based method is adopted to avoid the artificial checkerboard issues commonly existing in multi-resolution topology optimization methods. Numerical experiments are provided to demonstrate the efficacy of the proposed approach.",
                        "Citation Paper Authors": "Authors:Yue Li, Xuan Li, Minchen Li, Yixin Zhu, Bo Zhu, Chenfanfu Jiang"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "Li, S., Liu, W.K., 2002. Meshfree and particle methods and their applications. Appl. Mech. Rev. 55, 1\u201334. ",
                    "Citation Text": "Li, X., McWilliams, J., Li, M., Sung, C., Jiang, C., 2020. Soft hybrid aerial vehicle via bistable mechanism. arXiv\npreprint arXiv:2011.00426 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.00426",
                        "Citation Paper Title": "Title:Soft Hybrid Aerial Vehicle via Bistable Mechanism",
                        "Citation Paper Abstract": "Abstract:Unmanned aerial vehicles have been demonstrated successfully in a variety of tasks, including surveying and sampling tasks over large areas. These vehicles can take many forms. Quadrotors' agility and ability to hover makes them well suited for navigating potentially tight spaces, while fixed wing aircraft are capable of efficient flight over long distances. Hybrid aerial vehicles (HAVs) attempt to achieve both of these benefits by exhibiting multiple modes; however, morphing HAVs typically require extra actuators which add mass, reducing both agility and efficiency. We propose a morphing HAV with folding wings that exhibits both a quadrotor and a fixed wing mode without requiring any extra actuation. This is achieved by leveraging the motion of a bistable mechanism at the center of the aircraft to drive folding of the wing using only the existing motors and the inertia of the system. We optimize both the bistable mechanism and the folding wing using a topology optimization approach. The resulting mechanisms were fabricated on a 3D printer and attached to an existing quadrotor frame. Our prototype successfully transitions between both modes and our experiments demonstrate that the behavior of the fabricated prototype is consistent with that of the simulation.",
                        "Citation Paper Authors": "Authors:Xuan Li, Jessica McWilliams, Minchen Li, Cynthia Sung, Chenfanfu Jiang"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "Kashima, K., 2016. Nonlinear model reduction by deep autoencoder of noise response data, in: 2016 IEEE 55th\nConference on Decision and Control (CDC), pp. 5750\u20135755. doi: 10.1109/CDC.2016.7799153 . ",
                    "Citation Text": "Kim, B., Azevedo, V.C., Thuerey, N., Kim, T., Gross, M., Solenthaler, B., 2019. Deep \ufb02uids: A generative network\nfor parameterized \ufb02uid simulations, in: Computer Graphics Forum, Wiley Online Library. pp. 59\u201370.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.02071",
                        "Citation Paper Title": "Title:Deep Fluids: A Generative Network for Parameterized Fluid Simulations",
                        "Citation Paper Abstract": "Abstract:This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in-betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence-free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re-sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700x faster than re-simulating the data with the underlying CPU solver, while achieving compression rates of up to 1300x.",
                        "Citation Paper Authors": "Authors:Byungsoo Kim, Vinicius C. Azevedo, Nils Thuerey, Theodore Kim, Markus Gross, Barbara Solenthaler"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "Jiang, C., Schroeder, C., Teran, J., Stomakhin, A., Selle, A., 2016. The material point method for simulating\ncontinuum materials, in: ACM SIGGRAPH 2016 Courses, pp. 1\u201352. ",
                    "Citation Text": "Jiang, Y., Li, M., Jiang, C., Alonso-Marroquin, F., 2020. A hybrid material-point spheropolygon-element method for\nsolid and granular material interaction. International Journal for Numerical Methods in Engineering 121, 3021\u20133047.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13655",
                        "Citation Paper Title": "Title:A hybrid material-point spheropolygon-element method for solid and granular material interaction",
                        "Citation Paper Abstract": "Abstract:Capturing the interaction between objects that have an extreme difference in Young s modulus or geometrical scale is a highly challenging topic for numerical simulation. One of the fundamental questions is how to build an accurate multi-scale method with optimal computational efficiency. In this work, we develop a material-point-spheropolygon discrete element method (MPM-SDEM). Our approach fully couples the material point method (MPM) and the spheropolygon discrete element method (SDEM) through the exchange of contact force information. It combines the advantage of MPM for accurately simulating elastoplastic continuum materials and the high efficiency of DEM for calculating the Newtonian dynamics of discrete near-rigid objects. The MPM-SDEM framework is demonstrated with an explicit time integration scheme. Its accuracy and efficiency are further analysed against the analytical and experimental data. Results demonstrate this method could accurately capture the contact force and momentum exchange between materials while maintaining favourable computational stability and efficiency. Our framework exhibits great potential in the analysis of multi-scale, multi-physics phenomena",
                        "Citation Paper Authors": "Authors:Yupeng Jiang, Minchen Li, Chenfanfu Jiang, Fernando Alonso-marroquin"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Hu, Y., Li, T.M., Anderson, L., Ragan-Kelley, J., Durand, F., 2019b. Taichi: a language for high-performance\ncomputation on spatially sparse data structures. ACM Transactions on Graphics (TOG) 38, 1\u201316. ",
                    "Citation Text": "Hu, Y., Liu, J., Spielberg, A., Tenenbaum, J.B., Freeman, W.T., Wu, J., Rus, D., Matusik, W., 2019c. Chainqueen:\n32A real-time di\ufb00erentiable physical simulator for soft robotics, in: 2019 International conference on robotics and\nautomation (ICRA), IEEE. pp. 6265\u20136271.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.01054",
                        "Citation Paper Title": "Title:ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics",
                        "Citation Paper Abstract": "Abstract:Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and therefore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects including contact and can be seamlessly incorporated into inference, control and co-design systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of control tasks for soft robots, including problems with nearly 3,000 decision variables.",
                        "Citation Paper Authors": "Authors:Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B. Tenenbaum, William T. Freeman, Jiajun Wu, Daniela Rus, Wojciech Matusik"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "Holzapfel, G.A., 2002. Nonlinear solid mechanics: a continuum approach for engineering science. Meccanica 37,\n489\u2013490. ",
                    "Citation Text": "Hu, Y., Anderson, L., Li, T.M., Sun, Q., Carr, N., Ragan-Kelley, J., Durand, F., 2019a. Di\ufb00taichi: Di\ufb00erentiable\nprogramming for physical simulation. arXiv preprint arXiv:1910.00935 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.00935",
                        "Citation Paper Title": "Title:DiffTaichi: Differentiable Programming for Physical Simulation",
                        "Citation Paper Abstract": "Abstract:We present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using source code transformations that preserve arithmetic intensity and parallelism. A light-weight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation. We demonstrate the performance and productivity of our language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in our language is 4.2x shorter than the hand-engineered CUDA version yet runs as fast, and is 188x faster than the TensorFlow implementation. Using our differentiable programs, neural network controllers are typically optimized within only tens of iterations.",
                        "Citation Paper Authors": "Authors:Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, Fr\u00e9do Durand"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "Fei, Y., Guo, Q., Wu, R., Huang, L., Gao, M., 2021. Revisiting integration in the material point method: a scheme\nfor easier separation and less dissipation. ACM Transactions on Graphics (TOG) 40, 1\u201316. ",
                    "Citation Text": "Freno, B.A., Carlberg, K.T., 2019. Machine-learningerrormodelsforapproximatesolutions toparameterizedsystems\nof nonlinear equations. Computer Methods in Applied Mechanics and Engineering 348, 250\u2013296.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.02097",
                        "Citation Paper Title": "Title:Machine-learning error models for approximate solutions to parameterized systems of nonlinear equations",
                        "Citation Paper Abstract": "Abstract:This work proposes a machine-learning framework for constructing statistical models of errors incurred by approximate solutions to parameterized systems of nonlinear equations. These approximate solutions may arise from early termination of an iterative method, a lower-fidelity model, or a projection-based reduced-order model, for example. The proposed statistical model comprises the sum of a deterministic regression-function model and a stochastic noise model. The method constructs the regression-function model by applying regression techniques from machine learning (e.g., support vector regression, artificial neural networks) to map features (i.e., error indicators such as sampled elements of the residual) to a prediction of the approximate-solution error. The method constructs the noise model as a mean-zero Gaussian random variable whose variance is computed as the sample variance of the approximate-solution error on a test set; this variance can be interpreted as the epistemic uncertainty introduced by the approximate solution. This work considers a wide range of feature-engineering methods, data-set-construction techniques, and regression techniques that aim to ensure that (1) the features are cheaply computable, (2) the noise model exhibits low variance (i.e., low epistemic uncertainty introduced), and (3) the regression model generalizes to independent test data. Numerical experiments performed on several computational-mechanics problems and types of approximate solutions demonstrate the ability of the method to generate statistical models of the error that satisfy these criteria and significantly outperform more commonly adopted approaches for error modeling.",
                        "Citation Paper Authors": "Authors:Brian A. Freno, Kevin T. Carlberg"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "Drohmann, M., Haasdonk, B., Ohlberger, M., 2012. Reduced basis approximation for nonlinear parametrized evolu-\ntion equations based on empirical operator interpolation. SIAM Journal on Scienti\ufb01c Computing 34, A937\u2013A969. ",
                    "Citation Text": "Erichson, N.B., Muehlebach, M., Mahoney, M.W., 2019. Physics-informed autoencoders for lyapunov-stable \ufb02uid\n\ufb02ow prediction. arXiv preprint arXiv:1905.10866 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10866",
                        "Citation Paper Title": "Title:Physics-informed Autoencoders for Lyapunov-stable Fluid Flow Prediction",
                        "Citation Paper Abstract": "Abstract:In addition to providing high-profile successes in computer vision and natural language processing, neural networks also provide an emerging set of techniques for scientific problems. Such data-driven models, however, typically ignore physical insights from the scientific system under consideration. Among other things, a physics-informed model formulation should encode some degree of stability or robustness or well-conditioning (in that a small change of the input will not lead to drastic changes in the output), characteristic of the underlying scientific problem. We investigate whether it is possible to include physics-informed prior knowledge for improving the model quality (e.g., generalization performance, sensitivity to parameter tuning, or robustness in the presence of noisy data). To that extent, we focus on the stability of an equilibrium, one of the most basic properties a dynamic system can have, via the lens of Lyapunov analysis. For the prototypical problem of fluid flow prediction, we show that models preserving Lyapunov stability improve the generalization error and reduce the prediction uncertainty.",
                        "Citation Paper Authors": "Authors:N. Benjamin Erichson, Michael Muehlebach, Michael W. Mahoney"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "Daviet, G., Bertails-Descoubes, F., 2016. A semi-implicit material point method for the continuum simulation of\ngranular materials. ACM Transactions on Graphics (TOG) 35, 1\u201313. ",
                    "Citation Text": "Dosovitskiy, A., Tobias Springenberg, J., Brox, T., 2015. Learning to generate chairs with convolutional neural\nnetworks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1538\u20131546.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.5928",
                        "Citation Paper Title": "Title:Learning to Generate Chairs, Tables and Cars with Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Chen, P.Y., Chantharayukhonthorn, M., Yue, Y., Grinspun, E., Kamrin, K., 2021. Hybrid discrete-continuum\nmodeling of shear localization in granular media. Journal of the Mechanics and Physics of Solids 153, 104404. ",
                    "Citation Text": "Chen, Z., Zhang, H., 2019. Learning implicit \ufb01elds for generative shape modeling, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 5939\u20135948.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02822",
                        "Citation Paper Title": "Title:Learning Implicit Fields for Generative Shape Modeling",
                        "Citation Paper Abstract": "Abstract:We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiqin Chen, Hao Zhang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "Carlberg, K., Farhat, C., Cortial, J., Amsallem, D., 2013. The GNAT method for nonlinear model reduction: e\ufb00ective\nimplementation and application to computational \ufb02uid dynamics and turbulent \ufb02ows. Journal of Computational\nPhysics 242, 623\u2013647.\n31 ",
                    "Citation Text": "Carlberg, K., Tuminaro, R., Boggs, P., 2015. Preserving Lagrangian structure in nonlinear model reduction with\napplication to structural dynamics. SIAM Journal on Scienti\ufb01c Computing 37, B153\u2013B184.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1401.8044",
                        "Citation Paper Title": "Title:Preserving Lagrangian structure in nonlinear model reduction with application to structural dynamics",
                        "Citation Paper Abstract": "Abstract:This work proposes a model-reduction methodology that preserves Lagrangian structure (equivalently Hamiltonian structure) and achieves computational efficiency in the presence of high-order nonlinearities and arbitrary parameter dependence. As such, the resulting reduced-order model retains key properties such as energy conservation and symplectic time-evolution maps. We focus on parameterized simple mechanical systems subjected to Rayleigh damping and external forces, and consider an application to nonlinear structural dynamics. To preserve structure, the method first approximates the system's `Lagrangian ingredients'---the Riemannian metric, the potential-energy function, the dissipation function, and the external force---and subsequently derives reduced-order equations of motion by applying the (forced) Euler--Lagrange equation with these quantities. From the algebraic perspective, key contributions include two efficient techniques for approximating parameterized reduced matrices while preserving symmetry and positive definiteness: matrix gappy POD and reduced-basis sparsification (RBS). Results for a parameterized truss-structure problem demonstrate the importance of preserving Lagrangian structure and illustrate the proposed method's merits: it reduces computation time while maintaining high accuracy and stability, in contrast to existing nonlinear model-reduction techniques that do not preserve structure.",
                        "Citation Paper Authors": "Authors:Kevin Carlberg, Ray Tuminaro, Paul Boggs"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Carlberg, K., Farhat, C., 2011. A low-cost, goal-oriented \u2018compact proper orthogonal decomposition\u2019basis for model\nreduction of static systems. International Journal for Numerical Methods in Engineering 86, 381\u2013402. ",
                    "Citation Text": "Carlberg, K., Farhat, C., Cortial, J., Amsallem, D., 2013. The GNAT method for nonlinear model reduction: e\ufb00ective\nimplementation and application to computational \ufb02uid dynamics and turbulent \ufb02ows. Journal of Computational\nPhysics 242, 623\u2013647.\n31",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.1349",
                        "Citation Paper Title": "Title:The GNAT method for nonlinear model reduction: effective implementation and application to computational fluid dynamics and turbulent flows",
                        "Citation Paper Abstract": "Abstract:The Gauss--Newton with approximated tensors (GNAT) method is a nonlinear model reduction method that operates on fully discretized computational models. It achieves dimension reduction by a Petrov--Galerkin projection associated with residual minimization; it delivers computational efficency by a hyper-reduction procedure based on the `gappy POD' technique. Originally presented in Ref. [1], where it was applied to implicit nonlinear structural-dynamics models, this method is further developed here and applied to the solution of a benchmark turbulent viscous flow problem. To begin, this paper develops global state-space error bounds that justify the method's design and highlight its advantages in terms of minimizing components of these error bounds. Next, the paper introduces a `sample mesh' concept that enables a distributed, computationally efficient implementation of the GNAT method in finite-volume-based computational-fluid-dynamics (CFD) codes. The suitability of GNAT for parameterized problems is highlighted with the solution of an academic problem featuring moving discontinuities. Finally, the capability of this method to reduce by orders of magnitude the core-hours required for large-scale CFD computations, while preserving accuracy, is demonstrated with the simulation of turbulent flow over the Ahmed body. For an instance of this benchmark problem with over 17 million degrees of freedom, GNAT outperforms several other nonlinear model-reduction methods, reduces the required computational resources by more than two orders of magnitude, and delivers a solution that differs by less than 1% from its high-dimensional counterpart.",
                        "Citation Paper Authors": "Authors:Kevin Carlberg, Charbel Farhat, Julien Cortial, David Amsallem"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "Carlberg, K., 2015. Adaptive h-re\ufb01nement for reduced-order models. International Journal for Numerical Methods\nin Engineering 102, 1192\u20131210. ",
                    "Citation Text": "Carlberg, K., Barone, M., Antil, H., 2017. Galerkin v. least-squares Petrov\u2013Galerkin projection in nonlinear model\nreduction. Journal of Computational Physics 330, 693\u2013734.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.03749",
                        "Citation Paper Title": "Title:Galerkin v. least-squares Petrov--Galerkin projection in nonlinear model reduction",
                        "Citation Paper Abstract": "Abstract:Least-squares Petrov--Galerkin (LSPG) model-reduction techniques such as the Gauss--Newton with Approximated Tensors (GNAT) method have shown promise, as they have generated stable, accurate solutions for large-scale turbulent, compressible flow problems where standard Galerkin techniques have failed. However, there has been limited comparative analysis of the two approaches. This is due in part to difficulties arising from the fact that Galerkin techniques perform optimal projection associated with residual minimization at the time-continuous level, while LSPG techniques do so at the time-discrete level. This work provides a detailed theoretical and computational comparison of the two techniques for two common classes of time integrators: linear multistep schemes and Runge--Kutta schemes. We present a number of new findings, including conditions under which the LSPG ROM has a time-continuous representation, conditions under which the two techniques are equivalent, and time-discrete error bounds for the two approaches. Perhaps most surprisingly, we demonstrate both theoretically and computationally that decreasing the time step does not necessarily decrease the error for the LSPG ROM; instead, the time step should be `matched' to the spectral content of the reduced basis. In numerical experiments carried out on a turbulent compressible-flow problem with over one million unknowns, we show that increasing the time step to an intermediate value decreases both the error and the simulation time of the LSPG reduced-order model by an order of magnitude.",
                        "Citation Paper Authors": "Authors:Kevin Carlberg, Matthew Barone, Harbir Antil"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.01146v2": {
            "Paper Title": "Generalized Spectral Coarsening",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12924v3": {
            "Paper Title": "Joint stereo 3D object detection and implicit surface reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "points. Not using masks leads to inferior performances. We\nbelieve such background points can introduce noise to the Ha\nMethodAOS whenAP2D= 100 :00\nEasy Moderate Hard\nEgo-Net ",
                    "Citation Text": "S. Li, Z. Yan, H. Li, and K.-T. Cheng, \u201cExploring intermediate repre-\nsentation for monocular vehicle pose estimation,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 1873\u20131883.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.08464",
                        "Citation Paper Title": "Title:Exploring intermediate representation for monocular vehicle pose estimation",
                        "Citation Paper Abstract": "Abstract:We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.10463v3": {
            "Paper Title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with\n  Fine-Grain Utilization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.07737v2": {
            "Paper Title": "EGC2: Enhanced Graph Classification with Easy Graph Compression",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed a reinforcement learning-based attack approach and validated that\nGNNs are vulnerable to such attacks in graph classi\ufb01cation tasks by learning a generalized attack policy.\nTang et al. ",
                    "Citation Text": "Tang, H., Ma, G., Chen, Y ., Guo, L., Wang, W., Zeng, B., Zhan, L., 2020. Adversarial attack on\nhierarchical graph pooling neural networks. arXiv preprint arXiv:2005.11560 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11560",
                        "Citation Paper Title": "Title:Adversarial Attack on Hierarchical Graph Pooling Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recent years have witnessed the emergence and development of graph neural networks (GNNs), which have been shown as a powerful approach for graph representation learning in many tasks, such as node classification and graph classification. The research on the robustness of these models has also started to attract attentions in the machine learning field. However, most of the existing work in this area focus on the GNNs for node-level tasks, while little work has been done to study the robustness of the GNNs for the graph classification task. In this paper, we aim to explore the vulnerability of the Hierarchical Graph Pooling (HGP) Neural Networks, which are advanced GNNs that perform very well in the graph classification in terms of prediction accuracy. We propose an adversarial attack framework for this task. Specifically, we design a surrogate model that consists of convolutional and pooling operators to generate adversarial samples to fool the hierarchical GNN-based graph classification models. We set the preserved nodes by the pooling operator as our attack targets, and then we perturb the attack targets slightly to fool the pooling operator in hierarchical GNNs so that they will select the wrong nodes to preserve. We show the adversarial samples generated from multiple datasets by our surrogate model have enough transferability to attack current state-of-art graph classification models. Furthermore, we conduct the robust train on the target models and demonstrate that the retrained graph classification models are able to better defend against the attack from the adversarial samples. To the best of our knowledge, this is the first work on the adversarial attack against hierarchical GNN-based graph classification models.",
                        "Citation Paper Authors": "Authors:Haoteng Tang, Guixiang Ma, Yurong Chen, Lei Guo, Wei Wang, Bo Zeng, Liang Zhan"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "sorts the\ngraph vertices in descending order, then uses the sorted embedding to train traditional neural networks.\nHierarchical graph pooling methods learn hierarchical graph representations to capture the graph struc-\nture information. Ying et al. ",
                    "Citation Text": "Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., Leskovec, J., 2018. Hierarchical graph\nrepresentation learning with differentiable pooling. Advances in neural information processing\nsystems 31, 4805\u20134815.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08804",
                        "Citation Paper Title": "Title:Hierarchical Graph Representation Learning with Differentiable Pooling",
                        "Citation Paper Abstract": "Abstract:Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",
                        "Citation Paper Authors": "Authors:Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "introduced two strategies to enhance\nexisting GNN frameworks, that is, topological information enrichment through clustering coef\ufb01cients\nand structural redesign of the network through the addition of dense layers. UGformer, proposed by\nNguyen et al. ",
                    "Citation Text": "Nguyen, D.Q., Nguyen, T.D., Phung, D., 2022b. Universal graph transformer self-attention net-\nworks, in: Companion Proceedings of the Web Conference 2022, pp. 193\u2013196.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11855",
                        "Citation Paper Title": "Title:Universal Graph Transformer Self-Attention Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a transformer-based GNN model, named UGformer, to learn graph representations. In particular, we present two UGformer variants, wherein the first variant (publicized in September 2019) is to leverage the transformer on a set of sampled neighbors for each input node, while the second (publicized in May 2021) is to leverage the transformer on all input nodes. Experimental results demonstrate that the first UGformer variant achieves state-of-the-art accuracies on benchmark datasets for graph classification in both inductive setting and unsupervised transductive setting; and the second UGformer variant obtains state-of-the-art accuracies for inductive text classification. The code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "extracts features through the\nWL test of isomorphism on graphs. For persistence summaries, Zhao et al. ",
                    "Citation Text": "Zhao, Q., Wang, Y ., 2019. Learning metrics for persistence-based summaries and applications for\ngraph classi\ufb01cation. Advances in Neural Information Processing Systems 32, 9855\u20139866.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12189",
                        "Citation Paper Title": "Title:Learning metrics for persistence-based summaries and applications for graph classification",
                        "Citation Paper Abstract": "Abstract:Recently a new feature representation and data analysis methodology based on a topological tool called persistent homology (and its corresponding persistence diagram summary) has started to attract momentum. A series of methods have been developed to map a persistence diagram to a vector representation so as to facilitate the downstream use of machine learning tools, and in these approaches, the importance (weight) of different persistence features are often preset. However often in practice, the choice of the weight function should depend on the nature of the specific type of data one considers, and it is thus highly desirable to learn a best weight function (and thus metric for persistence diagrams) from labelled data. We study this problem and develop a new weighted kernel, called WKPI, for persistence summaries, as well as an optimization framework to learn a good metric for persistence summaries. Both our kernel and optimization problem have nice properties. We further apply the learned kernel to the challenging task of graph classification, and show that our WKPI-based classification framework obtains similar or (sometimes significantly) better results than the best results from a range of previous graph classification frameworks on a collection of benchmark datasets.",
                        "Citation Paper Authors": "Authors:Qi Zhao, Yusu Wang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "conducted\nadversarial training on target HGNNs by generating adversarial examples, thus improving the robustness\nof the HGNNs. Gao et al. ",
                    "Citation Text": "Gao, Z., Hu, R., Gong, Y ., 2020. Certi\ufb01ed robustness of graph classi\ufb01cation against topology attack\nwith randomized smoothing, in: IEEE Global Communications Conference, GLOBECOM 2020,\nVirtual Event, Taiwan, December 7-11, 2020, IEEE. pp. 1\u20136.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.05872",
                        "Citation Paper Title": "Title:Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:Graph classification has practical applications in diverse fields. Recent studies show that graph-based machine learning models are especially vulnerable to adversarial perturbations due to the non i.i.d nature of graph data. By adding or deleting a small number of edges in the graph, adversaries could greatly change the graph label predicted by a graph classification model. In this work, we propose to build a smoothed graph classification model with certified robustness guarantee. We have proven that the resulting graph classification model would output the same prediction for a graph under $l_0$ bounded adversarial perturbation. We also evaluate the effectiveness of our approach under graph convolutional network (GCN) based multi-class graph classification model.",
                        "Citation Paper Authors": "Authors:Zhidong Gao, Rui Hu, Yanmin Gong"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". They obtained graph-level embeddings through maximum pooling or average pooling to\nachieve graph-classi\ufb01cation tasks. Furthermore, Ullah et al. ",
                    "Citation Text": "Ullah, I., Manzo, M., Shah, M., Madden, M.G., 2022. Graph convolutional networks: analysis,\nimprovements and results. Applied Intelligence 52, 9033\u20139044.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.09592",
                        "Citation Paper Title": "Title:Graph Convolutional Networks: analysis, improvements and results",
                        "Citation Paper Abstract": "Abstract:In the current era of neural networks and big data, higher dimensional data is processed for automation of different application areas. Graphs represent a complex data organization in which dependencies between more than one object or activity occur. Due to the high dimensionality, this data creates challenges for machine learning algorithms. Graph convolutional networks were introduced to utilize the convolutional models concepts that shows good results. In this context, we enhanced two of the existing Graph convolutional network models by proposing four enhancements. These changes includes: hyper parameters optimization, convex combination of activation functions, topological information enrichment through clustering coefficients measure, and structural redesign of the network through addition of dense layers. We present extensive results on four state-of-art benchmark datasets. The performance is notable not only in terms of lesser computational cost compared to competitors, but also achieved competitive results for three of the datasets and state-of-the-art for the fourth dataset.",
                        "Citation Paper Authors": "Authors:Ihsan Ullah, Mario Manzo, Mitul Shah, Michael Madden"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.12564v3": {
            "Paper Title": "Spotlights: Probing Shapes from Spherical Viewpoints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10689v3": {
            "Paper Title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\n  Multi-view Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ": IDR can reconstruct\nsurface with high quality but requires foreground masks as supervision; Since IDR has demonstrated\nsuperior quality compared to another surface rendering based method \u2013 DVR ",
                    "Citation Text": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volu-\nmetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3504\u20133515,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.07372",
                        "Citation Paper Title": "Title:Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". We further tested\non 7 challenging scenes from the low-res set of the BlendedMVS dataset ",
                    "Citation Text": "Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long\nQuan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n1790\u20131799, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.10127",
                        "Citation Paper Title": "Title:BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks",
                        "Citation Paper Abstract": "Abstract:While deep learning has recently achieved great success on multi-view stereo (MVS), limited training data makes the trained model hard to be generalized to unseen scenarios. Compared with other computer vision tasks, it is rather difficult to collect a large-scale MVS dataset as it requires expensive active scanners and labor-intensive process to obtain ground truth 3D structures. In this paper, we introduce BlendedMVS, a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. To create the dataset, we apply a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, we render these mesh models to color images and depth maps. To introduce the ambient lighting information during training, the rendered color images are further blended with the input images to generate the training input. Our dataset contains over 17k high-resolution images covering a variety of scenes, including cities, architectures, sculptures and small objects. Extensive experiments demonstrate that BlendedMVS endows the trained model with significantly better generalization ability compared with other MVS datasets. The dataset and pretrained models are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.06642v3": {
            "Paper Title": "What's in a Decade? Transforming Faces Through Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13971v2": {
            "Paper Title": "3D Neural Sculpting (3DNS): Editing Neural Signed Distance Functions",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "by discretizing the input point\ncloud. State-of-the-art results have been attained by cou-\npling neural networks with data structures that retain lo-\ncalized spatial information. Octrees, which store learnable\nweights are used in ",
                    "Citation Text": "R. Chabra, J. E. Lenssen, E. Ilg, T. Schmidt, J.\nStraub, S. Lovegrove, and R. Newcombe, Deep lo-\ncal shapes: Learning local sdf priors for detailed\n3d reconstruction , 2020. arXiv: 2003 . 10983\n[cs.CV] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10983",
                        "Citation Paper Title": "Title:Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction",
                        "Citation Paper Abstract": "Abstract:Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.",
                        "Citation Paper Authors": "Authors:Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "extends this method to utilize\ninformation about the normal vectors of the surface. The\nauthors of ",
                    "Citation Text": "M. Atzmon, N. Haim, L. Yariv, O. Israelov, H.\nMaron, and Y . Lipman, Controlling neural level sets ,\n2019. arXiv: 1905.11911 [cs.LG] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.11911",
                        "Citation Paper Title": "Title:Controlling Neural Level Sets",
                        "Citation Paper Abstract": "Abstract:The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning.\nIn this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest.\nWe have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "use points sampled on an implicit surface to control\nits shape.\nRecently, the use of a neural network as the function ex-\npressing the surface was proposed in by three concurrent\nworks ",
                    "Citation Text": "J. J. Park, P. Florence, J. Straub, R. Newcombe, and\nS. Lovegrove, Deepsdf: Learning continuous signed\ndistance functions for shape representation , 2019.\narXiv: 1901.05103 [cs.CV] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ", for which the aforemen-\ntioned property has been theoretically explained ",
                    "Citation Text": "M. Tancik, P. P. Srinivasan, B. Mildenhall, S.\nFridovich-Keil, N. Raghavan, U. Singhal, R. Ra-\nmamoorthi, J. T. Barron, and R. Ng, Fourier features\nlet networks learn high frequency functions in low\ndimensional domains , 2020. arXiv: 2006.10739\n[cs.CV] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10739",
                        "Citation Paper Title": "Title:Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
                        "Citation Paper Abstract": "Abstract:We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "explores these representations, to which\nthe authors refer as Neural Fields, in depth.\n2.2. Neural SDF Editability\nThe research on editability is quite limited. DualSDF ",
                    "Citation Text": "Z. Hao, H. Averbuch-Elor, N. Snavely, and S. Be-\nlongie, \u201cDualsdf: Semantic shape manipulation using\na two-level representation,\u201d pp. 7631\u20137641, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.02869",
                        "Citation Paper Title": "Title:DualSDF: Semantic Shape Manipulation using a Two-Level Representation",
                        "Citation Paper Abstract": "Abstract:We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",
                        "Citation Paper Authors": "Authors:Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "produces highly realistic\nimages by expressing the radiance and density of a scene.\nA recent survey ",
                    "Citation Text": "Y . Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N.\nKhan, F. Tombari, J. Tompkin, V . Sitzmann, and S.\nSridhar, Neural fields in visual computing and be-\nyond , 2021. eprint: arXiv : 2111 . 11426 . [On-\nline]. Available: https://neuralfields.cs.\nbrown.edu/ .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "achieve\ninteractive framerates, as well.\nBesides training using raw geometric data like point\nclouds and meshes, there have been efforts to train neu-\nral SDFs directly from images ",
                    "Citation Text": "S. P. Bangaru, M. Gharbi, T.-M. Li, F. Luan, K.\nSunkavalli, M. Ha \u02c7san, S. Bi, Z. Xu, G. Bernstein,\nand F. Durand, Differentiable rendering of neural\nsdfs through reparameterization , 2022. DOI:10 .\n48550/ARXIV.2206.05344 . [Online]. Avail-\nable: https : / / arxiv . org / abs / 2206 .\n05344 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.05344",
                        "Citation Paper Title": "Title:Differentiable Rendering of Neural SDFs through Reparameterization",
                        "Citation Paper Abstract": "Abstract:We present a method to automatically compute correct gradients with respect to geometric scene parameters in neural SDF renderers. Recent physically-based differentiable rendering techniques for meshes have used edge-sampling to handle discontinuities, particularly at object silhouettes, but SDFs do not have a simple parametric form amenable to sampling. Instead, our approach builds on area-sampling techniques and develops a continuous warping function for SDFs to account for these discontinuities. Our method leverages the distance to surface encoded in an SDF and uses quadrature on sphere tracer points to compute this warping function. We further show that this can be done by subsampling the points to make the method tractable for neural SDFs. Our differentiable renderer can be used to optimize neural shapes from multi-view images and produces comparable 3D reconstructions to recent SDF-based inverse rendering methods, without the need for 2D segmentation masks to guide the geometry optimization and no volumetric approximations to the geometry.",
                        "Citation Paper Authors": "Authors:Sai Praveen Bangaru, Micha\u00ebl Gharbi, Tzu-Mao Li, Fujun Luan, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Sai Bi, Zexiang Xu, Gilbert Bernstein, Fr\u00e9do Durand"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", which uses sines instead of the\nusual RELUs, presented promising results in a variety of\ntasks including surface reconstruction. Convolutional net-\nworks are used in ",
                    "Citation Text": "J. Chibane, T. Alldieck, and G. Pons-Moll, Implicit\nfunctions in feature space for 3d shape reconstruc-\ntion and completion , 2020. arXiv: 2003 . 01456\n[cs.CV] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01456",
                        "Citation Paper Title": "Title:Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion",
                        "Citation Paper Abstract": "Abstract:While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",
                        "Citation Paper Authors": "Authors:Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05057v2": {
            "Paper Title": "HoloBeam: Paper-Thin Near-Eye Displays",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10699v2": {
            "Paper Title": "PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "and Bonsai, Kitchen\nand Room from the 360-degree Mip-NeRF360 dataset ",
                    "Citation Text": "Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance \ufb01elds. CVPR , 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12077",
                        "Citation Paper Title": "Title:Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "with learning rate set to 0:01,\nwhich takes no more than 2 hours in total.\nDatasets. We conduct experiments on scenes from three\nsources: Lego, Ficus, Ship and Hotdog from the NeRF\nBlender dataset ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nrepresenting scenes as neural radiance \ufb01elds for view synthe-\nsis.Commun. ACM , 65(1):99\u2013106, 2022. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "that learns a 3D feature \ufb01eld from the seman-\ntic feature maps predicted from state-of-the-art image-based\nsegmentation models (e.g., Lang-Seg ",
                    "Citation Text": "Boyi Li, Kilian Q. Weinberger, Serge J. Belongie, Vladlen\nKoltun, and Ren \u00b4e Ranftl. Language-driven semantic seg-\nmentation. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022 . OpenReview.net, 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.03546",
                        "Citation Paper Title": "Title:Language-driven Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., \"grass\" or \"building\") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at this https URL.",
                        "Citation Paper Authors": "Authors:Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, Ren\u00e9 Ranftl"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.14784v2": {
            "Paper Title": "Neural Volumetric Blendshapes: Computationally Efficient Physics-Based\n  Facial Blendshapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07245v2": {
            "Paper Title": "Autoencoder-Aided Visualization of Collections of Morse Complexes",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "created a new kind of neural network layer to embed persistence\ndiagrams into a learned latent space for further computation. And Hu\net al. ",
                    "Citation Text": "X. Hu, Y . Wang, L. Fuxin, D. Samaras, and C. Chen. Topology-\naware segmentation using discrete Morse theory. arXiv preprint\narXiv:2103.09992 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.09992",
                        "Citation Paper Title": "Title:Topology-Aware Segmentation Using Discrete Morse Theory",
                        "Citation Paper Abstract": "Abstract:In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.",
                        "Citation Paper Authors": "Authors:Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, Chao Chen"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "is even more\ndirect; they train a neural network to predict the winding number of\na curve (i.e. how many times it encircles the origin). Other studies,\nhowever, focus on using topological features internally. Hofer et al. ",
                    "Citation Text": "C. Hofer, R. Kwitt, M. Niethammer, and A. Uhl. Deep learning with\ntopological signatures. arXiv preprint arXiv:1707.04041 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.04041",
                        "Citation Paper Title": "Title:Deep Learning with Topological Signatures",
                        "Citation Paper Abstract": "Abstract:Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.",
                        "Citation Paper Authors": "Authors:Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ",\nthe authors train a support vector machine to predict persistence\ndiagrams from 3D shape datasets. Zhang et al. ",
                    "Citation Text": "P. Zhang, H. Shen, and H. Zhai. Machine learning topological invariants\nwith neural networks. Physical review letters , 120(6):066401, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.09401",
                        "Citation Paper Title": "Title:Machine Learning Topological Invariants with Neural Networks",
                        "Citation Paper Abstract": "Abstract:In this Letter we supervisedly train neural networks to distinguish different topological phases in the context of topological band insulators. After training with Hamiltonians of one-dimensional insulators with chiral symmetry, the neural network can predict their topological winding numbers with nearly 100% accuracy, even for Hamiltonians with larger winding numbers that are not included in the training data. These results show a remarkable success that the neural network can capture the global and nonlinear topological features of quantum phases from local inputs. By opening up the neural network, we confirm that the network does learn the discrete version of the winding number formula. We also make a couple of remarks regarding the role of the symmetry and the opposite effect of regularization techniques when applying machine learning to physical systems.",
                        "Citation Paper Authors": "Authors:Pengfei Zhang, Huitao Shen, Hui Zhai"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "have proposed variants in the simplest case of merge trees\nthat sacri\ufb01ce theoretical guarantees to produce more tractable com-\nputations. Recently, Yan et al. ",
                    "Citation Text": "L. Yan, Y . Wang, E. Munch, E. Gasparovic, and B. Wang. A structural\naverage of labeled merge trees for uncertainty visualization. IEEE\nTrans. Vis. Comput. Graph. , 26(1):832\u2013842, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00113",
                        "Citation Paper Title": "Title:A Structural Average of Labeled Merge Trees for Uncertainty Visualization",
                        "Citation Paper Abstract": "Abstract:Physical phenomena in science and engineering are frequently modeled using scalar fields. In scalar field topology, graph-based topological descriptors such as merge trees, contour trees, and Reeb graphs are commonly used to characterize topological changes in the (sub)level sets of scalar fields. One of the biggest challenges and opportunities to advance topology-based visualization is to understand and incorporate uncertainty into such topological descriptors to effectively reason about their underlying data. In this paper, we study a structural average of a set of labeled merge trees and use it to encode uncertainty in data. Specifically, we compute a 1-center tree that minimizes its maximum distance to any other tree in the set under a well-defined metric called the interleaving distance. We provide heuristic strategies that compute structural averages of merge trees whose labels do not fully agree. We further provide an interactive visualization system that resembles a numerical calculator that takes as input a set of merge trees and outputs a tree as their structural average. We also highlight structural similarities between the input and the average and incorporate uncertainty information for visual exploration. We develop a novel measure of uncertainty, referred to as consistency, via a metric-space view of the input trees. Finally, we demonstrate an application of our framework through merge trees that arise from ensembles of scalar fields. Our work is the first to employ interleaving distances and consistency to study a global, mathematically rigorous, structural average of merge trees in the context of uncertainty visualization.",
                        "Citation Paper Authors": "Authors:Lin Yan, Yusu Wang, Elizabeth Munch, Ellen Gasparovic, Bei Wang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". More generally, distance measures on level set topology\nhave been de\ufb01ned (see Bollen et al. ",
                    "Citation Text": "B. Bollen, E. W. Chambers, J. A. Levine, and E. Munch. Reeb graph\nmetrics from the ground up. CoRR , abs/2110.05631, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.05631",
                        "Citation Paper Title": "Title:Reeb Graph Metrics from the Ground Up",
                        "Citation Paper Abstract": "Abstract:The Reeb graph has been utilized in various applications including the analysis of scalar fields. Recently, research has been focused on using topological signatures such as the Reeb graph to compare multiple scalar fields by defining distance metrics on the topological signatures themselves. Here we survey five existing metrics that have been defined on Reeb graphs: the bottleneck distance, the interleaving distance, functional distortion distance, the Reeb graph edit distance, and the universal edit distance. Our goal is to (1) provide definitions and concrete examples of these distances in order to develop the intuition of the reader, (2) visit previously proven results of stability, universality, and discriminativity, (3) identify and complete any remaining properties which have only been proven (or disproven) for a subset of these metrics, (4) expand the taxonomy of the bottleneck distance to better distinguish between variations which have been commonly miscited, and (5) reconcile the various definitions and requirements on the underlying spaces for these metrics to be defined and properties to be proven.",
                        "Citation Paper Authors": "Authors:Brian Bollen, Erin Chambers, Joshua A. Levine, Elizabeth Munch"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "distances have been well-studied. Computing these distances re-\nquires a matching between points, which may be infeasible at scale,\nand thus authors in visualization have pioneered methods to compute\ndistances progressively ",
                    "Citation Text": "M. Soler, M. Plainchault, B. Conche, and J. Tierny. Lifted Wasserstein\nmatcher for fast and robust topology tracking. In 2018 IEEE 8th\nSymposium on Large Data Analysis and Visualization (LDAV) , pp.\n23\u201333. IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05870",
                        "Citation Paper Title": "Title:Lifted Wasserstein Matcher for Fast and Robust Topology Tracking",
                        "Citation Paper Abstract": "Abstract:This paper presents a robust and efficient method for tracking topological features in time-varying scalar data. Structures are tracked based on the optimal matching between persistence diagrams with respect to the Wasserstein metric. This fundamentally relies on solving the assignment problem, a special case of optimal transport, for all consecutive timesteps. Our approach relies on two main contributions. First, we revisit the seminal assignment algorithm by Kuhn and Munkres which we specifically adapt to the problem of matching persistence diagrams in an efficient way. Second, we propose an extension of the Wasserstein metric that significantly improves the geometrical stability of the matching of domain-embedded persistence pairs. We show that this geometrical lifting has the additional positive side-effect of improving the assignment matrix sparsity and therefore computing time. The global framework implements a coarse-grained parallelism by computing persistence diagrams and finding optimal matchings in parallel for every couple of consecutive timesteps. Critical trajectories are constructed by associating successively matched persistence pairs over time. Merging and splitting events are detected with a geometrical threshold in a post-processing stage. Extensive experiments on real-life datasets show that our matching approach is an order of magnitude faster than the seminal Munkres algorithm. Moreover, compared to a modern approximation method, our method provides competitive runtimes while yielding exact results. We demonstrate the utility of our global framework by extracting critical point trajectories from various simulated time-varying datasets and compare it to the existing methods based on associated overlaps of volumes. Robustness to noise and temporal resolution downsampling is empirically demonstrated.",
                        "Citation Paper Authors": "Authors:Maxime Soler, M\u00e9lanie Plainchault, Bruno Conche, Julien Tierny"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.09800v2": {
            "Paper Title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ").\nFurthermore, our method\u2019s ability to generalize to new edits\nand make correct associations between visual changes and\ntext instructions is limited by the human-written instruc-\ntions used to \ufb01ne-tune GPT-3 ",
                    "Citation Text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems , 33:1877\u20131901, 2020. 1, 2, 3,\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.07363v3": {
            "Paper Title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13932v2": {
            "Paper Title": "Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for\n  Scalar Data -- An Algorithm and A Benchmark",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", recently revisited for general cell complexes in higher\ndimensions ",
                    "Citation Text": "A. Garin, T. Heiss, K. Maggs, B. Bleile, and V . Robins. Duality in\npersistent homology of images. In SoCG , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04597",
                        "Citation Paper Title": "Title:Duality in Persistent Homology of Images",
                        "Citation Paper Abstract": "Abstract:We derive the relationship between the persistent homology barcodes of two dual filtered CW complexes. Applied to greyscale digital images, we obtain an algorithm to convert barcodes between the two different (dual) topological models of pixel connectivity.",
                        "Citation Paper Authors": "Authors:Ad\u00e9lie Garin, Teresa Heiss, Kelly Maggs, Bea Bleile, Vanessa Robins"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Among the above techniques, severalJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, MAY 2021 3\ndocumented accelerations share some conceptual similari-\nties with our approach. Speci\ufb01cally, Bauer et al. ",
                    "Citation Text": "U. Bauer, M. Kerber, and J. Reininghaus. Clear and Compress:\nComputing Persistent Homology in Chunks. In TopoInVis , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1303.0477",
                        "Citation Paper Title": "Title:Clear and Compress: Computing Persistent Homology in Chunks",
                        "Citation Paper Abstract": "Abstract:We present a parallelizable algorithm for computing the persistent homology of a filtered chain complex. Our approach differs from the commonly used reduction algorithm by first computing persistence pairs within local chunks, then simplifying the unpaired columns, and finally applying standard reduction on the simplified matrix. The approach generalizes a technique by G\u00fcnther et al., which uses discrete Morse Theory to compute persistence; we derive the same worst-case complexity bound in a more general context. The algorithm employs several practical optimization techniques which are of independent interest. Our sequential implementation of the algorithm is competitive with state-of-the-art methods, and we improve the performance through parallelized computation.",
                        "Citation Paper Authors": "Authors:Ulrich Bauer, Michael Kerber, Jan Reininghaus"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": ". In many applications\ninvolving data analysis, topological persistence quickly es-\ntablished itself as an appealing importance measure that\nhelps distinguish salient topological structures in the data.\nIn data visualization, except a few approaches dealing\nwith graph layouts ",
                    "Citation Text": "A. Suh, M. Hajij, B. Wang, C. Scheidegger, and P . A. Rosen.\nPersistent homology guided force-directed graph layouts. TVCG ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05548",
                        "Citation Paper Title": "Title:Persistent Homology Guided Force-Directed Graph Layouts",
                        "Citation Paper Abstract": "Abstract:Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets.",
                        "Citation Paper Authors": "Authors:Ashley Suh, Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.14503v2": {
            "Paper Title": "Parallel Compositing of Volumetric Depth Images for Interactive\n  Visualization of Distributed Volumes at High Frame Rates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.14314v3": {
            "Paper Title": "Generative Neural Articulated Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ", fully implicit networks [ 3\u20137], or a combination of low-\nresolution voxel grids combined with 2D CNN-based image upsampling layers [ 114,115]. Our 3D\nGAN architecture is most closely related to the recent work by Chan et al. ",
                    "Citation Text": "Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Ef\ufb01cient\ngeometry-aware 3D generative adversarial networks. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07945",
                        "Citation Paper Title": "Title:Efficient Geometry-aware 3D Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.",
                        "Citation Paper Authors": "Authors:Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 128,
                    "Sentence": "head model and apply\nthe state-of-the-art facial reconstruction method DECA ",
                    "Citation Text": "Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an animatable detailed 3D face\nmodel from in-the-wild images. ACM Transactions on Graphics (SIGGRAPH) , 40(4):1\u201313, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04012",
                        "Citation Paper Title": "Title:Learning an Animatable Detailed 3D Face Model from In-The-Wild Images",
                        "Citation Paper Abstract": "Abstract:While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Yao Feng, Haiwen Feng, Michael J. Black, Timo Bolkart"
                    }
                },
                {
                    "Sentence ID": 127,
                    "Sentence": "to remove backgrounds to stabilize the GAN training. To speed up training, we use\nGAN transfer learning ",
                    "Citation Text": "Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan\nRaducanu. Transferring gans: generating images from limited data. In European Conference on Computer\nVision (ECCV) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01677",
                        "Citation Paper Title": "Title:Transferring GANs: generating images from limited data",
                        "Citation Paper Abstract": "Abstract:Transferring the knowledge of pretrained networks to new domains by means of finetuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pretrained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when the target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pretrained model was trained without conditioning. Our results also suggest that density may be more important than diversity and a dataset with one or few densely sampled classes may be a better source model than more diverse datasets such as ImageNet or Places.",
                        "Citation Paper Authors": "Authors:Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, Bogdan Raducanu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03856v2": {
            "Paper Title": "Point Cloud Registration of non-rigid objects in sparse 3D Scans with\n  applications in Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05499v2": {
            "Paper Title": "Applicability limitations of differentiable full-reference image-quality",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "and were tested on 1,500 images from di\u000berent Vimeo90K sequences; this dataset isFigure 3: Hacking-detection heatmap.\nwidely used for image processing and has served in numerous research works ",
                    "Citation Text": "Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, and Chen Change Loy, \\Edvr:\nVideo restoration with enhanced deformable convolutional networks,\" in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\nWorkshops , June 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02716",
                        "Citation Paper Title": "Title:EDVR: Video Restoration with Enhanced Deformable Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, Chen Change Loy"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", the authors showed that an iterative algorithm can decrease LPIPS\nscores in a special way that yields negatively correlated MOS and LPIPS scores.\nSeveral methods for \fnding C for PSNR metrics have also emerged, including ",
                    "Citation Text": "Mahmood Sharif, Lujo Bauer, and Michael Reiter, \\On the suitability of lp-norms for\ncreating and preventing adversarial examples,\" 06 2018, pp. 1686{16868.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09653",
                        "Citation Paper Title": "Title:On the Suitability of $L_p$-norms for Creating and Preventing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Much research effort has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classified differently (i.e., misclassified). Both algorithms that create adversarial examples and strategies for defending against them typically use $L_p$-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an $L_p$-norm to be perceptually similar. In this work, we show that nearness according to an $L_p$-norm is not just unnecessary for perceptual similarity, but is also insufficient. Specifically, focusing on datasets (CIFAR10 and MNIST), $L_p$-norms, and thresholds used in prior work, we show through online user studies that \"adversarial examples\" that are closer to their benign counterparts than required by commonly used $L_p$-norm thresholds can nevertheless be perceptually different to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are \"near\" each other according to an $L_p$-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by $L_p$-norms is neither necessary nor sufficient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.",
                        "Citation Paper Authors": "Authors:Mahmood Sharif, Lujo Bauer, Michael K. Reiter"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "showed that VMAF Neg values are\nunalterable using this approach, but both VMAF and VMAF Neg proved vulnerable\nwhen using a di\u000berent pipeline in which images underwent compression with H.264.\nThe authors of ",
                    "Citation Text": "Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, and Alan C. Bovik, \\Prox-\niqa: A proxy approach to perceptual optimization of learned image compression,\" IEEE\nTransactions on Image Processing , vol. 30, pp. 360{373, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.08845",
                        "Citation Paper Title": "Title:ProxIQA: A Proxy Approach to Perceptual Optimization of Learned Image Compression",
                        "Citation Paper Abstract": "Abstract:The use of $\\ell_p$ $(p=1,2)$ norms has largely dominated the measurement of loss in neural networks due to their simplicity and analytical properties. However, when used to assess the loss of visual information, these simple norms are not very consistent with human perception. Here, we describe a different \"proximal\" approach to optimize image analysis networks against quantitative perceptual models. Specifically, we construct a proxy network, broadly termed ProxIQA, which mimics the perceptual model while serving as a loss layer of the network. We experimentally demonstrate how this optimization framework can be applied to train an end-to-end optimized image compression network. By building on top of an existing deep image compression model, we are able to demonstrate a bitrate reduction of as much as $31\\%$ over MSE optimization, given a specified perceptual quality (VMAF) level.",
                        "Citation Paper Authors": "Authors:Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, Alan C. Bovik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.14762v2": {
            "Paper Title": "Riemannian Functional Map Synchronization for Probabilistic Partial\n  Correspondence in Shape Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00349v2": {
            "Paper Title": "FLAME: Free-form Language-based Motion Synthesis & Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13462v1": {
            "Paper Title": "MVTN: Learning Multi-View Transformations for 3D Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12975v1": {
            "Paper Title": "Interactive Layout Drawing Interface with Shadow Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.00002v1": {
            "Paper Title": "Evaluating Alternative Glyph Design for Showing Large-Magnitude-Range\n  Quantum Spins",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "expands\nMacKinlay\u2019s APT by incorporating user tasks to guide\nvisualization generation. McColeman et al. ",
                    "Citation Text": "C. M. McColeman, F. Yang, T. F. Brady, and S. Franconeri, \u201cRe-\nthinking the ranks of visual channels,\u201d IEEE Transactions on Visu-\nalization and Computer Graphics , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.11367",
                        "Citation Paper Title": "Title:Rethinking the Ranks of Visual Channels",
                        "Citation Paper Abstract": "Abstract:Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is little existing work testing assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values.\nWe asked participants to immediately reproduce a set of values from memory. With a Bayesian multilevel modeling approach, we observed how the relevant rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice far more influence on performance, such as the number of values in the series (e.g. more marks led to larger errors), or the value of each mark (e.g. small values are systematically overestimated).\nRecall was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results show that we must move beyond two-value ratio judgments as a baseline for ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).",
                        "Citation Paper Authors": "Authors:Caitlyn M. McColeman, Fumeng Yang, Steven Franconeri, Timothy F. Brady"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.12871v1": {
            "Paper Title": "PaletteNeRF: Palette-based Color Editing for NeRFs",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "use hierarchical hash\nencoding to replace the costly MLP.\nAs for editing, EditingNeRF ",
                    "Citation Text": "S. Liu, X. Zhang, Z. Zhang, R. Zhang, J.-Y . Zhu, and B. Rus-\nsell. Editing conditional radiance \ufb01elds. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5773\u20135783, 2021. 1, 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.06466",
                        "Citation Paper Title": "Title:Editing Conditional Radiance Fields",
                        "Citation Paper Abstract": "Abstract:A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.",
                        "Citation Paper Authors": "Authors:Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "takes a set of objects\nwith similar shapes and colors, such as cars from Carla\ndataset ",
                    "Citation Text": "A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and\nV . Koltun. CARLA: An open urban driving simulator. In\nConference on robot learning , pages 1\u201316. PMLR, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "use empty space skipping and early ray ter-\nmination. In addition, KiloNeRF divides the scene into\nsmall grids, and uses a small and ef\ufb01cient MLP in each\ngrid to achieve further speedup. FastNeRF ",
                    "Citation Text": "S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and\nJ. Valentin. FastNeRF: High-Fidelity Neural Rendering at\n200FPS. pages 14346\u201314355, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10380",
                        "Citation Paper Title": "Title:FastNeRF: High-Fidelity Neural Rendering at 200FPS",
                        "Citation Paper Abstract": "Abstract:Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
                        "Citation Paper Authors": "Authors:Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", and scene\ncomposition [25, 47, 46, 44, 14, 27].\nTo improve NeRF\u2019s rendering speed, NSVF ",
                    "Citation Text": "L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt. Neu-\nral Sparse V oxel Fields. In NeurIPS , Jan. 2021. arXiv:\n2007.11571. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.12861v1": {
            "Paper Title": "An efficient quantum-classical hybrid algorithm for distorted\n  alphanumeric character identification",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "S. Caraiman and V . I. Manta, Image segmentation on\na quantum computer, Quantum. Info. Process. 14, 1693\n(2015). ",
                    "Citation Text": "N. Jiang, Y . Dang, and J. Wang, Quantum image match-\ning, Quantum. Info. Process. 15, 3543 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.09729",
                        "Citation Paper Title": "Title:Quantum Image Matching",
                        "Citation Paper Abstract": "Abstract:Quantum image processing (QIP) means the quantum based methods to speed up image processing algorithms. Many quantum image processing schemes claim that their efficiency are theoretically higher than their corresponding classical schemes. However, most of them do not consider the problem of measurement. As we all know, measurement will lead to collapse. That is to say, executing the algorithm once, users can only measure the final state one time. Therefore, if users want to regain the results (the processed images), they must execute the algorithms many times and then measure the final state many times to get all the pixels' values. If the measurement process is taken into account, whether or not the algorithms are really efficient needs to be reconsidered. In this paper, we try to solve the problem of measurement and give a quantum image matching algorithm. Unlike most of the QIP algorithms, our scheme interests only one pixel (the target pixel) instead of the whole image. It modifies the probability of pixels based on Grover's algorithm to make the target pixel to be measured with higher probability, and the measurement step is executed only once. An example is given to explain the algorithm more vividly. Complexity analysis indicates that the quantum scheme's complexity is $O(2^{n})$ in contradistinction to the classical scheme's complexity $O(2^{2n+2m})$, where $m$ and $n$ are integers related to the size of images.",
                        "Citation Paper Authors": "Authors:Nan Jiang, Yijie Dang, Jian Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "F. Yan, A. M. Iliyasu, B. Sun, S. E. Venegas-Andraca,\nF. Dong, and K. Hirota, A duple watermarking strategy for\nmulti-channel quantum images, Quantum Info. Process.\n14, 1675 (2015). ",
                    "Citation Text": "F. Yan, A. M. Iliyasu, Y . Guo, and H. Yang, Flexible rep-\nresentation and manipulation of audio signals on quantum\ncomputers, Theor. Comput. Sci. 752, 71 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.01291",
                        "Citation Paper Title": "Title:Flexible Representation and Manipulation of Audio Signals on Quantum Computers",
                        "Citation Paper Abstract": "Abstract:By analyzing the numerical representation of amplitude values in audio signals and integrating the time component, a representation for audio signals on quantum computers, FRQA, is proposed. The FRQA representation is a normalized state that facilitates basic audio signal operations targeting these parameters. The preparation and retrieval for FRQA are discussed and, based on the FRQA state, we realize the circuits to accomplish basic audio signal operations such as signal addition, signal inversion, signal delay, and signal reversal. These operations can be employed as the major components to build advanced operations for particular applications in the quantum computing domain.",
                        "Citation Paper Authors": "Authors:Fei Yan, Yiming Guo, Abdullah M. Iliyasu, Huamin Yang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "J.-E. Park, B. Quanz, S. Wood, H. Higgins, and R. Har-\nishankar, Practical application improvement to quantum\nsvm: theory to practice, arXiv preprint arXiv:2012.07725\n(2020).13 ",
                    "Citation Text": "I. Cong, S. Choi, and M. D. Lukin, Quantum convolu-\ntional neural networks, Nat. Phys. 15, 1273 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.03787",
                        "Citation Paper Title": "Title:Quantum Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce and analyze a novel quantum machine learning model motivated by convolutional neural networks. Our quantum convolutional neural network (QCNN) makes use of only $O(\\log(N))$ variational parameters for input sizes of $N$ qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. The QCNN architecture combines the multi-scale entanglement renormalization ansatz and quantum error correction. We explicitly illustrate its potential with two examples. First, QCNN is used to accurately recognize quantum states associated with 1D symmetry-protected topological phases. We numerically demonstrate that a QCNN trained on a small set of exactly solvable points can reproduce the phase diagram over the entire parameter regime and also provide an exact, analytical QCNN solution. As a second application, we utilize QCNNs to devise a quantum error correction scheme optimized for a given error model. We provide a generic framework to simultaneously optimize both encoding and decoding procedures and find that the resultant scheme significantly outperforms known quantum codes of comparable complexity. Finally, potential experimental realization and generalizations of QCNNs are discussed.",
                        "Citation Paper Authors": "Authors:Iris Cong, Soonwon Choi, Mikhail D. Lukin"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Y . Ma, H. Ma, and P. Chu, Demonstration of quantum im-\nage edge extraction enhancement through improved sobel\noperator, IEEE Access 8, 210277 (2020). ",
                    "Citation Text": "J.-E. Park, B. Quanz, S. Wood, H. Higgins, and R. Har-\nishankar, Practical application improvement to quantum\nsvm: theory to practice, arXiv preprint arXiv:2012.07725\n(2020).13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.07725",
                        "Citation Paper Title": "Title:Practical application improvement to Quantum SVM: theory to practice",
                        "Citation Paper Abstract": "Abstract:Quantum machine learning (QML) has emerged as an important area for Quantum applications, although useful QML applications would require many qubits. Therefore our paper is aimed at exploring the successful application of the Quantum Support Vector Machine (QSVM) algorithm while balancing several practical and technical considerations under the Noisy Intermediate-Scale Quantum (NISQ) assumption. For the quantum SVM under NISQ, we use quantum feature maps to translate data into quantum states and build the SVM kernel out of these quantum states, and further compare with classical SVM with radial basis function (RBF) kernels. As data sets are more complex or abstracted in some sense, classical SVM with classical kernels leads to less accuracy compared to QSVM, as classical SVM with typical classical kernels cannot easily separate different class data. Similarly, QSVM should be able to provide competitive performance over a broader range of data sets including ``simpler'' data cases in which smoother decision boundaries are required to avoid any model variance issues (i.e., overfitting). To bridge the gap between ``classical-looking'' decision boundaries and complex quantum decision boundaries, we propose to utilize general shallow unitary transformations to create feature maps with rotation factors to define a tunable quantum kernel, and added regularization to smooth the separating hyperplane model. We show in experiments that this allows QSVM to perform equally to SVM regardless of the complexity of the data sets and outperform in some commonly used reference data sets.",
                        "Citation Paper Authors": "Authors:Jae-Eun Park, Brian Quanz, Steve Wood, Heather Higgins, Ray Harishankar"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q.\nZhou, P. J. Love, A. Aspuru-Guzik, and J. L. O\u2019brien, A\nvariational eigenvalue solver on a photonic quantum pro-\ncessor, Nat. Commun. 5, 4213 (2014). ",
                    "Citation Text": "J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-\nGuzik, The theory of variational hybrid quantum-classical\nalgorithms, New J. Phys. 18, 023023 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.04279",
                        "Citation Paper Title": "Title:The theory of variational hybrid quantum-classical algorithms",
                        "Citation Paper Abstract": "Abstract:Many quantum algorithms have daunting resource requirements when compared to what is available today. To address this discrepancy, a quantum-classical hybrid optimization scheme known as \"the quantum variational eigensolver\" was developed with the philosophy that even minimal quantum resources could be made useful when used in conjunction with classical routines. In this work we extend the general theory of this algorithm and suggest algorithmic improvements for practical implementations. Specifically, we develop a variational adiabatic ansatz and explore unitary coupled cluster where we establish a connection from second order unitary coupled cluster to universal gate sets through relaxation of exponential splitting. We introduce the concept of quantum variational error suppression that allows some errors to be suppressed naturally in this algorithm on a pre-threshold quantum device. Additionally, we analyze truncation and correlated sampling in Hamiltonian averaging as ways to reduce the cost of this procedure. Finally, we show how the use of modern derivative free optimization techniques can offer dramatic computational savings of up to three orders of magnitude over previously used optimization techniques.",
                        "Citation Paper Authors": "Authors:Jarrod R. McClean, Jonathan Romero, Ryan Babbush, Al\u00e1n Aspuru-Guzik"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "D. S. Abrams and S. Lloyd, Simulation of many-body\nfermi systems on a universal quantum computer, Phys.\nRev. Lett. 79, 2586 (1997). ",
                    "Citation Text": "A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q.\nZhou, P. J. Love, A. Aspuru-Guzik, and J. L. O\u2019brien, A\nvariational eigenvalue solver on a photonic quantum pro-\ncessor, Nat. Commun. 5, 4213 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1304.3061",
                        "Citation Paper Title": "Title:A variational eigenvalue solver on a quantum processor",
                        "Citation Paper Abstract": "Abstract:Quantum computers promise to efficiently solve important problems that are intractable on a conventional computer. For quantum systems, where the dimension of the problem space grows exponentially, finding the eigenvalues of certain operators is one such intractable problem and remains a fundamental challenge. The quantum phase estimation algorithm can efficiently find the eigenvalue of a given eigenvector but requires fully coherent evolution. We present an alternative approach that greatly reduces the requirements for coherent evolution and we combine this method with a new approach to state preparation based on ans\u00e4tze and classical optimization. We have implemented the algorithm by combining a small-scale photonic quantum processor with a conventional computer. We experimentally demonstrate the feasibility of this approach with an example from quantum chemistry: calculating the ground state molecular energy for He-H+, to within chemical accuracy. The proposed approach, by drastically reducing the coherence time requirements, enhances the potential of the quantum resources available today and in the near future.",
                        "Citation Paper Authors": "Authors:Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J. Love, Al\u00e1n Aspuru-Guzik, Jeremy L. O'Brien"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09378v2": {
            "Paper Title": "A Multiple-View Geometric Model for Specularity Prediction on General\n  Curved Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.11263v1": {
            "Paper Title": "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "inferred part-segmentation through question an-\nswering on rendered images from PartNet ",
                    "Citation Text": "Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, and Kai\nXu. Partnet: A recursive part decomposition network for\n\ufb01ne-grained and hierarchical shape segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 9491\u20139500, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.00709",
                        "Citation Paper Title": "Title:PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation",
                        "Citation Paper Abstract": "Abstract:Deep learning approaches to 3D shape segmentation are typically formulated as a multi-class labeling problem. Existing models are trained for a fixed set of labels, which greatly limits their flexibility and adaptivity. We opt for top-down recursive decomposition and develop the first deep learning model for hierarchical segmentation of 3D shapes, based on recursive neural networks. Starting from a full shape represented as a point cloud, our model performs recursive binary decomposition, where the decomposition network at all nodes in the hierarchy share weights. At each node, a node classifier is trained to determine the type (adjacency or symmetry) and stopping criteria of its decomposition. The features extracted in higher level nodes are recursively propagated to lower level ones. Thus, the meaningful decompositions in higher levels provide strong contextual cues constraining the segmentations in lower levels. Meanwhile, to increase the segmentation accuracy at each node, we enhance the recursive contextual feature with the shape feature extracted for the corresponding part. Our method segments a 3D shape in point cloud into an unfixed number of parts, depending on the shape complexity, showing strong generality and flexibility. It achieves the state-of-the-art performance, both for fine-grained and semantic segmentation, on the public benchmark and a new benchmark of fine-grained segmentation proposed in this work. We also demonstrate its application for fine-grained part refinements in image-to-shape reconstruction.",
                        "Citation Paper Authors": "Authors:Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, Kai Xu"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "for learning seman-\ntic part segmentation. To alleviate the need for 3D anno-\ntations, unsupervised learning schemes utilize large collec-\ntions of unlabelled data [5,7,13,36,47]. For example, Hong\net al. ",
                    "Citation Text": "Yining Hong, Yilun Du, Chunru Lin, Josh Tenenbaum, and\nChuang Gan. 3d concept grounding on neural \ufb01elds. In An-\nnual Conference on Neural Information Processing Systems ,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.06403",
                        "Citation Paper Title": "Title:3D Concept Grounding on Neural Fields",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the challenging problem of 3D concept grounding (i.e. segmenting and learning visual concepts) by looking at RGBD images and reasoning about paired questions and answers. Existing visual reasoning approaches typically utilize supervised methods to extract 2D segmentation masks on which concepts are grounded. In contrast, humans are capable of grounding concepts on the underlying 3D representation of images. However, traditionally inferred 3D representations (e.g., point clouds, voxelgrids, and meshes) cannot capture continuous 3D features flexibly, thus making it challenging to ground concepts to 3D regions based on the language description of the object being referred to. To address both issues, we propose to leverage the continuous, differentiable nature of neural fields to segment and learn concepts. Specifically, each 3D coordinate in a scene is represented as a high-dimensional descriptor. Concept grounding can then be performed by computing the similarity between the descriptor vector of a 3D coordinate and the vector embedding of a language concept, which enables segmentations and concept learning to be jointly learned on neural fields in a differentiable fashion. As a result, both 3D semantic and instance segmentations can emerge directly from question answering supervision using a set of defined neural operators on top of neural fields (e.g., filtering and counting). Experimental results show that our proposed framework outperforms unsupervised/language-mediated segmentation models on semantic and instance segmentation tasks, as well as outperforms existing models on the challenging 3D aware visual reasoning tasks. Furthermore, our framework can generalize well to unseen shape categories and real scans.",
                        "Citation Paper Authors": "Authors:Yining Hong, Yilun Du, Chunru Lin, Joshua B. Tenenbaum, Chuang Gan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.00514v3": {
            "Paper Title": "Learning Spectral Unions of Partial Deformable 3D Shapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10062v1": {
            "Paper Title": "360$^\\circ$ Stereo Image Composition with Depth Adaption",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Recent advances\nin stereo depth estimation consist of deploying deep networks\nembedding all steps of traditional pipelines and combining effective\nlearning modules ",
                    "Citation Text": "Z. Li, X. Liu, N. Drenkow, A. Ding, F. X. Creighton, R. H. Taylor, and\nM. Unberath. Revisiting stereo depth estimation from a sequence-to-\nsequence perspective with transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pp. 6197\u20136206, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.02910",
                        "Citation Paper Title": "Title:Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers",
                        "Citation Paper Abstract": "Abstract:Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence estimates, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different domains, even without fine-tuning.",
                        "Citation Paper Authors": "Authors:Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X. Creighton, Russell H. Taylor, Mathias Unberath"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". The reconstruction and manipulation of\nhuman models have been signi\ufb01cantly advanced by neural network-\nbased technologies ",
                    "Citation Text": "S. Athar, Z. Shu, and D. Samaras. Flame-in-nerf: Neural control of radi-\nance \ufb01elds for free view face animation. arXiv preprint arXiv:2108.04913 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.04913",
                        "Citation Paper Title": "Title:FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation",
                        "Citation Paper Abstract": "Abstract:This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.",
                        "Citation Paper Authors": "Authors:ShahRukh Athar, Zhixin Shu, Dimitris Samaras"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". The estimated stereoscopic correspon-\ndences are used to conduct view-consistent image enhancement\noperations via deep networks, such as neural style transfer ",
                    "Citation Text": "D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. Stereoscopic neural style\ntransfer. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pp. 6654\u20136663, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.10591",
                        "Citation Paper Title": "Title:Stereoscopic Neural Style Transfer",
                        "Citation Paper Abstract": "Abstract:This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical real-time solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-to-end network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively.",
                        "Citation Paper Authors": "Authors:Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "developed a\nmethod of lighting and geometry estimation from 360 panoramic\nstereos. In the work of Li et al. ",
                    "Citation Text": "Y . Li, C. Barnes, K. Huang, and F.-L. Zhang. Deep 360 \u00b0optical \ufb02ow\nestimation based on multi-projection fusion. In European Conference on\nComputer Vision , pp. 336\u2013352. Springer, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.00776",
                        "Citation Paper Title": "Title:Deep 360$^\\circ$ Optical Flow Estimation Based on Multi-Projection Fusion",
                        "Citation Paper Abstract": "Abstract:Optical flow computation is essential in the early stages of the video processing pipeline. This paper focuses on a less explored problem in this area, the 360$^\\circ$ optical flow estimation using deep neural networks to support increasingly popular VR applications. To address the distortions of panoramic representations when applying convolutional neural networks, we propose a novel multi-projection fusion framework that fuses the optical flow predicted by the models trained using different projection methods. It learns to combine the complementary information in the optical flow results under different projections. We also build the first large-scale panoramic optical flow dataset to support the training of neural networks and the evaluation of panoramic optical flow estimation methods. The experimental results on our dataset demonstrate that our method outperforms the existing methods and other alternative deep networks that were developed for processing 360\u00b0 content.",
                        "Citation Paper Authors": "Authors:Yiheng Li, Connelly Barnes, Kun Huang, Fang-Lue Zhang"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "or improve the visual consistency between composited\nforeground and the target background ",
                    "Citation Text": "W. Cong, J. Zhang, L. Niu, L. Liu, Z. Ling, W. Li, and L. Zhang. Dovenet:\nDeep image harmonization via domain veri\ufb01cation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\n8394\u20138403, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.13239",
                        "Citation Paper Title": "Title:DoveNet: Deep Image Harmonization via Domain Verification",
                        "Citation Paper Abstract": "Abstract:Image composition is an important operation in image processing, but the inconsistency between foreground and background significantly degrades the quality of composite image. Image harmonization, aiming to make the foreground compatible with the background, is a promising yet challenging task. However, the lack of high-quality publicly available dataset for image harmonization greatly hinders the development of image harmonization techniques. In this work, we contribute an image harmonization dataset iHarmony4 by generating synthesized composite images based on COCO (resp., Adobe5k, Flickr, day2night) dataset, leading to our HCOCO (resp., HAdobe5k, HFlickr, Hday2night) sub-dataset. Moreover, we propose a new deep image harmonization method DoveNet using a novel domain verification discriminator, with the insight that the foreground needs to be translated to the same domain as background. Extensive experiments on our constructed dataset demonstrate the effectiveness of our proposed method. Our dataset and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, Liqing Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.11715v1": {
            "Paper Title": "GeoCode: Interpretable Shape Programs",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". The decoders of both these mod-\nels work in conjunction with a point cloud encoder to recon-\nstruct shapes from unannotated point clouds. As instructed\nby the authors, we train a point cloud encoder ",
                    "Citation Text": "Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\n9Net++: Deep hierarchical feature learning on point sets in a\nmetric space. In Adv. Neural Inform. Process. Syst. , 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "trained neural networks to parse 2D\nsketches into sequences of CAD commands, and Complex-\nGen ",
                    "Citation Text": "Haoxiang Guo, Shilin Liu, Hao Pan, Yang Liu, Xin Tong,\nand Baining Guo. ComplexGen: CAD reconstruction by b-\nrep chain complex generation. ACM Trans. Graph. , 2022.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.14573",
                        "Citation Paper Title": "Title:ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation",
                        "Citation Paper Abstract": "Abstract:We view the reconstruction of CAD models in the boundary representation (B-Rep) as the detection of geometric primitives of different orders, i.e. vertices, edges and surface patches, and the correspondence of primitives, which are holistically modeled as a chain complex, and show that by modeling such comprehensive structures more complete and regularized reconstructions can be achieved. We solve the complex generation problem in two steps. First, we propose a novel neural framework that consists of a sparse CNN encoder for input point cloud processing and a tri-path transformer decoder for generating geometric primitives and their mutual relationships with estimated probabilities. Second, given the probabilistic structure predicted by the neural network, we recover a definite B-Rep chain complex by solving a global optimization maximizing the likelihood under structural validness constraints and applying geometric refinements. Extensive tests on large scale CAD datasets demonstrate that the modeling of B-Rep chain complex structure enables more accurate detection for learning and more constrained reconstruction for optimization, leading to structurally more faithful and complete CAD B-Rep models than previous results.",
                        "Citation Paper Authors": "Authors:Haoxiang Guo, Shilin Liu, Hao Pan, Yang Liu, Xin Tong, Baining Guo"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "predict voxel grid and\ndepth with normal maps, respectively, which are then con-\nverted to meshes. Another approach is to produce a CAD\nshape to improve the structural integrity and edit capability\nof the reconstructed mesh. For example, Sketch2CAD ",
                    "Citation Text": "Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J Mi-\ntra. Sketch2CAD: Sequential CAD modeling by sketching\nin context. ACM Trans. Graph. , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.04927",
                        "Citation Paper Title": "Title:Sketch2CAD: Sequential CAD Modeling by Sketching in Context",
                        "Citation Paper Abstract": "Abstract:We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the \\emph{context} of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired sketching and CAD modeling sequences, we train our system by generating synthetic sequences of CAD operations that we render as line drawings. We present a proof of concept realization of our algorithm supporting four frequently used CAD operations. Using our system, participants are able to quickly model a large and diverse set of objects, demonstrating Sketch2CAD to be an alternate way of interacting with current CAD modeling systems.",
                        "Citation Paper Authors": "Authors:Changjian Li, Hao Pan, Adrien Bousseau, Niloy J. Mitra"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "Shape reconstruction. Several works used deep learning\nmethods to reconstruct a 3D object from a sketch. De-\nlanoy et al. ",
                    "Citation Text": "Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A\nEfros, and Adrien Bousseau. 3D sketching using multi-view\ndeep volumetric prediction. PACMCGIT , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.08390",
                        "Citation Paper Title": "Title:3D Sketching using Multi-View Deep Volumetric Prediction",
                        "Citation Paper Abstract": "Abstract:Sketch-based modeling strives to bring the ease and immediacy of drawing to the 3D world. However, while drawings are easy for humans to create, they are very challenging for computers to interpret due to their sparsity and ambiguity. We propose a data-driven approach that tackles this challenge by learning to reconstruct 3D shapes from one or more drawings. At the core of our approach is a deep convolutional neural network (CNN) that predicts occupancy of a voxel grid from a line drawing. This CNN provides us with an initial 3D reconstruction as soon as the user completes a single drawing of the desired shape. We complement this single-view network with an updater CNN that refines an existing prediction given a new drawing of the shape created from a novel viewpoint. A key advantage of our approach is that we can apply the updater iteratively to fuse information from an arbitrary number of viewpoints, without requiring explicit stroke correspondences between the drawings. We train both CNNs by rendering synthetic contour drawings from hand-modeled shape collections as well as from procedurally-generated abstract shapes. Finally, we integrate our CNNs in a minimal modeling interface that allows users to seamlessly draw an object, rotate it to see its 3D reconstruction, and refine it by re-drawing from another vantage point using the 3D reconstruction as guidance. The main strengths of our approach are its robustness to freehand bitmap drawings, its ability to adapt to different object categories, and the continuum it offers between single-view and multi-view sketch-based modeling.",
                        "Citation Paper Authors": "Authors:Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien Bousseau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.08496v2": {
            "Paper Title": "Dynamic Grass Color Scale Display Technique Based on Grass Length for\n  Green Landscape-Friendly Animation Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08010v3": {
            "Paper Title": "MoDi: Unconditional Motion Synthesis from Diverse Data",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". Thestyle codes are learned from the outputs of the mapping net-\nwork, using af\ufb01ne transformation.\nWe employ a discriminator ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 4, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.08790v1": {
            "Paper Title": "Estimating Cloth Elasticity Parameters Using Position-Based Simulation\n  of Compliant Constrained Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08526v1": {
            "Paper Title": "Unifying Human Motion Synthesis and Style Transfer with Denoising\n  Diffusion Probabilistic Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08365v1": {
            "Paper Title": "Geometric Rectification of Creased Document Images based on Isometric\n  Mapping",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "takes into account the slope angles of the texts in the reconstruction. Both DocTr ",
                    "Citation Text": "Hao Feng, Yuechen Wang, Wengang Zhou, Jiajun Deng, and Houqiang Li. Doctr: Document image\ntransformer for geometric unwarping and illumination correction. In Proceedings of the 29th ACM In-\nternational Conference on Multimedia , MM \u201921, page 273\u2013281, New York, NY, USA, 2021. Associa-\ntion for Computing Machinery. ISBN 9781450386517. doi: 10.1145/3474085.3475388. URL https:\n//doi.org/10.1145/3474085.3475388 .\n22",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.12942",
                        "Citation Paper Title": "Title:DocTr: Document Image Transformer for Geometric Unwarping and Illumination Correction",
                        "Citation Paper Abstract": "Abstract:In this work, we propose a new framework, called Document Image Transformer (DocTr), to address the issue of geometry and illumination distortion of the document images. Specifically, DocTr consists of a geometric unwarping transformer and an illumination correction transformer. By setting a set of learned query embedding, the geometric unwarping transformer captures the global context of the document image by self-attention mechanism and decodes the pixel-wise displacement solution to correct the geometric distortion. After geometric unwarping, our illumination correction transformer further removes the shading artifacts to improve the visual quality and OCR accuracy. Extensive evaluations are conducted on several datasets, and superior results are reported against the state-of-the-art methods. Remarkably, our DocTr achieves 20.02% Character Error Rate (CER), a 15% absolute improvement over the state-of-the-art methods. Moreover, it also shows high efficiency on running time and parameter count. The results will be available at this https URL for further comparison.",
                        "Citation Paper Authors": "Authors:Hao Feng, Yuechen Wang, Wengang Zhou, Jiajun Deng, Houqiang Li"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ", concerning the global distortions in the\nhorizontalandverticaldirections. Thestandarddeviationofeachrowandcolumnintheimageiscalculated\nbased on the results of DeepFlow.\n\u2022LPIPS. Learned Perceptual Image Patch Similarity (LPIPS) ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable e\ufb00ec-\ntiveness of deep features as a perceptual metric. In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 586\u2013595, 2018. doi: 10.1109/CVPR.2018.00068.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.08070v1": {
            "Paper Title": "NeRF-Art: Text-Driven Neural Radiance Fields Stylization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08051v1": {
            "Paper Title": "Objaverse: A Universe of Annotated 3D Objects",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": "ViT-B/32 24.1% 48.5% 46.9% 74.2% 22.8%\nViT-L/14 30.6% 56.8% 50.5% 77.0% 19.9%\nLAION-2B ",
                    "Citation Text": "Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402 , 2022. 1, 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.08402",
                        "Citation Paper Title": "Title:LAION-5B: An open large-scale dataset for training next generation image-text models",
                        "Citation Paper Abstract": "Abstract:Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page this https URL",
                        "Citation Paper Authors": "Authors:Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "RN50 21.4% 45.0% 43.9% 70.8% 22.5%\nViT-L/14 29.1% 54.5% 52.3% 77.2% 23.2%\nLAION-400M ",
                    "Citation Text": "Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114 , 2021. 1, 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.02114",
                        "Citation Paper Title": "Title:LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
                        "Citation Paper Abstract": "Abstract:Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.",
                        "Citation Paper Authors": "Authors:Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": ". Moreover, instances of objects often appear\nwith many styles, which is critical for training and evalu-\nating robust computer vision models ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021. 1, 2, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "released a dataset of 15,038 chairs and tables from\nShapeNet each with around 5 text captions, giving 75,344\ntotal text-shape pairs. ShapeGlot ",
                    "Citation Text": "Panos Achlioptas, Judy Fan, Robert Hawkins, Noah Good-\nman, and Leonidas J Guibas. Shapeglot: Learning language\nfor shape differentiation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 8938\u2013\n8947, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02925",
                        "Citation Paper Title": "Title:ShapeGlot: Learning Language for Shape Differentiation",
                        "Citation Paper Abstract": "Abstract:In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We first build a large scale, carefully controlled dataset of human utterances that each refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",
                        "Citation Paper Authors": "Authors:Panos Achlioptas, Judy Fan, Robert X.D. Hawkins, Noah D. Goodman, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "has dramat-\nically accelerated progress on a variety of tasks including\nclassi\ufb01cation, object detection, captioning, and more. Ever\nsince, the diversity and scale of datasets have continued to\ngrow. YFCC100M is a dataset of 99.2M images and 800K\nvideos ",
                    "Citation Text": "Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM , 59(2):64\u201373, 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.01817",
                        "Citation Paper Title": "Title:YFCC100M: The New Data in Multimedia Research",
                        "Citation Paper Abstract": "Abstract:We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released. The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development. We further present several new challenges in multimedia research that can now be expanded upon with our dataset.",
                        "Citation Paper Authors": "Authors:Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.13500v2": {
            "Paper Title": "Neural Neural Textures Make Sim2Real Consistent",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "are used to help train robots. Our paper uses an architecture based on Pfeiffer\net al. ",
                    "Citation Text": "M. Pfeiffer, I. Funke, M. R. Robu, S. Bodenstedt, L. Strenger, S. Engelhardt, T. Ro\u00df, M. J.\nClarkson, K. Gurusamy, B. R. Davidson, L. Maier-Hein, C. Riediger, T. Welsch, J. Weitz, and\nS. Speidel. Generating large labeled data sets for laparoscopic image processing tasks using\nunpaired image-to-image translation. arXiv , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.02882",
                        "Citation Paper Title": "Title:Generating large labeled data sets for laparoscopic image processing tasks using unpaired image-to-image translation",
                        "Citation Paper Abstract": "Abstract:In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual effort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This way, we are able to generate a large, fully labeled synthetic data set of laparoscopic images with realistic appearance. We show that this data set can be used to train models for the task of liver segmentation of laparoscopic images. We achieve average dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set will be made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (this http URL).",
                        "Citation Paper Authors": "Authors:Micha Pfeiffer, Isabel Funke, Maria R. Robu, Sebastian Bodenstedt, Leon Strenger, Sandy Engelhardt, Tobias Ro\u00df, Matthew J. Clarkson, Kurinchi Gurusamy, Brian R. Davidson, Lena Maier-Hein, Carina Riediger, Thilo Welsch, J\u00fcrgen Weitz, Stefanie Speidel"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". Extending to video to video translation, optical \ufb02ow is\noften used for stateful translation to improve temporal coherence [19, 20]. But they do not provide\ntemporal coherence over long time periods.\nView Synthesis NeRF ",
                    "Citation Text": "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view synthesis. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "Unsupervised Image to Image and Video to Video Translation Generative models have been\nvery successful in creating images unconditionally [9, 10], creating images conditioned on text [11,\n12] and creating images when trained on paired data ",
                    "Citation Text": "P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional\nadversarial networks, 2016. URL https://arxiv.org/abs/1611.07004 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07004",
                        "Citation Paper Title": "Title:Image-to-Image Translation with Conditional Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
                        "Citation Paper Authors": "Authors:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.07409v2": {
            "Paper Title": "Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "FLAME 1.21 1.54 1.31\n3DDFA-V2 3DMM 1.23 1.57 1.39\nDECA ",
                    "Citation Text": "Lars M. Mescheder, Andreas Geiger, and S. Nowozin.\nWhich training methods for GANs do actually converge? In\nICML , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.04406",
                        "Citation Paper Title": "Title:Which Training Methods for GANs do actually Converge?",
                        "Citation Paper Abstract": "Abstract:Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Andreas Geiger, Sebastian Nowozin"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "to search for the edit-\ning directions. To evaluate 3D shape reconstruction quality,\nwe use NoW benchmark ",
                    "Citation Text": "Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael\nBlack. Learning to regress 3D face shape and expression\nfrom an image without 3D supervision. In Proceedings IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) ,\nJune 2019. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06817",
                        "Citation Paper Title": "Title:Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual's face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces `not quite in-the-wild' (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes at this http URL.",
                        "Citation Paper Authors": "Authors:Soubhik Sanyal, Timo Bolkart, Haiwen Feng, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "and\nshows competitive performance compared with methods\ndesigned for 3D face reconstruction using basic models,\ne.g., 3DMM ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "analyzed the trade-offs between editability and \ufb01-\ndelity. However, they [1,2,37,44,55] all adopt global latent\ncode alone for GAN inversion task, thus failing to recover\nhigh-\ufb01delity details. Recently, HFGI ",
                    "Citation Text": "Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and\nQifeng Chen. High-Fidelity GAN inversion for image at-\ntribute editing. In CVPR , 2022. 1, 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.06590",
                        "Citation Paper Title": "Title:High-Fidelity GAN Inversion for Image Attribute Editing",
                        "Citation Paper Abstract": "Abstract:We present a novel high-fidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of high-fidelity GAN inversion from the perspective of lossy data compression. With a low bit-rate latent code, previous works have difficulties in preserving high-fidelity details in reconstructed and edited images. Increasing the size of a latent code can improve the accuracy of GAN inversion but at the cost of inferior editability. To improve image fidelity without compromising editability, we propose a distortion consultation approach that employs a distortion map as a reference for high-fidelity reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car domains show a clear improvement in both inversion and editing quality.",
                        "Citation Paper Authors": "Authors:Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "has shown promising results in generating pho-\ntorealistic images [6, 21, 22] and inspired researchers to\nput efforts on 3D aware generation [15, 29, 32]. How-\never, these methods use explicit shape representations,\ni.e., voxels [15, 29] and meshes ",
                    "Citation Text": "Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and\nPing Luo. Do 2D GANs know 3D shape? Unsupervised 3D\nShape Reconstruction from 2D Image GANs. In ICLR , 2021.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.00844",
                        "Citation Paper Title": "Title:Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs",
                        "Citation Paper Abstract": "Abstract:Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.08476v1": {
            "Paper Title": "SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "have re-\nceived signi\ufb01cant attention with photorealistic novel viewsynthesis performance. Meanwhile, the vanilla NeRF has\nseveral limitations.\nMany works have been conducted to address the\nlimitations of NeRF, including unseen scene general-\nization ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance \ufb01eld reconstruction from multi-view stereo.\nInProc. of the IEEE International Conf. on Computer Vision\n(ICCV) , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15595",
                        "Citation Paper Title": "Title:MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo",
                        "Citation Paper Abstract": "Abstract:We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.12639v2": {
            "Paper Title": "TreEnhance: A Tree Search Method For Low-Light Image Enhancement",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ".\nHu et al. proposed a white box-approach for im-\nage enhancement. This work is based on an actor-\ncritic algorithm to enhance the content of a low\nquality image ",
                    "Citation Text": "Y. Hu, H. He, C. Xu, B. Wang, S. Lin, Exposure:\nA white-box photo post-processing framework, ACM\nTransactions on Graphics (TOG) 37 (2) (2018) 1{17.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.09602",
                        "Citation Paper Title": "Title:Exposure: A White-Box Photo Post-Processing Framework",
                        "Citation Paper Abstract": "Abstract:Retouching can significantly elevate the visual appeal of photos, but many casual photographers lack the expertise to do this well. To address this problem, previous works have proposed automatic retouching systems based on supervised learning from paired training images acquired before and after manual editing. As it is difficult for users to acquire paired images that reflect their retouching preferences, we present in this paper a deep learning approach that is instead trained on unpaired data, namely a set of photographs that exhibits a retouching style the user likes, which is much easier to collect. Our system is formulated using deep convolutional neural networks that learn to apply different retouching operations on an input image. Network training with respect to various types of edits is enabled by modeling these retouching operations in a unified manner as resolution-independent differentiable filters. To apply the filters in a proper sequence and with suitable parameters, we employ a deep reinforcement learning approach that learns to make decisions on what action to take next, given the current state of the image. In contrast to many deep learning systems, ours provides users with an understandable solution in the form of conventional retouching edits, rather than just a \"black-box\" result. Through quantitative comparisons and user studies, we show that this technique generates retouching results consistent with the provided photo set.",
                        "Citation Paper Authors": "Authors:Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, Stephen Lin"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "Zhang proposed a pipeline based on decomposi-\ntion network and illumination adjustment to tune\nthe exposure in low-light images ",
                    "Citation Text": "Y. Zhang, J. Zhang, X. Guo, Kindling the darkness: A\npractical low-light image enhancer, in: Proceedings of\nthe 27th ACM international conference on multimedia,\n2019, pp. 1632{1640.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04161",
                        "Citation Paper Title": "Title:Kindling the Darkness: A Practical Low-light Image Enhancer",
                        "Citation Paper Abstract": "Abstract:Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradations, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify hidden artifacts. This work builds a simple yet effective network for \\textbf{Kin}dling the \\textbf{D}arkness (denoted as KinD), which, inspired by Retinex theory, decomposes images into two components. One component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting to be better regularized/learned. It is worth to note that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over state-of-the-art alternatives. Our KinD is robust against severe visual defects, and user-friendly to arbitrarily adjust light levels. In addition, our model spends less than 50ms to process an image in VGA resolution on a 2080Ti GPU. All the above merits make our KinD attractive for practical use.",
                        "Citation Paper Authors": "Authors:Yonghua Zhang, Jiawan Zhang, Xiaojie Guo"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ".\nThe use of vision transformers for color transition\nproblems has been proposed by Cai et al. ",
                    "Citation Text": "Y. Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y. Zhang,\nR. Timofte, L. Van Gool, Mask-guided spectral-wise\ntransformer for e\u000ecient hyperspectral image recon-\nstruction, in: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\n2022, pp. 17502{17511.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.07910",
                        "Citation Paper Title": "Title:Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction",
                        "Citation Paper Abstract": "Abstract:Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI restoration. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. Code and pre-trained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ".\nChai et al. presented an approach for parametric\ncolor enhancement. In this work, a convolutional\nneural network learns the parameters of a quadratic\ncolor transformation in a supervised learning sce-\nnario ",
                    "Citation Text": "Y. Chai, R. Giryes, L. Wolf, Supervised and unsuper-\nvised learning of parameterized color enhancement, in:\nProceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2020, pp. 992{1000.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.05843",
                        "Citation Paper Title": "Title:Supervised and Unsupervised Learning of Parameterized Color Enhancement",
                        "Citation Paper Abstract": "Abstract:We treat the problem of color enhancement as an image translation task, which we tackle using both supervised and unsupervised learning. Unlike traditional image to image generators, our translation is performed using a global parameterized color transformation instead of learning to directly map image information. In the supervised case, every training image is paired with a desired target image and a convolutional neural network (CNN) learns from the expert retouched images the parameters of the transformation. In the unpaired case, we employ two-way generative adversarial networks (GANs) to learn these parameters and apply a circularity constraint. We achieve state-of-the-art results compared to both supervised (paired data) and unsupervised (unpaired data) image enhancement methods on the MIT-Adobe FiveK benchmark. Moreover, we show the generalization capability of our method, by applying it on photos from the early 20th century and to dark video frames.",
                        "Citation Paper Authors": "Authors:Yoav Chai, Raja Giryes, Lior Wolf"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". Similarly to\nCycle-GAN, Sasaki et al. proposed an unpaired ap-\nproach with denoising di\u000busion probabilistic models\nfor image-to-image translation ",
                    "Citation Text": "H. Sasaki, C. G. Willcocks, T. P. Breckon, Unit-ddpm:\nUnpaired image translation with denoising di\u000busion\nprobabilistic models, arXiv preprint arXiv:2104.05358\n(2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.05358",
                        "Citation Paper Title": "Title:UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We propose a novel unpaired image-to-image translation method that uses denoising diffusion probabilistic models without requiring adversarial training. Our method, UNpaired Image Translation with Denoising Diffusion Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint distribution of images over both domains as a Markov chain by minimising a denoising score matching objective conditioned on the other domain. In particular, we update both domain translation models simultaneously, and we generate target domain images by a denoising Markov Chain Monte Carlo approach that is conditioned on the input source domain images, based on Langevin dynamics. Our approach provides stable model training for image-to-image translation and generates high-quality image outputs. This enables state-of-the-art Fr\u00e9chet Inception Distance (FID) performance on several public datasets, including both colour and multispectral imagery, significantly outperforming the contemporary adversarial image-to-image translation methods.",
                        "Citation Paper Authors": "Authors:Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Batzolis et al. pro-\nposed a conditional di\u000busion model able to reach\nstate-of-the-art performance in inpainting, super-\nresolution and edge-to-image tasks ",
                    "Citation Text": "G. Batzolis, J. Stanczuk, C.-B. Sch\u007f onlieb, C. Etmann,\nConditional image generation with score-based di\u000busion\nmodels, arXiv preprint arXiv:2111.13606 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.13606",
                        "Citation Paper Title": "Title:Conditional Image Generation with Score-Based Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Score-based diffusion models have emerged as one of the most promising frameworks for deep generative modelling. In this work we conduct a systematic comparison and theoretical analysis of different approaches to learning conditional probability distributions with score-based diffusion models. In particular, we prove results which provide a theoretical justification for one of the most successful estimators of the conditional score. Moreover, we introduce a multi-speed diffusion framework, which leads to a new estimator for the conditional score, performing on par with previous state-of-the-art approaches. Our theoretical and experimental findings are accompanied by an open source library MSDiff which allows for application and further research of multi-speed diffusion models.",
                        "Citation Paper Authors": "Authors:Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00f6nlieb, Christian Etmann"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Saharia\net al. proposed a conditional di\u000busion model and\napplied it to four di\u000berent image-to-image trans-\nlation tasks: colorization, JPEG restoration, in-\npainting and uncropping ",
                    "Citation Text": "C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Sal-\nimans, D. Fleet, M. Norouzi, Palette: Image-to-image\ndi\u000busion models, in: ACM SIGGRAPH 2022 Confer-\nence Proceedings, 2022, pp. 1{10.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05826",
                        "Citation Paper Title": "Title:Palette: Image-to-Image Diffusion Models",
                        "Citation Paper Abstract": "Abstract:This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out this https URL for an overview of the results.",
                        "Citation Paper Authors": "Authors:Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Lore et al.\nproposed a deep autoencoder approach for natural\nlow-light image enhancement. ",
                    "Citation Text": "K. G. Lore, A. Akintayo, S. Sarkar, Llnet: A deep au-\ntoencoder approach to natural low-light image enhance-\nment, Pattern Recognition 61 (2017) 650{662.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.03995",
                        "Citation Paper Title": "Title:LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement",
                        "Citation Paper Abstract": "Abstract:In surveillance, monitoring and tactical reconnaissance, gathering the right visual information from a dynamic environment and accurately processing such data are essential ingredients to making informed decisions which determines the success of an operation. Camera sensors are often cost-limited in ability to clearly capture objects without defects from images or videos taken in a poorly-lit environment. The goal in many applications is to enhance the brightness, contrast and reduce noise content of such images in an on-board real-time manner. We propose a deep autoencoder-based approach to identify signal features from low-light images handcrafting and adaptively brighten images without over-amplifying the lighter parts in images (i.e., without saturation of image pixels) in high dynamic range. We show that a variant of the recently proposed stacked-sparse denoising autoencoder can learn to adaptively enhance and denoise from synthetically darkened and noisy training examples. The network can then be successfully applied to naturally low-light environment and/or hardware degraded images. Results show significant credibility of deep learning based approaches both visually and by quantitative comparison with various popular enhancing, state-of-the-art denoising and hybrid enhancing-denoising techniques.",
                        "Citation Paper Authors": "Authors:Kin Gwn Lore, Adedotun Akintayo, Soumik Sarkar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.07098v1": {
            "Paper Title": "Interactive Sketching of Mannequin Poses",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "requires, i.e. to\nseparately annotate each stroke e.g. ridge/valley, curvature\nline, depth discontinuity, and boundary.\nDelanoy et al. ",
                    "Citation Text": "Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A.\nEfros, and Adrien Bousseau. 3d sketching using multi-view\ndeep volumetric prediction. Proc. ACM Comput. Graph. In-\nteract. Tech. , 1(1), July 2018. 4323",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.08390",
                        "Citation Paper Title": "Title:3D Sketching using Multi-View Deep Volumetric Prediction",
                        "Citation Paper Abstract": "Abstract:Sketch-based modeling strives to bring the ease and immediacy of drawing to the 3D world. However, while drawings are easy for humans to create, they are very challenging for computers to interpret due to their sparsity and ambiguity. We propose a data-driven approach that tackles this challenge by learning to reconstruct 3D shapes from one or more drawings. At the core of our approach is a deep convolutional neural network (CNN) that predicts occupancy of a voxel grid from a line drawing. This CNN provides us with an initial 3D reconstruction as soon as the user completes a single drawing of the desired shape. We complement this single-view network with an updater CNN that refines an existing prediction given a new drawing of the shape created from a novel viewpoint. A key advantage of our approach is that we can apply the updater iteratively to fuse information from an arbitrary number of viewpoints, without requiring explicit stroke correspondences between the drawings. We train both CNNs by rendering synthetic contour drawings from hand-modeled shape collections as well as from procedurally-generated abstract shapes. Finally, we integrate our CNNs in a minimal modeling interface that allows users to seamlessly draw an object, rotate it to see its 3D reconstruction, and refine it by re-drawing from another vantage point using the 3D reconstruction as guidance. The main strengths of our approach are its robustness to freehand bitmap drawings, its ability to adapt to different object categories, and the continuum it offers between single-view and multi-view sketch-based modeling.",
                        "Citation Paper Authors": "Authors:Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien Bousseau"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "takes on the task of 3D point cloud\nreconstruction from a single sketch. Their model is com-\nposed of an off-the-shelf sketch synthesis network ",
                    "Citation Text": "Runtao Liu, Qian Yu, and S. Yu. An unpaired sketch-to-\nphoto translation model. ArXiv , abs/1909.08313, 2019. 4322",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.08313",
                        "Citation Paper Title": "Title:Unsupervised Sketch-to-Photo Synthesis",
                        "Citation Paper Abstract": "Abstract:Humans can envision a realistic photo given a free-hand sketch that is not only spatially imprecise and geometrically distorted but also without colors and visual details. We study unsupervised sketch-to-photo synthesis for the first time, learning from unpaired sketch-photo data where the target photo for a sketch is unknown during training. Existing works only deal with style change or spatial deformation alone, synthesizing photos from edge-aligned line drawings or transforming shapes within the same modality, e.g., color images. Our key insight is to decompose unsupervised sketch-to-photo synthesis into a two-stage translation task: First shape translation from sketches to grayscale photos and then content enrichment from grayscale to color photos. We also incorporate a self-supervised denoising objective and an attention module to handle abstraction and style variations that are inherent and specific to sketches. Our synthesis is sketch-faithful and photo-realistic to enable sketch-based image retrieval in practice. An exciting corollary product is a universal and promising sketch generator that captures human visual perception beyond the edge map of a photo.",
                        "Citation Paper Authors": "Authors:Runtao Liu, Qian Yu, Stella Yu"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "infers depth and\nnormal maps to construct 3D point clouds.While the results\non these are remarkable, the network requires carefully ren-\ndered orthographic drawings from a side or frontal view. It\nis not suitable for amateur sketches.\nIn contrast, ",
                    "Citation Text": "Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen,\nand Stella X Yu. 3d shape reconstruction from free-hand\nsketches. arXiv preprint arXiv:2006.09694 , 2020. 4322",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09694",
                        "Citation Paper Title": "Title:3D Shape Reconstruction from Free-Hand Sketches",
                        "Citation Paper Abstract": "Abstract:Sketches are the most abstract 2D representations of real-world objects. Although a sketch usually has geometrical distortion and lacks visual cues, humans can effortlessly envision a 3D object from it. This suggests that sketches encode the information necessary for reconstructing 3D shapes. Despite great progress achieved in 3D reconstruction from distortion-free line drawings, such as CAD and edge maps, little effort has been made to reconstruct 3D shapes from free-hand sketches. We study this task and aim to enhance the power of sketches in 3D-related applications such as interactive design and VR/AR games.\nUnlike previous works, which mostly study distortion-free line drawings, our 3D shape reconstruction is based on free-hand sketches. A major challenge for free-hand sketch 3D reconstruction comes from the insufficient training data and free-hand sketch diversity, e.g. individualized sketching styles. We thus propose data generation and standardization mechanisms. Instead of distortion-free line drawings, synthesized sketches are adopted as input training data. Additionally, we propose a sketch standardization module to handle different sketch distortions and styles. Extensive experiments demonstrate the effectiveness of our model and its strong generalizability to various free-hand sketches. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen, Stella X. Yu"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", convo-\nlutional neural networks (CNNs) are being explored to han-\ndle sketches [36, 53, 34, 48, 28]. Given multiple sketches\nfrom different views, the network in ",
                    "Citation Text": "Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis,\nSubhransu Maji, and Rui Wang. 3d shape reconstruction\nfrom sketches via multi-view convolutional networks. In\n2017 International Conference on 3D Vision (3DV) , pages\n67\u201377. IEEE, 2017. 4322",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06375",
                        "Citation Paper Title": "Title:3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:We propose a method for reconstructing 3D shapes from 2D sketches in the form of line drawings. Our method takes as input a single sketch, or multiple sketches, and outputs a dense point cloud representing a 3D reconstruction of the input sketch(es). The point cloud is then converted into a polygon mesh. At the heart of our method lies a deep, encoder-decoder network. The encoder converts the sketch into a compact representation encoding shape information. The decoder converts this representation into depth and normal maps capturing the underlying surface from several output viewpoints. The multi-view maps are then consolidated into a 3D point cloud by solving an optimization problem that fuses depth and normals across all viewpoints. Based on our experiments, compared to other methods, such as volumetric networks, our architecture offers several advantages, including more faithful reconstruction, higher output surface resolution, better preservation of topology and shape structure.",
                        "Citation Paper Authors": "Authors:Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, Rui Wang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "further in the supplementary material. However,\nsince realistic sketch-to-3D paired data is dif\ufb01cult to ob-\ntain in large enough quantities, many approaches exploit the\nShapeNet ",
                    "Citation Text": "Angel X. Chang, T. Funkhouser, L. Guibas, P. Hanra-\nhan, Qixing Huang, Zimo Li, S. Savarese, M. Savva,\nShuran Song, H. Su, J. Xiao, L. Yi, and F. Yu.\nShapenet: An information-rich 3d model repository. ArXiv ,\nabs/1512.03012, 2015. 4322",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.06701v1": {
            "Paper Title": "A Novel Approach For Generating Customizable Light Field Datasets for\n  Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06310v1": {
            "Paper Title": "Structure-Guided Image Completion with Image-level and Object-level\n  Semantic Discriminators",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "and for the edge-guided in-\npainting, we compare our method with Edge-connect ",
                    "Citation Text": "Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,\nand Mehran Ebrahimi. Edgeconnect: Generative image\ninpainting with adversarial edge learning. arXiv preprint\narXiv:1901.00212 , 2019. 1, 2, 3, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00212",
                        "Citation Paper Title": "Title:EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning",
                        "Citation Paper Abstract": "Abstract:Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: this https URL",
                        "Citation Paper Authors": "Authors:Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "for a set of StyleGAN and semantic\ndiscriminators at both image level and object level D=\nfD;Ds;Dobj;Dobj\nsgand perceptual loss ",
                    "Citation Text": "Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean conference on computer vision , pages 694\u2013711.\nSpringer, 2016. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08155",
                        "Citation Paper Title": "Title:Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
                        "Citation Paper Abstract": "Abstract:We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Alexandre Alahi, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "are also used as guidance for inpainting.\n2.2. Discriminators for GANs\nThe initial Generative Adversarial Networks\n(GANs) ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems , pages 2672\u20132680,\n2014. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "for various tasks including inpaint-\ning [42, 69] and semantic image generation ",
                    "Citation Text": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2019. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07291",
                        "Citation Paper Title": "Title:Semantic Image Synthesis with Spatially-Adaptive Normalization",
                        "Citation Paper Abstract": "Abstract:We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "in an aligned setting where objects are carefully placed or\nregistered in the center of the image. However, generat-\ning unaligned objects in a complex natural scene is known\nto be challenging ",
                    "Citation Text": "Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\nxl: Scaling stylegan to large diverse datasets. arXiv preprint\narXiv:2202.00273 , 2022. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.00273",
                        "Citation Paper Title": "Title:StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets",
                        "Citation Paper Abstract": "Abstract:Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN's performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of $1024^2$ at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes.",
                        "Citation Paper Authors": "Authors:Axel Sauer, Katja Schwarz, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Dimproves the generated\nlocal textures.\nObject-level Discriminators. Recent progress on image\ngeneration [24, 25] demonstrates impressive results on gen-\nerating objects such as face, car, animal ",
                    "Citation Text": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196 , 2017. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10196",
                        "Citation Paper Title": "Title:Progressive Growing of GANs for Improved Quality, Stability, and Variation",
                        "Citation Paper Abstract": "Abstract:We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
                        "Citation Paper Authors": "Authors:Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "to discriminate the instance-\nlevel realism. Bene\ufb01ting from the comprehensive seman-\ntic concepts captured by pretrained vision models ",
                    "Citation Text": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,\nBolei Zhou, and Antonio Torralba. Understanding the role of\nindividual units in a deep neural network. Proceedings of the\nNational Academy of Sciences , 117(48):30071\u201330078, 2020.\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.05041",
                        "Citation Paper Title": "Title:Understanding the Role of Individual Units in a Deep Neural Network",
                        "Citation Paper Abstract": "Abstract:Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.",
                        "Citation Paper Authors": "Authors:David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", several\nworks [18, 21, 63, 66] design discriminators to predict\nthe pairwise relations between different modalities or the\nreal and fake samples. Likewise, the patch co-occurrence\ndiscriminator ",
                    "Citation Text": "Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli\nShechtman, Alexei A Efros, and Richard Zhang. Swapping\nautoencoder for deep image manipulation. arXiv preprint\narXiv:2007.00653 , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00653",
                        "Citation Paper Title": "Title:Swapping Autoencoder for Deep Image Manipulation",
                        "Citation Paper Abstract": "Abstract:Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models.",
                        "Citation Paper Authors": "Authors:Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, Richard Zhang"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "that trains an encoder-\ndecoder network to completes the missing region of an im-\nage, numerous approaches have been proposed to improve\nthe learning-based hole \ufb01lling. The proposed mechanisms\nincluding multi-stage networks [34, 39, 54, 56, 58\u201361, 65],\nattention mechanism ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 5505\u20135514,\n2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07892",
                        "Citation Paper Title": "Title:Generative Image Inpainting with Contextual Attention",
                        "Citation Paper Abstract": "Abstract:Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.06193v1": {
            "Paper Title": "ROAD: Learning an Implicit Recursive Octree Auto-Decoder to Efficiently\n  Encode 3D Shapes",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". However, in the context of 3D shape representation, extracting\nthe underlying surface from the implicit \ufb01eld typically involves expensive operations such as volume\nrendering ",
                    "Citation Text": "S. Lombardi, T. Simon, J. Saragih, G. Schwartz, A. Lehrmann, and Y . Sheikh. Neural volumes:\nLearning dynamic renderable volumes from images. arXiv , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07751",
                        "Citation Paper Title": "Title:Neural Volumes: Learning Dynamic Renderable Volumes from Images",
                        "Citation Paper Abstract": "Abstract:Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",
                        "Citation Paper Authors": "Authors:Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", compositionality [63, 64, 65], learning from data in the wild [66,\n67, 68] or test-time adaptation ",
                    "Citation Text": "S. Zakharov, W. Kehl, A. Bhargava, and A. Gaidon. Autolabeling 3d objects with differentiable\nrendering of sdf shape priors. In CVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11288",
                        "Citation Paper Title": "Title:Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors",
                        "Citation Paper Abstract": "Abstract:We present an automatic annotation pipeline to recover 9D cuboids and 3D shapes from pre-trained off-the-shelf 2D detectors and sparse LIDAR data. Our autolabeling method solves an ill-posed inverse problem by considering learned shape priors and optimizing geometric and physical parameters. To address this challenging problem, we apply a novel differentiable shape renderer to signed distance fields (SDF), leveraged together with normalized object coordinate spaces (NOCS). Initially trained on synthetic data to predict shape and coordinates, our method uses these predictions for projective and geometric alignment over real samples. Moreover, we also propose a curriculum learning strategy, iteratively retraining on samples of increasing difficulty in subsequent self-improving annotation rounds. Our experiments on the KITTI3D dataset show that we can recover a substantial amount of accurate cuboids, and that these autolabels can be used to train 3D vehicle detectors with state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Sergey Zakharov, Wadim Kehl, Arjun Bhargava, Adrien Gaidon"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ". Recursive parameterizations have also been employed in the con-\ntext of radiance \ufb01elds ",
                    "Citation Text": "G.-W. Yang, W.-Y . Zhou, H.-Y . Peng, D. Liang, T.-J. Mu, and S.-M. Hu. Recursive-nerf: An\nef\ufb01cient and dynamically growing nerf. arXiv , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.09103",
                        "Citation Paper Title": "Title:Recursive-NeRF: An Efficient and Dynamically Growing NeRF",
                        "Citation Paper Abstract": "Abstract:View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "and SLAM [48, 49]. For an overview of\nrecent methods and applications please consult ",
                    "Citation Text": "Y . Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V . Sitzmann,\nand S. Sridhar. Neural \ufb01elds in visual computing and beyond. arXiv , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", object pose\nestimation and re\ufb01nement [10, 43, 24, 44], object and surface reconstruction from sparse and noisy\ndata [5, 45], multi-modal perception ",
                    "Citation Text": "R. Gao, Y .-Y . Chang, S. Mall, L. Fei-Fei, and J. Wu. Objectfolder: A dataset of objects with\nimplicit visual, auditory, and tactile representations. In CoRL , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.07991",
                        "Citation Paper Title": "Title:ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations",
                        "Citation Paper Abstract": "Abstract:Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.",
                        "Citation Paper Authors": "Authors:Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ",\nand robotics [35, 36, 37, 38, 39, 40]. Neural Fields have been used in robotics to represent 3D ge-\nometry and appearance with applications in grasping [41, 42], trajectory planning ",
                    "Citation Text": "M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson, J. Bohg, and M. Schwa-\nger. Vision-only robot navigation in a neural radiance world. RA-L , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00168",
                        "Citation Paper Title": "Title:Vision-Only Robot Navigation in a Neural Radiance World",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at this https URL .",
                        "Citation Paper Authors": "Authors:Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.06125v1": {
            "Paper Title": "Neural Assets: Volumetric Object Capture and Rendering for Interactive\n  Environments",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "uses a\nhash function instead, creating a volumetric storage for fea-\ntures that are processed by a small neural network; ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance \ufb01elds. arXiv preprint\narXiv:2203.09517 , 2022. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09517",
                        "Citation Paper Title": "Title:TensoRF: Tensorial Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "uses another strategy: predic-\ntion caching. This, however, is only applied at test time\nand has high memory requirements. MobileNeRF ",
                    "Citation Text": "Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for ef\ufb01cient neural \ufb01eld rendering on mo-\nbile architectures. arXiv preprint arXiv:2208.00277 , 2022.\n3, 7\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.00277",
                        "Citation Paper Title": "Title:MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.",
                        "Citation Paper Authors": "Authors:Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "use an octree acceleration structure to\nspeed up volume traversal at render time and store view-\ndependent information in a spherical harmonics encoding.\nSNeRG ",
                    "Citation Text": "Peter Hedman, Pratul P Srinivasan, Ben Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance \ufb01elds for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5875\u20135884, 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14645",
                        "Citation Paper Title": "Title:Baking Neural Radiance Fields for Real-Time View Synthesis",
                        "Citation Paper Abstract": "Abstract:Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",
                        "Citation Paper Authors": "Authors:Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". Over-\nall, many existing radiance \ufb01eld approaches have too slow\nrender speeds for interactive experiences or are not suitable\nfor implementations using shading languages.\nReal-time neural rendering. Several approaches have\nbeen created speci\ufb01cally to address rendering speed:\nPlenOctrees ",
                    "Citation Text": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time renderingof neural radiance \ufb01elds. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 5752\u2013\n5761, 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14024",
                        "Citation Paper Title": "Title:PlenOctrees for Real-time Rendering of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "uses a learned sampling strategy (oracle network)\nfor this purpose\u2014however, this leads to problems at ob-\nject boundaries and with \ufb01ne details, a use case that we\nfocus on. Fast-NeRF ",
                    "Citation Text": "Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-\ufb01delity neural\nrendering at 200fps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 14346\u2013\n14355, 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10380",
                        "Citation Paper Title": "Title:FastNeRF: High-Fidelity Neural Rendering at 200FPS",
                        "Citation Paper Abstract": "Abstract:Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
                        "Citation Paper Authors": "Authors:Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04981v1": {
            "Paper Title": "LoopDraw: a Loop-Based Autoregressive Model for Shape Synthesis and\n  Editing",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "among others [12, 20].\nAs an alternative to generating shapes with \ufb01xed-topology,\nsome techniques produce individual parts ( e.g. meshes or\ncuboids) that are stitched together [15, 30]. Recently, Tet-\nGAN ",
                    "Citation Text": "William Gao, April Wang, Gal Metzer, Raymond A Yeh,\nand Rana Hanocka. Tetgan: A convolutional neural net-\nwork for tetrahedral mesh generation. arXiv preprint\narXiv:2210.05735 , 2022. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.05735",
                        "Citation Paper Title": "Title:TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation",
                        "Citation Paper Abstract": "Abstract:We present TetGAN, a convolutional neural network designed to generate tetrahedral meshes. We represent shapes using an irregular tetrahedral grid which encodes an occupancy and displacement field. Our formulation enables defining tetrahedral convolution, pooling, and upsampling operations to synthesize explicit mesh connectivity with variable topological genus. The proposed neural network layers learn deep features over each tetrahedron and learn to extract patterns within spatial regions across multiple scales. We illustrate the capabilities of our technique to encode tetrahedral meshes into a semantically meaningful latent-space which can be used for shape editing and synthesis. Our project page is at this https URL.",
                        "Citation Paper Authors": "Authors:William Gao, April Wang, Gal Metzer, Raymond A. Yeh, Rana Hanocka"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ",\nwhich represents surfaces with occupancy \ufb01elds; the V AE\nvariant of ShapeGAN ",
                    "Citation Text": "Marian Kleineberg, Matthias Fey, and Frank Weichert. Ad-\nversarial generation of continuous implicit shape representa-\ntions. arXiv preprint arXiv:2002.00349 , 2020. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.00349",
                        "Citation Paper Title": "Title:Adversarial Generation of Continuous Implicit Shape Representations",
                        "Citation Paper Abstract": "Abstract:This work presents a generative adversarial architecture for generating three-dimensional shapes based on signed distance representations. While the deep generation of shapes has been mostly tackled by voxel and surface point cloud approaches, our generator learns to approximate the signed distance for any point in space given prior latent information. Although structurally similar to generative point cloud approaches, this formulation can be evaluated with arbitrary point density during inference, leading to fine-grained details in generated outputs. Furthermore, we study the effects of using either progressively growing voxel- or point-processing networks as discriminators, and propose a refinement scheme to strengthen the generator's capabilities in modeling the zero iso-surface decision boundary of shapes. We train our approach on the ShapeNet benchmark dataset and validate, both quantitatively and qualitatively, its performance in generating realistic 3D shapes.",
                        "Citation Paper Authors": "Authors:Marian Kleineberg, Matthias Fey, Frank Weichert"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "generates a sequence of mesh\nvertices and then learns to connect them together. Other\nworks with diverse approaches to the sequence-based 3D\nshape modeling problem include DeepCAD ",
                    "Citation Text": "Rundi Wu, Chang Xiao, and Changxi Zheng. DeepCAD: A\ndeep generative network for computer-aided design models.\n2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.09492",
                        "Citation Paper Title": "Title:DeepCAD: A Deep Generative Network for Computer-Aided Design Models",
                        "Citation Paper Abstract": "Abstract:Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",
                        "Citation Paper Authors": "Authors:Rundi Wu, Chang Xiao, Changxi Zheng"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "Our approach joins a number of other works in the \ufb01eld\nof shape generation, speci\ufb01cally those that leverage se-\nquences. Among autoregressive sequence-based generative\nframeworks, PolyGen ",
                    "Citation Text": "Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Pe-\nter W. Battaglia. Polygen: An autoregressive generative\nmodel of 3d meshes. ICML , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10880",
                        "Citation Paper Title": "Title:PolyGen: An Autoregressive Generative Model of 3D Meshes",
                        "Citation Paper Abstract": "Abstract:Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.",
                        "Citation Paper Authors": "Authors:Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04970v1": {
            "Paper Title": "Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in\n  Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04741v1": {
            "Paper Title": "Physically Plausible Animation of Human Upper Body from a Single Image",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". The autoregressive model has also\nproved to be effective for synthesizing long-term 3D motion\nfrom scratch, such as the acLSTM model ",
                    "Citation Text": "Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, Zeng Huang,\nand Hao Li. Auto-conditioned recurrent networks for ex-\ntended complex human motion synthesis. arXiv preprint\narXiv:1707.05363 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05363",
                        "Citation Paper Title": "Title:Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a real-time method for synthesizing highly complex human motions using a novel training regime we call the auto-conditioned Recurrent Neural Network (acRNN). Recently, researchers have attempted to synthesize new motion by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acRNN is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles.",
                        "Citation Paper Authors": "Authors:Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, Zeng Huang, Hao Li"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", as\nwell as 3D methods that take advantages of dense pose ",
                    "Citation Text": "Natalia Neverova, Riza Alp Guler, and Iasonas Kokkinos.\nDense pose transfer. In Proceedings of the European Confer-\nence on Computer Vision , pages 123\u2013138, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01995",
                        "Citation Paper Title": "Title:Dense Pose Transfer",
                        "Citation Paper Abstract": "Abstract:In this work we integrate ideas from surface-based modeling with neural synthesis: we propose a combination of surface-based pose estimation and deep generative models that allows us to perform accurate pose transfer, i.e. synthesize a new image of a person based on a single image of that person and the image of a pose donor. We use a dense pose estimation system that maps pixels from both images to a common surface-based coordinate system, allowing the two images to be brought in correspondence with each other. We inpaint and refine the source image intensities in the surface coordinate system, prior to warping them onto the target pose. These predictions are fused with those of a convolutional predictive module through a neural synthesis module allowing for training the whole pipeline jointly end-to-end, optimizing a combination of adversarial and perceptual losses. We show that dense pose estimation is a substantially more powerful conditioning input than landmark-, or mask-based alternatives, and report systematic improvements over state of the art generators on DeepFashion and MVC datasets.",
                        "Citation Paper Authors": "Authors:Natalia Neverova, Riza Alp Guler, Iasonas Kokkinos"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", estimated body part\nspatial transformations [ 1,25], attention maps ",
                    "Citation Text": "Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei\nWang, and Xiang Bai. Progressive pose attention transfer\nfor person image generation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2347\u20132356, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03349",
                        "Citation Paper Title": "Title:Progressive Pose Attention Transfer for Person Image Generation",
                        "Citation Paper Abstract": "Abstract:This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency. Codes and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, Xiang Bai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04386v1": {
            "Paper Title": "Multi-View Mesh Reconstruction with Neural Deferred Shading",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". Neural ren-\ndering has been used as integral part of 3D reconstruction\nmethods with neural scene representations.\nIntroduced by Mildenhall et al . ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV , 2020. 1, 2, 8, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04360v1": {
            "Paper Title": "MIME: Human-Aware 3D Scene Generation",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "predict the \u201c3D \ufb02oor plan\u201d from\na 2D human walking trajectory in a deterministic way. The\napproach only indicates the room layout and furniture foot-\nprints and does not model objects or contact. Nie et al. ",
                    "Citation Text": "Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias\nNie\u00dfner. Pose2room: understanding 3d scenes from human\nactivities. In European Conference on Computer Vision , pages\n425\u2013443. Springer, 2022. 2, 3, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.03030",
                        "Citation Paper Title": "Title:Pose2Room: Understanding 3D Scenes from Human Activities",
                        "Citation Paper Abstract": "Abstract:With wearable IMU sensors, one can estimate human poses from wearable devices without requiring visual input~\\cite{von2017sparse}. In this work, we pose the question: Can we reason about object structure in real-world environments solely from human trajectory information? Crucially, we observe that human motion and interactions tend to give strong information about the objects in a scene -- for instance a person sitting indicates the likely presence of a chair or sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of the objects in a scene characterized by their class categories and oriented 3D bounding boxes, based on an input observed human trajectory in the environment. P2R-Net models the probability distribution of object class as well as a deep Gaussian mixture model for object boxes, enabling sampling of multiple, diverse, likely modes of object configurations from an observed human trajectory. In our experiments we show that P2R-Net can effectively learn multi-modal distributions of likely objects for human motions, and produce a variety of plausible object structures of the environment, even without any visual information. The results demonstrate that P2R-Net consistently outperforms the baselines on the PROX dataset and the VirtualHome platform.",
                        "Citation Paper Authors": "Authors:Yinyu Nie, Angela Dai, Xiaoguang Han, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "is the positional encoding for the\ntranslationt, rotationrand sizes.\nFurniture Encoder. The furniture encoder computes the\nembedding of existing objects in the room:\nE\u0012: (Ij= 0;kj;tj;rj;sj)!(0;\u0015(kj);p(tj);p(rj);p(sj)):\n4Figure 4. Scene re\ufb01nement with the collision and contact loss from\nMOVER ",
                    "Citation Text": "Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas,\nMuhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus\nThies, and Michael J. Black. Human-aware object placement\nfor visual environment reconstruction. In Computer Vision\nand Pattern Recognition (CVPR) , 2022. 5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.03609",
                        "Citation Paper Title": "Title:Human-Aware Object Placement for Visual Environment Reconstruction",
                        "Citation Paper Abstract": "Abstract:Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets. The code and data are available for research purposes at this https URL.",
                        "Citation Paper Authors": "Authors:Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas, Muhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus Thies, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", to predict the feature ^qthat\nis used to generate the next object:\n\u001c\u0012(F;TM+N\ni=1;q)!^q:\nTo decode the attribute distribution (^k;^t;^r;^s)of the gener-\nated objectoM+1from ^q, we follow the same design from\nATISS ",
                    "Citation Text": "Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten\nKreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres-\nsive transformers for indoor scene synthesis. Conference on\nNeural Information Processing Systems (NeurIPS) , 34:12013\u2013\n12026, 2021. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.03675",
                        "Citation Paper Title": "Title:ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
                        "Citation Paper Abstract": "Abstract:The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
                        "Citation Paper Authors": "Authors:Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "for a comprehensive review. These\ndatasets contain only humans, forgoing the 3D environments\nwhich the subjects interact with, e.g., \ufb02oor plane, walls,\nfurniture. In contrast, real 3D scene datasets such as Mat-\nterport3D ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3D: Learning from rgb-d\ndata in indoor environments. International Conference on 3D\nVision (3DV) , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03848v2": {
            "Paper Title": "NeRFEditor: Differentiable Style Decomposition for Full 3D Scene Editing",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "was the\n\ufb01rst work to edit the shape and color of NeRF on local parts\nof simple objects. CLIP-NeRF ",
                    "Citation Text": "Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manip-\nulation of neural radiance \ufb01elds. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 3835\u20133844, 2022. 1, 2, 5, 6, 7, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05139",
                        "Citation Paper Title": "Title:CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manipulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled conditional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation field to the positional encoding and deferring color conditioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reflect the targeted editing. The mappers are trained with a CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". For our method, we report on\ntwo variants: the model without 3D-consistent style editing,\ni.e. Ours (w/o 3D) and the whole model, i.e. Ours. For a fair\ncomparison, we apply PTI ",
                    "Citation Text": "Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Transactions on Graphics (TOG) , 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05744",
                        "Citation Paper Title": "Title:Pivotal Tuning for Latent-based Editing of Real Images",
                        "Citation Paper Abstract": "Abstract:Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "conditioning on SIREN, leading to the view-\nconsistent synthesis. FENeRF ",
                    "Citation Text": "Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi\nZhang, Yebin Liu, and Jue Wang. Fenerf: Face editing in\nneural radiance \ufb01elds. In Proceedings of the IEEE/CVF Con-\n9ference on Computer Vision and Pattern Recognition , pages\n7672\u20137682, 2022. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.15490",
                        "Citation Paper Title": "Title:FENeRF: Face Editing in Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Previous portrait image generation methods roughly fall into two categories: 2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but with low view consistency. 3D-aware GAN methods can maintain view consistency but their generated images are not locally editable. To overcome these limitations, we propose FENeRF, a 3D-aware generator that can produce view-consistent and locally-editable portrait images. Our method uses two decoupled latent codes to generate corresponding facial semantics and texture in a spatial aligned 3D volume with shared geometry. Benefiting from such underlying 3D representation, FENeRF can jointly render the boundary-aligned image and semantic mask and use the semantic mask to edit the 3D volume via GAN inversion. We further show such 3D representation can be learned from widely available monocular image and semantic mask pairs. Moreover, we reveal that joint learning semantics and texture helps to generate finer geometry. Our experiments demonstrate that FENeRF outperforms state-of-the-art methods in various face editing tasks.",
                        "Citation Paper Authors": "Authors:Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04248v1": {
            "Paper Title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion\n  Priors",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ": The dataset consists of 100,000 utter-\nances from 1,251 celebrities. 100 videos with 25 identi-\nties in total were randomly chosen for the test.\n\u2022VoxCeleb2 ",
                    "Citation Text": "Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.\nV oxceleb2: Deep speaker recognition. arXiv preprint\narXiv:1806.05622 , 2018. 5, 9, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05622",
                        "Citation Paper Title": "Title:VoxCeleb2: Deep Speaker Recognition",
                        "Citation Paper Abstract": "Abstract:The objective of this paper is speaker recognition under noisy and unconstrained conditions.\nWe make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.\nSecond, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.",
                        "Citation Paper Authors": "Authors:Joon Son Chung, Arsha Nagrani, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ") using ad-hoc\nmethods, or they simply do not support inferring other facial\nmotions ",
                    "Citation Text": "KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nambood-\niri, and CV Jawahar. A lip sync expert is all you need for\nspeech to lip generation in the wild. In Proceedings of the\n28th ACM International Conference on Multimedia , pages\n484\u2013492, 2020. 2, 3, 5, 6, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10010",
                        "Citation Paper Title": "Title:A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild",
                        "Citation Paper Abstract": "Abstract:In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: \\url{this http URL}. The code and models are released at this GitHub repository: \\url{this http URL}. You can also try out the interactive demo at this link: \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "introduced text-conditional diffusion model\nand showed that classi\ufb01er free guidance has better perfor-\nmance than CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021. 3, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "described as follows:\nLsimple =En0\u0018q(n0jfa\nl);t\u0018[1;T][\r\rn0\u0000Pa2nl(nt;t;fa\nl)\r\r\n2];\n(7)\nwhere we use n0to represent n0\n1:Lfor convenience.\nWe implement P a2nlwith a transformer encoder ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 4, 6, 9\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". MLP land MLP nlare applied after\nEvto map the visual feature fvinto lip space and non-lip\nspace, respectively. The dimension of MLP lis512\u0002470\nand the dimension of MLP nlis512\u000242.\n\u2022Audio Encoder E a, a ResNet34 encoder from ",
                    "Citation Text": "Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae\nLee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sungh-\nwan Jung, Bong-Jin Lee, and Icksang Han. In defence\nof metric learning for speaker recognition. arXiv preprint\narXiv:2003.11982 , 2020. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11982",
                        "Citation Paper Title": "Title:In defence of metric learning for speaker recognition",
                        "Citation Paper Abstract": "Abstract:The objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-speaker and large inter-speaker distance.\nA popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most popular loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our proposed metric learning objective outperform state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sunghwan Jung, Bong-Jin Lee, Icksang Han"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "along\nthis direction, but they either only infer one facial attribute\n(i.e., emotion in EVP ",
                    "Citation Text": "Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu,\nChen Change Loy, Xun Cao, and Feng Xu. Audio-driven\nemotional video portraits. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2021. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07452",
                        "Citation Paper Title": "Title:Audio-Driven Emotional Video Portraits",
                        "Citation Paper Abstract": "Abstract:Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the correlation between speech content and the mouth shape. Facial emotion, which is one of the most important features on natural human faces, is always neglected in their methods. In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Specifically, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, i.e., a duration-independent emotion space and a duration dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced. Then we propose the Target-Adaptive Face Synthesis technique to generate the final high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualitatively and quantitatively.",
                        "Citation Paper Authors": "Authors:Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "discussed how multi-\nview \u201cmodality\u201d can be jointly unitized to boost the intrin-\nsic representation through contrastive learning rather than\npredictive (or reconstruction) learning, it also demonstrated\nthat the more views, the better. Concurrently, MMV ",
                    "Citation Text": "Jean-Baptiste Alayrac, Adri `a Recasens, Rosalia Schneider,\nRelja Arandjelovi \u00b4c, Jason Ramapuram, Jeffrey De Fauw, Lu-\ncas Smaira, Sander Dieleman, and Andrew Zisserman. Self-\nSupervised MultiModal Versatile Networks. In NeurIPS ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16228",
                        "Citation Paper Title": "Title:Self-Supervised MultiModal Versatile Networks",
                        "Citation Paper Abstract": "Abstract:Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Alayrac, Adri\u00e0 Recasens, Rosalia Schneider, Relja Arandjelovi\u0107, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "used cross-\nmodal supervision to disentangle speech content and emo-\ntion from the audio signal with landmark as the intermediate\nrepresentation. Recently, CMC ",
                    "Citation Text": "Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\ntrastive multiview coding. arXiv preprint arXiv:1906.05849 ,\n2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05849",
                        "Citation Paper Title": "Title:Contrastive Multiview Coding",
                        "Citation Paper Abstract": "Abstract:Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: this http URL.",
                        "Citation Paper Authors": "Authors:Yonglong Tian, Dilip Krishnan, Phillip Isola"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03635v1": {
            "Paper Title": "Non-uniform Sampling Strategies for NeRF on 360{\\textdegree} images",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "proposes a parameterization to handle unbounded scenes\nunder the conical frustum proposed in mip-NeRF ",
                    "Citation Text": "Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-\naliasing neural radiance \ufb01elds. ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", a learning-based method based on continuous implicit represen-\ntation, has achieved high-resolution rendering by taking advantage of volume rendering with\ncontinuous neural radiance \ufb01elds. To improve the performance of NeRF, several studies have\ncombined multiple representations (e.g., point clouds ",
                    "Citation Text": "K. Deng, A. Liu, J. Y . Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and\nfaster training for free. CVPR , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.02791",
                        "Citation Paper Title": "Title:Depth-supervised NeRF: Fewer Views and Faster Training for Free",
                        "Citation Paper Abstract": "Abstract:A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as \"free\" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.",
                        "Citation Paper Authors": "Authors:Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "emerged, discrete representations of scenes have\nbeen used in the novel view synthesis task. Several approaches with discrete representations\nof scenes employ point clouds ",
                    "Citation Text": "A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-\nshot view synthesis. In ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00677",
                        "Citation Paper Title": "Title:Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360\u00b0 scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",
                        "Citation Paper Authors": "Authors:Ajay Jain, Matthew Tancik, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05559v2": {
            "Paper Title": "Unifying Diffusion Models' Latent Space, with Applications to\n  CycleDiffusion and Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.09147v2": {
            "Paper Title": "Neural Implicit Mapping via Nested Neighborhoods",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nRecently, neural networks have been used to represent\nSDFs [11, 23, 26]. Sinusoidal networks are an important\nexample, which are multilayer perceptrons (MLPs) having\nthe sine as its activation function. We use the framework\nin ",
                    "Citation Text": "T. Novello, G. Schardong, L. Schirmer, V . da Silva,\nH. Lopes, and L. Velho. Exploring differential ge-\nometry in neural implicits. Computers & Graphics ,\n108, 2022. ISSN 0097-8493. doi: https : / / doi .\norg / 10 . 1016 / j . cag . 2022 . 09 . 003. URL https://\ndsilvavinicius . github . io / differential _\ngeometry_in_neural_implicits/ . 1, 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.09263",
                        "Citation Paper Title": "Title:Exploring Differential Geometry in Neural Implicits",
                        "Citation Paper Abstract": "Abstract:We introduce a neural implicit framework that exploits the differentiable properties of neural networks and the discrete geometry of point-sampled surfaces to approximate them as the level sets of neural implicit functions.\nTo train a neural implicit function, we propose a loss functional that approximates a signed distance function, and allows terms with high-order derivatives, such as the alignment between the principal directions of curvature, to learn more geometric details. During training, we consider a non-uniform sampling strategy based on the curvatures of the point-sampled surface to prioritize points with more geometric details. This sampling implies faster learning while preserving geometric accuracy when compared with previous approaches.\nWe also use the analytical derivatives of a neural implicit function to estimate the differential measures of the underlying point-sampled surface.",
                        "Citation Paper Authors": "Authors:Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vinicius da Silva, Helio Lopes, Luiz Velho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.02837v1": {
            "Paper Title": "Pretrained Diffusion Models for Unified Human Motion Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05744v4": {
            "Paper Title": "More Control for Free! Image Synthesis with Semantic Diffusion Guidance",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ".\nWe present quantitative results and comparison with pre-\nvious work in Table 1 with the following evaluation metrics.\nFID for image quality evaluation. We report FID\nscore ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in Neural Information Processing Sys-\ntems, pages 6626\u20136637, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", we\nfurther explore whether diffusion models can be semanti-\ncally guided by text or image, or both to synthesize images.\nCLIP-guided Generation and Manipulation\nCLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "is a powerful vision-language joint embed-\nding model trained on large-scale images and texts. Its\nrepresentations have been shown to be robust and general\nenough to perform zero-shot classi\ufb01cation and various\nvision-language tasks on diverse datasets. StyleCLIP ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 2085\u20132094,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "proposes a way to iteratively inject image guid-\nance to a diffusion model, yet it exhibits limited structural\ndiversity of the generated images. Instance-Conditioned\nGAN ",
                    "Citation Text": "Arantxa Casanova, Marl `ene Careil, Jakob Verbeek, Michal\nDrozdzal, and Adriana Romero-Soriano. Instance-\nconditioned gan. arXiv preprint arXiv:2109.05070 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05070",
                        "Citation Paper Title": "Title:Instance-Conditioned GAN",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) can generate near photo realistic images in narrow domains such as human faces. Yet, modeling complex distributions of datasets such as ImageNet and COCO-Stuff remains challenging in unconditional settings. In this paper, we take inspiration from kernel density estimation techniques and introduce a non-parametric approach to modeling distributions of complex datasets. We partition the data manifold into a mixture of overlapping neighborhoods described by a datapoint and its nearest neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN), which learns the distribution around each datapoint. Experimental results on ImageNet and COCO-Stuff show that IC-GAN significantly improves over unconditional models and unsupervised data partitioning baselines. Moreover, we show that IC-GAN can effortlessly transfer to datasets not seen during training by simply changing the conditioning instances, and still generate realistic images. Finally, we extend IC-GAN to the class-conditional case and show semantically controllable generation and competitive quantitative results on ImageNet; while improving over BigGAN on ImageNet-LT. Code and trained models to reproduce the reported results are available at this https URL.",
                        "Citation Paper Authors": "Authors:Arantxa Casanova, Marl\u00e8ne Careil, Jakob Verbeek, Michal Drozdzal, Adriana Romero-Soriano"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.01672v1": {
            "Paper Title": "MaRF: Representing Mars as Neural Radiance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10960v3": {
            "Paper Title": "Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams)",
            "Sentences": [
                {
                    "Sentence ID": 93,
                    "Sentence": ". Experiments were performed on the benchmark\nof public ensembles ",
                    "Citation Text": "M. Pont, J. Vidal, J. Delon, and J. Tierny. Wasserstein Distances,\nGeodesics and Barycenters of Merge Trees \u2013 Ensemble Benchmark.\nhttps://github.com/MatPont/WassersteinMergeTreesData, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.07789",
                        "Citation Paper Title": "Title:Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
                        "Citation Paper Abstract": "Abstract:This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance [106] and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the L2-Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks [112] for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach -- with barycenter computations in the orders of minutes for the largest examples -- as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results.",
                        "Citation Paper Authors": "Authors:Mathieu Pont, Jules Vidal, Julie Delon, Julien Tierny"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "introduced a\nframework for computing a 1-center of a set of merge trees (i.e.\nminimizing its maximum distance to the set), for an interleav-\ning distance ",
                    "Citation Text": "E. Gasparovic, E. Munch, S. Oudot, K. Turner, B. Wang, and Y . Wang.\nIntrinsic interleaving distance for merge trees. CoRR , 1908.00063,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00063",
                        "Citation Paper Title": "Title:Intrinsic Interleaving Distance for Merge Trees",
                        "Citation Paper Abstract": "Abstract:Merge trees are a type of graph-based topological summary that tracks the evolution of connected components in the sublevel sets of scalar functions. They enjoy widespread applications in data analysis and scientific visualization. In this paper, we consider the problem of comparing two merge trees via the notion of interleaving distance in the metric space setting. We investigate various theoretical properties of such a metric. In particular, we show that the interleaving distance is intrinsic on the space of labeled merge trees and provide an algorithm to construct metric 1-centers for collections of labeled merge trees. We further prove that the intrinsic property of the interleaving distance also holds for the space of unlabeled merge trees. Our results are a first step toward performing statistics on graph-based topological summaries.",
                        "Citation Paper Authors": "Authors:Ellen Gasparovic, Elizabeth Munch, Steve Oudot, Katharine Turner, Bei Wang, Yusu Wang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "have been introduced\nfor the estimation of barycenters of persistence diagrams (or\nvectorized variants ",
                    "Citation Text": "H. Adams, S. Chepushtanova, T. Emerson, E. Hanson, M. Kirby,\nF. Motta, R. Neville, C. Peterson, P. Shipman, and L. Ziegelmeier.\nPersistence Images: A Stable Vector Representation of Persistent Ho-\nmology. Journal of Machine Learning Research , 18(8):1\u201335, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.06217",
                        "Citation Paper Title": "Title:Persistence Images: A Stable Vector Representation of Persistent Homology",
                        "Citation Paper Abstract": "Abstract:Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.",
                        "Citation Paper Authors": "Authors:Henry Adams, Sofya Chepushtanova, Tegan Emerson, Eric Hanson, Michael Kirby, Francis Motta, Rachel Neville, Chris Peterson, Patrick Shipman, Lori Ziegelmeier"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": ", thereby enabling the ef\ufb01cient computation of distances,\ngeodesics and barycenters of merge trees.\nRegarding the estimation of a representative object from a\nset of topological representations, multiple approaches emerged\nrecently. Several methods ",
                    "Citation Text": "T. Lacombe, M. Cuturi, and S. Oudot. Large Scale computation of\nMeans and Clusters for Persistence Diagrams using Optimal Transport.\nInNIPS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08331",
                        "Citation Paper Title": "Title:Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport",
                        "Citation Paper Abstract": "Abstract:Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of complex data. Despite several appealing properties, incorporating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. Indeed, this was recently exemplified in a string of papers which show that the simple task of averaging a few PDs can be computationally prohibitive. We propose in this article a tractable framework to carry out standard tasks on PDs at scale, notably evaluating distances, estimating barycenters and performing clustering. This framework builds upon a reformulation of PD metrics as optimal transport (OT) problems. Doing so, we can exploit recent computational advances: the OT problem on a planar grid, when regularized with entropy, is convex can be solved in linear time using the Sinkhorn algorithm and convolutions. This results in scalable computations that can stream on GPUs. We demonstrate the efficiency of our approach by carrying out clustering with diagrams metrics on several thousands of PDs, a scale never seen before in the literature.",
                        "Citation Paper Authors": "Authors:Th\u00e9o Lacombe, Marco Cuturi, Steve Oudot"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". However, since\nthese measures are not metrics (the preservation of the triangle\ninequality is not speci\ufb01cally enforced), they are not conducive\nto the computation of geodesics. Stable distance metrics between\nReeb graphs ",
                    "Citation Text": "U. Bauer, X. Ge, and Y . Wang. Measuring distance between Reeb\ngraphs. In Symposium on Computational Geometry , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1307.2839",
                        "Citation Paper Title": "Title:Measuring Distance between Reeb Graphs",
                        "Citation Paper Abstract": "Abstract:One of the prevailing ideas in geometric and topological data analysis is to provide descriptors that encode useful information about hidden objects from observed data. The Reeb graph is one such descriptor for a given scalar function. The Reeb graph provides a simple yet meaningful abstraction of the input domain, and can also be computed efficiently.\nGiven the popularity of the Reeb graph in applications, it is important to understand its stability and robustness with respect to changes in the input function, as well as to be able to compare the Reeb graphs resulting from different functions.\nIn this paper, we propose a metric for Reeb graphs, called the functional distortion distance. Under this distance measure, the Reeb graph is stable against small changes of input functions. At the same time, it remains discriminative at differentiating input functions. In particular, the main result is that the functional distortion distance between two Reeb graphs is bounded from below by (and thus more discriminative than) the bottleneck distance between both the ordinary and extended persistence diagrams for appropriate dimensions.\nAs an application of our results, we analyze a natural simplification scheme for Reeb graphs, and show that persistent features in Reeb graph remains persistent under simplification. Understanding the stability of important features of the Reeb graph under simplification is an interesting problem on its own right, and critical to the practical usage of Reeb graphs.",
                        "Citation Paper Authors": "Authors:Ulrich Bauer, Xiaoyin Ge, Yusu Wang"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Popular topological3\nrepresentations in data visualization include the persistence dia-\ngram ",
                    "Citation Text": "U. Bauer, M. Kerber, and J. Reininghaus. Distributed computation of\npersistent homology. In Algorithm Engineering and Experiments , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.0710",
                        "Citation Paper Title": "Title:Distributed computation of persistent homology",
                        "Citation Paper Abstract": "Abstract:Persistent homology is a popular and powerful tool for capturing topological features of data. Advances in algorithms for computing persistent homology have reduced the computation time drastically -- as long as the algorithm does not exhaust the available memory. Following up on a recently presented parallel method for persistence computation on shared memory systems, we demonstrate that a simple adaption of the standard reduction algorithm leads to a variant for distributed systems. Our algorithmic design ensures that the data is distributed over the nodes without redundancy; this permits the computation of much larger instances than on a single machine. Moreover, we observe that the parallelism at least compensates for the overhead caused by communication between nodes, and often even speeds up the computation compared to sequential and even parallel shared memory algorithms. In our experiments, we were able to compute the persistent homology of filtrations with more than a billion (10^9) elements within seconds on a cluster with 32 nodes using less than 10GB of memory per node.",
                        "Citation Paper Authors": "Authors:Ulrich Bauer, Michael Kerber, Jan Reininghaus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.16677v1": {
            "Paper Title": "3D Neural Field Generation using Triplane Diffusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13408v2": {
            "Paper Title": "View Synthesis of Dynamic Scenes based on Deep 3D Mask Volume",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.15460v1": {
            "Paper Title": "Fragment-History Volumes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.14902v1": {
            "Paper Title": "3inGAN: Learning a 3D Generative Model from Images of a Self-similar\n  Scene",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": ", such an approach results in limited\nquality (blurry) results. Instead, we build our approach using\ntheir proposed grid-based ReLU Field representation. This\nrepresentation is detailed in brief in section 3.1.\nA na \u00a8\u0131ve extension of SinGAN ",
                    "Citation Text": "Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan:\nLearning a generative model from a single natural image. In\nICCV , pages 4570\u20134580, 2019. 2, 3, 4, 5, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01164",
                        "Citation Paper Title": "Title:SinGAN: Learning a Generative Model from a Single Natural Image",
                        "Citation Paper Abstract": "Abstract:We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.",
                        "Citation Paper Authors": "Authors:Tamar Rott Shaham, Tali Dekel, Tomer Michaeli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.14822v1": {
            "Paper Title": "Adjustable Method Based on Body Parts for Improving the Accuracy of 3D\n  Reconstruction in Visually Important Body Parts from Silhouettes",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "is mapping the measures, such \nas breadths of the shoulder, waist, and hip of a silhouette and \ntemplate model . In work by  Pavlakos et al. ",
                    "Citation Text": "Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and \nKostas Daniilidis. Learning to estimate 3D human pose \nand shape from a sing le color image. In CVPR, 2018. 1, \n2, 3, 6, 8 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04092",
                        "Citation Paper Title": "Title:Learning to Estimate 3D Human Pose and Shape from a Single Color Image",
                        "Citation Paper Abstract": "Abstract:This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.",
                        "Citation Paper Authors": "Authors:Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "utilized  iterative fitting  on 2D  joints \nto train  a CNN . Bogo et al. ",
                    "Citation Text": "Federica Bogo, Angjoo Kanazawa, Christoph Lassner, \nPeter Gehler, Javier Romero, and Michael J Black. Keep \nit SMPL: Automatic estimation of 3D human pose and \nshape from a single image. In ECCV, 2016. 1, 3, 4, 8.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08128",
                        "Citation Paper Title": "Title:Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image",
                        "Citation Paper Abstract": "Abstract:We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.",
                        "Citation Paper Authors": "Authors:Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J. Black"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.10658v2": {
            "Paper Title": "EDGE: Editable Dance Generation From Music",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", or\na variation of the frame inbetweening scheme used in Ho et\nal. ",
                    "Citation Text": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High de\ufb01nition video generation with diffusion mod-\nels.arXiv preprint arXiv:2210.02303 , 2022. 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02303",
                        "Citation Paper Title": "Title:Imagen Video: High Definition Video Generation with Diffusion Models",
                        "Citation Paper Abstract": "Abstract:We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.",
                        "Citation Paper Authors": "Authors:Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ", an autoregressive model introduced to-\ngether with the AIST++ dataset\n\u2022Bailando ",
                    "Citation Text": "Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,\nChen Qian, Chen Change Loy, and Ziwei Liu. Bailando:\n3d dance generation by actor-critic gpt with choreographic\nmemory. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 11050\u2013\n11059, 2022. 2, 4, 5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13055",
                        "Citation Paper Title": "Title:Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory",
                        "Citation Paper Abstract": "Abstract:Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.",
                        "Citation Paper Authors": "Authors:Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, Ziwei Liu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "de\ufb01-\nnition of diffusion as a Markov noising process with la-\n2Figure 2. EDGE Pipeline Overview: EDGE learns to denoise dance sequences from time t=Ttot= 0, conditioned on music. Music\nembedding information is provided by a frozen Jukebox model ",
                    "Citation Text": "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook\nKim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-\nerative model for music. arXiv preprint arXiv:2005.00341 ,\n2020. 2, 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00341",
                        "Citation Paper Title": "Title:Jukebox: A Generative Model for Music",
                        "Citation Paper Abstract": "Abstract:We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at this https URL, along with model weights and code at this https URL",
                        "Citation Paper Authors": "Authors:Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". Timestep information is incorporated both\nwith a token concatenated with the music conditioning and\nfeature-wise linear modulation (FiLM) ",
                    "Citation Text": "Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence , volume 32, 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.03017",
                        "Citation Paper Title": "Title:Learning Visual Reasoning Without Strong Priors",
                        "Citation Paper Abstract": "Abstract:Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.",
                        "Citation Paper Authors": "Authors:Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "introduced classi\ufb01er guidance for image generation,\nwhere the output of a diffusion model may be \u201csteered\u201d to-\nwards a target, such as a class label, using the gradients of a\ndifferentiable auxiliary model. Saharia et al. ",
                    "Citation Text": "Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In\nACM SIGGRAPH 2022 Conference Proceedings , pages 1\u2013\n10, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05826",
                        "Citation Paper Title": "Title:Palette: Image-to-Image Diffusion Models",
                        "Citation Paper Abstract": "Abstract:This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out this https URL for an overview of the results.",
                        "Citation Paper Authors": "Authors:Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.14604v1": {
            "Paper Title": "Reduced Representation of Deformation Fields for Effective Non-rigid\n  Shape Matching",
            "Sentences": [
                {
                    "Sentence ID": 80,
                    "Sentence": ". We train our method on a subset of 1000\nSURREAL shapes ",
                    "Citation Text": "G\u00fcl Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev,\nand Cordelia Schmid. Learning from synthetic humans. In CVPR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.01370",
                        "Citation Paper Title": "Title:Learning from Synthetic Humans",
                        "Citation Paper Abstract": "Abstract:Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data.",
                        "Citation Paper Authors": "Authors:G\u00fcl Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "consisting of 20 shapes of the same human in 20 distinct poses. We further augment the challenge\n7Method Correspondence Error\nType Name #Tr data SHREC\u201919 FAUST(NI) SCAPE(PC+N)\nSpectral GeoFMap ",
                    "Citation Text": "Nicolas Donati, Abhishek Sharma, and Maks Ovsjanikov. Deep geometric functional maps:\nRobust feature learning for shape correspondence. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 8592\u20138601, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14286",
                        "Citation Paper Title": "Title:Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence",
                        "Citation Paper Abstract": "Abstract:We present a novel learning-based approach for computing correspondences between non-rigid 3D shapes. Unlike previous methods that either require extensive training data or operate on handcrafted input descriptors and thus generalize poorly across diverse datasets, our approach is both accurate and robust to changes in shape structure. Key to our method is a feature-extraction network that learns directly from raw shape geometry, combined with a novel regularized map extraction layer and loss, based on the functional map representation. We demonstrate through extensive experiments in challenging shape matching scenarios that our method can learn from less training data than existing supervised approaches and generalizes significantly better than current descriptor-based learning methods. Our source code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Donati, Abhishek Sharma, Maks Ovsjanikov"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "Ours GT\nFigure 3: Color-coded correspondences on the SCAPE (PC+N) dataset. \u201cTwist\u201d is a challenging\narticulation as a wrong deformation can lead to large geodesic error (see Cheese-Pull effect in ",
                    "Citation Text": "Marvin Eisenberger, Zorah L\u00e4hner, and Daniel Cremers. Smooth shells: Multi-scale shape\nregistration with functional maps. 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 12262\u201312271, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12512",
                        "Citation Paper Title": "Title:Smooth Shells: Multi-Scale Shape Registration with Functional Maps",
                        "Citation Paper Abstract": "Abstract:We propose a novel 3D shape correspondence method based on the iterative alignment of so-called smooth shells. Smooth shells define a series of coarse-to-fine shape approximations designed to work well with multiscale algorithms. The main idea is to first align rough approximations of the geometry and then add more and more details to refine the correspondence. We fuse classical shape registration with Functional Maps by embedding the input shapes into an intrinsic-extrinsic product space. Moreover, we disambiguate intrinsic symmetries by applying a surrogate based Markov chain Monte Carlo initialization. Our method naturally handles various types of noise that commonly occur in real scans, like non-isometry or incompatible meshing. Finally, we demonstrate state-of-the-art quantitative results on several datasets and show that our pipeline produces smoother, more realistic results than other automatic matching methods in real world applications.",
                        "Citation Paper Authors": "Authors:Marvin Eisenberger, Zorah L\u00e4hner, Daniel Cremers"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": ", instead of learning the weights of the least\nsquares function, we instead learn deformation values at nodes and demonstrate our method to be\napplicable in wide-range of downstream tasks. Alternatively, Eisenberger et al. ",
                    "Citation Text": "Marvin Eisenberger, Zorah L\u00e4hner, and Daniel Cremers. Divergence-free shape correspondence\nby deformation. Computer Graphics Forum , 38, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.10417",
                        "Citation Paper Title": "Title:Divergence-Free Shape Interpolation and Correspondence",
                        "Citation Paper Abstract": "Abstract:We present a novel method to model and calculate deformation fields between shapes embedded in $\\mathbb{R}^D$. Our framework combines naturally interpolating the two input shapes and calculating correspondences at the same time. The key idea is to compute a divergence-free deformation field represented in a coarse-to-fine basis using the Karhunen-Lo\u00e8ve expansion. The advantages are that there is no need to discretize the embedding space and the deformation is volume-preserving. Furthermore, the optimization is done on downsampled versions of the shapes but the morphing can be applied to any resolution without a heavy increase in complexity. We show results for shape correspondence, registration, inter- and extrapolation on the TOSCA and FAUST data sets.",
                        "Citation Paper Authors": "Authors:Marvin Eisenberger, Zorah L\u00e4hner, Daniel Cremers"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", animation [ 40,70,71] and more recently in a\ndata-driven framework ",
                    "Citation Text": "Meitar Shechter, Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. DeepMLS:\ngeometry-aware control point deformation. Proc. Eurographics Short Papers , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.01873",
                        "Citation Paper Title": "Title:NeuralMLS: Geometry-Aware Control Point Deformation",
                        "Citation Paper Abstract": "Abstract:We introduce NeuralMLS, a space-based deformation technique, guided by a set of displaced control points. We leverage the power of neural networks to inject the underlying shape geometry into the deformation parameters. The goal of our technique is to enable a realistic and intuitive shape deformation. Our method is built upon moving least-squares (MLS), since it minimizes a weighted sum of the given control point displacements. Traditionally, the influence of each control point on every point in space (i.e., the weighting function) is defined using inverse distance heuristics. In this work, we opt to learn the weighting function, by training a neural network on the control points from a single input shape, and exploit the innate smoothness of neural networks. Our geometry-aware control point deformation is agnostic to the surface representation and quality; it can be applied to point clouds or meshes, including non-manifold and disconnected surface soups. We show that our technique facilitates intuitive piecewise smooth deformations, which are well suited for manufactured objects. We show the advantages of our approach compared to existing surface and space-based deformation techniques, both quantitatively and qualitatively.",
                        "Citation Paper Authors": "Authors:Meitar Shechter, Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.16210v1": {
            "Paper Title": "PaCMO: Partner Dependent Human Motion Generation in Dyadic Human\n  Activity using Neural Operators",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "provides a framework\nto generate action-conditioned single and multi-person mo-\ntion sequences using a transformer architecture. MUGL ",
                    "Citation Text": "Shubh Maheshwari, Debtanu Gupta, and Ravi Kiran Sar-\nvadevabhatla. Mugl: Large scale multi person conditional\naction generation with locomotion. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 257\u2013265, 2022. 1, 2, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.11460",
                        "Citation Paper Title": "Title:MUGL: Large Scale Multi Person Conditional Action Generation with Locomotion",
                        "Citation Paper Abstract": "Abstract:We introduce MUGL, a novel deep neural model for large-scale, diverse generation of single and multi-person pose-based action sequences with locomotion. Our controllable approach enables variable-length generations customizable by action category, across more than 100 categories. To enable intra/inter-category diversity, we model the latent generative space using a Conditional Gaussian Mixture Variational Autoencoder. To enable realistic generation of actions involving locomotion, we decouple local pose and global trajectory components of the action sequence. We incorporate duration-aware feature representations to enable variable-length sequence generation. We use a hybrid pose sequence representation with 3D pose sequences sourced from videos and 3D Kinect-based sequences of NTU-RGBD-120. To enable principled comparison of generation quality, we employ suitably modified strong baselines during evaluation. Although smaller and simpler compared to baselines, MUGL provides better quality generations, paving the way for practical and controllable large-scale human action generation.",
                        "Citation Paper Authors": "Authors:Shubh Maheshwari, Debtanu Gupta, Ravi Kiran Sarvadevabhatla"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "datasets.\n\u2022NTU-RGB+D 120 is the extended version of original\nNTU-RGB+D ",
                    "Citation Text": "Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 1010\u20131019, 2016. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.02808",
                        "Citation Paper Title": "Title:NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis",
                        "Citation Paper Abstract": "Abstract:Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.",
                        "Citation Paper Authors": "Authors:Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "is also a text-conditioned human motion gen-\nerator but it provides temporal control over the generated\nmotion. MotionCLIP [14, 45] leverages CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nInInternational Conference on Machine Learning , pages\n8748\u20138763. PMLR, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "generates\nhuman motion with the speci\ufb01c goal of grasping objects.\nTEACH ",
                    "Citation Text": "Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG\u00fcl Varol. Teach: Temporal action composition for 3d hu-\nmans. arXiv preprint arXiv:2209.04066 , 2022. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.04066",
                        "Citation Paper Title": "Title:TEACH: Temporal Action Composition for 3D Humans",
                        "Citation Paper Abstract": "Abstract:Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for \"TEmporal Action Compositions for Human motions\", produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at our $\\href{this http URL}{\\text{website}}$.",
                        "Citation Paper Authors": "Authors:Nikos Athanasiou, Mathis Petrovich, Michael J. Black, G\u00fcl Varol"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "use a trans-\nformer architecture to generate human motion, whereas the\nformer uses a pre-trained language model DistilBERT ",
                    "Citation Text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas\nWolf. Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.01108",
                        "Citation Paper Title": "Title:DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                        "Citation Paper Abstract": "Abstract:As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
                        "Citation Paper Authors": "Authors:Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.14054v1": {
            "Paper Title": "CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic\n  Synthetic Data Generation for the Manufacturing Industry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13887v1": {
            "Paper Title": "TPA-Net: Generate A Dataset for Text to Physics-based Animation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "is one of the\niconic synthetic V&L datasets. Besides, there are billions\nof image-text pairs [35, 37, 47] available on the internet.\nHowever, high-quality video-text datasets are much less\navailable. Existing work includes HowTo100M ",
                    "Citation Text": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2630\u20132640, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03327",
                        "Citation Paper Title": "Title:HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
                        "Citation Paper Abstract": "Abstract:Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: this http URL.",
                        "Citation Paper Authors": "Authors:Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "further applies text-to-3D generation to Avatar.\nVision-Language Datasets Microsoft COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740\u2013755.\nSpringer, 2014. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "incorporates\nCLIP with NeRF, enabling simple text-editable 3D object\nmanipulation; ",
                    "Citation Text": "Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human motion\ngeneration to clip space. arXiv preprint arXiv:2203.08063 ,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.08063",
                        "Citation Paper Title": "Title:MotionCLIP: Exposing Human Motion Generation to CLIP Space",
                        "Citation Paper Abstract": "Abstract:We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent embedding that is disentangled, well behaved, and supports highly semantic textual descriptions. MotionCLIP gains its unique power by aligning its latent space with that of the Contrastive Language-Image Pre-training (CLIP) model. Aligning the human motion manifold to CLIP space implicitly infuses the extremely rich semantic knowledge of CLIP into the manifold. In particular, it helps continuity by placing semantically similar motions close to one another, and disentanglement, which is inherited from the CLIP-space structure. MotionCLIP comprises a transformer-based motion auto-encoder, trained to reconstruct motion while being aligned to its text label's position in CLIP-space. We further leverage CLIP's unique visual understanding and inject an even stronger signal through aligning motion to rendered frames in a self-supervised manner. We show that although CLIP has never seen the motion domain, MotionCLIP offers unprecedented text-to-motion abilities, allowing out-of-domain actions, disentangled editing, and abstract language specification. For example, the text prompt \"couch\" is decoded into a sitting down motion, due to lingual similarity, and the prompt \"Spiderman\" results in a web-swinging-like solution that is far from seen during training. In addition, we show how the introduced latent space can be leveraged for motion interpolation, editing and recognition.",
                        "Citation Paper Authors": "Authors:Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "uses texts to control lighting\nconditions in rendering. Besides, several works use CLIP to\nenable text-to-3D representations. For example, ",
                    "Citation Text": "Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Popa Tiberiu. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. December\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13333",
                        "Citation Paper Title": "Title:CLIP-Mesh: Generating textured meshes from text using pretrained image-text models",
                        "Citation Paper Abstract": "Abstract:We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.",
                        "Citation Paper Authors": "Authors:Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "to multi-\nmodal generation. Similarly, [59, 60] apply GAN variants\nand further enhance the quality of the generated images\nwith improved image-text alignments. Other works, such\nas DALL-E ",
                    "Citation Text": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning , pages 8821\u20138831. PMLR, 2021.\n1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12092",
                        "Citation Paper Title": "Title:Zero-Shot Text-to-Image Generation",
                        "Citation Paper Abstract": "Abstract:Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13752v1": {
            "Paper Title": "Sketch-Guided Text-to-Image Diffusion Models",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "proposes to iteratively re\ufb01ne the diffusion pro-\ncess using a noisy reference image at each time-step dur-\ning inference, enabling control of the amount of high-level\nsemantics being adapted from the images. In addition,\nSDEdit ",
                    "Citation Text": "Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-\nYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and\nediting with stochastic differential equations. arXiv preprint\narXiv:2108.01073 , 2021. 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.01073",
                        "Citation Paper Title": "Title:SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
                        "Citation Paper Abstract": "Abstract:Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.",
                        "Citation Paper Authors": "Authors:Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "samples\nwith their class names as captions (e.g., \u201cshoes\u201d). The cor-\nresponding edge maps were generated with the edge pre-\ndiction model of ",
                    "Citation Text": "Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi\nTian, Matti Pietik \u00a8ainen, and Li Liu. Pixel difference net-\nworks for ef\ufb01cient edge detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 5117\u20135127, 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.07009",
                        "Citation Paper Title": "Title:Pixel Difference Networks for Efficient Edge Detection",
                        "Citation Paper Abstract": "Abstract:Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection with the rich and abstract edge representation capacities. However, the high performance of CNN based edge detection is achieved with a large pretrained CNN backbone, which is memory and energy consuming. In addition, it is surprising that the previous wisdom from the traditional edge detectors, such as Canny, Sobel, and LBP are rarely investigated in the rapid-developing deep learning era. To address these issues, we propose a simple, lightweight yet effective architecture named Pixel Difference Network (PiDiNet) for efficient edge detection. Extensive experiments on BSDS500, NYUD, and Multicue are provided to demonstrate its effectiveness, and its high training and inference efficiency. Surprisingly, when training from scratch with only the BSDS500 and VOC datasets, PiDiNet can surpass the recorded result of human perception (0.807 vs. 0.803 in ODS F-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A faster version of PiDiNet with less than 0.1M parameters can still achieve comparable performance among state of the arts with 200 FPS. Results on the NYUD and Multicue datasets show similar observations. The codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietik\u00e4inen, Li Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "that suggests to use a pretrained unconditional diffu-\nsion model for various image translation tasks, by training\na specialized, per-task, encoder ",
                    "Citation Text": "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding\nin style: a stylegan encoder for image-to-image translation.\narXiv preprint arXiv:2008.00951 , 2020. 2\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.00951",
                        "Citation Paper Title": "Title:Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.",
                        "Citation Paper Authors": "Authors:Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "allow the user a\nparticular type of control by enabling them to provide a se-\nmantic segmentation map to control the composition of the\nelements in the synthesized image.\nILVR ",
                    "Citation Text": "Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. Ilvr: Conditioning method for\ndenoising diffusion probabilistic models. arXiv preprint\narXiv:2108.02938 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.02938",
                        "Citation Paper Title": "Title:ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.",
                        "Citation Paper Authors": "Authors:Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "minimizes the distances between the em-\nbeddings of the input sketch and the corresponding ground\ntruth real image in the vector-quantized space of a VQ-\nGAN ",
                    "Citation Text": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 12873\u201312883, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09841",
                        "Citation Paper Title": "Title:Taming Transformers for High-Resolution Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .",
                        "Citation Paper Authors": "Authors:Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "lever-\nages conditional GAN with joint image-sketch representa-\ntion. CoGS ",
                    "Citation Text": "Cusuh Ham, Gemma Canet Tarres, Tu Bui, James Hays,\nZhe Lin, and John Collomosse. Cogs: Controllable gen-\neration and search from sketch and style. arXiv preprint\narXiv:2203.09554 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09554",
                        "Citation Paper Title": "Title:CoGS: Controllable Generation and Search from Sketch and Style",
                        "Citation Paper Abstract": "Abstract:We present CoGS, a novel method for the style-conditioned, sketch-driven synthesis of images. CoGS enables exploration of diverse appearance possibilities for a given sketched object, enabling decoupled control over the structure and the appearance of the output. Coarse-grained control over object structure and appearance are enabled via an input sketch and an exemplar \"style\" conditioning image to a transformer-based sketch and style encoder to generate a discrete codebook representation. We map the codebook representation into a metric space, enabling fine-grained control over selection and interpolation between multiple synthesis options before generating the image via a vector quantized GAN (VQGAN) decoder. Our framework thereby unifies search and synthesis tasks, in that a sketch and style pair may be used to run an initial synthesis which may be refined via combination with similar results in a search corpus to produce an image more closely matching the user's intent. We show that our model, trained on the 125 object classes of our newly created Pseudosketches dataset, is capable of producing a diverse gamut of semantic content and appearance styles.",
                        "Citation Paper Authors": "Authors:Cusuh Ham, Gemma Canet Tarres, Tu Bui, James Hays, Zhe Lin, John Collomosse"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "uses\nedge-preserving image augmentations to train a Generative\nAdversarial Network (GAN), ContextualGAN ",
                    "Citation Text": "Yongyi Lu, Shangzhe Wu, Yu-Wing Tai, and Chi-Keung\nTang. Image generation from sketch constraint using con-\ntextual gan. In Proceedings of the European conference on\ncomputer vision (ECCV) , pages 205\u2013220, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08972",
                        "Citation Paper Title": "Title:Image Generation from Sketch Constraint Using Contextual GAN",
                        "Citation Paper Abstract": "Abstract:In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint image's point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.",
                        "Citation Paper Authors": "Authors:Yongyi Lu, Shangzhe Wu, Yu-Wing Tai, Chi-Keung Tang"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "2.1. Image-to-Image Translation\nImage-to-image translation has been a long-standing\ntask in the computer vision domain with a myriad of ex-\nplorations and prior works ",
                    "Citation Text": "Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen.\nImage-to-image translation: Methods and applications.\nIEEE Transactions on Multimedia , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08629",
                        "Citation Paper Title": "Title:Image-to-Image Translation: Methods and Applications",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.",
                        "Citation Paper Authors": "Authors:Yingxue Pang, Jianxin Lin, Tao Qin, Zhibo Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13494v1": {
            "Paper Title": "Immersive Neural Graphics Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10618v2": {
            "Paper Title": "Fully implicit frictional dynamics with soft constraints",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "favored\na more traditional root-finding solver combining friction and con-\ntact forces with elastic equations of motion. In contrast, Li et al. ",
                    "Citation Text": "Minchen Li, Zachary Ferguson, Teseo Schneider, Timothy Langlois, Denis Zorin,\nDaniele Panozzo, Chenfanfu Jiang, and Danny M. Kaufman. 2020. Incremental\nPotential Contact: Intersection- and Inversion-free Large Deformation Dynamics.\nACM Transactions on Graphics 39, 4 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.15908",
                        "Citation Paper Title": "Title:Convergent Incremental Potential Contact",
                        "Citation Paper Abstract": "Abstract:Recent advances in the simulation of frictionally contacting elastodynamics with the Incremental Potential Contact (IPC) model have enabled inversion and intersection-free simulation via the application of mollified barriers, filtered line-search, and optimization-based solvers for time integration. In its current formulation the IPC model is constructed via a discrete constraint model, replacing non-interpenetration constraints with barrier potentials on an already spatially discretized domain. However, while effective, this purely discrete formulation prohibits convergence under refinement. To enable a convergent IPC model we reformulate IPC potentials in the continuous setting and provide a first, convergent discretization thereof. We demonstrate and analyze the convergence behavior of this new model and discretization on a range of elastostatic and dynamic contact problems, and evaluate its accuracy on both analytical benchmarks and application-driven examples.",
                        "Citation Paper Authors": "Authors:Minchen Li, Zachary Ferguson, Teseo Schneider, Timothy Langlois, Denis Zorin, Daniele Panozzo, Chenfanfu Jiang, Danny M. Kaufman"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". More recently, more\nattention was brought towards modelling friction as a smoothlychanging force at the stick-slip limit [ 16,27]. This allows each sim-\nulation step to remain differentiable. Geilinger et al. ",
                    "Citation Text": "Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz B\u00e4cher, Bernhard\nThomaszewski, and Stelian Coros. 2020. ADD: analytically differentiable dy-\nnamics for multi-body systems with frictional contact. ACM Transactions on\nGraphics 39, 6 (Nov. 2020), 1\u201315. https://doi.org/10.1145/3414685.3417766",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00987",
                        "Citation Paper Title": "Title:ADD: Analytically Differentiable Dynamics for Multi-Body Systems with Frictional Contact",
                        "Citation Paper Abstract": "Abstract:We present a differentiable dynamics solver that is able to handle frictional contact for rigid and deformable objects within a unified framework. Through a principled mollification of normal and tangential contact forces, our method circumvents the main difficulties inherent to the non-smooth nature of frictional contact. We combine this new contact model with fully-implicit time integration to obtain a robust and efficient dynamics solver that is analytically differentiable. In conjunction with adjoint sensitivity analysis, our formulation enables gradient-based optimization with adaptive trade-offs between simulation accuracy and smoothness of objective function landscapes. We thoroughly analyse our approach on a set of simulation examples involving rigid bodies, visco-elastic materials, and coupled multi-body systems. We furthermore showcase applications of our differentiable simulator to parameter estimation for deformable objects, motion planning for robotic manipulation, trajectory optimization for compliant walking robots, as well as efficient self-supervised learning of control policies.",
                        "Citation Paper Authors": "Authors:Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz B\u00e4cher, Bernhard Thomaszewski, Stelian Coros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13333v1": {
            "Paper Title": "Learning to Rasterize Differentiable",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "proposed softening the z-buffer by\na weighted softmax defined over depth. Dedicated aggre-\ngation functions for silhouette computation have also been\nproposed, which differentiate scenes in binary color and are\nindependent from depth. Later, Petersen et al. ",
                    "Citation Text": "Felix Petersen, Bastian Goldluecke, Christian Borgelt, and\nOliver Deussen. Gendr: A generalized differentiable ren-\nderer. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 4002\u2013\n4011, June 2022. 2, 4, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.13845",
                        "Citation Paper Title": "Title:GenDR: A Generalized Differentiable Renderer",
                        "Citation Paper Abstract": "Abstract:In this work, we present and study a generalized family of differentiable renderers. We discuss from scratch which components are necessary for differentiable rendering and formalize the requirements for each component. We instantiate our general differentiable renderer, which generalizes existing differentiable renderers like SoftRas and DIB-R, with an array of different smoothing distributions to cover a large spectrum of reasonable settings. We evaluate an array of differentiable renderer instantiations on the popular ShapeNet 3D reconstruction benchmark and analyze the implications of our results. Surprisingly, the simple uniform distribution yields the best overall results when averaged over 13 classes; in general, however, the optimal choice of distribution heavily depends on the task.",
                        "Citation Paper Authors": "Authors:Felix Petersen, Bastian Goldluecke, Christian Borgelt, Oliver Deussen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.03017v2": {
            "Paper Title": "Learning-based Inverse Rendering of Complex Indoor Scenes with\n  Differentiable Monte Carlo Raytracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.07706v2": {
            "Paper Title": "ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D\n  Human Motion Generation",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". The\ndata distribution of different action categories in BABEL is\nremarkably long-tailed. Therefore, we adopt a square-root\nsampling ",
                    "Citation Text": "Bingyi Kang, Saining Xie, Marcus Rohrbach,\nZhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis\nKalantidis. Decoupling representation and classi\ufb01er\nfor long-tailed recognition. In ICLR . OpenReview.net,\n2020. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.09217",
                        "Citation Paper Title": "Title:Decoupling Representation and Classifier for Long-Tailed Recognition",
                        "Citation Paper Abstract": "Abstract:The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposes novel regulariza-\ntion methods to improve the stability of Transformer-based\nGAN training. ",
                    "Citation Text": "Ricard Durall, Stanislav Frolov, Andreas Dengel, and\nJanis Keuper. Combining transformer generators with\nconvolutional discriminators. CoRR , abs/2105.10189,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.10189",
                        "Citation Paper Title": "Title:Combining Transformer Generators with Convolutional Discriminators",
                        "Citation Paper Abstract": "Abstract:Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator.",
                        "Citation Paper Authors": "Authors:Ricard Durall, Stanislav Frolov, J\u00f6rn Hees, Federico Raue, Franz-Josef Pfreundt, Andreas Dengel, Janis Keupe"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "is the \ufb01rst pure Transformer-based GAN archi-\ntecture. Later ViTGAN ",
                    "Citation Text": "Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang,\nZhuowen Tu, and Ce Liu. Vitgan: Training gans with\nvision transformers. CoRR , abs/2107.04589, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.04589",
                        "Citation Paper Title": "Title:ViTGAN: Training GANs with Vision Transformers",
                        "Citation Paper Abstract": "Abstract:Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.",
                        "Citation Paper Authors": "Authors:Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "in visual recognition\ntasks inspires its application in generation tasks. Trans-\nGAN ",
                    "Citation Text": "Yifan Jiang, Shiyu Chang, and Zhangyang Wang.\nTransgan: Two transformers can make one strong\nGAN. CoRR , abs/2102.07074, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07074",
                        "Citation Paper Title": "Title:TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up",
                        "Citation Paper Abstract": "Abstract:The recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures. On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Specifically, TransGAN sets new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2. When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high fidelity and impressive texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yifan Jiang, Shiyu Chang, Zhangyang Wang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "adopts the pooling module\nto aggregate information across multiple persons. ",
                    "Citation Text": "Vida Adeli, Mahsa Ehsanpour, Ian D. Reid, Juan Car-\nlos Niebles, Silvio Savarese, Ehsan Adeli, and\nHamid Rezato\ufb01ghi. Tripod: Human trajectory and\npose dynamics forecasting in the wild. CoRR ,\nabs/2104.04029, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.04029",
                        "Citation Paper Title": "Title:TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild",
                        "Citation Paper Abstract": "Abstract:Joint forecasting of human trajectory and pose dynamics is a fundamental building block of various applications ranging from robotics and autonomous driving to surveillance systems. Predicting body dynamics requires capturing subtle information embedded in the humans' interactions with each other and with the objects present in the scene. In this paper, we propose a novel TRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph attentional networks to model the human-human and human-object interactions both in the input space and the output space (decoded future output). The model is supplemented by a message passing interface over the graphs to fuse these different levels of interactions efficiently. Furthermore, to incorporate a real-world challenge, we propound to learn an indicator representing whether an estimated body joint is visible/invisible at each frame, e.g. due to occlusion or being outside the sensor field of view. Finally, we introduce a new benchmark for this joint task based on two challenging datasets (PoseTrack and 3DPW) and propose evaluation metrics to measure the effectiveness of predictions in the global space, even when there are invisible cases of joints. Our evaluation shows that TRiPOD outperforms all prior work and state-of-the-art specifically designed for each of the trajectory and pose forecasting tasks.",
                        "Citation Paper Authors": "Authors:Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Carlos Niebles, Silvio Savarese, Ehsan Adeli, Hamid Rezatofighi"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "to predict dance motions con-\nditioned on music.\nMulti-person interactions have also been considered in\nmotion prediction. ",
                    "Citation Text": "Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio\nSavarese, and Alexandre Alahi. Social GAN: socially\nacceptable trajectories with generative adversarial net-\nworks. In CVPR , pages 2255\u20132264. Computer Vision\nFoundation / IEEE Computer Society, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.10892",
                        "Citation Paper Title": "Title:Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",
                        "Citation Paper Authors": "Authors:Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.09834v2": {
            "Paper Title": "Real-time Model Predictive Control and System Identification Using\n  Differentiable Physics Simulation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". Moreover,\n[10, 11] solve and update system parameters using stochastic\noptimization. Recent learning-based approaches attempted\nto improve modeling accuracy and model unknown time-\nvarying components. For example, Yu et al. ",
                    "Citation Text": "W. Yu, J. Tan, C. K. Liu, and G. Turk, \u201cPreparing for the\nunknown: Learning a universal policy with online system\nidenti\ufb01cation,\u201d in RSS, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02453",
                        "Citation Paper Title": "Title:Preparing for the Unknown: Learning a Universal Policy with Online System Identification",
                        "Citation Paper Abstract": "Abstract:We present a new method of learning control policies that successfully operate under unknown dynamic models. We create such policies by leveraging a large number of training examples that are generated using a physical simulator. Our system is made of two components: a Universal Policy (UP) and a function for Online System Identification (OSI). We describe our control policy as universal because it is trained over a wide array of dynamic models. These variations in the dynamic model may include differences in mass and inertia of the robots' components, variable friction coefficients, or unknown mass of an object to be manipulated. By training the Universal Policy with this variation, the control policy is prepared for a wider array of possible conditions when executed in an unknown environment. The second part of our system uses the recent state and action history of the system to predict the dynamics model parameters mu. The value of mu from the Online System Identification is then provided as input to the control policy (along with the system state). Together, UP-OSI is a robust control policy that can be used across a wide range of dynamic models, and that is also responsive to sudden changes in the environment. We have evaluated the performance of this system on a variety of tasks, including the problem of cart-pole swing-up, the double inverted pendulum, locomotion of a hopper, and block-throwing of a manipulator. UP-OSI is effective at these tasks across a wide range of dynamic models. Moreover, when tested with dynamic models outside of the training range, UP-OSI outperforms the Universal Policy alone, even when UP is given the actual value of the model dynamics. In addition to the benefits of creating more robust controllers, UP-OSI also holds out promise of narrowing the Reality Gap between simulated and real physical systems.",
                        "Citation Paper Authors": "Authors:Wenhao Yu, Jie Tan, C. Karen Liu, Greg Turk"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "learned a\nneural network to model time varying parameters. Jiang et al. ",
                    "Citation Text": "Y . Jiang, T. Zhang, D. Ho, Y . Bai, C. K. Liu, S. Levine, and\nJ. Tan, \u201cSimgan: Hybrid simulator identi\ufb01cation for domainadaptation via adversarial reinforcement learning,\u201d in ICRA ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06005",
                        "Citation Paper Title": "Title:SimGAN: Hybrid Simulator Identification for Domain Adaptation via Adversarial Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:As learning-based approaches progress towards automating robot controllers design, transferring learned policies to new domains with different dynamics (e.g. sim-to-real transfer) still demands manual effort. This paper introduces SimGAN, a framework to tackle domain adaptation by identifying a hybrid physics simulator to match the simulated trajectories to the ones from the target domain, using a learned discriminative loss to address the limitations associated with manual loss design. Our hybrid simulator combines neural networks and traditional physics simulation to balance expressiveness and generalizability, and alleviates the need for a carefully selected parameter set in System ID. Once the hybrid simulator is identified via adversarial reinforcement learning, it can be used to refine policies for the target domain, without the need to interleave data collection and policy refinement. We show that our approach outperforms multiple strong baselines on six robotic locomotion tasks for domain adaptation.",
                        "Citation Paper Authors": "Authors:Yifeng Jiang, Tingnan Zhang, Daniel Ho, Yunfei Bai, C. Karen Liu, Sergey Levine, Jie Tan"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "and nonlinear\nsystems [ 4,5]. Of\ufb02ine SysID can be achieved by differentiable\nphysics engine [ 6,7,8], as well as learned DNNs that\napproximate dynamic systems ",
                    "Citation Text": "M. Lutter, C. Ritter, and J. Peters, \u201cDeep lagrangian networks:\nUsing physics as model prior for deep learning,\u201d in ICLR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.04490",
                        "Citation Paper Title": "Title:Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep learning has achieved astonishing results on many tasks with large amounts of data and generalization within the proximity of training data. For many important real-world applications, these requirements are unfeasible and additional prior knowledge on the task domain is required to overcome the resulting problems. In particular, learning physics models for model-based control requires robust extrapolation from fewer samples - often collected online in real-time - and model errors may lead to drastic damages of the system. Directly incorporating physical insight has enabled us to obtain a novel deep model learning approach that extrapolates well while requiring fewer samples. As a first example, we propose Deep Lagrangian Networks (DeLaN) as a deep network structure upon which Lagrangian Mechanics have been imposed. DeLaN can learn the equations of motion of a mechanical system (i.e., system dynamics) with a deep network efficiently while ensuring physical plausibility. The resulting DeLaN network performs very well at robot tracking control. The proposed method did not only outperform previous model learning approaches at learning speed but exhibits substantially improved and more robust extrapolation to novel trajectories and learns online in real-time",
                        "Citation Paper Authors": "Authors:Michael Lutter, Christian Ritter, Jan Peters"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11931v1": {
            "Paper Title": "Layered-Garment Net: Generating Multiple Implicit Garment Layers from a\n  Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11416v1": {
            "Paper Title": "Fairing-PIA: Progressive iterative approximation for fairing curve and\n  surface generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10885v2": {
            "Paper Title": "KiloNeuS: A Versatile Neural Implicit Surface Representation for\n  Real-Time Rendering",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ".\nKiloNeRF produces very close results, but performs con-\nsiderably better over the perceptual metric LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ",\na ray-marching heuristic allowing for real-time rendering.\nNeural Radiance Fields Neural radiance \ufb01elds were pro-\nposed in ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In ECCV , 2020. 1, 3, 4, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11319v1": {
            "Paper Title": "VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "(SD), a text-to-image latent diffusion\nmodel. While these models produce high-\ufb01delity images,\nthey cannot be directly transformed into vector graphics.\nA number of works generate vector graphics from input\nimages. We extend the work of Layer-wise Image Vectoriza-\ntion (LIVE) ",
                    "Citation Text": "Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,\nNikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-wise\nimage vectorization. In Proceedings of the IEEE conference\non computer vision and pattern recognition , 2022. 3, 5, 7, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.04655",
                        "Citation Paper Title": "Title:Towards Layer-wise Image Vectorization",
                        "Citation Paper Abstract": "Abstract:Image rasterization is a mature technique in computer graphics, while image vectorization, the reverse path of rasterization, remains a major challenge. Recent advanced deep learning-based models achieve vectorization and semantic interpolation of vector graphs and demonstrate a better topology of generating new figures. However, deep models cannot be easily generalized to out-of-domain testing data. The generated SVGs also contain complex and redundant shapes that are not quite convenient for further editing. Specifically, the crucial layer-wise topology and fundamental semantics in images are still not well understood and thus not fully explored. In this work, we propose Layer-wise Image Vectorization, namely LIVE, to convert raster images to SVGs and simultaneously maintain its image topology. LIVE can generate compact SVG forms with layer-wise structures that are semantically consistent with human perspective. We progressively add new bezier paths and optimize these paths with the layer-wise framework, newly designed loss functions, and component-wise path initialization technique. Our experiments demonstrate that LIVE presents more plausible vectorized forms than prior works and can be generalized to new images. With the help of this newly learned topology, LIVE initiates human editable SVGs for both designers and other downstream applications. Codes are made available at this https URL.",
                        "Citation Paper Authors": "Authors:Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, Humphrey Shi"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "uses diffusion models as priors for conditional im-\nage generation, segmentation, and more. DreamFusion ",
                    "Citation Text": "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv , 2022. 3,\n4, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14988",
                        "Citation Paper Title": "Title:DreamFusion: Text-to-3D using 2D Diffusion",
                        "Citation Paper Abstract": "Abstract:Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",
                        "Citation Paper Authors": "Authors:Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ", or\noptimizing sketches to match a reference image in CLIP\nfeature space ",
                    "Citation Text": "Yael Vinker, Ehsan Pajouheshgar, Jessica Y . Bo, Roman Chris-\ntian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir\nZamir, and Ariel Shamir. Clipasso: Semantically-aware ob-\nject sketching, 2022. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05822",
                        "Citation Paper Title": "Title:CLIPasso: Semantically-Aware Object Sketching",
                        "Citation Paper Abstract": "Abstract:Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of B\u00e9zier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.",
                        "Citation Paper Authors": "Authors:Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "used SDS with the Imagen pixel space\ndiffusion model to learn the parameters of a 3D Neural Ra-\ndiance Field ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance \ufb01elds for view synthe-\nsis.ECCV , 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "can be optimized\nwithLSDSto bring it toward the conditional distribution of\nthe teacher. This is inspired by probability density distilla-\ntion ",
                    "Citation Text": "A\u00a8aron van den Oord, Yazhe Li, Igor Babuschkin, Karen Si-\nmonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den\nDriessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg,\nNorman Casagrande, Dominik Grewe, Seb Noury, Sander\nDieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex\nGraves, Helen King, Tom Walters, Dan Belov, and Demis\nHassabis. Parallel WaveNet: Fast high-\ufb01delity speech synthe-\nsis.ICML , 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10433",
                        "Citation Paper Title": "Title:Parallel WaveNet: Fast High-Fidelity Speech Synthesis",
                        "Citation Paper Abstract": "Abstract:The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, Demis Hassabis"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "train on a reduced spatial resolution by\ncompressing 512\u0002512images into a relatively compact 64\u0002\n64, 4-channel latent space with a VQGAN-like autoencoder\n(E;D ) ",
                    "Citation Text": "Patrick Esser, Robin Rombach, and Bj \u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis, 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09841",
                        "Citation Paper Title": "Title:Taming Transformers for High-Resolution Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .",
                        "Citation Paper Authors": "Authors:Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "uses\nan evolutionary approach to create image collages. Though\nwe also use pretrained vision-language models, we use a\ngenerative model, Stable Diffusion ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models, 2021. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". This bound reduces\nto a weighted mixture of denoising objectives ",
                    "Citation Text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS , 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11239",
                        "Citation Paper Title": "Title:Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Ajay Jain, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ":\nLDDPM (\u001e;x) =Et;\u000f[w(t)k\u000f\u001e(\u000btx+\u001bt\u000f)\u0000\u000fk2\n2](1)\nwhere xis a real data sample and t2f1;2;:::Tgis a uni-\nformly sampled timestep scalar that indexes noise schedules\n\u000bt;\u001bt ",
                    "Citation Text": "Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. NeurIPS , 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.00630",
                        "Citation Paper Title": "Title:Variational Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", we use diffusion\nmodels as transferable priors for vector graphics. Concurrent\nwork ",
                    "Citation Text": "Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.07600",
                        "Citation Paper Title": "Title:Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures",
                        "Citation Paper Abstract": "Abstract:Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As NeRFs operate in image space, a naive solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering. Implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ".\nWe also take inspiration from inverse graphics with dif-\nfusion models. Diffusion models have been used in zero-\nshot for image-to-image tasks like inpainting ",
                    "Citation Text": "Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.09865",
                        "Citation Paper Title": "Title:RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks.\nRePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions.\nGithub Repository: this http URL",
                        "Citation Paper Authors": "Authors:Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "rather than a dis-\ncriminative model.\nRecent work has shown the success of text-to-image gen-\neration. DALL-E 2 ",
                    "Citation Text": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents, 2022. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.06125",
                        "Citation Paper Title": "Title:Hierarchical Text-Conditional Image Generation with CLIP Latents",
                        "Citation Paper Abstract": "Abstract:Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.06787v2": {
            "Paper Title": "HDR-Plenoxels: Self-Calibrating High Dynamic Range Radiance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10003v1": {
            "Paper Title": "3d human motion generation from the text via gesture action\n  classification and the autoregressive model",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ". There have been ef-\nforts of creating large-scale PLM, such as BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, \u201cBert: Pre-training of deep bidirectional\ntransformers for language understanding,\u201d arXiv preprint\narXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "dataset. Its 3D motion data is from Mi-\ncrosoft Kinect readout, which is known to be unreliable and tem-\nporally inconsistent ",
                    "Citation Text": "Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun,\nAnnan Deng, Minglun Gong, and Li Cheng, \u201cAction2motion:\nConditioned generation of 3d human motions,\u201d in Proceed-\nings of the 28th ACM International Conference on Multimedia ,\n2020, pp. 2021\u20132029.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15240",
                        "Citation Paper Title": "Title:Action2Motion: Conditioned Generation of 3D Human Motions",
                        "Citation Paper Abstract": "Abstract:Action recognition is a relatively established task, where givenan input sequence of human motion, the goal is to predict its ac-tion category. This paper, on the other hand, considers a relativelynew problem, which could be thought of as an inverse of actionrecognition: given a prescribed action type, we aim to generateplausible human motion sequences in 3D. Importantly, the set ofgenerated motions are expected to maintain itsdiversityto be ableto explore the entire action-conditioned motion space; meanwhile,each sampled sequence faithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by these objectives, we followthe physics law of human kinematics by adopting the Lie Algebratheory to represent thenaturalhuman motions; we also propose atemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of the motion space. A new 3D human motion dataset, HumanAct12, is also constructed. Empirical experiments overthree distinct human motion datasets (including ours) demonstratethe effectiveness of our approach.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.04529v2": {
            "Paper Title": "Foveated Rendering: Motivation, Taxonomy, and Research Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09037v1": {
            "Paper Title": "Complementary Textures. A Novel Approach to Object Alignment in Mixed\n  Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08702v1": {
            "Paper Title": "PointInverter: Point Cloud Reconstruction and Editing via a Generative\n  Model with Shape Priors",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "proposed to use GAN inversion for 3D point clouds to ad-\ndress the shape completion task ",
                    "Citation Text": "Xuelin Chen, Baoquan Chen, and Niloy J Mitra. Unpaired\npoint cloud completion on real scans using adversarial train-\ning. In Proceedings of the International Conference on Learn-\ning Representations (ICLR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00069",
                        "Citation Paper Title": "Title:Unpaired Point Cloud Completion on Real Scans using Adversarial Training",
                        "Citation Paper Abstract": "Abstract:As 3D scanning solutions become increasingly popular, several deep learning setups have been developed geared towards that task of scan completion, i.e., plausibly filling in regions there were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, i.e., partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training data. We develop a first approach that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. We evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport, KITTI), quantitatively on 3D-EPN shape completion benchmark dataset, and demonstrate realistic completions under varying levels of incompleteness.",
                        "Citation Paper Authors": "Authors:Xuelin Chen, Baoquan Chen, Niloy J. Mitra"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". Subse-\nquent methods include the use of autoregressive model ",
                    "Citation Text": "Yongbin Sun, Yue Wang, Ziwei Liu, Joshua Siegel, and San-\njay Sarma. PointGrow: Autoregressively learned point cloudgeneration with self-attention. In The IEEE Winter Confer-\nence on Applications of Computer Vision (WACV) , pages\n61\u201370, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05591",
                        "Citation Paper Title": "Title:PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention",
                        "Citation Paper Abstract": "Abstract:Generating 3D point clouds is challenging yet highly desired. This work presents a novel autoregressive model, PointGrow, which can generate diverse and realistic point cloud samples from scratch or conditioned on semantic contexts. This model operates recurrently, with each point sampled according to a conditional distribution given its previously-generated points, allowing inter-point correlations to be well-exploited and 3D shape generative processes to be better interpreted. Since point cloud object shapes are typically encoded by long-range dependencies, we augment our model with dedicated self-attention modules to capture such relations. Extensive evaluations show that PointGrow achieves satisfying performance on both unconditional and conditional point cloud generation tasks, with respect to realism and diversity. Several important applications, such as unsupervised feature learning and shape arithmetic operations, are also demonstrated.",
                        "Citation Paper Authors": "Authors:Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E. Siegel, Sanjay E. Sarma"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "as the pioneering method that \ufb01rst learns per-point\nfeatures and then pools them into a global feature vector,\nPointNet++ ",
                    "Citation Text": "Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Advances in Neural Informa-\ntion Processing Systems (NIPS) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.07969v1": {
            "Paper Title": "Foveated Rendering: a State-of-the-Art Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07600v1": {
            "Paper Title": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "pretrains a disentangled\nNeRF representation network on rendered object datasets,\nwhich is then used to constraint a NeRF scene optimiza-\ntion under CLIP loss, between random renderings of the\nNeRF and target image or text CLIP embedding. Dream-\nFusion ",
                    "Citation Text": "Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988 , 2022. 1, 2, 3, 4, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.14988",
                        "Citation Paper Title": "Title:DreamFusion: Text-to-3D using 2D Diffusion",
                        "Citation Paper Abstract": "Abstract:Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",
                        "Citation Paper Authors": "Authors:Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "follows a similar opti-\nmization scheme, while improving results by considering\nan explicit shading model. CLIP-Mesh ",
                    "Citation Text": "Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Popa Tiberiu. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. SIGGRAPH\nAsia 2022 Conference Papers , 2022. 2, 5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13333",
                        "Citation Paper Title": "Title:CLIP-Mesh: Generating textured meshes from text using pretrained image-text models",
                        "Citation Paper Abstract": "Abstract:We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.",
                        "Citation Paper Authors": "Authors:Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "introduced, for the \ufb01rst time, the use of largely\nsuccessful pretrained 2D diffusion models for text-guided\n3D object generation. DreamFusion uses a proprietary 2D\ndiffusion model ",
                    "Citation Text": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding, 2022.\n1, 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11487",
                        "Citation Paper Title": "Title:Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                        "Citation Paper Abstract": "Abstract:We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                        "Citation Paper Authors": "Authors:Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "employs CLIP guid-\nance as well, but uses NeRFs to represent the 3D object\ninstead of an explicit triangular mesh, together with a dedi-\ncated sparsity loss. CLIPNeRF ",
                    "Citation Text": "Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manip-\nulation of neural radiance \ufb01elds. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 3835\u20133844, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05139",
                        "Citation Paper Title": "Title:CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manipulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled conditional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation field to the positional encoding and deferring color conditioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reflect the targeted editing. The mappers are trained with a CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "intro-\nduced mesh colorization and geometric \ufb01ne-tuning by op-\ntimizing an initial mesh through differential rendering and\nCLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748\u20138763. PMLR, 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ". These gener-\nators are adversarially trained with a dataset of 2D images.\nIn ",
                    "Citation Text": "Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad\nNorouzi. Novel view synthesis with diffusion models.\narXiv:2210.04628 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.04628",
                        "Citation Paper Title": "Title:Novel View Synthesis with Diffusion Models",
                        "Citation Paper Abstract": "Abstract:We present 3DiM, a diffusion model for 3D novel view synthesis, which is able to translate a single input view into consistent and sharp completions across many views. The core component of 3DiM is a pose-conditional image-to-image diffusion model, which takes a source view and its pose as inputs, and generates a novel view for a target pose as output. 3DiM can generate multiple views that are 3D consistent using a novel technique called stochastic conditioning. The output views are generated autoregressively, and during the generation of each novel view, one selects a random conditioning view from the set of available views at each denoising step. We demonstrate that stochastic conditioning significantly improves the 3D consistency of a naive sampler for an image-to-image diffusion model, which involves conditioning on a single fixed view. We compare 3DiM to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM's generated completions from a single view achieve much higher fidelity, while being approximately 3D consistent. We also introduce a new evaluation methodology, 3D consistency scoring, to measure the 3D consistency of a generated object by training a neural field on the model's output views. 3DiM is geometry free, does not rely on hyper-networks or test-time optimization for novel view synthesis, and allows a single model to easily scale to a large number of scenes.",
                        "Citation Paper Authors": "Authors:Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.07422v1": {
            "Paper Title": "Regression-based Monte Carlo Integration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07331v1": {
            "Paper Title": "Floor Plan Exploration Framework Based on Similarity Distances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07301v1": {
            "Paper Title": "SVS: Adversarial refinement for sparse novel view synthesis",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". This model works on only three input images and is generalisable to\ndifferent scenes. Further developments were made based on geometric constraints ",
                    "Citation Text": "Mohammad Mahdi Johari, Yann Lepoittevin, and Fran\u00e7ois Fleuret. GeoNeRF: Gener-\nalizing NeRF With Geometry Priors. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.13539",
                        "Citation Paper Title": "Title:GeoNeRF: Generalizing NeRF with Geometry Priors",
                        "Citation Paper Abstract": "Abstract:We present GeoNeRF, a generalizable photorealistic novel view synthesis method based on neural radiance fields. Our approach consists of two main stages: a geometry reasoner and a renderer. To render a novel view, the geometry reasoner first constructs cascaded cost volumes for each nearby source view. Then, using a Transformer-based attention mechanism and the cascaded cost volumes, the renderer infers geometry and appearance, and renders detailed images via classical volume rendering techniques. This architecture, in particular, allows sophisticated occlusion reasoning, gathering information from consistent source views. Moreover, our method can easily be fine-tuned on a single scene, and renders competitive results with per-scene optimized neural rendering methods with a fraction of computational cost. Experiments show that GeoNeRF outperforms state-of-the-art generalizable neural rendering models on various synthetic and real datasets. Lastly, with a slight modification to the geometry reasoner, we also propose an alternative model that adapts to RGBD images. This model directly exploits the depth information often available thanks to depth sensors. The implementation code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Mohammad Mahdi Johari, Yann Lepoittevin, Fran\u00e7ois Fleuret"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "try to achieve this by adding Gaussian noise to the output \u03c3values during opti-\nmisation. But this does not eliminate all geometry artefacts, and reduces the reconstruction\nquality. Instead, we follow Barron et al. ",
                    "Citation Text": "Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\nMip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.12077",
                        "Citation Paper Title": "Title:Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "reconstructs an encoding volume based on a 3D feature Plane\nSweep V olume ",
                    "Citation Text": "John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. DeepStereo: Learning to\nPredict New Views from the World\u2019s Imagery. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2016. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.06825",
                        "Citation Paper Title": "Title:DeepStereo: Learning to Predict New Views from the World's Imagery",
                        "Citation Paper Abstract": "Abstract:Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision, but their use in graphics problems has been limited. In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. To verify our method we show that it can convincingly reproduce known test views from nearby imagery. Additionally we show images rendered from novel viewpoints. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.",
                        "Citation Paper Authors": "Authors:John Flynn, Ivan Neulander, James Philbin, Noah Snavely"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "models partial single objects\nusing periodic activation functions. All of these models aim to disentangle image composition\nfor scene editing, or are limited to simple scenes comprised of one or a few simple objects.\nDeVris et al. ",
                    "Citation Text": "Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and\nJoshua M. Susskind. Unconstrained Scene Generation with Locally Conditioned Ra-\ndiance Fields. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) ,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00670",
                        "Citation Paper Title": "Title:Unconstrained Scene Generation with Locally Conditioned Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",
                        "Citation Paper Authors": "Authors:Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "trained a generator system to produce random NeRF volumes which could\nthen be combined with a decoder for GAN inversion. GNeRF ",
                    "Citation Text": "Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and\nJingyi Yu. GNeRF: GAN-based Neural Radiance Field without Posed Camera. 2021\nIEEE/CVF International Conference on Computer Vision (ICCV) , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15606",
                        "Citation Paper Title": "Title:GNeRF: GAN-based Neural Radiance Field without Posed Camera",
                        "Citation Paper Abstract": "Abstract:We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",
                        "Citation Paper Authors": "Authors:Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "incorporates compositional 3D scene structure to the model to handle multi-object scenes.\nPix2NeRF ",
                    "Citation Text": "Shengqu Cai, Anton Obukhov, Dengxin Dai, and Luc Van Gool. Pix2NeRF: Unsuper-\nvised Conditional $\\pi$-GAN for Single Image to Neural Radiance Fields Translation.\nInIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2022.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.13162",
                        "Citation Paper Title": "Title:Pix2NeRF: Unsupervised Conditional $\u03c0$-GAN for Single Image to Neural Radiance Fields Translation",
                        "Citation Paper Abstract": "Abstract:We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object or a scene of a specific class, conditioned on a single input image. This is a challenging task, as training NeRF requires multiple views of the same scene, coupled with corresponding poses, which are hard to obtain. Our method is based on $\\pi$-GAN, a generative model for unconditional 3D-aware image synthesis, which maps random latent codes to radiance fields of a class of objects. We jointly optimize (1) the $\\pi$-GAN objective to utilize its high-fidelity 3D-aware generation and (2) a carefully designed reconstruction objective. The latter includes an encoder coupled with $\\pi$-GAN generator to form an auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is unsupervised, capable of being trained with independent images without 3D, multi-view, or pose supervision. Applications of our pipeline include 3d avatar generation, object-centric novel view synthesis with a single input image, and 3d-aware super-resolution, to name a few.",
                        "Citation Paper Authors": "Authors:Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "achieves disentanglement of object properties while not\nrequiring 3D supervision. All single-view methods base their 3D representations on a single\n2D image, which suffer from single-view spatial ambiguities. Nanbo et al. ",
                    "Citation Text": "Li Nanbo, Cian Eastwood, and Robert B. Fisher. Learning Object-Centric Representa-\ntions of Multi-Object Scenes from Multiple Views. NIPS\u201920: Proceedings of the 34th\nInternational Conference on Neural Information Processing Systems , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.07117",
                        "Citation Paper Title": "Title:Learning Object-Centric Representations of Multi-Object Scenes from Multiple Views",
                        "Citation Paper Abstract": "Abstract:Learning object-centric representations of multi-object scenes is a promising approach towards machine intelligence, facilitating high-level reasoning and control from visual sensory data. However, current approaches for unsupervised object-centric scene representation are incapable of aggregating information from multiple observations of a scene. As a result, these \"single-view\" methods form their representations of a 3D scene based only on a single 2D observation (view). Naturally, this leads to several inaccuracies, with these methods falling victim to single-view spatial ambiguities. To address this, we propose The Multi-View and Multi-Object Network (MulMON) -- a method for learning accurate, object-centric representations of multi-object scenes by leveraging multiple views. In order to sidestep the main technical difficulty of the multi-object-multi-view scenario -- maintaining object correspondences across views -- MulMON iteratively updates the latent object representations for a scene over multiple views. To ensure that these iterative updates do indeed aggregate spatial information to form a complete 3D scene understanding, MulMON is asked to predict the appearance of the scene from novel viewpoints during training. Through experiments, we show that MulMON better-resolves spatial ambiguities than single-view methods -- learning more accurate and disentangled object representations -- and also achieves new functionality in predicting object segmentations for novel viewpoints.",
                        "Citation Paper Authors": "Authors:Li Nanbo, Cian Eastwood, Robert B. Fisher"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", it has become\npossible to generate novel photo-realistic content [ 9,10,25,26]. Several works have applied\nadversarial methods to the controllable novel view synthesis of objects. HoloGAN ",
                    "Citation Text": "Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang.\nHoloGAN: Unsupervised learning of 3D representations from natural images. The\nIEEE International Conference on Computer Vision (ICCV) , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01326",
                        "Citation Paper Title": "Title:HoloGAN: Unsupervised learning of 3D representations from natural images",
                        "Citation Paper Abstract": "Abstract:We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",
                        "Citation Paper Authors": "Authors:Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "emulates classical stereo matching\ntechniques by learning an ensemble of pair-wise similarities. But the results are very blurry,\ncannot handle specularities, and the model is very expensive to run. PixelNeRF ",
                    "Citation Text": "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural Radi-\nance Fields From One or Few Images. In CVPR , 2021. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02190",
                        "Citation Paper Title": "Title:pixelNeRF: Neural Radiance Fields from One or Few Images",
                        "Citation Paper Abstract": "Abstract:We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Despite this, all these approaches struggle to\ngeneralise across scenes, require dense input images, and are very costly to run. In particular\nrecent works have focused on introducing additional data augmentation ",
                    "Citation Text": "Tianlong Chen, Peihao Wang, Zhiwen Fan, and Zhangyang Wang. Aug-NeRF: Training\nStronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmenta-\ntions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.01164",
                        "Citation Paper Title": "Title:Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Physically-Grounded Augmentations",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Field (NeRF) regresses a neural parameterized scene by differentially rendering multi-view images with ground-truth supervision. However, when interpolating novel views, NeRF often yields inconsistent and visually non-smooth geometric results, which we consider as a generalization gap between seen and unseen views. Recent advances in convolutional neural networks have demonstrated the promise of advanced robust data augmentations, either random or learned, in enhancing both in-distribution and out-of-distribution generalization. Inspired by that, we propose Augmented NeRF (Aug-NeRF), which for the first time brings the power of robust data augmentations into regularizing the NeRF training. Particularly, our proposal learns to seamlessly blend worst-case perturbations into three distinct levels of the NeRF pipeline with physical grounds, including (1) the input coordinates, to simulate imprecise camera parameters at image capture; (2) intermediate features, to smoothen the intrinsic feature manifold; and (3) pre-rendering output, to account for the potential degradation factors in the multi-view image supervision. Extensive results demonstrate that Aug-NeRF effectively boosts NeRF performance in both novel view synthesis (up to 1.5dB PSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the implicit smooth prior injected by the triple-level augmentations, Aug-NeRF can even recover scenes from heavily corrupted images, a highly challenging setting untackled before. Our codes are available in this https URL.",
                        "Citation Paper Authors": "Authors:Tianlong Chen, Peihao Wang, Zhiwen Fan, Zhangyang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.07296v1": {
            "Paper Title": "Optimizing Placements of 360-degree Panoramic Cameras in Indoor\n  Environments by Integer Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13599v4": {
            "Paper Title": "VectorAdam for Rotation Equivariant Geometry Optimization",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "introduces a term to rectify the variance\nof the adaptive learning rate, among many others. Most similar to this work, Zhang et al. ",
                    "Citation Text": "Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam.\narXiv preprint arXiv:1709.04546 , 2017.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.04546",
                        "Citation Paper Title": "Title:Normalized Direction-preserving Adam",
                        "Citation Paper Abstract": "Abstract:Adaptive optimization algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in some scenarios. However, recent studies show that they often lead to worse generalization performance than SGD, especially for training deep neural networks (DNNs). In this work, we identify the reasons that Adam generalizes worse than SGD, and develop a variant of Adam to eliminate the generalization gap. The proposed method, normalized direction-preserving Adam (ND-Adam), enables more precise control of the direction and step size for updating weight vectors, leading to significantly improved generalization performance. Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits. By bridging the gap between SGD and Adam, we also hope to shed light on why certain optimization algorithms generalize better than others.",
                        "Citation Paper Authors": "Authors:Zijun Zhang, Lin Ma, Zongpeng Li, Chuan Wu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "incorporates Nestorov momentum into Adam\u2019s\nformulation to improve rate of convergence, and RAdam ",
                    "Citation Text": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,\nand Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint\narXiv:1908.03265 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03265",
                        "Citation Paper Title": "Title:On the Variance of the Adaptive Learning Rate and Beyond",
                        "Citation Paper Abstract": "Abstract:The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.05125v1": {
            "Paper Title": "ChromoSkein: Untangling Three-Dimensional Chromatin Fiber With a\n  Web-Based Visualization Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05123v1": {
            "Paper Title": "Up to 58 Tets/Hex to untangle Hex meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.04045v1": {
            "Paper Title": "Fast GPU-Based Two-Way Continuous Collision Handling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13871v2": {
            "Paper Title": "A Repulsive Force Unit for Garment Collision Handling in Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03568v1": {
            "Paper Title": "CASA: Category-agnostic Skeletal Animal Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "provides\na benchmark for common static 3D object shape reconstruction. Among all of these datasets, only a\nfew aim for dynamic object reconstruction ",
                    "Citation Text": "Yang Li, Hikari Takehara, Takafumi, Taketomi, Bo Zheng, and Matthias Nie\u00dfner. 4dcomplete:\nNon-rigid motion estimation beyond the observable surface. In ICCV , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01905",
                        "Citation Paper Title": "Title:4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface",
                        "Citation Paper Abstract": "Abstract:Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and incompleteness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.",
                        "Citation Paper Authors": "Authors:Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", face [ 29,3], and animals [ 85,2]. Template-\nfree methods [ 19,30,25,64,57,70,23,44,69] study the problem of predicting nonrigid objects\ndirectly for a speci\ufb01c category. However, these methods heavily rely on strong category priors\nsuch as key-points annotations ",
                    "Citation Text": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning category-\nspeci\ufb01c mesh reconstruction from image collections. In ECCV , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07549",
                        "Citation Paper Title": "Title:Learning Category-Specific Mesh Reconstruction from Image Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Category-speci\ufb01c nonrigid reconstruction. Parametric model (template) is a classic approach for\ncategory-speci\ufb01c nonrigid reconstruction[ 40,18,2,78,77,36,32,54] . Templates are built for\nvarious objects including human body [ 34,1], hand ",
                    "Citation Text": "Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: Modeling and\ncapturing hands and bodies together. SIGGRAPH ASIA , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02610",
                        "Citation Paper Title": "Title:Embodied Hands: Modeling and Capturing Hands and Bodies Together",
                        "Citation Paper Abstract": "Abstract:Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (this http URL).",
                        "Citation Paper Authors": "Authors:Javier Romero, Dimitrios Tzionas, Michael J. Black"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01566v1": {
            "Paper Title": "NaRPA: Navigation and Rendering Pipeline for Astronautics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01466v1": {
            "Paper Title": "Inverse Kinematics with Dual-Quaternions, Exponential-Maps, and Joint\n  Limits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01174v1": {
            "Paper Title": "Hypergraph Convolutional Network based Weakly Supervised Point Cloud\n  Semantic Segmentation with Scene-Level Annotations",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "used sparse scribble-based la bels\nfor 3D shapes segmentation. The labeling information was\nautomatically propagated to the unlabeled parts of the trai ning\n3D shapes and given as a part of the optimization results.\nWei et al. ",
                    "Citation Text": "J. Wei, G. Lin, K.-H. Yap, T.-Y . Hung, and L. Xie, \u201cMulti- path\nregion mining for weakly supervised 3d semantic segmentati on on point\nclouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 4384\u20134393.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.13035",
                        "Citation Paper Title": "Title:Multi-Path Region Mining For Weakly Supervised 3D Semantic Segmentation on Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds provide intrinsic geometric information and surface context for scene understanding. Existing methods for point cloud segmentation require a large amount of fully labeled data. Using advanced depth sensors, collection of large scale 3D dataset is no longer a cumbersome process. However, manually producing point-level label on the large scale dataset is time and labor-intensive. In this paper, we propose a weakly supervised approach to predict point-level results using weak labels on 3D point clouds. We introduce our multi-path region mining module to generate pseudo point-level label from a classification network trained with weak labels. It mines the localization cues for each class from various aspects of the network feature using different attention modules. Then, we use the point-level pseudo labels to train a point cloud segmentation network in a fully supervised manner. To the best of our knowledge, this is the first method that uses cloud-level weak labels on raw 3D space to train a point cloud semantic segmentation network. In our setting, the 3D weak labels only indicate the classes that appeared in our input sample. We discuss both scene- and subcloud-level weakly labels on raw 3D point cloud data and perform in-depth experiments on them. On ScanNet dataset, our result trained with subcloud-level labels is compatible with some fully supervised methods.",
                        "Citation Paper Authors": "Authors:Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, Lihua Xie"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "proposed a dynamic\nedge convolution algorithm for semantic segmentation of po int\nclouds. The EdgeConv dynamically computed node adjacency\nat each graph layer using the distance between point feature s.\nLi et al. ",
                    "Citation Text": "G. Li, M. Muller, A. Thabet, and B. Ghanem, \u201cDeepgcns: Ca n gcns\ngo as deep as cnns?\u201d in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2019, pp. 9267\u20139276.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03751",
                        "Citation Paper Title": "Title:DeepGCNs: Can GCNs Go as Deep as CNNs?",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.",
                        "Citation Paper Authors": "Authors:Guohao Li, Matthias M\u00fcller, Ali Thabet, Bernard Ghanem"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.00330v1": {
            "Paper Title": "Real-Time Character Inverse Kinematics using the Gauss-Seidel Iterative\n  Approximation Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.10309v2": {
            "Paper Title": "A Fully Implicit Method for Robust Frictional Contact Handling in\n  Elastic Rods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00166v1": {
            "Paper Title": "Decorrelating ReSTIR Samplers via MCMC Mutations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.17344v1": {
            "Paper Title": "gCoRF: Generative Compositional Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "dataset to obtain semantic map for portrait im-\nages, and DatasetGAN ",
                    "Citation Text": "Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-\nFrancois La\ufb02eche, Adela Barriuso, Antonio Torralba, and\nSanja Fidler. Datasetgan: Ef\ufb01cient labeled data factory with\nminimal human effort. In CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06490",
                        "Citation Paper Title": "Title:DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort",
                        "Citation Paper Abstract": "Abstract:We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods, which in some cases require as much as 100x more annotated data as our method.",
                        "Citation Paper Authors": "Authors:Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "datasets. We model different semantic parts of\nthe portraits like hair, eyes, eyebrows and nose using our\nmethod. We used BiSeNet ",
                    "Citation Text": "Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Bisenet: Bilateral segmentation\nnetwork for real-time semantic segmentation. In European\nConference on Computer Vision , pages 334\u2013349. Springer,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.00897",
                        "Citation Paper Title": "Title:BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.",
                        "Citation Paper Authors": "Authors:Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "learn to directly generate vox-\nels and meshes respectively, but show artifacts due to the\ndif\ufb01culty in training. Recently, the prosperous of NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In ECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed a compositional representation that models\ncoarse structure with voxel grid and \ufb01ne detials with NeRF.\nStelzner et al. ",
                    "Citation Text": "Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. De-\ncomposing 3d scenes into objects via unsupervised volume\nsegmentation. arXiv:2104.01148 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.01148",
                        "Citation Paper Title": "Title:Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation",
                        "Citation Paper Abstract": "Abstract:We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.",
                        "Citation Paper Authors": "Authors:Karl Stelzner, Kristian Kersting, Adam R. Kosiorek"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "can decomposite a scene into objects given the su-\npervision of object masks or bounding boxes. uORF ",
                    "Citation Text": "Hong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsu-\npervised discovery of object radiance \ufb01elds. arXiv preprint\narXiv:2107.07905 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.07905",
                        "Citation Paper Title": "Title:Unsupervised Discovery of Object Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets.",
                        "Citation Paper Authors": "Authors:Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "learns the NeRF\nfor each object independently, which naturally allows com-\nposition. The approaches of Ost et al . ",
                    "Citation Text": "Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and\nFelix Heide. Neural scene graphs for dynamic scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 2856\u20132865,\nJune 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.10379",
                        "Citation Paper Title": "Title:Neural Scene Graphs for Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",
                        "Citation Paper Authors": "Authors:Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.15904v1": {
            "Paper Title": "Self-Supervised Learning with Multi-View Rendering for 3D Point Cloud\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15897v1": {
            "Paper Title": "Single-Image HDR Reconstruction by Multi-Exposure Generation",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "to\nrecover the final HDR image from the predicted exposures.\nThe peak signal-to-noise ratio (PSNR), structure similarity\n(SSIM), LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep fea-\ntures as a perceptual metric. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 586\u2013595, 2018. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The aforementioned methods,\nhowever, often fail to reconstruct the desired HDR image\nproperly, leading to artifacts, ghosting, and tearing in theApproach MethodDataset\navailable Training\nDirect\nInput: LDR\nOutput: HDREilertsen et al. ",
                    "Citation Text": "Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa\u0142 K.\nMantiuk, and Jonas Unger. Hdr image reconstruction from a\nsingle exposure using deep cnns. ACM Trans. Graph. , 36(6),\nnov 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07480",
                        "Citation Paper Title": "Title:HDR image reconstruction from a single exposure using deep CNNs",
                        "Citation Paper Abstract": "Abstract:Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods.",
                        "Citation Paper Authors": "Authors:Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa\u0142 K. Mantiuk, Jonas Unger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05800v2": {
            "Paper Title": "Time-of-Day Neural Style Transfer for Architectural Photographs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13676v1": {
            "Paper Title": "Computing Medial Axis Transform with Feature Preservation via Restricted\n  Power Diagram",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13305v1": {
            "Paper Title": "BoundED: Neural Boundary and Edge Detection in 3D Point Clouds via Local\n  Neighborhood Statistics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.12398v1": {
            "Paper Title": "NeARportation: A Remote Real-time Neural Rendering Framework",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "employs volume rendering to compose new view\nimages and can handle light attenuation and transparent objects.\nNeRF has developed remarkably within the past few years ",
                    "Citation Text": "Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Nu-\nmair Khan, Federico Tombari, James Tompkin, Vincent sitzmann, and Sri-\nnath Sridhar. 2022. Neural Fields in Visual Computing and Beyond. Com-\nputer Graphics Forum 41, 2 (2022), 641\u2013676. https://doi.org/10.1111/cgf.14505\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14505",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", mak-\ning it unsuitable for AR/VR devices with relatively low computing\npower and less energy for extended use.\nNeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi\nRamamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\nFields for View Synthesis. In ECCV .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.01937v4": {
            "Paper Title": "FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11463v1": {
            "Paper Title": "Breaking Bad: A Dataset for Geometric Fracture and Reassembly",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "settings, since data is often scarce or even not available\nin real world. In robotics, our dataset facilitates the development of sequential decision-making\nalgorithms ",
                    "Citation Text": "Seyed Kamyar Seyed Ghasemipour, Daniel Freeman, Byron David, Satoshi Kataoka, Igor\nMordatch, et al. Blocks assemble! learning to assemble with large-scale structured reinforcement\nlearning. arXiv preprint arXiv:2203.13733 , 2022. 1, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.13733",
                        "Citation Paper Title": "Title:Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Assembly of multi-part physical structures is both a valuable end product for autonomous robotics, as well as a valuable diagnostic task for open-ended training of embodied intelligent agents. We introduce a naturalistic physics-based environment with a set of connectable magnet blocks inspired by children's toy kits. The objective is to assemble blocks into a succession of target blueprints. Despite the simplicity of this objective, the compositional nature of building diverse blueprints from a set of blocks leads to an explosion of complexity in structures that agents encounter. Furthermore, assembly stresses agents' multi-step planning, physical reasoning, and bimanual coordination. We find that the combination of large-scale reinforcement learning and graph-based policies -- surprisingly without any additional complexity -- is an effective recipe for training agents that not only generalize to complex unseen blueprints in a zero-shot manner, but even operate in a reset-free setting without being trained to do so. Through extensive experiments, we highlight the importance of large-scale training, structured representations, contributions of multi-task vs. single-task learning, as well as the effects of curriculums, and discuss qualitative behaviors of trained agents.",
                        "Citation Paper Authors": "Authors:Seyed Kamyar Seyed Ghasemipour, Daniel Freeman, Byron David, Shixiang Shane Gu, Satoshi Kataoka, Igor Mordatch"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "or distorted parts. Another interesting direction would be studying geometric shape\nassembly in few-shot ",
                    "Citation Text": "Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few\nexamples: A survey on few-shot learning. ACM Computing Surveys , 2020. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05046",
                        "Citation Paper Title": "Title:Generalizing from a Few Examples: A Survey on Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",
                        "Citation Paper Authors": "Authors:Yaqing Wang, Quanming Yao, James Kwok, Lionel M. Ni"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": ", our dataset can be used to study the problems of missing fractured\npieces ",
                    "Citation Text": "Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir G Kim, and Hao\nZhang. Coalesce: Component assembly by learning to synthesize connections. In 3DV, 2020.\n3, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01936",
                        "Citation Paper Title": "Title:COALESCE: Component Assembly by Learning to Synthesize Connections",
                        "Citation Paper Abstract": "Abstract:We introduce COALESCE, the first data-driven framework for component-based shape assembly which employs deep learning to synthesize part connections. To handle geometric and topological mismatches between parts, we remove the mismatched portions via erosion, and rely on a joint synthesis step, which is learned from data, to fill the gap and arrive at a natural and plausible part joint. Given a set of input parts extracted from different objects, COALESCE automatically aligns them and synthesizes plausible joints to connect the parts into a coherent 3D object represented by a mesh. The joint synthesis network, designed to focus on joint regions, reconstructs the surface between the parts by predicting an implicit shape representation that agrees with existing parts, while generating a smooth and topologically meaningful connection. We employ test-time optimization to further ensure that the synthesized joint region closely aligns with the input parts to create realistic component assemblies from diverse input parts. We demonstrate that our method significantly outperforms prior approaches including baseline deep models for 3D shape synthesis, as well as state-of-the-art methods for shape completion.",
                        "Citation Paper Authors": "Authors:Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir G. Kim, Hao Zhang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "or formulate part assembly as a graph\nlearning problem [ 16,20]. These methods use the\nPartNet dataset ",
                    "Citation Text": "Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and\nHao Su. PartNet: A large-scale benchmark for \ufb01ne-grained and hierarchical part-level 3D object\n11understanding. In CVPR , 2019. 2, 3, 4, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02713",
                        "Citation Paper Title": "Title:PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding",
                        "Citation Paper Abstract": "Abstract:We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-of-the-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose a novel method for part instance segmentation and demonstrate its superior performance over existing methods.",
                        "Citation Paper Authors": "Authors:Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, Hao Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.10807v1": {
            "Paper Title": "Self-Supervised Representation Learning for CAD",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ". Several techniques for learn-\ning on B-reps use message passing networks. BRepNet ",
                    "Citation Text": "Joseph G. Lambourne, Karl D. D. Willis, Pradeep Ku-\nmar Jayaraman, Aditya Sanghi, Peter Meltzer, and Hooman\nShayani. BRepNet: A topological message passing system\nfor solid models. arXiv:2104.00706 [cs] , Apr. 2021. arXiv:\n2104.00706. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00706",
                        "Citation Paper Title": "Title:BRepNet: A topological message passing system for solid models",
                        "Citation Paper Abstract": "Abstract:Boundary representation (B-rep) models are the standard way 3D shapes are described in Computer-Aided Design (CAD) applications. They combine lightweight parametric curves and surfaces with topological information which connects the geometric entities to describe manifolds. In this paper we introduce BRepNet, a neural network architecture designed to operate directly on B-rep data structures, avoiding the need to approximate the model as meshes or point clouds. BRepNet defines convolutional kernels with respect to oriented coedges in the data structure. In the neighborhood of each coedge, a small collection of faces, edges and coedges can be identified and patterns in the feature vectors from these entities detected by specific learnable parameters. In addition, to encourage further deep learning research with B-reps, we publish the Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep models annotated with information about the modeling operations which created each face. We demonstrate that BRepNet can segment these models with higher accuracy than methods working on meshes, and point clouds.",
                        "Citation Paper Authors": "Authors:Joseph G. Lambourne, Karl D.D. Willis, Pradeep Kumar Jayaraman, Aditya Sanghi, Peter Meltzer, Hooman Shayani"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", this dataset contains\nmostly designs created by novice CAD users, not captur-\ning the design process of CAD experts. The Fusion 360\ndataset ",
                    "Citation Text": "Karl D. D. Willis, Yewen Pu, Jieliang Luo, Hang Chu,\nTao Du, Joseph G. Lambourne, Armando Solar-Lezama,\nand Wojciech Matusik. Fusion 360 Gallery: A Dataset\nand Environment for Programmatic CAD Reconstruction.\narXiv:2010.02392 [cs] , Oct. 2020. arXiv: 2010.02392. 2,\n3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02392",
                        "Citation Paper Title": "Title:Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD Construction from Human Design Sequences",
                        "Citation Paper Abstract": "Abstract:Parametric computer-aided design (CAD) is a standard paradigm used to design manufactured objects, where a 3D shape is represented as a program supported by the CAD software. Despite the pervasiveness of parametric CAD and a growing interest from the research community, currently there does not exist a dataset of realistic CAD models in a concise programmatic form. In this paper we present the Fusion 360 Gallery, consisting of a simple language with just the sketch and extrude modeling operations, and a dataset of 8,625 human design sequences expressed in this language. We also present an interactive environment called the Fusion 360 Gym, which exposes the sequential construction of a CAD program as a Markov decision process, making it amendable to machine learning approaches. As a use case for our dataset and environment, we define the CAD reconstruction task of recovering a CAD program from a target geometry. We report results of applying state-of-the-art methods of program synthesis with neurally guided search on this task.",
                        "Citation Paper Authors": "Authors:Karl D.D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G. Lambourne, Armando Solar-Lezama, Wojciech Matusik"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", randomly perturb ori-\nentation and predict the upright position ",
                    "Citation Text": "Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In ICLR 2018 , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07728",
                        "Citation Paper Title": "Title:Unsupervised Representation Learning by Predicting Image Rotations",
                        "Citation Paper Abstract": "Abstract:Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: this https URL .",
                        "Citation Paper Authors": "Authors:Spyros Gidaris, Praveer Singh, Nikos Komodakis"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "Learning from CAD Collections Recently, interest in\nlearning from CAD data has increased due to advances\nin machine learning and the release of large CAD data-\nsets, including collections of B-reps and program struc-\nture [20,36], CAD sketches ",
                    "Citation Text": "Ari Seff, Yaniv Ovadia, Wenda Zhou, and Ryan P.\nAdams. SketchGraphs: A Large-Scale Dataset for Mod-\neling Relational Geometry in Computer-Aided Design.\narXiv:2007.08506 [cs, stat] , July 2020. arXiv: 2007.08506.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08506",
                        "Citation Paper Title": "Title:SketchGraphs: A Large-Scale Dataset for Modeling Relational Geometry in Computer-Aided Design",
                        "Citation Paper Abstract": "Abstract:Parametric computer-aided design (CAD) is the dominant paradigm in mechanical engineering for physical design. Distinguished by relational geometry, parametric CAD models begin as two-dimensional sketches consisting of geometric primitives (e.g., line segments, arcs) and explicit constraints between them (e.g., coincidence, perpendicularity) that form the basis for three-dimensional construction operations. Training machine learning models to reason about and synthesize parametric CAD designs has the potential to reduce design time and enable new design workflows. Additionally, parametric CAD designs can be viewed as instances of constraint programming and they offer a well-scoped test bed for exploring ideas in program synthesis and induction. To facilitate this research, we introduce SketchGraphs, a collection of 15 million sketches extracted from real-world CAD models coupled with an open-source data processing pipeline. Each sketch is represented as a geometric constraint graph where edges denote designer-imposed geometric relationships between primitives, the nodes of the graph. We demonstrate and establish benchmarks for two use cases of the dataset: generative modeling of sketches and conditional generation of likely constraints given unconstrained geometry.",
                        "Citation Paper Authors": "Authors:Ari Seff, Yaniv Ovadia, Wenda Zhou, Ryan P. Adams"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.01685v2": {
            "Paper Title": "Differentiable Subdivision Surface Fitting",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "or other visual queues such as\noptical \ufb02ow [7,41]. Recently, with the development of differentiable\nrenderers, rendered image difference metrics can be used to optimize\nshape ",
                    "Citation Text": "A. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik. Learning category-\nspeci\ufb01c mesh reconstruction from image collections. In Proceedings of\nthe European Conference on Computer Vision (ECCV) , pp. 371\u2013386,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07549",
                        "Citation Paper Title": "Title:Learning Category-Specific Mesh Reconstruction from Image Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ".\nSharma et al. described a method using neural networks to \ufb01t B-\nspline patches to input point cloud data ",
                    "Citation Text": "G. Sharma, D. Liu, S. Maji, E. Kalogerakis, S. Chaudhuri, and R. M \u02c7ech.\nParsenet: A parametric surface \ufb01tting network for 3d point clouds. In\nEuropean Conference on Computer Vision , pp. 261\u2013276. Springer,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12181",
                        "Citation Paper Title": "Title:ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:We propose a novel, end-to-end trainable, deep network called ParSeNet that decomposes a 3D point cloud into parametric surface patches, including B-spline patches as well as basic geometric primitives. ParSeNet is trained on a large-scale dataset of man-made 3D shapes and captures high-level semantic priors for shape decomposition. It handles a much richer class of primitives than prior work, and allows us to represent surfaces with higher fidelity. It also produces repeatable and robust parametrizations of a surface compared to purely geometric approaches. We present extensive experiments to validate our approach against analytical and learning-based alternatives. Our source code is publicly available at: this https URL.",
                        "Citation Paper Authors": "Authors:Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis, Siddhartha Chaudhuri, Radom\u00edr M\u011bch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.10859v2": {
            "Paper Title": "Wavelet-Based Fast Decoding of 360-Degree Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08535v1": {
            "Paper Title": "Realistic, Animatable Human Reconstructions for Virtual Fit-On",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "is an advancement of PIFu mainly focus on the accuracy\nand details of the 3-D model. Although the details have been\nimproved, still the body model is stationary. Octopus ",
                    "Citation Text": "Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatna-\ngar, Christian Theobalt, and Gerard Pons-Moll, \u201cLearn-\ning to reconstruct people in clothing from a single rgb\ncamera,\u201d in CVPR , 2019, pp. 1175\u20131186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.05885",
                        "Citation Paper Title": "Title:Learning to Reconstruct People in Clothing from a Single RGB Camera",
                        "Citation Paper Abstract": "Abstract:We present a learning-based model to infer the personalized 3D shape of people from a few frames (1-8) of a monocular video in which the person is moving, in less than 10 seconds with a reconstruction accuracy of 5mm. Our model learns to predict the parameters of a statistical body model and instance displacements that add clothing and hair to the shape. The model achieves fast and accurate predictions based on two key design choices. First, by predicting shape in a canonical T-pose space, the network learns to encode the images of the person into pose-invariant latent codes, where the information is fused. Second, based on the observation that feed-forward predictions are fast but do not always align with the input images, we predict using both, bottom-up and top-down streams (one per view) allowing information to flow in both directions. Learning relies only on synthetic 3D data. Once learned, the model can take a variable number of frames as input, and is able to reconstruct shapes even from a single image with an accuracy of 6mm. Results on 3 different datasets demonstrate the efficacy and accuracy of our approach.",
                        "Citation Paper Authors": "Authors:Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, Gerard Pons-Moll"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08083v1": {
            "Paper Title": "Reference Based Color Transfer for Medical Volume Rendering",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "network\nthat has been pre-trained on the colored ImageNet dataset ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.\nBerg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV) 115, 3 (2015), 211\u2013252. https:\n//doi.org/10.1007/s11263-015-0816-y",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "used a deep neural network to colorize an image where they pose\nthe colorization problem as a regression problem. With compara-\ntively larger set of data, Zhang et al. ",
                    "Citation Text": "Richard Zhang, Phillip Isola, and Alexei A Efros. 2016. Colorful Image Coloriza-\ntion. In ECCV .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08511",
                        "Citation Paper Title": "Title:Colorful Image Colorization",
                        "Citation Paper Abstract": "Abstract:Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \"colorization Turing test,\" asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08080v1": {
            "Paper Title": "Deep Learning based Super-Resolution for Medical Volume Visualization\n  with Direct Volume Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.15258v2": {
            "Paper Title": "Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D\n  Camera",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "optimize an underlying\ncontinuous scene function for novel view synthesis. Some NeRF-like methods [ 37,49,58,18,46,47]\nachieve promising results on dynamic scenes without prior templates. Ner\ufb01es ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Jonathan T Barron, So\ufb01en Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Ner\ufb01es: Deformable neural radiance \ufb01elds. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 5865\u20135874,\n2021. 3, 4, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "models them with the help of Neural Radiance Fields (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,\nand Ren Ng. Nerf: Representing scenes as neural radiance \ufb01elds for view synthesis. In European\nconference on computer vision (ECCV) , pages 405\u2013421. Springer, 2020. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07650v1": {
            "Paper Title": "DART: Articulated Hand Model with Diverse Accessories and Rich Textures",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "as the backbone of the \ufb01rst three networks, and HRNet ",
                    "Citation Text": "Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu,\nMingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition.\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07919",
                        "Citation Paper Title": "Title:Deep High-Resolution Representation Learning for Visual Recognition",
                        "Citation Paper Abstract": "Abstract:High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \\emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \\emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\\url{this https URL}}.",
                        "Citation Paper Authors": "Authors:Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "is an RGB-D hand dataset, that is constructed by posing the articulated hand model with real mocap\ndata, together with interaction and occlusion introduced by objects and clusters. Hasson et al. ",
                    "Citation Text": "Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J Black, Ivan Laptev,\nand Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In\nComputer Vision and Pattern Recognition (CVPR), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05767",
                        "Citation Paper Title": "Title:Learning joint reconstruction of hands and manipulated objects",
                        "Citation Paper Abstract": "Abstract:Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.",
                        "Citation Paper Authors": "Authors:Yana Hasson, G\u00fcl Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "choose to synthesize hand\ndata from Mixamo characters with a limited diversity of pose and skin colors. Rogez et al. ",
                    "Citation Text": "Gr\u00e9gory Rogez, Maryam Khademi, James Steven Supancic, J. M. M. Montiel, and Deva Ramanan. 3D\nHand Pose Detection in Egocentric RGB-D Images. European Conference on Computer Vision (ECCV) ,\n2014.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.0065",
                        "Citation Paper Title": "Title:3D Hand Pose Detection in Egocentric RGB-D Images",
                        "Citation Paper Abstract": "Abstract:We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images.",
                        "Citation Paper Authors": "Authors:Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "propose Crossing Net, which models the statistical\ncorrelation of hand pose and its corresponding depth image by combining GANs and V AEs with\na shared latent space. Oberweger et al. ",
                    "Citation Text": "Markus Oberweger, Gernot Riegler, Paul Wohlhart, and Vincent Lepetit. Ef\ufb01ciently Creating 3D Training\nData for Fine Hand Pose Estimation. Computer Vision and Pattern Recognition (CVPR), 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03389",
                        "Citation Paper Title": "Title:Efficiently Creating 3D Training Data for Fine Hand Pose Estimation",
                        "Citation Paper Abstract": "Abstract:While many recent hand pose estimation methods critically rely on a training set of labelled frames, the creation of such a dataset is a challenging task that has been overlooked so far. As a result, existing datasets are limited to a few sequences and individuals, with limited accuracy, and this prevents these methods from delivering their full potential. We propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3D locations of the joints: The user is asked to provide only an estimate of the 2D reprojections of the visible joints in some reference frames, which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function. We then exploit spatial, temporal, and appearance constraints to retrieve the full 3D poses of the hand over the complete sequence. We show that this data can be used to train a recent state-of-the-art hand pose estimation method, leading to increased accuracy. The code and dataset can be found on our website this https URL",
                        "Citation Paper Authors": "Authors:Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ". We won\u2019t discuss them in details since\nthey are beyond the scope of this paper. Existing hand synthesizing methods can be grouped into\nthree categories, summarized in Tab. 3\n1) GAN/V AE-based generation : Based on CycleGAN ",
                    "Citation Text": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image Translation Using\nCycle-Consistent Adversarial Networks. International Conference on Computer Vision (ICCV), 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10593",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                        "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "have proved their usefulness in downstream vision tasks, such as 3D pose estimation [ 9,17], clothed\nhuman reconstruction ",
                    "Citation Text": "Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans\nObtained from Normals. In Computer Vision and Pattern Recognition (CVPR), 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.09127",
                        "Citation Paper Title": "Title:ICON: Implicit Clothed humans Obtained from Normals",
                        "Citation Paper Abstract": "Abstract:Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.",
                        "Citation Paper Authors": "Authors:Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, Michael J. Black"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.15585v2": {
            "Paper Title": "Decomposing NeRF for Editing via Feature Field Distillation",
            "Sentences": [
                {
                    "Sentence ID": 104,
                    "Sentence": "show that neural implicit representations can be combined with the supervision of semantic labels.\nYang et al. ",
                    "Citation Text": "Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang,\nand Zhaopeng Cui. Learning object-compositional neural radiance \ufb01eld for editable scene\nrendering. In International Conference on Computer Vision (ICCV) , October 2021. URL\nhttps://arxiv.org/abs/2109.01847 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01847",
                        "Citation Paper Title": "Title:Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering",
                        "Citation Paper Abstract": "Abstract:Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",
                        "Citation Paper Authors": "Authors:Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "for ef\ufb01ciently teaching robots object\ngrasping. Instead of a pre-trained object-centric 3D model, we use 2D vision models as teacher\nnetworks via distillation, exploiting recent progress in pre-trained foundation models ",
                    "Citation Text": "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\nBrynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa\nDoumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho,\nJenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha\nKalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh,\nMark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks\nof foundation models. arXiv , 2021. URL https://arxiv.org/abs/2108.07258 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.07258",
                        "Citation Paper Title": "Title:On the Opportunities and Risks of Foundation Models",
                        "Citation Paper Abstract": "Abstract:AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
                        "Citation Paper Authors": "Authors:Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang\n\n\n        , Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang\n\n\n    et al. (14 additional authors not shown)\n\u00a0You must enable JavaScript to view entire author list."
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "use foreground-background decomposition and atlas representation for time-consistent,\nlocal editing; Loeschcke et al. ",
                    "Citation Text": "Sebastian Loeschcke, Serge Belongie, and Sagie Benaim. Text-driven stylization of video\nobjects. arXiv , 2022. URL https://arxiv.org/abs/2206.12396 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.12396",
                        "Citation Paper Title": "Title:Text-Driven Stylization of Video Objects",
                        "Citation Paper Abstract": "Abstract:We tackle the task of stylizing video objects in an intuitive and semantic manner following a user-specified text prompt. This is a challenging task as the resulting video must satisfy multiple properties: (1) it has to be temporally consistent and avoid jittering or similar artifacts, (2) the resulting stylization must preserve both the global semantics of the object and its fine-grained details, and (3) it must adhere to the user-specified text prompt. To this end, our method stylizes an object in a video according to two target texts. The first target text prompt describes the global semantics and the second target text prompt describes the local semantics. To modify the style of an object, we harness the representational power of CLIP to get a similarity score between (1) the local target text and a set of local stylized views, and (2) a global target text and a set of stylized global views. We use a pretrained atlas decomposition network to propagate the edits in a temporally consistent manner. We demonstrate that our method can generate consistent style changes over time for a variety of objects and videos, that adhere to the specification of the target texts. We also show how varying the specificity of the target texts and augmenting the texts with a set of prefixes results in stylizations with different levels of detail. Full results are given on our project webpage: this https URL",
                        "Citation Paper Authors": "Authors:Sebastian Loeschcke, Serge Belongie, Sagie Benaim"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "supervised by click or scribble annotations. Lastly, in a different but related task, video editing, Kasten\net al. ",
                    "Citation Text": "Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent\nvideo editing. ACM Transactions on Graphics (TOG) , 2021. URL http://arxiv.org/abs/\n2109.11418v1 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.11418",
                        "Citation Paper Title": "Title:Layered Neural Atlases for Consistent Video Editing",
                        "Citation Paper Abstract": "Abstract:We present a method that decomposes, or \"unwraps\", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.",
                        "Citation Paper Authors": "Authors:Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel"
                    }
                },
                {
                    "Sentence ID": 113,
                    "Sentence": "-based neural \ufb01elds. Other concurrent studies\nexplore decomposition through training scene-speci\ufb01c segmentation \ufb01eld ",
                    "Citation Text": "Shuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton, Tristan Laidlow, and Andrew J.\nDavison. iLabel: Interactive neural scene labelling. arXiv , 2021.\n18",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14637",
                        "Citation Paper Title": "Title:ILabel: Interactive Neural Scene Labelling",
                        "Citation Paper Abstract": "Abstract:Joint representation of geometry, colour and semantics using a 3D neural field enables accurate dense labelling from ultra-sparse interactions as a user reconstructs a scene in real-time using a handheld RGB-D sensor. Our iLabel system requires no training data, yet can densely label scenes more accurately than standard methods trained on large, expensively labelled image datasets. Furthermore, it works in an 'open set' manner, with semantic classes defined on the fly by the user.\nILabel's underlying model is a multilayer perceptron (MLP) trained from scratch in real-time to learn a joint neural scene representation. The scene model is updated and visualised in real-time, allowing the user to focus interactions to achieve efficient labelling. A room or similar scene can be accurately labelled into 10+ semantic categories with only a few tens of clicks. Quantitative labelling accuracy scales powerfully with the number of clicks, and rapidly surpasses standard pre-trained semantic segmentation methods. We also demonstrate a hierarchical labelling variant.",
                        "Citation Paper Authors": "Authors:Shuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton, Tristan Laidlow, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "also explores the same training framework\nand, in particular, investigates how fused features are improved from 2D teacher networks. It\nalso complementarily shows the results with other teacher models (MoCo-v3 ",
                    "Citation Text": "Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvision transformers. In Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 9640\u20139649, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.02057",
                        "Citation Paper Title": "Title:An Empirical Study of Training Self-Supervised Vision Transformers",
                        "Citation Paper Abstract": "Abstract:This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.",
                        "Citation Paper Authors": "Authors:Xinlei Chen, Saining Xie, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "demonstrate that given view-consistent ground-truth instance segmentation masks\nduring training, NeRF can be trained to represent each object as different volumes, although such an\nannotation is expensive in practice. Concurrently, Benaim et al. ",
                    "Citation Text": "Sagie Benaim, Frederik Warburg, Peter Ebert Christensen, and Serge Belongie. V olumetric\ndisentanglement for 3d scene manipulation. arXiv , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.02776",
                        "Citation Paper Title": "Title:Volumetric Disentanglement for 3D Scene Manipulation",
                        "Citation Paper Abstract": "Abstract:Recently, advances in differential volumetric rendering enabled significant breakthroughs in the photo-realistic and fine-detailed reconstruction of complex 3D scenes, which is key for many virtual reality applications. However, in the context of augmented reality, one may also wish to effect semantic manipulations or augmentations of objects within a scene. To this end, we propose a volumetric framework for (i) disentangling or separating, the volumetric representation of a given foreground object from the background, and (ii) semantically manipulating the foreground object, as well as the background. Our framework takes as input a set of 2D masks specifying the desired foreground object for training views, together with the associated 2D views and poses, and produces a foreground-background disentanglement that respects the surrounding illumination, reflections, and partial occlusions, which can be applied to both training and novel views. Our method enables the separate control of pixel color and depth as well as 3D similarity transformations of both the foreground and background objects. We subsequently demonstrate the applicability of our framework on a number of downstream manipulation tasks including object camouflage, non-negative 3D object inpainting, 3D object translation, 3D object inpainting, and 3D text-based object manipulation. Full results are given in our project webpage at this https URL",
                        "Citation Paper Authors": "Authors:Sagie Benaim, Frederik Warburg, Peter Ebert Christensen, Serge Belongie"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nGeometric Decomposition of Neural Scene Representations Kohli et al. ",
                    "Citation Text": "Amit Kohli, Vincent Sitzmann, and Gordon Wetzstein. Semantic implicit neural scene repre-\nsentations with semi-supervised training. In International Conference on 3D Vision (3DV) .\nIEEE, 2020. URL http://arxiv.org/abs/2003.12673v2 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12673",
                        "Citation Paper Title": "Title:Semantic Implicit Neural Scene Representations With Semi-Supervised Training",
                        "Citation Paper Abstract": "Abstract:The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.",
                        "Citation Paper Authors": "Authors:Amit Kohli, Vincent Sitzmann, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "use intermediate\nfeatures that emerge in a 3D occupancy \ufb01eld network ",
                    "Citation Text": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\nGeiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. URL\nhttp://arxiv.org/abs/1812.03828v2 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03828",
                        "Citation Paper Title": "Title:Occupancy Networks: Learning 3D Reconstruction in Function Space",
                        "Citation Paper Abstract": "Abstract:With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07466v1": {
            "Paper Title": "Synthetic-to-real Composite Semantic Segmentation in Additive\n  Manufacturing",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "due to its ef\ufb01ciency and simplicity.\nC. Image-to-image translation\nTo potentially improve the ef\ufb01ciency of semantic segmenta-\ntion, the application of the unpaired image-to-image translation\nmethod based on the CycleGAN network ",
                    "Citation Text": "J. -Y . Zhu, T. Park, P. Isola and A. A. Efros, \u201dUnpaired Image-to-Image\nTranslation Using Cycle-Consistent Adversarial Networks,\u201d in 2017 IEEE\nInternational Conference on Computer Vision (ICCV), pp. 2242-2251,\ndoi: 10.1109/ICCV .2017.244.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10593",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                        "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "presented\na comprehensive overview of the modern research state in\nthe \ufb01eld of semantic segmentation. As can be seen from the\nworks ",
                    "Citation Text": "O. Ronneberger, P. Fischer, T. Brox, \u201dU-Net: Convolutional Networks\nfor Biomedical Image Segmentation,\u201d 2015, arXiv:1505.04597.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "outlined\nthe most promising approaches to integrating synthesized data\ninto deep learning pipelines. Ward, Moghadam, and Hudson ",
                    "Citation Text": "D. Ward, P. Moghadam, N. Hudson, \u201dDeep Leaf Segmentation Using\nSynthetic Data,\u201d 2018, arXiv:1807.10931.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10931",
                        "Citation Paper Title": "Title:Deep Leaf Segmentation Using Synthetic Data",
                        "Citation Paper Abstract": "Abstract:Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to tackle leaf instance segmentation requires a large amount of manually annotated training data. Currently, the benchmark datasets for leaf segmentation contain only a few hundred labeled training images. In this paper, we propose a framework for leaf instance segmentation by augmenting real plant datasets with generated synthetic images of plants inspired by domain randomisation. We train a state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a combination of real and synthetic images of Arabidopsis plants. Our proposed approach achieves 90% leaf segmentation score on the A1 test set outperforming the-state-of-the-art approaches for the CVPPP Leaf Segmentation Challenge (LSC). Our approach also achieves 81% mean performance over all five test datasets.",
                        "Citation Paper Authors": "Authors:Daniel Ward, Peyman Moghadam, Nicolas Hudson"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". The success of simulated labeled data is\nclearly illustrated in already classic GTA5 ",
                    "Citation Text": "S.R. Richter, V . Vineet, S. Roth, V . Koltun, \u201dPlaying for Data: Ground\nTruth from Computer Games,\u201d 2016, arXiv:1608.02192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.02192",
                        "Citation Paper Title": "Title:Playing for Data: Ground Truth from Computer Games",
                        "Citation Paper Abstract": "Abstract:Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.10363v2": {
            "Paper Title": "Towards Device Efficient Conditional Image Generation",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "highlight the instability in GAN minimax training and not being\nable to achieve Nash\u2019s equilibrium for retaining performance metrics (FID), on applying\nclassification based pruning methods for conditional image generation. Similarly, Shu et\nal. ",
                    "Citation Text": "Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, and\nChang Xu. Co-evolutionary compression for unpaired image translation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 3235\u20133244,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.10804",
                        "Citation Paper Title": "Title:Co-Evolutionary Compression for Unpaired Image Translation",
                        "Citation Paper Abstract": "Abstract:Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators.",
                        "Citation Paper Authors": "Authors:Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, Chang Xu"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "\u2019s\ndistiller framework does adaptive search for operators types and channel widths. AutoML ",
                    "Citation Text": "Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan\ncompression: Efficient architectures for interactive conditional gans. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition , pages 5284\u20135294,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08936",
                        "Citation Paper Title": "Title:GAN Compression: Efficient Architectures for Interactive Conditional GANs",
                        "Citation Paper Abstract": "Abstract:Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by 9x, paving the way for interactive image synthesis.",
                        "Citation Paper Authors": "Authors:Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "speed up exploration for better architectures via network transformation. Fu et al. ",
                    "Citation Text": "Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, and Zhangyang\nWang. Autogan-distiller: Searching to compress generative adversarial networks. arXiv\npreprint arXiv:2006.08198 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.08198",
                        "Citation Paper Title": "Title:AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, Zhangyang Wang"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "search for\ntransferable network blocks, surpasses manually designed architectures [ 14,47]. While Cai et\nal. ",
                    "Citation Text": "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Reinforcement learning\nfor architecture search by network transformation. arXiv preprint arXiv:1707.04873 , 4,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.04873",
                        "Citation Paper Title": "Title:Efficient Architecture Search by Network Transformation",
                        "Citation Paper Abstract": "Abstract:Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23\\% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.",
                        "Citation Paper Authors": "Authors:Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, Jun Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.07533v2": {
            "Paper Title": "Not As Easy As You Think -- Experiences and Lessons Learnt from Trying\n  to Create a Bottom-Up Visualization Image Typology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05736v2": {
            "Paper Title": "Vision Transformer for NeRF-Based View Synthesis from a Single Input\n  Image",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "as our transformer encoder. The\ntransformer encoder has 12 layers ( J=12), where each layer\nuses the LayerNorm ",
                    "Citation Text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450 ,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06450",
                        "Citation Paper Title": "Title:Layer Normalization",
                        "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                        "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "3.\nAs shown in Table 2, our method achieves state-of-the-\nart performance against existing approaches in terms of\nPSNR, SSIM, and LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "and follow-up\nworks [38, 54] demonstrated that applying a transformer to\na sequence of patches (split from an image) achieves com-\npetitive performance on discriminative tasks ( e.g., image\nclassi\ufb01cation). Wang et al. ",
                    "Citation Text": "Dan Wang, Xinrui Cui, Xun Chen, Zhengxia Zou, Tianyang\nShi, Septimiu Salcudean, Z Jane Wang, and Rabab Ward.\nMulti-view 3D reconstruction with transformers. In ICCV ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12957",
                        "Citation Paper Title": "Title:Multi-view 3D Reconstruction with Transformer",
                        "Citation Paper Abstract": "Abstract:Deep CNN-based methods have so far achieved the state of the art results in multi-view 3D object reconstruction. Despite the considerable progress, the two core modules of these methods - multi-view feature extraction and fusion, are usually investigated separately, and the object relations in different views are rarely explored. In this paper, inspired by the recent great success in self-attention-based Transformer models, we reformulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and propose a new framework named 3D Volume Transformer (VolT) for such a task. Unlike previous CNN-based methods using a separate design, we unify the feature extraction and view fusion in a single Transformer network. A natural advantage of our design lies in the exploration of view-to-view relationships using self-attention among multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction benchmark dataset, our method achieves a new state-of-the-art accuracy in multi-view reconstruction with fewer parameters ($70\\%$ less) than other CNN-based methods. Experimental results also suggest the strong scaling capability of our method. Our code will be made publicly available.",
                        "Citation Paper Authors": "Authors:Dan Wang, Xinrui Cui, Xun Chen, Zhengxia Zou, Tianyang Shi, Septimiu Salcudean, Z. Jane Wang, Rabab Ward"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.00938v2": {
            "Paper Title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ\n  Labs Instagram Page",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06373v1": {
            "Paper Title": "Learning Multi-resolution Functional Maps with Spectral Attention for\n  Robust Shape Matching",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ". This\nbenchmark consists of shapes from a large-scale animation dataset DeformingThings4D ",
                    "Citation Text": "Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nie\u00dfner. 4DComplete:\nNon-rigid motion estimation beyond the observable surface. In ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01905",
                        "Citation Paper Title": "Title:4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface",
                        "Citation Paper Abstract": "Abstract:Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and incompleteness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.",
                        "Citation Paper Authors": "Authors:Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "is not necessary because of the\nregularization imposed in Eq. (1).\nImplementation We implement our network with PyTorch ",
                    "Citation Text": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,\nhigh-performance deep learning library. NeurIPS , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", but not in the spectral space.\nAttention-based spectral learning The attention mechanism was originally introduced in deep\nlearning for natural language processing, and consists in putting relative weights on different words of\nan input sentence ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03529v2": {
            "Paper Title": "Mesh-Tension Driven Expression-Based Wrinkles for Synthetic Faces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.05735v1": {
            "Paper Title": "TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nTo prepare the training and validation data, we use a randomized 5 =1 train/validation\nsplit on the dataset. We process the ShapeNet models to be manifold and watertight using\nthe method by Huang et al. ",
                    "Citation Text": "Jingwei Huang, Hao Su, and Leonidas Guibas. Robust watertight manifold surface\ngeneration method for ShapeNet models. arXiv preprint arXiv:1802.01698 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.01698",
                        "Citation Paper Title": "Title:Robust Watertight Manifold Surface Generation Method for ShapeNet Models",
                        "Citation Paper Abstract": "Abstract:In this paper, we describe a robust algorithm for 2-Manifold generation of various kinds of ShapeNet Models. The input of our pipeline is a triangle mesh, with a set of vertices and triangular faces. The output of our pipeline is a 2-Manifold with vertices roughly uniformly distributed on the geometry surface. Our algorithm uses an octree to represent the original mesh, and construct the surface by isosurface extraction. Finally, we project the vertices to the original mesh to achieve high precision. As a result, our method can be adopted efficiently to all ShapeNet models with the guarantee of correct 2-Manifold topology.",
                        "Citation Paper Authors": "Authors:Jingwei Huang, Hao Su, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". Cubic voxel-based techniques can generate shapes with varied\ngenus ",
                    "Citation Text": "Zhiqin Chen, Vladimir G Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Sid-\ndhartha Chaudhuri. DECOR-GAN: 3D shape detailization by conditional re\ufb01nement.\nInProc. IEEE Conf. Computer Vision and Pattern Recognition , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09159",
                        "Citation Paper Title": "Title:DECOR-GAN: 3D Shape Detailization by Conditional Refinement",
                        "Citation Paper Abstract": "Abstract:We introduce a deep generative network for 3D shape detailization, akin to stylization with the style being geometric details. We address the challenge of creating large varieties of high-resolution and detailed 3D geometry from a small set of exemplars by treating the problem as that of geometric detail transfer. Given a low-resolution coarse voxel shape, our network refines it, via voxel upsampling, into a higher-resolution shape enriched with geometric details. The output shape preserves the overall structure (or content) of the input, while its detail generation is conditioned on an input \"style code\" corresponding to a detailed exemplar. Our 3D detailization via conditional refinement is realized by a generative adversarial network, coined DECOR-GAN. The network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D PatchGAN discriminator to enforce local patches of the generated model to be similar to those in the training detailed shapes. During testing, a style code is fed into the generator to condition the refinement. We demonstrate that our method can refine a coarse shape into a variety of detailed shapes with different styles. The generated results are evaluated in terms of content preservation, plausibility, and diversity. Comprehensive ablation studies are conducted to validate our network designs. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05665v1": {
            "Paper Title": "HiFECap: Monocular High-Fidelity and Expressive Capture of Human\n  Performances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.05626v1": {
            "Paper Title": "Semantic Segmentation under Adverse Conditions: A Weather and\n  Nighttime-aware Synthetic Data-based Approach",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "Baseline 0.47 0.57 0.44 0.21 0.42 0.82\nFnT on AWSS 0.48 0.58 0.48 0.26 0.45 0.74\nPSPNet ",
                    "Citation Text": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.\nPyramid scene parsing network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 2881\u20132890, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.01105",
                        "Citation Paper Title": "Title:Pyramid Scene Parsing Network",
                        "Citation Paper Abstract": "Abstract:Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
                        "Citation Paper Authors": "Authors:Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "Baseline 0.41 0.46 0.36 0.17 0.35 0.78\nFnT on AWSS 0.44 0.48 0.47 0.19 0.39 0.59\nHRNet ",
                    "Citation Text": "Yuhui Yuan, Xiaokang Chen, Xilin Chen, and Jingdong Wang. Segmenta-\ntion transformer: Object-contextual representations for semantic segmenta-\ntion. arXiv preprint arXiv:1909.11065 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11065",
                        "Citation Paper Title": "Title:Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Our submission \"HRNet + OCR + SegFix\" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: this https URL and this https URL. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in~Section3.3.",
                        "Citation Paper Authors": "Authors:Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "Baseline 0.46 0.42 0.41 0.09 0.35 0.75\nFnT on AWSS 0.47 0.49 0.35 0.14 0.36 0.51\nDANet ",
                    "Citation Text": "Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Han-\nqing Lu. Dual attention network for scene segmentation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition , pages\n3146\u20133154, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.02983",
                        "Citation Paper Title": "Title:Dual Attention Network for Scene Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data. We make the code and trained model publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "and Synscapes [39, 44]\nas clearly demonstrated in Table 1. Recently, SHIFT ",
                    "Citation Text": "Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt\nSchiele, Federico Tombari, and Fisher Yu. Shift: A synthetic driving\ndataset for continuous multi-task domain adaptation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n21371\u201321382, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.08367",
                        "Citation Paper Title": "Title:SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous driving systems. Existing image and video driving datasets, however, fall short of capturing the mutable nature of the real world. In this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, SHIFT. It presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. Featuring a comprehensive sensor suite and annotations for several mainstream perception tasks, SHIFT allows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assess model robustness and generality. Our dataset and benchmark toolkit are publicly available at this http URL.",
                        "Citation Paper Authors": "Authors:Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, Fisher Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05250v1": {
            "Paper Title": "Digital twins for city simulation: Automatic, efficient, and robust mesh\n  generation for large-scale city modeling and simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14313v2": {
            "Paper Title": "AniVis: Generating Animated Transitions Between Statistical Charts with\n  a Tree Model",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", which\nbreaks down an animation into multiple steps, can help viewers\neasily follow visual transitions and better apprehend rich types\nof changes involved. Researchers have explored various aspects\nof staging design for animated transitions. For example, Chalbi\net al. ",
                    "Citation Text": "Amira Chalbi, Jacob Ritchie, Deokgun Park, Jungu Choi, Nicolas Roussel, Niklas\nElmqvist, and Fanny Chevalier. 2020. Common Fate for Animated Transitions\nin Visualization. IEEE Trans. Vis. Comput. Graph. 26, 1 (2020), 386\u2013396. https:\n//doi.org/10.1109/TVCG.2019.2934288",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00661",
                        "Citation Paper Title": "Title:Common Fate for Animated Transitions in Visualization",
                        "Citation Paper Abstract": "Abstract:The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion > (dynamic luminance, size, luminance); dynamic size > (dynamic luminance, position); and dynamic luminance > size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.",
                        "Citation Paper Authors": "Authors:Amira Chalbi, Jacob Ritchie, Deokgun Park, Jungu Choi, Nicolas Roussel, Niklas Elmqvist, Fanny Chevalier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05093v1": {
            "Paper Title": "Crack Modeling via Minimum-Weight Surfaces in 3d Voronoi Diagrams",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04514v1": {
            "Paper Title": "Self-Supervised 3D Human Pose Estimation in Static Video Via Neural\n  Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03287v3": {
            "Paper Title": "NeMF: Neural Motion Fields for Kinematic Animation",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". Many works demonstrated success in\nreal-time interactive kinematic character controls [ 13,40\u201342,47]. The auto-regressive approach is\nalso suitable for embracing the uncertainty of future predictions with generative models, for example,\nin the form of V AE as in HuMoR ",
                    "Citation Text": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas.\nHumor: 3d human motion model for robust pose estimation. In International Conference on Computer\nVision (ICCV) , 2021. 3, 5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04668",
                        "Citation Paper Title": "Title:HuMoR: 3D Human Motion Model for Robust Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos.",
                        "Citation Paper Authors": "Authors:Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ",\nwhich regularizes the skeletal topology to obtain better results ",
                    "Citation Text": "Dario Pavllo, David Grangier, and Michael Auli. Quaternet: A quaternion-based recurrent model for\nhuman motion. In British Machine Vision Conference (BMVC) , 2018. 1, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06485",
                        "Citation Paper Title": "Title:QuaterNet: A Quaternion-based Recurrent Model for Human Motion",
                        "Citation Paper Abstract": "Abstract:Deep learning for predicting or generating 3D human pose sequences is an active research area. Previous work regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angle or exponential map parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. Our recurrent network, QuaterNet, represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. On short-term predictions, QuaterNet improves the state-of-the-art quantitatively. For long-term generation, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature.",
                        "Citation Paper Authors": "Authors:Dario Pavllo, David Grangier, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", and then compute the geodesic\ndistance to measure the rotational difference:\nLrot=TX\nt=1arccosTr\u0010\nRt(^Rt)\u00001\u0011\n\u00001\n2;Lori=TX\nt=1arccosTr\u0010\nRo\nt(^Ro\nt)\u00001\u0011\n\u00001\n2: (3)\nWe then evaluate the L1loss on local joint positions obtained from forward kinematics (FK) ",
                    "Citation Text": "Ruben Villegas, Jimei Yang, Duygu Ceylan, and Honglak Lee. Neural kinematic networks for unsupervised\nmotion retargetting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June\n2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05653",
                        "Citation Paper Title": "Title:Neural Kinematic Networks for Unsupervised Motion Retargetting",
                        "Citation Paper Abstract": "Abstract:We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator.",
                        "Citation Paper Authors": "Authors:Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": ", we represent the local motion at time tas a\nmatrix Xtcomposed of joint positions xp\nt2RJ\u00023, velocities _xp\nt2RJ\u00023, rotations xr\nt2RJ\u00026in\n6D rotation form ",
                    "Citation Text": "Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representa-\ntions in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5745\u20135753, 2019. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07035",
                        "Citation Paper Title": "Title:On the Continuity of Rotation Representations in Neural Networks",
                        "Citation Paper Abstract": "Abstract:In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.",
                        "Citation Paper Authors": "Authors:Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "demonstrated long-term motion in-betweening by learning the space-time kinematic prior\nwithout supervision using Generative Adversarial Networks (GAN) ",
                    "Citation Text": "Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. Cyclical\nannealing schedule: A simple approach to mitigating kl vanishing. arXiv preprint arXiv:1903.10145 , 2019.\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10145",
                        "Citation Paper Title": "Title:Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing",
                        "Citation Paper Abstract": "Abstract:Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter \\beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for \\beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing \\beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",
                        "Citation Paper Authors": "Authors:Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, Lawrence Carin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.04047v1": {
            "Paper Title": "Motion Planning on Visual Manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.01582v2": {
            "Paper Title": "A Psychoacoustic Quality Criterion for Path-Traced Sound Propagation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03815v1": {
            "Paper Title": "Scene-level Tracking and Reconstruction without Object Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04628v1": {
            "Paper Title": "Novel View Synthesis with Diffusion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03007v1": {
            "Paper Title": "XDGAN: Multi-Modal 3D Shape Generation in 2D Space",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". Striking progress\nin the quality and resolution of GAN-generated images has been achieved in recent\nyears [5, 22, 23, 24], including in conditional settings ",
                    "Citation Text": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic\nimage synthesis with spatially-adaptive normalization. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07291",
                        "Citation Paper Title": "Title:Semantic Image Synthesis with Spatially-Adaptive Normalization",
                        "Citation Paper Abstract": "Abstract:We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ".\n4.1 Datasets and Experimental Setup\nFor the quantitative comparisons, we use the ShapeNetV2 ",
                    "Citation Text": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qix-\ning Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su,\net al. Shapenet: An information-rich 3d model repository. arXiv preprint\narXiv:1512.03012 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "for geometry\ngeneration, but nothing suggests that our approach will not generalize to other GAN\narchitectures (see Supplemental for experiments with StyleGAN2 ",
                    "Citation Text": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and\nTimo Aila. Analyzing and improving the image quality of stylegan. 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 8107\u20138116, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", which proposes to use an implicit decoder IM-NET\nin conjunction with features learned by a latent-GAN model ",
                    "Citation Text": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas J. Guibas.\nLearning representations and generative models for 3d point clouds. In ICML ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.02392",
                        "Citation Paper Title": "Title:Learning Representations and Generative Models for 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",
                        "Citation Paper Authors": "Authors:Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". While some degree of\nview control of such models has been exploited for downstream 3D tasks ",
                    "Citation Text": "Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio\nTorralba, and Sanja Fidler. Image gans meet differentiable rendering for inverse\ngraphics and interpretable 3d neural rendering. In International Conference on\nLearning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09125",
                        "Citation Paper Title": "Title:Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Differentiable rendering has paved the way to training neural networks to perform \"inverse graphics\" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \"neural renderer\", complementing traditional graphics renderers.",
                        "Citation Paper Authors": "Authors:Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "Generative Adversarial Networks (GANs) have become a popular technique for im-\nage generation since their introduction by Goodfellow et al. ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adver-\nsarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems\n27, pages 2672\u20132680. Curran Associates, Inc., 2014. URL http://papers.\nnips.cc/paper/5423-generative-adversarial-nets.pdf .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.02780v2": {
            "Paper Title": "GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ", 3D reconstruction from 2D or 2.5D images [ 48,49,50],\nand point cloud classi\ufb01cation and segmentation tasks [ 51,52,53]. However, meta-learning has not\nbeen extensively investigated in the context of SDFs. The most relevant method is MetaSDF ",
                    "Citation Text": "Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level\nof detail: Real-time rendering with implicit 3d shapes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 11358\u201311367, 2021. 4, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.10994",
                        "Citation Paper Title": "Title:Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
                        "Citation Paper Abstract": "Abstract:Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
                        "Citation Paper Authors": "Authors:Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ".\nAcronym is a post-processed subset of the popular ShapeNet ",
                    "Citation Text": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012 , 2015. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "that uses plane projection to learn local geometric features, and parallel\nUNets ",
                    "Citation Text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In Medical Image Computing and Computer-Assisted In-\ntervention \u2013 MICCAI 2015 , pages 234\u2013241, Cham, 2015. Springer International Publishing.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "coordinates and scene attributes, which offers a fully-differentiable and space-\nef\ufb01cient ",
                    "Citation Text": "Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-\nplicit neural representations with periodic activation functions. Advances in Neural Information\nProcessing Systems , 33:7462\u20137473, 2020. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09661",
                        "Citation Paper Title": "Title:Implicit Neural Representations with Periodic Activation Functions",
                        "Citation Paper Abstract": "Abstract:Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.01868v1": {
            "Paper Title": "Capturing and Animation of Body and Clothing from Monocular Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01651v1": {
            "Paper Title": "SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating\n  Video",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Recently, some researchers have focused\non applying the neural radiance \ufb01eld to human reconstruc-\ntion. Neuralbody ",
                    "Citation Text": "Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2021. 1, 2, 4, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15838",
                        "Citation Paper Title": "Title:Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans",
                        "Citation Paper Abstract": "Abstract:This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "mod-\ni\ufb01es the geometric representation to SDF to get \ufb01ner ge-\nometry and normal, so they can simultaneously estimate\ndetailed 3D geometry and the unshaded surface color to-\ngether with the scene illumination. ICON ",
                    "Citation Text": "Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J\nBlack. Icon: Implicit clothed humans obtained from nor-\nmals. arXiv preprint arXiv:2112.09127 , 2021. 3, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.09127",
                        "Citation Paper Title": "Title:ICON: Implicit Clothed humans Obtained from Normals",
                        "Citation Paper Abstract": "Abstract:Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.",
                        "Citation Paper Authors": "Authors:Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "infers a 3D\nclothed human meshes from a color image by utilizing a\nbody-guided normal prediction model and a local-feature-\nbased implicit 3D representation conditioned on SMPL(-\nX). SelfRecon ",
                    "Citation Text": "Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-\nfrecon: Self reconstruction your digital avatar from monocu-\nlar video. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , 2022. 2, 3, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.12792",
                        "Citation Paper Title": "Title:SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video",
                        "Citation Paper Abstract": "Abstract:We propose SelfRecon, a clothed human body reconstruction method that combines implicit and explicit representations to recover space-time coherent geometries from a monocular self-rotating human video. Explicit methods require a predefined template mesh for a given sequence, while the template is hard to acquire for a specific subject. Meanwhile, the fixed topology limits the reconstruction accuracy and clothing types. Implicit representation supports arbitrary topology and can represent high-fidelity geometry shapes due to its continuous nature. However, it is difficult to integrate multi-frame information to produce a consistent registration sequence for downstream applications. We propose to combine the advantages of both representations. We utilize differential mask loss of the explicit mesh to obtain the coherent overall shape, while the details on the implicit surface are refined with the differentiable neural rendering. Meanwhile, the explicit mesh is updated periodically to adjust its topology changes, and a consistency loss is designed to match both representations. Compared with existing methods, SelfRecon can produce high-fidelity surfaces for arbitrary clothed humans with self-supervised optimization. Extensive experimental results demonstrate its effectiveness on real captured monocular videos. The source code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Boyi Jiang, Yang Hong, Hujun Bao, Juyong Zhang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "utilizes a set of latent codes an-\nchored to a deformable mesh which is shared at different\nframes. H-NeRF ",
                    "Citation Text": "Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.\nH-nerf: Neural radiance \ufb01elds for rendering and temporal\nreconstruction of humans in motion. Advances in Neural In-\nformation Processing Systems , 34, 2021. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.13746",
                        "Citation Paper Title": "Title:H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion",
                        "Citation Paper Abstract": "Abstract:We present neural radiance fields for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance field with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject -- including both body and clothing -- and to regularize the radiance field to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities significantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.",
                        "Citation Paper Authors": "Authors:Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "introduces the volume alignment feature and rel-\native z-offset when giving a pair of stereo videos, which\ncan effectively alleviate the depth ambiguity and restore ab-\nsolute scale information. BCNet ",
                    "Citation Text": "Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang\nLiu, and Hujun Bao. Bcnet: Learning body and cloth shape\nfrom a single image. In European Conference on Computer\nVision , pages 18\u201335. Springer, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00214",
                        "Citation Paper Title": "Title:BCNet: Learning Body and Cloth Shape from A Single Image",
                        "Citation Paper Abstract": "Abstract:In this paper, we consider the problem to automatically reconstruct garment and body shapes from a single near-front view RGB image. To this end, we propose a layered garment representation on top of SMPL and novelly make the skinning weight of garment independent of the body mesh, which significantly improves the expression ability of our garment model. Compared with existing methods, our method can support more garment categories and recover more accurate geometry. To train our model, we construct two large scale datasets with ground truth body and garment geometries as well as paired color images. Compared with single mesh or non-parametric representation, our method can achieve more flexible control with separate meshes, makes applications like re-pose, garment transfer, and garment texture mapping possible. Code and some data is available at this https URL.",
                        "Citation Paper Authors": "Authors:Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, Hujun Bao"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "concatenates pixel\u2019s aligned feature\nand depth of a given query point as the input of an MLP\nto obtain a 3D human occupancy \ufb01eld. PIFuHD ",
                    "Citation Text": "Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 84\u201393, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00452",
                        "Citation Paper Title": "Title:PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization",
                        "Citation Paper Abstract": "Abstract:Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",
                        "Citation Paper Authors": "Authors:Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "\ufb01rst\ncalculates per-frame poses using the SMPL model, then\noptimizes for the subject\u2019s shape in the canonical T-pose.\nKolotouros et al. ",
                    "Citation Text": "Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-\nilidis. Convolutional mesh regression for single-image hu-\nman shape reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 4501\u20134510, 2019. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03244",
                        "Citation Paper Title": "Title:Convolutional Mesh Regression for Single-Image Human Shape Reconstruction",
                        "Citation Paper Abstract": "Abstract:This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with model-based approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graph-based mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose estimation approaches.",
                        "Citation Paper Authors": "Authors:Nikos Kolotouros, Georgios Pavlakos, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "model to rep-\nresent human body and obtain per-frame parameters via op-\ntimization. SMPL+D based method Videoavatars ",
                    "Citation Text": "Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Video based reconstruction\nof 3d people models. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 8387\u2013\n8397, 2018. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04758",
                        "Citation Paper Title": "Title:Video Based Reconstruction of 3D People Models",
                        "Citation Paper Abstract": "Abstract:This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.",
                        "Citation Paper Authors": "Authors:Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "adopts a representation\nconsisting of a density voxel grid and a feature voxel grid\nwith a network for view-dependent appearance. DIVeR ",
                    "Citation Text": "Liwen Wu, Jae Yong Lee, Anand Bhattad, Yuxiong Wang,\nand David Forsyth. Diver: Real-time and accurate neural ra-\ndiance \ufb01elds with deterministic integration for volume ren-\ndering. arXiv preprint arXiv:2111.10427 , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.10427",
                        "Citation Paper Title": "Title:DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic Integration for Volume Rendering",
                        "Citation Paper Abstract": "Abstract:DIVeR builds on the key ideas of NeRF and its variants -- density models and volume rendering -- to learn 3D object models that can be rendered realistically from small numbers of images. In contrast to all previous NeRF methods, DIVeR uses deterministic rather than stochastic estimates of the volume rendering integral. DIVeR's representation is a voxel based field of features. To compute the volume rendering integral, a ray is broken into intervals, one per voxel; components of the volume rendering integral are estimated from the features for each interval using an MLP, and the components are aggregated. As a result, DIVeR can render thin translucent structures that are missed by other integrators. Furthermore, DIVeR's representation has semantics that is relatively exposed compared to other such methods -- moving feature vectors around in the voxel space results in natural edits. Extensive qualitative and quantitative comparisons to current state-of-the-art methods show that DIVeR produces models that (1) render at or above state-of-the-art quality, (2) are very small without being baked, (3) render very fast without being baked, and (4) can be edited in natural ways.",
                        "Citation Paper Authors": "Authors:Liwen Wu, Jae Yong Lee, Anand Bhattad, Yuxiong Wang, David Forsyth"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "represent a scene as a sparse 3D grid\nwith spherical harmonics and thus can be optimized without\nany neural components. DVGO ",
                    "Citation Text": "Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance \ufb01elds\nreconstruction. arXiv preprint arXiv:2111.11215 , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11215",
                        "Citation Paper Title": "Title:Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction",
                        "Citation Paper Abstract": "Abstract:We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene.",
                        "Citation Paper Authors": "Authors:Cheng Sun, Min Sun, Hwann-Tzong Chen"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "adopts\nthousands of tiny MLPs instead of one single large MLP,\nwhich could achieve real-time rendering and can train 2\u02dc3x\n2faster. Plenoxels ",
                    "Citation Text": "Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenox-\nels: Radiance \ufb01elds without neural networks. arXiv preprint\narXiv:2112.05131 , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05131",
                        "Citation Paper Title": "Title:Plenoxels: Radiance Fields without Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.",
                        "Citation Paper Authors": "Authors:Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "utilizes the depth information supplied by 3D\npoint clouds to speed up convergence and synthesize better\nresults from fewer training views. KiloNeRF ",
                    "Citation Text": "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance \ufb01elds with\nthousands of tiny mlps. In International Conference on Com-\nputer Vision (ICCV) , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13744",
                        "Citation Paper Title": "Title:KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
                        "Citation Paper Abstract": "Abstract:NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
                        "Citation Paper Authors": "Authors:Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "could generate high-\ufb01delity novel view\nsynthesis, its long training time cannot be accepted in prac-\ntical use. Therefore, how to improve the training speed of\nNeRF has been widely studied since its emergence of NeRF.\nDS-NeRF ",
                    "Citation Text": "Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised NeRF: Fewer views and faster\ntraining for free. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\nJune 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.02791",
                        "Citation Paper Title": "Title:Depth-supervised NeRF: Fewer Views and Faster Training for Free",
                        "Citation Paper Abstract": "Abstract:A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as \"free\" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.",
                        "Citation Paper Authors": "Authors:Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "employs a conditional NeRF to generate audio-\ndriven talking head. HeadNeRF ",
                    "Citation Text": "Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong\nZhang. Headnerf: A real-time nerf-based parametric head\nmodel. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05637",
                        "Citation Paper Title": "Title:HeadNeRF: A Real-time NeRF-based Parametric Head Model",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model that integrates the neural radiance field to the parametric representation of the human head. It can render high fidelity head images in real-time on modern GPUs, and supports directly controlling the generated images' rendering pose and various semantic attributes. Different from existing related parametric models, we use the neural radiance fields as a novel 3D proxy instead of the traditional 3D textured mesh, which makes that HeadNeRF is able to generate high fidelity images. However, the computationally expensive rendering process of the original NeRF hinders the construction of the parametric NeRF model. To address this issue, we adopt the strategy of integrating 2D neural rendering to the rendering process of NeRF and design novel loss terms. As a result, the rendering speed of HeadNeRF can be significantly accelerated, and the rendering time of one frame is reduced from 5s to 25ms. The well designed loss terms also improve the rendering accuracy, and the fine-level details of the human head, such as the gaps between teeth, wrinkles, and beards, can be represented and synthesized by HeadNeRF. Extensive experimental results and several applications demonstrate its effectiveness. The trained parametric model is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "introduces a temporal trans-\nformer and a multi-view transformer to aggregate corre-\nsponding features across space and time. Weng et al. ",
                    "Citation Text": "Chung-Yi Weng, Brian Curless, Pratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video. arXiv preprint arXiv:2201.04127 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.04127",
                        "Citation Paper Title": "Title:HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video",
                        "Citation Paper Abstract": "Abstract:We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is particularly challenging, as it requires synthesizing photorealistic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesizing fine details such as cloth folds and facial appearance. Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, produced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios.",
                        "Citation Paper Authors": "Authors:Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, Ira Kemelmacher-Shlizerman"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposes skeleton embedding serves\nas a common reference that links constraints across time.\nNeural Human Performer ",
                    "Citation Text": "Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry\nFuchs. Neural human performer: Learning generalizable ra-\ndiance \ufb01elds for human performance rendering. Advances in\nNeural Information Processing Systems , 34, 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.07448",
                        "Citation Paper Title": "Title:Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering",
                        "Citation Paper Abstract": "Abstract:In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "integrates texture map fea-\ntures to re\ufb01ne volume rendering. HumanNeRF ",
                    "Citation Text": "Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang\nZhang, Jingyi Yu, and Lan Xu. Humannerf: Generaliz-\nable neural human radiance \ufb01eld from sparse inputs. arXiv\npreprint arXiv:2112.02789 , 2021. 1, 2\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.02789",
                        "Citation Paper Title": "Title:HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs",
                        "Citation Paper Abstract": "Abstract:Recent neural human representations can produce high-quality multi-view rendering but require using dense multi-view inputs and costly training. They are hence largely limited to static models as training each frame is infeasible. We present HumanNeRF - a generalizable neural representation - for high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated pixel-alignment feature across multi-view inputs along with a pose embedded non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To further improve the rendering quality, we augment our solution with an appearance blending module for combining the benefits of both neural volumetric rendering and neural texture blending. Extensive experiments on various multi-view dynamic human datasets demonstrate the generalizability and effectiveness of our approach in synthesizing photo-realistic free-view humans under challenging motions and with very sparse camera view inputs.",
                        "Citation Paper Authors": "Authors:Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, Lan Xu"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "de\ufb01nes the neural scene representation on the mesh surface\npoints and signed distances from the surface of a human\nbody mesh. Neural Actor ",
                    "Citation Text": "Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt. Neural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol. ACM Transactions on Graphics (TOG) , 40(6):1\u201316,\n2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.02019",
                        "Citation Paper Title": "Title:Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control",
                        "Citation Paper Abstract": "Abstract:We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "introduces deformation \ufb01elds\nbased on neural blend weight \ufb01elds to generate observation-\nto-canonical correspondences. Surface-Aligned NeRF ",
                    "Citation Text": "Tianhan Xu, Yasuhiro Fujita, and Eiichi Matsumoto.\nSurface-aligned neural radiance \ufb01elds for controllable 3d hu-\nman synthesis. arXiv preprint arXiv:2201.01683 , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.01683",
                        "Citation Paper Title": "Title:Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis",
                        "Citation Paper Abstract": "Abstract:We propose a new method for reconstructing controllable implicit 3D human models from sparse multi-view RGB videos. Our method defines the neural scene representation on the mesh surface points and signed distances from the surface of a human body mesh. We identify an indistinguishability issue that arises when a point in 3D space is mapped to its nearest surface point on a mesh for learning surface-aligned neural scene representation. To address this issue, we propose projecting a point onto a mesh surface using a barycentric interpolation with modified vertex normals. Experiments with the ZJU-MoCap and Human3.6M datasets show that our approach achieves a higher quality in a novel-view and novel-pose synthesis than existing methods. We also demonstrate that our method easily supports the control of body shape and clothes. Project page: this https URL.",
                        "Citation Paper Authors": "Authors:Tianhan Xu, Yasuhiro Fujita, Eiichi Matsumoto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.01602v1": {
            "Paper Title": "Self-improving Multiplane-to-layer Images for Novel View Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "assumed that the size of this stack may differ between pix-\nels. In addition, any connections between pixels were not\nimposed. Although later manuscripts introduced explicit\nlocal connectivity of neighboring pixels ",
                    "Citation Text": "M. L. Shih, S. Y . Su, J. Kopf, and J. B. Huang. 3d photogra-\nphy using context-aware layered depth inpainting. In CVPR ,\n2020. 2, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04727",
                        "Citation Paper Title": "Title:3D Photography using Context-aware Layered Depth Inpainting",
                        "Citation Paper Abstract": "Abstract:We propose a method for converting a single RGB-D input image into a 3D photo - a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner. The resulting 3D photos can be efficiently rendered with motion parallax using standard graphics engines. We validate the effectiveness of our method on a wide range of challenging everyday scenes and show fewer artifacts compared with the state of the arts.",
                        "Citation Paper Authors": "Authors:Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". How-\never, massive queries to the neural network at inference time\nresult in slow rendering speed. Moreover, IBRNet ",
                    "Citation Text": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P.\nSrinivasan, Howard Zhou, Jonathan T. Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser.\nIBRNet: Learning Multi-View Image-Based Rendering. In\nCVPR , 2021. 1, 2, 5, 7, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.13090",
                        "Citation Paper Title": "Title:IBRNet: Learning Multi-View Image-Based Rendering",
                        "Citation Paper Abstract": "Abstract:We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "paper to the\ncase of only two input frames.\nRecently, several methods [48, 3, 39, 44, 4, 37] have at-\ntempted to estimate the implicit 3D representation from in-\nput images in the form of a neural radiance field ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV , 2020. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "efficient for single-image NVS, due to its\nsparsity and simplicity of rendering. At the same time, es-timating this representation in a differentiable way without\nrelaxation is still challenging ",
                    "Citation Text": "Shubham Tulsiani, Richard Tucker, and Noah Snavely.\nLayer-structured 3d scene inference via view synthesis. In\nECCV , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10264",
                        "Citation Paper Title": "Title:Layer-structured 3D Scene Inference via View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present an approach to infer a layer-structured 3D representation of a scene from a single input image. This allows us to infer not only the depth of the visible pixels, but also to capture the texture and depth for content in the scene that is not directly visible. We overcome the challenge posed by the lack of direct supervision by instead leveraging a more naturally available multi-view supervisory signal. Our insight is to use view synthesis as a proxy task: we enforce that our representation (inferred from a single image), when rendered from a novel perspective, matches the true observed image. We present a learning framework that operationalizes this insight using a new, differentiable novel view renderer. We provide qualitative and quantitative validation of our approach in two different settings, and demonstrate that we can learn to capture the hidden aspects of a scene.",
                        "Citation Paper Authors": "Authors:Shubham Tulsiani, Richard Tucker, Noah Snavely"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.03380v2": {
            "Paper Title": "Shape, Light, and Material Decomposition from Images using Monte Carlo\n  Rendering and Denoising",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03480v2": {
            "Paper Title": "SHRED: 3D Shape Region Decomposition with Learned Local Operations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.15632v1": {
            "Paper Title": "ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11224v3": {
            "Paper Title": "VToonify: Controllable High-Resolution Portrait Video Style Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14697v2": {
            "Paper Title": "Creative Painting with Latent Diffusion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.15172v1": {
            "Paper Title": "Understanding Pure CLIP Guidance for Voxel Grid NeRF Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02268v2": {
            "Paper Title": "Visual Analysis of Multiple Dynamic Sensitivities along Ascending\n  Trajectories in the Atmosphere",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14537v1": {
            "Paper Title": "GPU-based Data-parallel Rendering of Large, Unstructured, and\n  Non-convexly Partitioned Data",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ", which reduces delays and\ncommunications by implementing a spatiotemporally-aware composi-\ntor. Their approach uses \u201cchains\u201d that determine the blending order of\neach strip of the image. Usher et al. ",
                    "Citation Text": "W. Usher, I. Wald, J. Amstutz, J. G\u00fcnther, C. Brownlee, and V . Pascucci.\nScalable ray tracing using the distributed framebuffer. Computer Graphics\nForum , 38(3):455\u2013466, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.07083",
                        "Citation Paper Title": "Title:Scalable Ray Tracing Using the Distributed FrameBuffer",
                        "Citation Paper Abstract": "Abstract:Image- and data-parallel rendering across multiple nodes on high-performance computing systems is widely used in visualization to provide higher frame rates, support large data sets, and render data in situ. Specifically for in situ visualization, reducing bottlenecks incurred by the visualization and compositing is of key concern to reduce the overall simulation runtime. Moreover, prior algorithms have been designed to support either image- or data-parallel rendering and impose restrictions on the data distribution, requiring different implementations for each configuration. In this paper, we introduce the Distributed FrameBuffer, an asynchronous image-processing framework for multi-node rendering. We demonstrate that our approach achieves performance superior to the state of the art for common use cases, while providing the flexibility to support a wide range of parallel rendering algorithms and data distributions. By building on this framework, we extend the open-source ray tracing library OSPRay with a data-distributed API, enabling its use in data-distributed and in situ visualization applications.",
                        "Citation Paper Authors": "Authors:Will Usher, Ingo Wald, Jefferson Amstutz, Johannes G\u00fcnther, Carson Brownlee, Valerio Pascucci"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02545v3": {
            "Paper Title": "New Properties and Invariants of Harmonic Polygons",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ".\nThe more elementary Brocard porism of triangles is studied in [ 6,7,16,26]. In ",
                    "Citation Text": "Garcia, R., Reznik, D. (2022). Loci of the Brocard points over selected triangle families. Intl.\nJ. of Geom. , 11(2): 35\u201345.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08561",
                        "Citation Paper Title": "Title:Loci of the Brocard Points over Selected Triangle Families",
                        "Citation Paper Abstract": "Abstract:We study the loci of the Brocard points over two selected families of triangles: (i) 2 vertices fixed on a circumference and a third one which sweeps it, (ii) Poncelet 3-periodics in the homothetic ellipse pair. Loci obtained include circles, ellipses, and teardrop-like curves. We derive expressions for both curves and their areas. We also study the locus of the vertices of Brocard triangles over the homothetic and Brocard-poristic Poncelet families.",
                        "Citation Paper Authors": "Authors:Ronaldo Garcia, Dan Reznik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.13875v1": {
            "Paper Title": "A General Scattering Phase Function for Inverse Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13514v1": {
            "Paper Title": "StyleSwap: Style-Based Generator Empowers Robust Face Swapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13254v1": {
            "Paper Title": "Identifying and Extracting Football Features from Real-World Media\n  Sources using Only Synthetic Training Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.12101v1": {
            "Paper Title": "3D Reconstruction using Structured Light from off-the-shelf components",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.10845v2": {
            "Paper Title": "DIG: Draping Implicit Garment over the Human Body",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11856v1": {
            "Paper Title": "WordStream Maker: A Lightweight End-to-end Visualization Platform for\n  Qualitative Time-series Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07556v2": {
            "Paper Title": "ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11223v1": {
            "Paper Title": "UniColor: A Unified Framework for Multi-Modal Colorization with\n  Transformer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07850v2": {
            "Paper Title": "HF-NeuS: Improved Surface Reconstruction Using High-Frequency Details",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "models the signed distance function of\nthe shape and uses a sphere tracking algorithm to render 2D images. A signi\ufb01cant milestone in 3D\nreconstruction was the development of NeRF ",
                    "Citation Text": "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing\nscenes as neural radiance \ufb01elds for view synthesis. In European conference on computer vision , pages\n405\u2013421. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "utilizes surface rendering\nto model the occupancy function of a 3D shape, which uses a root search approach to obtain the\nlocation of the surface and predicts a 2D image. IDR ",
                    "Citation Text": "L. Yariv, Y . Kasten, D. Moran, M. Galun, M. Atzmon, B. Ronen, and Y . Lipman. Multiview neural surface\nreconstruction by disentangling geometry and appearance. Advances in Neural Information Processing\nSystems , 33:2492\u20132502, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09852",
                        "Citation Paper Title": "Title:Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance",
                        "Citation Paper Abstract": "Abstract:In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",
                        "Citation Paper Authors": "Authors:Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "presents an integrated positional encoding to control\nfrequency in different scales. Park et al. ",
                    "Citation Text": "K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla. Ner\ufb01es:\nDeformable neural radiance \ufb01elds. International Conference on Vomputer Vision , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "proposes to use the sin function as\nactivation function in the network. MipNeRF ",
                    "Citation Text": "J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan. Mip-nerf:\nA multiscale representation for anti-aliasing neural radiance \ufb01elds. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 5855\u20135864, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "derive an unbiased density function equation using logistic sigmoid\nfunctions and introduce a learnable parameter to control the slope of the function during rendering\nand sampling. Concurrent to our work, NeuralPatch ",
                    "Citation Text": "F. Darmon, B. Bascle, J.-C. Devaux, P. Monasse, and M. Aubry. Improving neural implicit surfaces\ngeometry with patch warping. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 6260\u20136269, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.09648",
                        "Citation Paper Title": "Title:Improving neural implicit surfaces geometry with patch warping",
                        "Citation Paper Abstract": "Abstract:Neural implicit surfaces have become an important technique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difficulty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping entire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their similarity with a robust structural similarity (SSIM); (ii) handling visibility and occlusion in such a way that incorrect warps are not given too much importance while encouraging a reconstruction as complete as possible. We evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20% on both datasets.",
                        "Citation Paper Authors": "Authors:Fran\u00e7ois Darmon, B\u00e9n\u00e9dicte Bascle, Jean-Cl\u00e9ment Devaux, Pascal Monasse, Mathieu Aubry"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.16988v1": {
            "Paper Title": "Data Bricks Space Mission: Teaching Kids about Data with Physicalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.10174v1": {
            "Paper Title": "Learning Reconstructability for Drone Aerial Path Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09965v1": {
            "Paper Title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.15236v2": {
            "Paper Title": "Stochastic Poisson Surface Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09807v1": {
            "Paper Title": "Reflections and Considerations on Running Creative Visualization\n  Learning Activities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09712v1": {
            "Paper Title": "Process Mining Meets Visual Analytics: The Case of Conformance Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.05297v5": {
            "Paper Title": "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for\n  Conversational Gestures Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11940v2": {
            "Paper Title": "Wassersplines for Neural Vector Field--Controlled Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09344v1": {
            "Paper Title": "Understanding reinforcement learned crowds",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "combine DRL with an ORCA layer that ensures\ncollision-free movement.\nDRL is also used to generate more interesting, higher-\nquality trajectories. Xu and Karamouzas ",
                    "Citation Text": "Xu, P, Karamouzas, I. Human-inspired multi-agent navigation using\nknowledge distillation. In: 2021 IEEE /RSJ International Conference on\nIntelligent Robots and Systems (IROS). 2021, p. 8105\u20138112. doi: 10.\n1109/IROS51168.2021.9636463 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10000",
                        "Citation Paper Title": "Title:Human-Inspired Multi-Agent Navigation using Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Despite significant advancements in the field of multi-agent navigation, agents still lack the sophistication and intelligence that humans exhibit in multi-agent settings. In this paper, we propose a framework for learning a human-like general collision avoidance policy for agent-agent interactions in fully decentralized, multi-agent environments. Our approach uses knowledge distillation with reinforcement learning to shape the reward function based on expert policies extracted from human trajectory demonstrations through behavior cloning. We show that agents trained with our approach can take human-like trajectories in collision avoidance and goal-directed steering tasks not provided by the demonstrations, outperforming the experts as well as learning-based agents trained without knowledge distillation.",
                        "Citation Paper Authors": "Authors:Pei Xu, Ioannis Karamouzas"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ".\nCrowd Simulation via DRL. There is a number of prior\npapers which train DRL agents on the task of crowd simu-\nlation. Long et al. ",
                    "Citation Text": "Long, P, Fan, T, Liao, X, Liu, W, Zhang, H, Pan, J. Towards Opti-\nmally Decentralized Multi-Robot Collision Avoidance via Deep Rein-\nforcement Learning. arXiv:170910082 [cs] 2018;ArXiv: 1709.10082.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.10082",
                        "Citation Paper Title": "Title:Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generate its paths without observing other robots' states and intents. While other distributed multi-robot collision avoidance systems exist, they often require extracting agent-level features to plan a local collision-free action, which can be computationally prohibitive and not robust. More importantly, in practice the performance of these methods are much lower than their centralized counterparts.\nWe present a decentralized sensor-level collision avoidance policy for multi-robot systems, which directly maps raw sensor measurements to an agent's steering commands in terms of movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to find an optimal policy which is trained over a large number of robots on rich, complex environments simultaneously using a policy gradient based reinforcement learning algorithm. We validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient, collision-free paths for a large-scale robot system. We also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period, including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots. Videos are available at this https URL",
                        "Citation Paper Authors": "Authors:Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao Zhang, Jia Pan"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "perform a large-scale study of im-\nplementation details and code-level optimizations that a \u000bect\nthe performance of TRPO and PPO in robotic control tasks.\nSimilarly, Andrychowicz et al. ",
                    "Citation Text": "Andrychowicz, M, Raichuk, A, Sta \u00b4nczyk, P, Orsini, M, Girgin,\nS, Marinier, R, et al. What Matters In On-Policy Reinforcement\nLearning? A Large-Scale Empirical Study. arXiv:200605990 [cs, stat]\n2020;ArXiv: 2006.05990.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05990",
                        "Citation Paper Title": "Title:What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study",
                        "Citation Paper Abstract": "Abstract:In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",
                        "Citation Paper Authors": "Authors:Marcin Andrychowicz, Anton Raichuk, Piotr Sta\u0144czyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "explore how the performance of RL agents\nis a\u000bected by parameters like the initial state distribution,\ncomponents of the state representation, or the control fre-\nquency. Engstrom et al. ",
                    "Citation Text": "Engstrom, L, Ilyas, A, Santurkar, S, Tsipras, D, Janoos, F, Rudolph,\nL, et al. Implementation Matters in Deep Policy Gradients: A Case\nStudy on PPO and TRPO. arXiv:200512729 [cs, stat] 2020;ArXiv:\n2005.12729.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12729",
                        "Citation Paper Title": "Title:Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO",
                        "Citation Paper Abstract": "Abstract:We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty and importance of attributing performance gains in deep reinforcement learning. Code for reproducing our results is available at this https URL .",
                        "Citation Paper Authors": "Authors:Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". A more modern version\nthat follows the same principle is the Proximal Policy Opti-\nmization (PPO) algorithm, introduced by Schulman et al. ",
                    "Citation Text": "Schulman, J, Wolski, F, Dhariwal, P, Radford, A, Klimov, O. Prox-\nimal Policy Optimization Algorithms. arXiv:170706347 [cs] 2017;.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ",\nwhich typically involve designing rule-based systems to up-\ndate each agent\u2019s velocity based on their context. There also\nexists a body of work using RL for controlling virtual char-\nacters, including crowd scenarios ",
                    "Citation Text": "Kwiatkowski, A, Alvarado, E, Kalogeiton, V , Liu, CK, Pettr \u00b4e, J,\nvan de Panne, M, et al. A Survey on Reinforcement Learning Methods\nin Character Animation. Computer Graphics Forum 2022;41(2):613\u2013\n639. doi: 10.1111/cgf.14504 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.04735",
                        "Citation Paper Title": "Title:A Survey on Reinforcement Learning Methods in Character Animation",
                        "Citation Paper Abstract": "Abstract:Reinforcement Learning is an area of Machine Learning focused on how agents can be trained to make sequential decisions, and achieve a particular goal within an arbitrary environment. While learning, they repeatedly take actions based on their observation of the environment, and receive appropriate rewards which define the objective. This experience is then used to progressively improve the policy controlling the agent's behavior, typically represented by a neural network. This trained module can then be reused for similar problems, which makes this approach promising for the animation of autonomous, yet reactive characters in simulators, video games or virtual reality environments. This paper surveys the modern Deep Reinforcement Learning methods and discusses their possible applications in Character Animation, from skeletal control of a single, physically-based character to navigation controllers for individual agents and virtual crowds. It also describes the practical side of training DRL systems, comparing the different frameworks available to build such agents.",
                        "Citation Paper Authors": "Authors:Ariel Kwiatkowski, Eduardo Alvarado, Vicky Kalogeiton, C. Karen Liu, Julien Pettr\u00e9, Michiel van de Panne, Marie-Paule Cani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05462v2": {
            "Paper Title": "Designing Personalized Garments with Body Movement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09029v1": {
            "Paper Title": "BareSkinNet: De-makeup and De-lighting via 3D Face Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08791v1": {
            "Paper Title": "DifferSketching: How Differently Do People Sketch 3D Objects?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08725v1": {
            "Paper Title": "Neural Wavelet-domain Diffusion for 3D Shape Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08531v1": {
            "Paper Title": "Progressive tearing and cutting of soft-bodies in high-performance\n  virtual reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08468v1": {
            "Paper Title": "Human Performance Modeling and Rendering via Neural Animated Mesh",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07806v1": {
            "Paper Title": "SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid\n  Shape Correspondence",
            "Sentences": [
                {
                    "Sentence ID": 91,
                    "Sentence": ",P1andP2are partially overlapped and thus onlypoints sparsely sampled in the overlap region are consid-\nered in the above formulation.\nAnother example of contrastive learning is its application\nto the graph matching problem ",
                    "Citation Text": "Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learn-\ning combinatorial embedding networks for deep graph\nmatching. In CVPR , pages 3056\u20133065, 2019. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00597",
                        "Citation Paper Title": "Title:Learning Combinatorial Embedding Networks for Deep Graph Matching",
                        "Citation Paper Abstract": "Abstract:Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.",
                        "Citation Paper Authors": "Authors:Runzhong Wang, Junchi Yan, Xiaokang Yang"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ". The third category is su-\npervised learning approaches, including FMNet ",
                    "Citation Text": "Or Litany, Tal Remez, Emanuele Rodola, Alex Bronstein,\nand Michael Bronstein. Deep functional maps: Structured\nprediction for dense shape correspondence. In ICCV , 2017.\n2, 3, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.08686",
                        "Citation Paper Title": "Title:Deep Functional Maps: Structured Prediction for Dense Shape Correspondence",
                        "Citation Paper Abstract": "Abstract:We introduce a new framework for learning dense correspondence between deformable 3D shapes. Existing learning based approaches model shape correspondence as a labelling problem, where each point of a query shape receives a label identifying a point on some reference domain; the correspondence is then constructed a posteriori by composing the label predictions of two input shapes. We propose a paradigm shift and design a structured prediction model in the space of functional maps, linear operators that provide a compact representation of the correspondence. We model the learning process via a deep residual network which takes dense descriptor fields defined on two shapes as input, and outputs a soft map between the two given objects. The resulting correspondence is shown to be accurate on several challenging benchmarks comprising multiple categories, synthetic models, real scans with acquisition artifacts, topological noise, and partiality.",
                        "Citation Paper Authors": "Authors:Or Litany, Tal Remez, Emanuele Rodol\u00e0, Alex M. Bronstein, Michael M. Bronstein"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": ".\nThe second category is unsupervised learning approaches,\nincluding SURFMNet ",
                    "Citation Text": "Jean-Michel Roufosse, Abhishek Sharma, and Maks Ovs-\njanikov. Unsupervised deep learning for structured shape\nmatching. In ICCV , 2019. 2, 3, 4, 5, 6, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03794",
                        "Citation Paper Title": "Title:Unsupervised Deep Learning for Structured Shape Matching",
                        "Citation Paper Abstract": "Abstract:We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely \\emph{unsupervised setting}, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.",
                        "Citation Paper Authors": "Authors:Jean-Michel Roufosse, Abhishek Sharma, Maks Ovsjanikov"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". Besides, there also exist\nrecent works performing the contrasting at the local patch\nlevel ",
                    "Citation Text": "Bi\u2019an Du, Xiang Gao, Wei Hu, and Xin Li. Self-contrastive\nlearning with hard negative sampling for self-supervised\npoint cloud learning. In ACM MM , pages 3133\u20133142, 2021.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.01886",
                        "Citation Paper Title": "Title:Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning",
                        "Citation Paper Abstract": "Abstract:Point clouds have attracted increasing attention. Significant progress has been made in methods for point cloud analysis, which often requires costly human annotation as supervision. To address this issue, we propose a novel self-contrastive learning for self-supervised point cloud representation learning, aiming to capture both local geometric patterns and nonlocal semantic primitives based on the nonlocal self-similarity of point clouds. The contributions are two-fold: on the one hand, instead of contrasting among different point clouds as commonly employed in contrastive learning, we exploit self-similar point cloud patches within a single point cloud as positive samples and otherwise negative ones to facilitate the task of contrastive learning. On the other hand, we actively learn hard negative samples that are close to positive samples for discriminative feature learning. Experimental results show that the proposed method achieves state-of-the-art performance on widely used benchmark datasets for self-supervised point cloud segmentation and transfer learning for classification.",
                        "Citation Paper Authors": "Authors:Bi'an Du, Xiang Gao, Wei Hu, Xin Li"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "per-\nform local feature contrasting at the point level between two\ntransformed 3D scene fragments. The learned feature repre-\nsentation is shown to be useful in downstream 3D tasks like\nsegmentation and detection ",
                    "Citation Text": "Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nCVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04405",
                        "Citation Paper Title": "Title:ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at this http URL.",
                        "Citation Paper Authors": "Authors:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.03258v3": {
            "Paper Title": "DoodleFormer: Creative Sketch Drawing with Transformers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06814v1": {
            "Paper Title": "Neural Point-based Shape Modeling of Humans in Challenging Clothing",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "successfully model clothing or clothed humans\nwith point clouds, and further demonstrate in ",
                    "Citation Text": "Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black.\nThe power of points for modeling humans in clothing. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , pages 10974\u201310984, Oct. 2021. 1,\n2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01137",
                        "Citation Paper Title": "Title:The Power of Points for Modeling Humans in Clothing",
                        "Citation Paper Abstract": "Abstract:Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along -- the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes.",
                        "Citation Paper Authors": "Authors:Qianli Ma, Jinlong Yang, Siyu Tang, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "and \ufb01rst encode the UV-positional map (with a resolution of\n128\u0002128) of the unclothed posed body by a U-Net ",
                    "Citation Text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention (MICCAI) , pages\n234\u2013241, 2015. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.06614v1": {
            "Paper Title": "Cluster-based multidimensional scaling embedding tool for data\n  visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06421v1": {
            "Paper Title": "A Transfer Function Design Using A Knowledge Database based on Deep\n  Image and Primitive Intensity Profile Features Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05753v1": {
            "Paper Title": "Neural3Points: Learning to Generate Physically Realistic Full-body\n  Motion for Virtual Reality Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03494v1": {
            "Paper Title": "Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D\n  Image Representations",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". DINO and MoCo-v3 are self-\nsupervised whereas DeiT is trained with supervision (image\nlabels). Features on all scenes are pre-computed using the\npublicly available weights (pre-trained on ImageNet ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. IJCV , 115(3):211\u2013252,\n2015. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", \ufb01tting\nthe latter to 2D images via differentiable rendering. Neural\nRadiance Fields (NeRFs) ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In Proc. ECCV , pages 405\u2013421, 2020. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "We summarize relevant background work in feature ex-\ntraction, reconstruction and neural rendering.\nSelf-supervised visual features. While N3F can work\non top of any 2D dense image features, including recent\nones based on Vision Transformers (ViT) ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "uses NeRF as a means to integrate coarse 3D and noisy 2D\nlabels and render re\ufb01ned 2D panoptic maps, while ",
                    "Citation Text": "Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Car-\noline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi,\nFrank Dellaert, and Thomas Funkhouser. Panoptic neural\n\ufb01elds: A semantic object-aware neural scene representation.\nInProc. CVPR , pages 12871\u201312881, 2022. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.04334",
                        "Citation Paper Title": "Title:Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation",
                        "Citation Paper Abstract": "Abstract:We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff). Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direction, and time and outputs density and radiance. The background stuff is represented by a similar MLP that additionally outputs semantic labels. Each object MLPs are instance-specific and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-specific priors incorporated via meta-learned initialization. Our model builds a panoptic radiance field representation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly optimize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color images and pseudo-supervision from predicted semantic segmentations. During experiments with real-world dynamic scenes, we find that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmentation, 3D scene editing, and multiview depth prediction.",
                        "Citation Paper Authors": "Authors:Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": ") to achieve\nsemantically aware synthesis [22, 66].\nMore related to our work, however, are methods that ex-\ntend radiance \ufb01elds to also predict semantics [25, 65, 79,\n80]. For example, Semantic-NeRF ",
                    "Citation Text": "Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-\ndrew Davison. In-place scene labelling and understanding\nwith implicit scene representation. In Proc. ICCV , pages\n15838\u201315847, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15875",
                        "Citation Paper Title": "Title:In-Place Scene Labelling and Understanding with Implicit Scene Representation",
                        "Citation Paper Abstract": "Abstract:Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties.\nWe extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.",
                        "Citation Paper Authors": "Authors:Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "have popularized such ideas\nby applying them in a powerful manner to a comparatively\nsimple setting: novel view synthesis from a collection of\nimages of a single static scene. They combine radiance\n\ufb01elds with internal learning ",
                    "Citation Text": "Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.\n10Deep image prior. In Proc. CVPR , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10925",
                        "Citation Paper Title": "Title:Deep Image Prior",
                        "Citation Paper Abstract": "Abstract:Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.\nApart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at this https URL .",
                        "Citation Paper Authors": "Authors:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". Recently, authors have proposed to represent im-\nplicit functions with deep neural networks for the repre-\nsentation of geometry ",
                    "Citation Text": "Jeong Joon Park, Peter Florence, Julian Straub, Richard A.\nNewcombe, and Steven Lovegrove. DeepSDF: Learning\ncontinuous signed distance functions for shape representa-\ntion. In Proc. CVPR , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05252v1": {
            "Paper Title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video\n  Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03083v1": {
            "Paper Title": "Interactive Visual Analysis of Structure-borne Noise Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03245v1": {
            "Paper Title": "The HoloLens in Medicine: A systematic Review and Taxonomy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02316v1": {
            "Paper Title": "Wavelet-based Loss for High-frequency Interface Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13663v2": {
            "Paper Title": "AccuStripes: Adaptive Binning for the Visual Comparison of Univariate\n  Data Distributions",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", where each bin contains the\nsame number of data points. For this reason the position of the bin\nboundaries is arbitrary. A more sophisticated adaptive binning tech-\nnique is Bayesian Blocks binning (BB) ",
                    "Citation Text": "B. Pollack, S. Bhattacharya, and M. Schmitt. Bayesian Block Histogram-\nming for High Energy Physics. arXiv preprint arXiv:1708.00810 , Aug.\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.00810",
                        "Citation Paper Title": "Title:Bayesian Block Histogramming for High Energy Physics",
                        "Citation Paper Abstract": "Abstract:The Bayesian Block algorithm, originally developed for applications in astronomy, can be used to improve the binning of histograms in high energy physics. The visual improvement can be dramatic, as shown here with two simple examples. More importantly, this algorithm and the histogram is produces is a non-parametric density estimate, providing a description of background distributions that does not suffer from the arbitrariness of ad hoc analytical functions. The statistical power of an hypothesis test based on Bayesian Blocks is nearly as good as that obtained by fitting analytical functions. Two examples are provided: a narrow peak on a smoothly-falling background, and an excess in the tail of a background that falls rapidly over several orders of magnitude. These examples show the usefulness of the binning provided by the Bayesian Blocks algorithm both for presentation of data and when searching for new physics.",
                        "Citation Paper Authors": "Authors:Brian Pollack, Saptaparna Bhattacharya, Michael Schmitt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.12038v5": {
            "Paper Title": "Riemannian Geometry Approach for Minimizing Distortion and its\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.01846v1": {
            "Paper Title": "Evaluating Situated Visualization in AR with Eye Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.01746v1": {
            "Paper Title": "SPCNet: Stepwise Point Cloud Completion Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00682v1": {
            "Paper Title": "Zero-Shot Multi-Modal Artist-Controlled Retrieval and Exploration of 3D\n  Object Sets",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "where the authors used it to ensure\nincreasingly abstract sketches that are semantically similar to the\nsupervisory text prompt. Concurrent work to ours called TASK-\nformer ",
                    "Citation Text": "Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, and James Hays. 2022. A Sketch\nis Worth a Thousand Words: Image Retrieval with Text and Sketch. European\nConference on Computer Vision, ECCV (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.03354",
                        "Citation Paper Title": "Title:A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch",
                        "Citation Paper Abstract": "Abstract:We address the problem of retrieving images with both a sketch and a text query. We present TASK-former (Text And SKetch transformer), an end-to-end trainable model for image retrieval using a text description and a sketch as input. We argue that both input modalities complement each other in a manner that cannot be achieved easily by either one alone. TASK-former follows the late-fusion dual-encoder approach, similar to CLIP, which allows efficient and scalable retrieval since the retrieval set can be indexed independently of the queries. We empirically demonstrate that using an input sketch (even a poorly drawn one) in addition to text considerably increases retrieval recall compared to traditional text-based image retrieval. To evaluate our approach, we collect 5,000 hand-drawn sketches for images in the test set of the COCO dataset. The collected sketches are available a this https URL.",
                        "Citation Paper Authors": "Authors:Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, James Hays"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.09965v3": {
            "Paper Title": "Few-Shot Font Generation by Learning Fine-Grained Local Styles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00116v1": {
            "Paper Title": "LaserSVG: Responsive Laser-Cutter Templates",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ", for example, take a 3D model and convert it into planar sections. Mesh2Fab ",
                    "Citation Text": "Yong-Liang Yang, Jun Wang, and Niloy J. Mitra. 2014. Mesh2Fab: Reforming Shapes for Material-specific Fabrication. CoRR abs/1411.3632 (2014).\narXiv:1411.3632 http://arxiv.org/abs/1411.3632",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.3632",
                        "Citation Paper Title": "Title:Mesh2Fab: Reforming Shapes for Material-specific Fabrication",
                        "Citation Paper Abstract": "Abstract:As humans, we regularly associate shape of an object with its built material. In the context of geometric modeling, however, this interrelation between form and material is rarely explored. In this work, we propose a novel data-driven reforming (i.e., reshaping) algorithm that adapts an input multi-component model for a target fabrication material. The algorithm adapts both the part geometry and the inter-part topology of the input shape to better align with material specific fabrication requirements. As output, we produce the reshaped model along with respective part dimensions and inter-part junction specifications. We evaluate our algorithm on a range of man-made models and demonstrate non-trivial model reshaping examples focusing only on metal and wooden materials. We also appraise the output of our algorithm using a user study.",
                        "Citation Paper Authors": "Authors:Yong-Liang Yang, Jun Wang, Niloy J. Mitra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.11774v2": {
            "Paper Title": "The Esports Frontier: Rendering for Competitive Games",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". In the local computer latency case, small\nsteps in already low latency have been shown to still impact player\nperformance for FPS aiming ",
                    "Citation Text": "Josef Spjut, Ben Boudaoud, and Joohwan Kim. 2021. A Case Study of First Person\nAiming at Low Latency for Esports. arXiv preprint arXiv:2105.10498 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.10498",
                        "Citation Paper Title": "Title:A Case Study of First Person Aiming at Low Latency for Esports",
                        "Citation Paper Abstract": "Abstract:Lower computer system input-to-output latency substantially reduces many task completion times. In fact, literature shows that reduction in targeting task completion time from decreased latency often exceeds the decrease in latency alone. However, for aiming in first person shooter (FPS) games, some prior work has demonstrated diminishing returns below 40 ms of local input-to-output computer system latency. In this paper, we review this prior art and provide an additional case study with data demonstrating the importance of local system latency improvement, even at latency values below 20 ms. Though other factors may determine victory in a particular esports challenge, ensuring balanced local computer latency among competitors is essential to fair competition.",
                        "Citation Paper Authors": "Authors:Josef Spjut, Ben Boudaoud, Joohwan Kim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.14433v1": {
            "Paper Title": "A Portable Multiscopic Camera for Novel View and Time Synthesis in\n  Dynamic Scenes",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "can\nmodel the scenes as a continuous function of spatial locations\nand other important geometry parameters. Those parameters\ncan then be optimised via training.\nNeural Radiance Field NeRF ",
                    "Citation Text": "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-\nthi, and R. Ng, \u201cNerf: Representing scenes as neural radiance \ufb01elds\nfor view synthesis,\u201d in ECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "have been proposed to visualize three-dimensional\nscenes, but their interactivity capabilities are inferior. Re-\ncently, with the emergence of simulators such as Gibson ",
                    "Citation Text": "F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, \u201cGibson\nenv: Real-world perception for embodied agents,\u201d in CVPR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10654",
                        "Citation Paper Title": "Title:Gibson Env: Real-World Perception for Embodied Agents",
                        "Citation Paper Abstract": "Abstract:Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, \"Goggles\", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",
                        "Citation Paper Authors": "Authors:Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "could balance portability, lightweight, and suf\ufb01cient con-\nsistency constraints. Besides real-world data, synthetic data\nis also widely employed in researching synthesis methods.\nSome 3D datasets ",
                    "Citation Text": "A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva,\nS. Song, A. Zeng, and Y . Zhang, \u201cMatterport3d: Learning from rgb-d\ndata in indoor environments,\u201d in 3DV. IEEE, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "combine \ufb02ow\nmodels with NeRF to warp source frames to target frames\nand thus it could model the temporal dimension. ",
                    "Citation Text": "W. Xian, J.-B. Huang, J. Kopf, and C. Kim, \u201cSpace-time neural\nirradiance \ufb01elds for free-viewpoint video,\u201d in CVPR , 2021, pp. 9421\u2013\n9431.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12950",
                        "Citation Paper Title": "Title:Space-time Neural Irradiance Fields for Free-Viewpoint Video",
                        "Citation Paper Abstract": "Abstract:We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",
                        "Citation Paper Authors": "Authors:Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.14261v1": {
            "Paper Title": "Shape-Guided Mixed Metro Map Layout",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02417v1": {
            "Paper Title": "Volume Rendering Digest (for NeRF)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.00307v2": {
            "Paper Title": "Learning to Get Up",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12763v1": {
            "Paper Title": "Leveraging Synthetic Data to Learn Video Stabilization Under Adverse\n  Conditions",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "followed this procedure, generating videos for\ntraining CNNs for video stabilization. The network learns a warping transformation of multi-grids given the shaky video\nframes and the previously stabilized ones. On the other hand, Liu et al. ",
                    "Citation Text": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Hybrid neural fusion for full-frame\nvideo stabilization. arXiv preprint arXiv:2102.06205 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06205",
                        "Citation Paper Title": "Title:Hybrid Neural Fusion for Full-frame Video Stabilization",
                        "Citation Paper Abstract": "Abstract:Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",
                        "Citation Paper Authors": "Authors:Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", who showed how to obtain an optical \ufb02ow large-scale dataset, MPI-Sintel, following a\nsystematic and easy process. Dosovitskiy et al. ",
                    "Citation Text": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Conference on robot learning , pages 1\u201316. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "and developed\nfor computer vision and robotics research. The system supports autonomous driving and \ufb02ying research in different\nenvironments. GANcraft is another work by Hao et al. ",
                    "Citation Text": "Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu. GANcraft: Unsupervised 3D Neural Rendering of Minecraft\nWorlds. arXiv preprint arXiv:2104.07659 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07659",
                        "Citation Paper Title": "Title:GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds",
                        "Citation Paper Abstract": "Abstract:We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at this https URL .",
                        "Citation Paper Authors": "Authors:Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", who modi\ufb01ed the game Grand Theft Auto V for that purpose. At the same time, another work by\nShafaei et al. ",
                    "Citation Text": "Alireza Shafaei, James J Little, and Mark Schmidt. Play and Learn: Using Video Games to Train Computer Vision Models.\narXiv:1608.01745 , 2016.\n13Leveraging Synthetic Data to Learn Video Stabilization Under Adverse Conditions",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.01745",
                        "Citation Paper Title": "Title:Play and Learn: Using Video Games to Train Computer Vision Models",
                        "Citation Paper Abstract": "Abstract:Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.",
                        "Citation Paper Authors": "Authors:Alireza Shafaei, James J. Little, Mark Schmidt"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "was presented as an unarchived workshop paper.\n3Leveraging Synthetic Data to Learn Video Stabilization Under Adverse Conditions\ncannot re\ufb02ect scene parallax ",
                    "Citation Text": "Nianjin Ye, Chuan Wang, Haoqiang Fan, and Shuaicheng Liu. Motion basis learning for unsupervised deep homography\nestimation with subspace projection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages\n13117\u201313125, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15346",
                        "Citation Paper Title": "Title:Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a new framework for unsupervised deep homography estimation. Our contributions are 3 folds. First, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. Second, considering a homography contains 8 Degree-of-Freedoms (DOFs) that is much less than the rank of the network features, we propose a Low Rank Representation (LRR) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. Last, we propose a Feature Identity Loss (FIL) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. With this constraint, the unsupervised optimization is achieved more effectively and more stable features are learned. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark datasets both qualitatively and quantitatively. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Nianjin Ye, Chuan Wang, Haoqiang Fan, Shuaicheng Liu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "stabilize\nthe shaky camera trajectory using L1-norm optimization under constraints. Similarly, Bradley et al. ",
                    "Citation Text": "Arwen Bradley, Jason Klivington, Joseph Triscari, and Rudolph van der Merwe. Cinematic-L1 Video Stabilization with a\nLog-Homography Model. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages\n1041\u20131049, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.08144",
                        "Citation Paper Title": "Title:Cinematic-L1 Video Stabilization with a Log-Homography Model",
                        "Citation Paper Abstract": "Abstract:We present a method for stabilizing handheld video that simulates the camera motions cinematographers achieve with equipment like tripods, dollies, and Steadicams. We formulate a constrained convex optimization problem minimizing the $\\ell_1$-norm of the first three derivatives of the stabilized motion. Our approach extends the work of Grundmann et al. [9] by solving with full homographies (rather than affinities) in order to correct perspective, preserving linearity by working in log-homography space. We also construct crop constraints that preserve field-of-view; model the problem as a quadratic (rather than linear) program to allow for an $\\ell_2$ term encouraging fidelity to the original trajectory; and add constraints and objectives to reduce distortion. Furthermore, we propose new methods for handling salient objects via both inclusion constraints and centering objectives. Finally, we describe a windowing strategy to approximate the solution in linear time and bounded memory. Our method is computationally efficient, running at 300fps on an iPhone XS, and yields high-quality results, as we demonstrate with a collection of stabilized videos, quantitative and qualitative comparisons to [9] and other methods, and an ablation study.",
                        "Citation Paper Authors": "Authors:Arwen Bradley, Jason Klivington, Joseph Triscari, Rudolph van der Merwe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12408v1": {
            "Paper Title": "User-Controllable Latent Transformer for StyleGAN Image Layout Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12278v1": {
            "Paper Title": "Learning Continuous Implicit Representation for Near-Periodic Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12078v1": {
            "Paper Title": "Learning to regulate 3D head shape by removing occluding hair from\n  in-the-wild images",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "in Table 2, and upper head accuracy on the\nCoMA ",
                    "Citation Text": "A. Ranjan, T. Bolkart, S. Sanyal, and M. J. Black. Generating 3D faces\nusing convolutional mesh autoencoders. In European Conference on\nComputer Vision (ECCV) , pp. 725\u2013741, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10267",
                        "Citation Paper Title": "Title:Generating 3D faces using Convolutional Mesh Autoencoders",
                        "Citation Paper Abstract": "Abstract:Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at this http URL",
                        "Citation Paper Authors": "Authors:Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "dataset. Using\nthe skin mask prevents the in\ufb02uence of occlusions on the loss\nimproving the quality of the reconstructed texture.\nIdentity loss: Regardless of expression, pose or illumination, em-\nbeddingsfrom the face recognition network ",
                    "Citation Text": "Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vg-\ngface2: A dataset for recognising faces across pose and age. CoRR ,\nabs/1710.08092, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.08092",
                        "Citation Paper Title": "Title:VGGFace2: A dataset for recognising faces across pose and age",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.",
                        "Citation Paper Authors": "Authors:Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12674v1": {
            "Paper Title": "Automatic Testing and Validation of Level of Detail Reductions Through\n  Supervised Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10906v1": {
            "Paper Title": "DualSmoke: Sketch-Based Smoke Illustration Design with Two-Stage\n  Generative Model",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "for LCS generator GLCS from hand-drawn\nsketches S, and VF generator GVFfrom the generated LCS\n\ufb01eldL(Figure 2). The generators are based on the U-Net\nstructure with skip connections between the decoder and en-\ncoder networks ",
                    "Citation Text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention , pages 234\u2013241.\nSpringer, 2015. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "learned relations between low- and high-resolution\nvelocity \ufb01elds using CNN (Convolutional Neural Network)\nfor enhancing details of the smoke \ufb02ow. Xie et al. ",
                    "Citation Text": "You Xie, Erik Franz, Mengyu Chu, and Nils Thuerey. tem-\npogan: A temporally coherent, volumetric gan for super-\nresolution \ufb02uid \ufb02ow. ACM Transactions on Graphics (TOG) ,\n37(4):1\u201315, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.09710",
                        "Citation Paper Title": "Title:tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow",
                        "Citation Paper Abstract": "Abstract:We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate advected quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.",
                        "Citation Paper Authors": "Authors:You Xie, Erik Franz, Mengyu Chu, Nils Thuerey"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.08667v1": {
            "Paper Title": "SDA-SNE: Spatial Discontinuity-Aware Surface Normal Estimation via\n  Multi-Directional Dynamic Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08580v1": {
            "Paper Title": "MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07982v1": {
            "Paper Title": "MosaicSets: Embedding Set Systems into Grid Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08274v1": {
            "Paper Title": "SMPL-IK: Learned Morphology-Aware Inverse Kinematics for AI Driven\n  Artistic Workflows",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06413v1": {
            "Paper Title": "Generating Pixel Art Character Sprites using GANs",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nThere are skip connections from the output of the ithencoder\nlayer to the nth\u0000idecoder one, with n= 6, to preserve\nspatial information.\nThe loss function for the generator is similar to the one in ",
                    "Citation Text": "P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-Image\nTranslation with Conditional Adversarial Networks,\u201d in 2017 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) ,\nvol. 2017-Janua. IEEE, 7 2017, pp. 5967\u20135976. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/8100115/",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07004",
                        "Citation Paper Title": "Title:Image-to-Image Translation with Conditional Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
                        "Citation Paper Authors": "Authors:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.07010v1": {
            "Paper Title": "Automatic Landmark Detection and Registration of Brain Cortical Surfaces\n  via Quasi-Conformal Geometry and Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06952v1": {
            "Paper Title": "A Novel Tree Visualization to Guide Interactive Exploration of\n  Multi-dimensional Topological Hierarchies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06358v3": {
            "Paper Title": "Do Inpainting Yourself: Generative Facial Inpainting Guided by Exemplars",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06710v1": {
            "Paper Title": "Progressive Multi-scale Light Field Networks",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "to encode color and geometry simultane-\nously or with VIINTER ",
                    "Citation Text": "Brandon Feng, Susmija Jabbireddy, and Amitabh Varshney.\nViinter: View interpolation with implicit neural representa-\ntions of images. In SIGGRAPH ASIA , 2022. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.00722",
                        "Citation Paper Title": "Title:VIINTER: View Interpolation with Implicit Neural Representations of Images",
                        "Citation Paper Abstract": "Abstract:We present VIINTER, a method for view interpolation by interpolating the implicit neural representation (INR) of the captured images. We leverage the learned code vector associated with each image and interpolate between these codes to achieve viewpoint transitions. We propose several techniques that significantly enhance the interpolation quality. VIINTER signifies a new way to achieve view interpolation without constructing 3D structure, estimating camera poses, or computing pixel correspondence. We validate the effectiveness of VIINTER on several multi-view scenes with different types of camera layout and scene composition. As the development of INR of images (as opposed to surface or volume) has centered around tasks like image fitting and super-resolution, with VIINTER, we show its capability for view interpolation and offer a promising outlook on using INR for image manipulation tasks.",
                        "Citation Paper Authors": "Authors:Brandon Yushan Feng, Susmija Jabbireddy, Amitabh Varshney"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "use a vector-quantized auto-\ndecoder to compress feature grids into lookup indices and a\nlearned codebook. Levels of detail are obtained by learning\ncompressed feature grids at multiple resolutions in a sparse\noctree. In parallel, Streamable Neural Fields ",
                    "Citation Text": "Junwoo Cho, Seungtae Nam, Daniel Rho, Jong Hwan Ko,\nand Eunbyung Park. Streamable neural \ufb01elds. In ECCV ,\n2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.09663",
                        "Citation Paper Title": "Title:Streamable Neural Fields",
                        "Citation Paper Abstract": "Abstract:Neural fields have emerged as a new data representation paradigm and have shown remarkable success in various signal representations. Since they preserve signals in their network parameters, the data transfer by sending and receiving the entire model parameters prevents this emerging technology from being used in many practical scenarios. We propose streamable neural fields, a single model that consists of executable sub-networks of various widths. The proposed architectural and training techniques enable a single network to be streamable over time and reconstruct different qualities and parts of signals. For example, a smaller sub-network produces smooth and low-frequency signals, while a larger sub-network can represent fine details. Experimental results have shown the effectiveness of our method in various domains, such as 2D images, videos, and 3D signed distance functions. Finally, we demonstrate that our proposed method improves training stability, by exploiting parameter sharing.",
                        "Citation Paper Authors": "Authors:Junwoo Cho, Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "offers levels of detail for vari-\nous scene representations including 2D images and radiance\n\ufb01elds with different scales encoding different bandwidths in\nFourier space. PINs ",
                    "Citation Text": "Zoe Landgraf, Alexander Sorkine Hornung, and Ricardo Sil-\nveira Cabral. PINs: Progressive Implicit Networks for Multi-\nScale Neural Representations. In International Conference\non Machine Learning (ICML 2022) , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.04713",
                        "Citation Paper Title": "Title:PINs: Progressive Implicit Networks for Multi-Scale Neural Representations",
                        "Citation Paper Abstract": "Abstract:Multi-layer perceptrons (MLP) have proven to be effective scene encoders when combined with higher-dimensional projections of the input, commonly referred to as \\textit{positional encoding}. However, scenes with a wide frequency spectrum remain a challenge: choosing high frequencies for positional encoding introduces noise in low structure areas, while low frequencies result in poor fitting of detailed regions. To address this, we propose a progressive positional encoding, exposing a hierarchical MLP structure to incremental sets of frequency encodings. Our model accurately reconstructs scenes with wide frequency bands and learns a scene representation at progressive level of detail \\textit{without explicit per-level supervision}. The architecture is modular: each level encodes a continuous implicit representation that can be leveraged separately for its respective resolution, meaning a smaller network for coarser reconstructions. Experiments on several 2D and 3D datasets show improvements in reconstruction accuracy, representational capacity and training speed compared to baselines.",
                        "Citation Paper Authors": "Authors:Zoe Landgraf, Alexander Sorkine Hornung, Ricardo Silveira Cabral"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", generalization across scenes [66, 41],\ngenerative modelling [47, 35, 60], videos [24, 57, 55],\nsparse views [8, 34, 19], fast training [32, 64], and even\nlarge-scale scenes [43, 58, 54].\nAmong NeRF research, Mip-NeRF [3, 4] and BA-\nCON ",
                    "Citation Text": "David B. Lindell, Dave Van Veen, Jeong Joon Park, and\nGordon Wetzstein. BACON: Band-limited coordinate net-\nworks for multiscale scene representation. arXiv preprint\narXiv:2112.04645 , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.04645",
                        "Citation Paper Title": "Title:BACON: Band-limited Coordinate Networks for Multiscale Scene Representation",
                        "Citation Paper Abstract": "Abstract:Coordinate-based networks have emerged as a powerful tool for 3D representation and scene reconstruction. These networks are trained to map continuous input coordinates to the value of a signal at each point. Still, current architectures are black boxes: their spectral characteristics cannot be easily analyzed, and their behavior at unsupervised points is difficult to predict. Moreover, these networks are typically trained to represent a signal at a single scale, so naive downsampling or upsampling results in artifacts. We introduce band-limited coordinate networks (BACON), a network architecture with an analytical Fourier spectrum. BACON has constrained behavior at unsupervised points, can be designed based on the spectral characteristics of the represented signal, and can represent signals at multiple scales without per-scale supervision. We demonstrate BACON for multiscale neural representation of images, radiance fields, and 3D scenes using signed distance functions and show that it outperforms conventional single-scale coordinate networks in terms of interpretability and quality.",
                        "Citation Paper Authors": "Authors:David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "develop Mul-\ntiresolution Deep Implicit Functions (MDIF) to represent\n3D shapes which combines a global SDF representation\nwith local residuals. For radiance \ufb01elds, Yang et al. ",
                    "Citation Text": "Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun\nLiang, Tai-Jiang Mu, and Shi-Min Hu. Recursive-nerf: An\nef\ufb01cient and dynamically growing nerf, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.09103",
                        "Citation Paper Title": "Title:Recursive-NeRF: An Efficient and Dynamically Growing NeRF",
                        "Citation Paper Abstract": "Abstract:View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "use Pl \u00a8ucker coor-\ndinates which combine the ray direction rdand ray origin\nrointo a 6-dimensional vector (rd;ro\u0002rd). Light \ufb01elds\ncan also be combined with explicit representations ",
                    "Citation Text": "Julian Ost, Issam Laradji, Alejandro Newell, Yuval Ba-\nhat, and Felix Heide. Neural point light \ufb01elds. CoRR ,\nabs/2112.01473, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01473",
                        "Citation Paper Title": "Title:Neural Point Light Fields",
                        "Citation Paper Abstract": "Abstract:We introduce Neural Point Light Fields that represent scenes implicitly with a light field living on a sparse point cloud. Combining differentiable volume rendering with learned implicit density representations has made it possible to synthesize photo-realistic images for novel views of small scenes. As neural volumetric rendering methods require dense sampling of the underlying functional scene representation, at hundreds of samples along a ray cast through the volume, they are fundamentally limited to small scenes with the same objects projected to hundreds of training views. Promoting sparse point clouds to neural implicit light fields allows us to represent large scenes effectively with only a single radiance evaluation per ray. These point light fields are a function of the ray direction, and local point feature neighborhood, allowing us to interpolate the light field conditioned training images without dense object coverage and parallax. We assess the proposed method for novel view synthesis on large driving scenarios, where we synthesize realistic unseen views that existing implicit approaches fail to represent. We validate that Neural Point Light Fields make it possible to predict videos along unseen trajectories previously only feasible to generate by explicitly modeling the scene.",
                        "Citation Paper Authors": "Authors:Julian Ost, Issam Laradji, Alejandro Newell, Yuval Bahat, Felix Heide"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "pair\nthe traditional two-plane parameterization with Gegenbauer\npolynomials while Sitzmann et al. ",
                    "Citation Text": "Vincent Sitzmann, Semon Rezchikov, William T. Freeman,\nJoshua B. Tenenbaum, and Fredo Durand. Light \ufb01eld net-\nworks: Neural scene representations with single-evaluation\nrendering. In arXiv , 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.02634",
                        "Citation Paper Title": "Title:Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering",
                        "Citation Paper Abstract": "Abstract:Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a single network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.06143v1": {
            "Paper Title": "PRIF: Primary Ray-based Implicit Function",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.05821v1": {
            "Paper Title": "HiTailor: Interactive Transformation and Visualization for Hierarchical\n  Tabular Data",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "choose the\nmulti-task learning architecture for spreadsheets\u2019 structure extraction.\nThe method comprises two steps: extracting cell features and modelling\ntable structures, while TabularNet ",
                    "Citation Text": "L. Du, F. Gao, X. Chen, R. Jia, J. Wang, J. Zhang, S. Han, and D. Zhang.\nTabularnet: A neural network architecture for understanding semantic\nstructures of tabular data. In Proc. ACM Conf. Knowledge Discovery and\nData Mining (SIGKDD) , pp. 322\u2013331, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03096",
                        "Citation Paper Title": "Title:TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data",
                        "Citation Paper Abstract": "Abstract:Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Lun Du, Fei Gao, Xu Chen, Ran Jia, Junshan Wang, Jiang Zhang, Shi Han, Dongmei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.05775v1": {
            "Paper Title": "PSUMNet: Unified Modality Part Streams are All You Need for Efficient\n  Pose-based Action Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.02166v2": {
            "Paper Title": "FauxThrow: Exploring the Effects of Incorrect Point of Release in\n  Throwing Motions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04598v1": {
            "Paper Title": "UnderPressure: Deep Learning for Foot Contact Detection, Ground Reaction\n  Force Estimation and Footskate Cleanup",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04448v1": {
            "Paper Title": "NeuralVDB: High-resolution Sparse Volume Representation using\n  Hierarchical Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "and/or applying mixed-precision inference. Specifically for\nencoding/training, data-driven approaches like MetaSDF ",
                    "Citation Text": "Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wet-\nzstein. 2020. Metasdf: Meta-learning signed distance functions. Advances in\nNeural Information Processing Systems 33 (2020), 10136\u201310147.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09662",
                        "Citation Paper Title": "Title:MetaSDF: Meta-learning Signed Distance Functions",
                        "Citation Paper Abstract": "Abstract:Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "are encoded using neural networks. Most of\nthese studies utilize coordinate-based neural networks and feature\nmapping/encoding techniques such as SIREN ",
                    "Citation Text": "Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon\nWetzstein. 2020. Implicit neural representations with periodic activation functions.\nAdvances in Neural Information Processing Systems 33 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09661",
                        "Citation Paper Title": "Title:Implicit Neural Representations with Periodic Activation Functions",
                        "Citation Paper Abstract": "Abstract:Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ".\n2.3 Neural Representation\nAlong with the rise of deep learning, representing volumes using\nneural networks became actively explored. From occupancy field [ 26,\n41], implicit surface like SDF [ 27,38], to multi-dimensional data like\nradiance field ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi\nRamamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance\nfields for view synthesis. In European conference on computer vision . Springer,\n405\u2013421.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.04370v1": {
            "Paper Title": "CLIP-based Neural Neighbor Style Transfer for 3D Assets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04364v1": {
            "Paper Title": "A Rotation-Strain Method to Model Surfaces using Plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.03914v1": {
            "Paper Title": "Interpretable Disentangled Parametrization of Measured BRDF with\n  $\u03b2$-VAE",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.09772v2": {
            "Paper Title": "In Defence of Visual Analytics Systems: Replies to Critics",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ".\nData mining algorithm 8 We propose a dynamic clustering algorithm to enable the ef\ufb01cient clustering of fast-paced incoming streaming data ",
                    "Citation Text": "J. Knittel, S. Koch, T. Tang, W. Chen, Y . Wu, S. Liu, and T. Ertl. Real-time\nvisual analysis of high-volume social media posts. IEEE Transactions on\nVisualization and Computer Graphics , 28(1):879\u2013889, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.03052",
                        "Citation Paper Title": "Title:Real-Time Visual Analysis of High-Volume Social Media Posts",
                        "Citation Paper Abstract": "Abstract:Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system.",
                        "Citation Paper Authors": "Authors:Johannes Knittel, Steffen Koch, Tan Tang, Wei Chen, Yingcai Wu, Shixia Liu, Thomas Ertl"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ".\nV A design/work\ufb02ow/framework 17 We introduce a risk-aware framework, namely, VideoModerator, to facilitate the ef\ufb01cient moderation of e-commerce videos ",
                    "Citation Text": "T. Tang, Y . Wu, Y . Wu, L. Yu, and Y . Li. Videomoderator: A risk-\naware framework for multimodal video moderation in e-commerce. IEEE\nTransactions on Visualization and Computer Graphics , 28(1):846\u2013856,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.03479",
                        "Citation Paper Title": "Title:VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
                        "Citation Paper Abstract": "Abstract:Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.",
                        "Citation Paper Authors": "Authors:Tan Tang, Yanhong Wu, Lingyun Yu, Yuhong Li, Yingcai Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.00774v2": {
            "Paper Title": "Interaction Mix and Match: Synthesizing Close Interaction using\n  Conditional Hierarchical GAN with Multi-Hot Class Embedding",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". We are interested in incorporating the contact consistency loss proposed in\nGANimator ",
                    "Citation Text": "Peizhuo Li, K\ufb01r Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. Ganimator: Neural motion\nsynthesis from a single sequence. ACM Transactions on Graphics (TOG) , 41(4):138, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.02625",
                        "Citation Paper Title": "Title:GANimator: Neural Motion Synthesis from a Single Sequence",
                        "Citation Paper Abstract": "Abstract:We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. Existing data-driven techniques for motion synthesis require a large motion dataset which contains the desired and specific skeletal structure. By contrast, GANimator only requires training on a single motion sequence, enabling novel motion synthesis for a variety of skeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework contains a series of generative and adversarial neural networks, each responsible for generating motions in a specific frame rate. The framework progressively learns to synthesize motion from random noise, enabling hierarchical control over the generated motion content across varying levels of detail. We show a number of applications, including crowd simulation, key-frame editing, style transfer, and interactive control, which all learn from a single input sequence. Code and data for this paper are at this https URL.",
                        "Citation Paper Authors": "Authors:Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, Olga Sorkine-Hornung"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "proposed a Lie Algebra based Variational Auto-Encoder (V AE) framework to generate motions according to the\ninput action label. Petrovich et al. ",
                    "Citation Text": "Mathis Petrovich, Michael J. Black, and G\u00fcl Varol. Action-conditioned 3D human motion synthesis with\ntransformer V AE. In International Conference on Computer Vision (ICCV) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.05670",
                        "Citation Paper Title": "Title:Action-Conditioned 3D Human Motion Synthesis with Transformer VAE",
                        "Citation Paper Abstract": "Abstract:We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.",
                        "Citation Paper Authors": "Authors:Mathis Petrovich, Michael J. Black, G\u00fcl Varol"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.02114v1": {
            "Paper Title": "Solving Inverse PDE Problems using Grid-Free Monte Carlo Estimators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01772v1": {
            "Paper Title": "A Dataset and Benchmark for Mesh Parameterization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01626v1": {
            "Paper Title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "successfully perform local\nmanipulations using user-provided masks for guidance.\nWhile most works that require only text (i.e., no masks) are limited to global editing [9, 23], Bar-Tal et al. ",
                    "Citation Text": "Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven\nlayered image and video editing. arXiv preprint arXiv:2204.02491 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02491",
                        "Citation Paper Title": "Title:Text2LIVE: Text-Driven Layered Image and Video Editing",
                        "Citation Paper Abstract": "Abstract:We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.",
                        "Citation Paper Authors": "Authors:Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, Tali Dekel"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "show how to perform global changes, whereas Avrahami et al. ",
                    "Citation Text": "Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural\nimages. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 18208\u201318218, 2022.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14818",
                        "Citation Paper Title": "Title:Blended Diffusion for Text-driven Editing of Natural Images",
                        "Citation Paper Abstract": "Abstract:Natural language offers a highly intuitive interface for image editing. In this paper, we introduce the first solution for performing local (region-based) edits in generic natural images, based on a natural language description along with an ROI mask. We achieve our goal by leveraging and combining a pretrained language-image model (CLIP), to steer the edit towards a user-provided text prompt, with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results. To seamlessly fuse the edited region with the unchanged parts of the image, we spatially blend noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels. In addition, we show that adding augmentations to the diffusion process mitigates adversarial results. We compare against several baselines and related methods, both qualitatively and quantitatively, and show that our method outperforms these solutions in terms of overall realism, ability to preserve the background and matching the text. Finally, we show several text-driven editing applications, including adding a new object to an image, removing/replacing/altering existing objects, background replacement, and image extrapolation. Code is available at: this https URL",
                        "Citation Paper Authors": "Authors:Omri Avrahami, Dani Lischinski, Ohad Fried"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", e.g., human faces, they\nstruggle over large and diverse datasets.\nTo obtain more expressive generation capabilities, Crowson et al. ",
                    "Citation Text": "Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castri-\ncato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language\nguidance. arXiv preprint arXiv:2204.08583 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.08583",
                        "Citation Paper Title": "Title:VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance",
                        "Citation Paper Abstract": "Abstract:Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.",
                        "Citation Paper Authors": "Authors:Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, Edward Raff"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "further demonstrated how to use masks provided\nby the user, to localize the text-based editing and restrict the change to a speci\ufb01c spatial region. However,\nwhile GAN-based image editing approaches succeed on highly-curated datasets ",
                    "Citation Text": "Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel Cohen-Or, and\nMichal Irani. Self-distilled stylegan: Towards generation from internet photos. In Special Interest Group\non Computer Graphics and Interactive Techniques Conference Proceedings , pages 1\u20139, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.12211",
                        "Citation Paper Title": "Title:Self-Distilled StyleGAN: Towards Generation from Internet Photos",
                        "Citation Paper Abstract": "Abstract:StyleGAN is known to produce high-fidelity images, while also offering unprecedented semantic editing. However, these fascinating abilities have been demonstrated only on a limited set of datasets, which are usually structurally aligned and well curated. In this paper, we show how StyleGAN can be adapted to work on raw uncurated images collected from the Internet. Such image collections impose two main challenges to StyleGAN: they contain many outlier images, and are characterized by a multi-modal distribution. Training StyleGAN on such raw image collections results in degraded image synthesis quality. To meet these challenges, we proposed a StyleGAN-based self-distillation approach, which consists of two main components: (i) A generative-based self-filtering of the dataset to eliminate outlier images, in order to generate an adequate training set, and (ii) Perceptual clustering of the generated images to detect the inherent data modalities, which are then employed to improve StyleGAN's \"truncation trick\" in the image synthesis process. The presented technique enables the generation of high-quality images, while minimizing the loss in diversity of the data. Through qualitative and quantitative evaluation, we demonstrate the power of our approach to new challenging and diverse domains collected from the Internet. New datasets and pre-trained models are available at this https URL .",
                        "Citation Paper Authors": "Authors:Ron Mokady, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.01618v1": {
            "Paper Title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01471v1": {
            "Paper Title": "Procedural Generation and Rendering of Realistic, Navigable Forest\n  Environments: An Open-Source Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13889v4": {
            "Paper Title": "Free-Viewpoint RGB-D Human Performance Capture and Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00949v1": {
            "Paper Title": "VolTeMorph: Realtime, Controllable and Generalisable Animation of\n  Volumetric Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14313v3": {
            "Paper Title": "Learning to Use Chopsticks in Diverse Gripping Styles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.14782v1": {
            "Paper Title": "Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal\n  Charts and Distortion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13091v3": {
            "Paper Title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space\n  Exploration of Ensemble Simulations",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "which\nreduces the training time and memory cost with minimal impact. Sec-\nond, spectral normalization ",
                    "Citation Text": "T. Miyato, T. Kataoka, M. Koyama, and Y . Yoshida. Spectral Normaliza-\ntion for Generative Adversarial Networks. In Proc. International Confer-\nence on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05957",
                        "Citation Paper Title": "Title:Spectral Normalization for Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
                        "Citation Paper Authors": "Authors:Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.14606v1": {
            "Paper Title": "WISE: Whitebox Image Stylization by Example-based Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01397v2": {
            "Paper Title": "Dynamic Structured Illumination Microscopy with a Neural Space-time\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00790v1": {
            "Paper Title": "Skeleton-free Pose Transfer for Stylized 3D Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.14080v1": {
            "Paper Title": "Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows -- An\n  Experimental Study",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "for a description of alternative metrics between topological descrip-\ntors. Several techniques have been proposed for summarizing the\ntopological features in an ensemble or analyzing their variability.\nFavelier et al. ",
                    "Citation Text": "G. Favelier, N. Faraj, B. Summa, and J. Tierny. Persistence Atlas for\nCritical Point Variability in Ensembles. IEEE TVCG , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11212",
                        "Citation Paper Title": "Title:Persistence Atlas for Critical Point Variability in Ensembles",
                        "Citation Paper Abstract": "Abstract:This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed.",
                        "Citation Paper Authors": "Authors:Guillaume Favelier, Noura Faraj, Brian Summa, Julien Tierny"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.14067v1": {
            "Paper Title": "Neural Strands: Learning Hair Geometry and Appearance from Multi-View\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13904v1": {
            "Paper Title": "CaminAR: Supporting Walk-and-talk Experiences for Remote Dyads using\n  Augmented Reality on Smart Glasses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10312v2": {
            "Paper Title": "AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance\n  Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13784v1": {
            "Paper Title": "AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion\n  Sensing",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rota-\ntion representations in neural networks. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 5745{5753\n(2019) 5 ",
                    "Citation Text": "Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: De-\nformable transformers for end-to-end object detection. In: International\nConference on Learning Representations (2020) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04159",
                        "Citation Paper Title": "Title:Deformable DETR: Deformable Transformers for End-to-End Object Detection",
                        "Citation Paper Abstract": "Abstract:DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at this https URL.",
                        "Citation Paper Authors": "Authors:Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ".\n3.2 Input and Output Representation\nSince the 6D representation of rotations has proved e\u000bective for training neural\nnetworks due to its continuity ",
                    "Citation Text": "Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rota-\ntion representations in neural networks. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 5745{5753\n(2019) 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07035",
                        "Citation Paper Title": "Title:On the Continuity of Rotation Representations in Neural Networks",
                        "Citation Paper Abstract": "Abstract:In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.",
                        "Citation Paper Authors": "Authors:Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "was \frst to apply Transformer models\nto vertex-vertex and vertex-joint interactions for 3D human pose and mesh re-\nconstruction from a single image. PoseFormer ",
                    "Citation Text": "Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding, Z.: 3d human\npose estimation with spatial and temporal transformers. Proceedings of the\nIEEE International Conference on Computer Vision (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10455",
                        "Citation Paper Title": "Title:3D Human Pose Estimation with Spatial and Temporal Transformers",
                        "Citation Paper Abstract": "Abstract:Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, Zhengming Ding"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "Zhao, J., Badler, N.I.: Inverse kinematics positioning using nonlinear pro-\ngramming for highly articulated \fgures. ACM Transactions on Graphics\n(TOG) 13(4), 313{336 (1994) 4 ",
                    "Citation Text": "Zhao, Z., Wu, Z., Zhang, Y., Li, B., Jia, J.: Tracking objects as pixel-wise\ndistributions. In: Proceedings of the European Conference on Computer\nVision (2022) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.05518",
                        "Citation Paper Title": "Title:Tracking Objects as Pixel-wise Distributions",
                        "Citation Paper Abstract": "Abstract:Multi-object tracking (MOT) requires detecting and associating objects through frames. Unlike tracking via detected bounding boxes or tracking objects as points, we propose tracking objects as pixel-wise distributions. We instantiate this idea on a transformer-based architecture, P3AFormer, with pixel-wise propagation, prediction, and association. P3AFormer propagates pixel-wise features guided by flow information to pass messages between frames. Furthermore, P3AFormer adopts a meta-architecture to produce multi-scale object feature maps. During inference, a pixel-wise association procedure is proposed to recover object connections through frames based on the pixel-wise prediction. P3AFormer yields 81.2\\% in terms of MOTA on the MOT17 benchmark -- the first among all transformer networks to reach 80\\% MOTA in literature. P3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.",
                        "Citation Paper Authors": "Authors:Zelin Zhao, Ze Wu, Yueqing Zhuang, Boxun Li, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.:\nRestormer: E\u000ecient transformer for high-resolution image restoration. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 5728{5739 (2022) 4AvatarPoser 19 ",
                    "Citation Text": "Zhang, X., Bhatnagar, B.L., Guzov, V., Starke, S., Pons-Moll, G.: Couch:\nTowards controllable human-chair interactions. In: European Conference on\nComputer Vision). Springer (October 2022) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.00541",
                        "Citation Paper Title": "Title:COUCH: Towards Controllable Human-Chair Interactions",
                        "Citation Paper Abstract": "Abstract:Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts.",
                        "Citation Paper Authors": "Authors:Xiaohan Zhang, Bharat Lal Bhatnagar, Vladimir Guzov, Sebastian Starke, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "Yi, X., Zhou, Y., Xu, F.: Transpose: real-time 3d human translation and\npose estimation with six inertial sensors. ACM Transactions on Graphics\n(TOG) 40(4), 1{13 (2021) 4 ",
                    "Citation Text": "Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.:\nRestormer: E\u000ecient transformer for high-resolution image restoration. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 5728{5739 (2022) 4AvatarPoser 19",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09881",
                        "Citation Paper Title": "Title:Restormer: Efficient Transformer for High-Resolution Image Restoration",
                        "Citation Paper Abstract": "Abstract:Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "Yang, D., Kim, D., Lee, S.H.: Lobstr: Real-time lower-body pose prediction\nfrom sparse upper-body tracking signals. In: Computer Graphics Forum.\nvol. 40, pp. 265{275. Wiley Online Library (2021) 3, 4, 8, 9, 13 ",
                    "Citation Text": "Yi, X., Zhou, Y., Habermann, M., Shimada, S., Golyanik, V., Theobalt, C.,\nXu, F.: Physical inertial poser (pip): Physics-aware real-time human motion\ntracking from sparse inertial sensors. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 13167{13178\n(2022) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.08528",
                        "Citation Paper Title": "Title:Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors",
                        "Citation Paper Abstract": "Abstract:Motion capture from sparse inertial sensors has shown great potential compared to image-based approaches since occlusions do not lead to a reduced tracking quality and the recording space is not restricted to be within the viewing frustum of the camera. However, capturing the motion and global position only from a sparse set of inertial sensors is inherently ambiguous and challenging. In consequence, recent state-of-the-art methods can barely handle very long period motions, and unrealistic artifacts are common due to the unawareness of physical constraints. To this end, we present the first method which combines a neural kinematics estimator and a physics-aware motion optimizer to track body motions with only 6 inertial sensors. The kinematics module first regresses the motion status as a reference, and then the physics module refines the motion to satisfy the physical constraints. Experiments demonstrate a clear improvement over the state of the art in terms of capture accuracy, temporal stability, and physical correctness.",
                        "Citation Paper Authors": "Authors:Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian Theobalt, Feng Xu"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "Waltemate, T., Gall, D., Roth, D., Botsch, M., Latoschik, M.E.: The im-\npact of avatar personalization and immersion on virtual body ownership,\npresence, and emotional response. IEEE Transactions on Visualization and\nComputer Graphics 24(4), 1643{1652 (2018) 2 ",
                    "Citation Text": "Wang, J., Liu, L., Xu, W., Sarkar, K., Theobalt, C.: Estimating egocentric\n3d human pose in global space. In: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision. pp. 11500{11509 (2021) 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13454",
                        "Citation Paper Title": "Title:Estimating Egocentric 3D Human Pose in Global Space",
                        "Citation Paper Abstract": "Abstract:Egocentric 3D human pose estimation using a single fisheye camera has become popular recently as it allows capturing a wide range of daily activities in unconstrained environments, which is difficult for traditional outside-in motion capture with external cameras. However, existing methods have several limitations. A prominent problem is that the estimated poses lie in the local coordinate system of the fisheye camera, rather than in the world coordinate system, which is restrictive for many applications. Furthermore, these methods suffer from limited accuracy and temporal instability due to ambiguities caused by the monocular setup and the severe occlusion in a strongly distorted egocentric perspective. To tackle these limitations, we present a new method for egocentric global 3D body pose estimation using a single head-mounted fisheye camera. To achieve accurate and temporally stable global poses, a spatio-temporal optimization is performed over a sequence of frames by minimizing heatmap reprojection errors and enforcing local and global body motion priors learned from a mocap dataset. Experimental results show that our approach outperforms state-of-the-art methods both quantitatively and qualitatively.",
                        "Citation Paper Authors": "Authors:Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "Villegas, R., Yang, J., Ceylan, D., Lee, H.: Neural kinematic networks for\nunsupervised motion retargetting. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. pp. 8639{8648 (2018) 4 ",
                    "Citation Text": "Von Marcard, T., Rosenhahn, B., Black, M.J., Pons-Moll, G.: Sparse inertial\nposer: Automatic 3d human pose estimation from sparse imus. In: Computer\ngraphics forum. vol. 36, pp. 349{360. Wiley Online Library (2017) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.08014",
                        "Citation Paper Title": "Title:Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs",
                        "Citation Paper Abstract": "Abstract:We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.",
                        "Citation Paper Authors": "Authors:Timo von Marcard, Bodo Rosenhahn, Michael J. Black, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30(2017) 4 ",
                    "Citation Text": "Villegas, R., Yang, J., Ceylan, D., Lee, H.: Neural kinematic networks for\nunsupervised motion retargetting. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. pp. 8639{8648 (2018) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05653",
                        "Citation Paper Title": "Title:Neural Kinematic Networks for Unsupervised Motion Retargetting",
                        "Citation Paper Abstract": "Abstract:We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator.",
                        "Citation Paper Authors": "Authors:Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "Troje, N.F.: Decomposing biological motion: A framework for analysis and\nsynthesis of human gait patterns. Journal of vision 2(5), 2{2 (2002) 9 ",
                    "Citation Text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30(2017) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "Sun, P., Cao, J., Jiang, Y., Zhang, R., Xie, E., Yuan, Z., Wang, C., Luo,\nP.: Transtrack: Multiple object tracking with transformer. arXiv preprint\narXiv:2012.15460 (2020) 4 ",
                    "Citation Text": "Sun, Z., Cao, S., Yang, Y., Kitani, K.M.: Rethinking transformer-based set\nprediction for object detection. In: Proceedings of the IEEE/CVF interna-\ntional conference on computer vision. pp. 3611{3620 (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.10881",
                        "Citation Paper Title": "Title:Rethinking Transformer-based Set Prediction for Object Detection",
                        "Citation Paper Abstract": "Abstract:DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.",
                        "Citation Paper Authors": "Authors:Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "Sumner, R.W., Zwicker, M., Gotsman, C., Popovi\u0013 c, J.: Mesh-based inverse\nkinematics. ACM transactions on graphics (TOG) 24(3), 488{495 (2005) 418 Jiang et al. ",
                    "Citation Text": "Sun, P., Cao, J., Jiang, Y., Zhang, R., Xie, E., Yuan, Z., Wang, C., Luo,\nP.: Transtrack: Multiple object tracking with transformer. arXiv preprint\narXiv:2012.15460 (2020) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15460",
                        "Citation Paper Title": "Title:TransTrack: Multiple Object Tracking with Transformer",
                        "Citation Paper Abstract": "Abstract:In this work, we propose TransTrack, a simple but efficient scheme to solve the multiple object tracking problems. TransTrack leverages the transformer architecture, which is an attention-based query-key mechanism. It applies object features from the previous frame as a query of the current frame and introduces a set of learned object queries to enable detecting new-coming objects. It builds up a novel joint-detection-and-tracking paradigm by accomplishing object detection and object association in a single shot, simplifying complicated multi-step settings in tracking-by-detection methods. On MOT17 and MOT20 benchmark, TransTrack achieves 74.5\\% and 64.5\\% MOTA, respectively, competitive to the state-of-the-art methods. We expect TransTrack to provide a novel perspective for multiple object tracking. The code is available at: \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "Mari\u0013 c, F., Giamou, M., Hall, A.W., Khoubyarian, S., Petrovi\u0013 c, I., Kelly, J.:\nRiemannian optimization for distance-geometric inverse kinematics. IEEE\nTransactions on Robotics 38(3), 1703{1722 (2021) 4 ",
                    "Citation Text": "Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C.: Trackformer:\nMulti-object tracking with transformers. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 8844{8854\n(2022) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.02702",
                        "Citation Paper Title": "Title:TrackFormer: Multi-Object Tracking with Transformers",
                        "Citation Paper Abstract": "Abstract:The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph Feichtenhofer"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "Li, W., Liu, H., Tang, H., Wang, P., Van Gool, L.: Mhformer: Multi-\nhypothesis transformer for 3d human pose estimation. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 13147{13156 (2022) 4 ",
                    "Citation Text": "Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R.: Swinir:\nImage restoration using swin transformer. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 1833{1844 (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10257",
                        "Citation Paper Title": "Title:SwinIR: Image Restoration Using Swin Transformer",
                        "Citation Paper Abstract": "Abstract:Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\\textbf{up to 0.14$\\sim$0.45dB}$, while the total number of parameters can be reduced by $\\textbf{up to 67%}$.",
                        "Citation Paper Authors": "Authors:Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In:\nInternational Conference on Learning Representations (2015) 8, 9 ",
                    "Citation Text": "Li, J., Xu, C., Chen, Z., Bian, S., Yang, L., Lu, C.: Hybrik: A hybrid\nanalytical-neural inverse kinematics solution for 3d human pose and shape\nestimation. In: Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition. pp. 3383{3393 (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.14672",
                        "Citation Paper Title": "Title:HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation",
                        "Citation Paper Abstract": "Abstract:Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh for the human body by estimating several parameters. However, learning the abstract parameters is a highly non-linear process and suffers from image-model misalignment, leading to mediocre model performance. In contrast, 3D keypoint estimation methods combine deep CNN network with the volumetric representation to achieve pixel-level localization accuracy but may predict unrealistic body structure. In this paper, we address the above issues by bridging the gap between body mesh estimation and 3D keypoint estimation. We propose a novel hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms accurate 3D joints to relative body-part rotations for 3D body mesh reconstruction, via the twist-and-swing decomposition. The swing rotation is analytically solved with 3D joints, and the twist rotation is derived from the visual cues through the neural network. We show that HybrIK preserves both the accuracy of 3D pose and the realistic body structure of the parametric human model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than the pure 3D keypoint estimation methods. Without bells and whistles, the proposed method surpasses the state-of-the-art methods by a large margin on various 3D human pose and shape benchmarks. As an illustrative example, HybrIK outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW dataset. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Heidicker, P., Langbehn, E., Steinicke, F.: In\ruence of avatar appearance\non presence in social vr. In: 2017 IEEE Symposium on 3D User Interfaces\n(3DUI). pp. 233{234 (2017) 2 ",
                    "Citation Text": "Huang, Y., Kaufmann, M., Aksan, E., Black, M.J., Hilliges, O., Pons-Moll,\nG.: Deep inertial poser: Learning to reconstruct human pose from sparse\ninertial measurements in real time. ACM Transactions on Graphics, (Proc.\nSIGGRAPH Asia) 37, 185:1{185:15 (Nov 2018) 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04703",
                        "Citation Paper Title": "Title:Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time",
                        "Citation Paper Abstract": "Abstract:We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$ sequences with $330\\,000$ time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.",
                        "Citation Paper Authors": "Authors:Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar Hilliges, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Duka, A.V.: Neural network based inverse kinematics solution for trajectory\ntracking of a robotic arm. Procedia Technology 12, 20{27 (2014) 4 ",
                    "Citation Text": "Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer,\nC.: Multiscale vision transformers. In: Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision. pp. 6824{6835 (2021) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11227",
                        "Citation Paper Title": "Title:Multiscale Vision Transformers",
                        "Citation Paper Abstract": "Abstract:We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: this https URL",
                        "Citation Paper Authors": "Authors:Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q., Salakhutdinov, R.:\nTransformer-xl: Attentive language models beyond a \fxed-length context.\nIn: Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics. pp. 2978{2988 (2019) 4 ",
                    "Citation Text": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep\nbidirectional transformers for language understanding. Annual Conference\nof the North American Chapter of the Association for Computational Lin-\nguistics (2019) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "Csiszar, A., Eilers, J., Verl, A.: On solving the inverse kinematics problem\nusing neural networks. In: 2017 24th International Conference on Mecha-\ntronics and Machine Vision in Practice (M2VIP). pp. 1{6. IEEE (2017)\n4 ",
                    "Citation Text": "Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q., Salakhutdinov, R.:\nTransformer-xl: Attentive language models beyond a \fxed-length context.\nIn: Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics. pp. 2978{2988 (2019) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02860",
                        "Citation Paper Title": "Title:Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                        "Citation Paper Abstract": "Abstract:Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
                        "Citation Paper Authors": "Authors:Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "B\u0013 ocsi, B., Nguyen-Tuong, D., Csat\u0013 o, L., Schoelkopf, B., Peters, J.: Learning\ninverse kinematics with structured prediction. In: 2011 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems. pp. 698{703. IEEE\n(2011) 4 ",
                    "Citation Text": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko,\nS.: End-to-end object detection with transformers. In: European conference\non computer vision. pp. 213{229. Springer (2020) 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12872",
                        "Citation Paper Title": "Title:End-to-End Object Detection with Transformers",
                        "Citation Paper Abstract": "Abstract:We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.13751v1": {
            "Paper Title": "GAUDI: A Neural Architect for Immersive 3D Scene Generation",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". We sample 5k images from predicted\nand target distributions for each model and dataset and report both FID ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint\narXiv:1706.08500 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ": Replica is a dataset comprised of 18realistic scenes from which trajectories are\nrendered via Habitat ",
                    "Citation Text": "Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian\nStraub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In\nProceedings of the IEEE/CVF International Conference on Computer Vision , pages 9339\u20139347, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01201",
                        "Citation Paper Title": "Title:Habitat: A Platform for Embodied AI Research",
                        "Citation Paper Abstract": "Abstract:We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.\nThese large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",
                        "Citation Paper Authors": "Authors:Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ": VLN-CE is a dataset originally designed for vision and language navigation in\ncontinuous environments. This dataset is composed of 3:6K trajectories of an agent navigating\nbetween two points in a 3D scene from the 3D dataset ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran\nSong, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments.\narXiv preprint arXiv:1709.06158 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13907v2": {
            "Paper Title": "Pose Representations for Deep Skeletal Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13220v1": {
            "Paper Title": "Beyond Visuals : Examining the Experiences of Geoscience Professionals\n  With Vision Disabilities in Accessing Data Visualizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.12746v1": {
            "Paper Title": "Voreen -- An Open-source Framework for Interactive Visualization and\n  Processing of Large Volume Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.05677v4": {
            "Paper Title": "HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense\n  Contact Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10257v2": {
            "Paper Title": "Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for\n  Editable Portrait Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.07224v2": {
            "Paper Title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in\n  Particle Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01806v2": {
            "Paper Title": "Neural Scene Decoration from a Single Photograph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.10310v3": {
            "Paper Title": "Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance\n  Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11911v1": {
            "Paper Title": "NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for\n  Geometry and Texture Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11757v1": {
            "Paper Title": "Learning Generalizable Light Field Networks from Few Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11617v1": {
            "Paper Title": "Face Deblurring using Dual Camera Fusion on Mobile Phones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.17261v2": {
            "Paper Title": "R2L: Distilling Neural Radiance Field to Neural Light Field for\n  Efficient Novel View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11318v1": {
            "Paper Title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and\n  Nonparametric Noise Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02308v2": {
            "Paper Title": "MoFaNeRF: Morphable Facial Neural Radiance Field",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10860v1": {
            "Paper Title": "Transformer with Implicit Edges for Particle-based Physics Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00584v2": {
            "Paper Title": "The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D\n  Shapes from Parts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.07159v2": {
            "Paper Title": "A Level Set Theory for Neural Implicit Evolution under Explicit Flows",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10663v1": {
            "Paper Title": "Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10606v1": {
            "Paper Title": "Approximate Differentiable Rendering with Algebraic Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05140v2": {
            "Paper Title": "NeRF for Outdoor Scene Relighting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04382v2": {
            "Paper Title": "CLIP-Actor: Text-Driven Recommendation and Stylization for Animating\n  Human Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.09978v1": {
            "Paper Title": "NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation\n  on Point Clouds",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "\ufb01rst labels points\nwith semantic prediction and center votes, and then cluster\npoints into segments as the instance proposals. Follow-up\nworks [2, 44] further enhance the clustering method in dif-\nferent aspects. HAIS ",
                    "Citation Text": "Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and\nXinggang Wang. Hierarchical Aggregation for 3d Instance\nSegmentation. In ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.02350",
                        "Citation Paper Title": "Title:Hierarchical Aggregation for 3D Instance Segmentation",
                        "Citation Paper Abstract": "Abstract:Instance segmentation on point clouds is a fundamental task in 3D scene perception. In this work, we propose a concise clustering-based framework named HAIS, which makes full use of spatial relation of points and point sets. Considering clustering-based methods may result in over-segmentation or under-segmentation, we introduce the hierarchical aggregation to progressively generate instance proposals, i.e., point aggregation for preliminarily clustering points to sets and set aggregation for generating complete instances from sets. Once the complete 3D instances are obtained, a sub-network of intra-instance prediction is adopted for noisy points filtering and mask quality scoring. HAIS is fast (only 410ms per frame) and does not require non-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving the highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods by a large margin. Besides, the SOTA results on the S3DIS dataset validate the good generalization ability. Code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, Xinggang Wang"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "directly predicts a \ufb01xed set of 3D bounding boxes, while\nGSPN ",
                    "Citation Text": "Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J\nGuibas. Gspn: Generative Shape Proposal Network for 3d\nInstance Segmentation in Point Cloud. In CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03320",
                        "Citation Paper Title": "Title:GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud",
                        "Citation Paper Abstract": "Abstract:We introduce a novel 3D object proposal approach named Generative Shape Proposal Network (GSPN) for instance segmentation in point cloud data. Instead of treating object proposal as a direct bounding box regression problem, we take an analysis-by-synthesis strategy and generate proposals by reconstructing shapes from noisy observations in a scene. We incorporate GSPN into a novel 3D instance segmentation framework named Region-based PointNet (R-PointNet) which allows flexible proposal refinement and instance segmentation generation. We achieve state-of-the-art performance on several 3D instance segmentation tasks. The success of GSPN largely comes from its emphasis on geometric understandings during object proposal, which greatly reducing proposals with low objectness.",
                        "Citation Paper Authors": "Authors:Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", we train a 2-layer MLP that predicts the bound-\ning box, parameterized by its two corners relative to the\nquery. We further compare against V oteNet ",
                    "Citation Text": "Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep Hough V oting for 3d Object Detection in Point\nClouds. In CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09664",
                        "Citation Paper Title": "Title:Deep Hough Voting for 3D Object Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "whose shape\nis described by the feature f, and that is evaluated at loca-\ntionx. We opt to model Cwith CvxNet ",
                    "Citation Text": "Boyang Deng, Kyle Genova, Soroosh Yazdani, So\ufb01en\nBouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. Cvxnet:\nLearnable convex decomposition. In CVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.05736",
                        "Citation Paper Title": "Title:CvxNet: Learnable Convex Decomposition",
                        "Citation Paper Abstract": "Abstract:Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental in computer graphics, where it provides one of the most common ways to approximate geometry, for example, in real-time physics simulation. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. However, at testing time, convexes can also generate explicit representations -- polygonal meshes -- which can then be used in any downstream application. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an auto-encoding process. We investigate the applications of this architecture including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval.",
                        "Citation Paper Authors": "Authors:Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, Andrea Tagliasacchi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nTransformer-based instance segmentation . The intro-\nduction of end-to-end transformer-based instance segmen-\ntation ",
                    "Citation Text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12872",
                        "Citation Paper Title": "Title:End-to-End Object Detection with Transformers",
                        "Citation Paper Abstract": "Abstract:We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "detects a set of bounding boxes\nas the initial instance proposals, and then applies a seg-\nmentation module and NMS to output the \ufb01nal mask. Po-\nlarMask ",
                    "Citation Text": "Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo\nLiu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:\nSingle Shot Instance Segmentation with Polar Representa-\ntion. In CVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13226",
                        "Citation Paper Title": "Title:PolarMask: Single Shot Instance Segmentation with Polar Representation",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used as a mask prediction module for instance segmentation, by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on challenging COCO dataset. For the first time, we demonstrate a much simpler and flexible instance segmentation framework achieving competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation tasks. Code is available at: this http URL.",
                        "Citation Paper Authors": "Authors:Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Ding Liang, Chunhua Shen, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "for 2D instance seg-\nmentation.\n2D instance segmentation . Top-down methods [16, 50]\npredict redundant instance proposals for sampled locations\nin images, which typically requires NMS to remove the\noverlap. Mask-RCNN ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.12121v1": {
            "Paper Title": "Cross-Modal Contrastive Representation Learning for Audio-to-Image\n  Generation",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "introduces self -attention and cross -attention modules that are included in the \nCMCGAN. The research in ",
                    "Citation Text": "Bin Duan,Wei Wang, Hao Tang, Hugo Latapie, and Yan  Yan, \u201cCascade attention guided \nresidue learning gan  for cross -modal translation,\u201d in 2020 25th International  Conference on \nPattern Recognition (ICPR) . IEEE, 2021,  pp. 1336 \u20131343.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.01826",
                        "Citation Paper Title": "Title:Cascade Attention Guided Residue Learning GAN for Cross-Modal Translation",
                        "Citation Paper Abstract": "Abstract:Since we were babies, we intuitively develop the ability to correlate the input from different cognitive sensors such as vision, audio, and text. However, in machine learning, this cross-modal learning is a nontrivial task because different modalities have no homogeneous properties. Previous works discover that there should be bridges among different modalities. From neurology and psychology perspective, humans have the capacity to link one modality with another one, e.g., associating a picture of a bird with the only hearing of its singing and vice versa. Is it possible for machine learning algorithms to recover the scene given the audio signal? In this paper, we propose a novel Cascade Attention-Guided Residue GAN (CAR-GAN), aiming at reconstructing the scenes given the corresponding audio signals. Particularly, we present a residue module to mitigate the gap between different modalities progressively. Moreover, a cascade attention guided network with a novel classification loss function is designed to tackle the cross-modal learning task. Our model keeps the consistency in high-level semantic label domain and is able to balance two different modalities. The experimental results demonstrate that our model achieves the state-of-the-art cross-modal audio-visual generation on the challenging Sub-URMP dataset. Code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Bin Duan, Wei Wang, Hao Tang, Hugo Latapie, Yan Yan"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", but the internal details \nof the structure are different. First, CGANs is replaced with a Self -Attention Generative \nAdversarial Networks (SAGAN) ",
                    "Citation Text": "Han Zhang , Ian Goodfellow, Dimitris Metaxas, and  Augustus Odena, \u201cSelf -attention \ngenerative adversarial  networks,\u201d in International conference on machine  learning . PMLR, 2019, \npp. 7354 \u20137363.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08318",
                        "Citation Paper Title": "Title:Self-Attention Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
                        "Citation Paper Authors": "Authors:Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.09729v2": {
            "Paper Title": "REALY: Rethinking the Evaluation of 3D Face Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.04286v3": {
            "Paper Title": "Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.13339v3": {
            "Paper Title": "An Overview of Color Transfer and Style Transfer for Images and Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.09019v1": {
            "Paper Title": "Structure-aware Editable Morphable Model for 3D Facial Detail Animation\n  and Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08890v1": {
            "Paper Title": "NeuForm: Adaptive Overfitting for Neural Shape Editing",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ", learn task-speci\ufb01c\ngeometry of 2D domain [ 10,32], or force the surface to agree with an implicit function ",
                    "Citation Text": "POURSAEED , O., F ISHER , M., A IGERMAN , N., AND KIM, V. G. Coupling explicit and implicit surface\nrepresentations for generative 3d modeling. ECCV (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.10294",
                        "Citation Paper Title": "Title:Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling",
                        "Citation Paper Abstract": "Abstract:We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.",
                        "Citation Paper Authors": "Authors:Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Other approaches model shapes using their 2D parameterizations [ 14,\n39]. Improved versions of such methods optimize for low-distortion atlases ",
                    "Citation Text": "BEDNARIK , J., P ARASHAR , S., G UNDOGDU , E., S ALZMANN , M., AND FUA, P.Shape reconstruction by\nlearning differentiable surface representations. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2020), pp. 4716\u20134725.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11227",
                        "Citation Paper Title": "Title:Shape Reconstruction by Learning Differentiable Surface Representations",
                        "Citation Paper Abstract": "Abstract:Generative models that produce point clouds have emerged as a powerful tool to represent 3D surfaces, and the best current ones rely on learning an ensemble of parametric representations. Unfortunately, they offer no control over the deformations of the surface patches that form the ensemble and thus fail to prevent them from either overlapping or collapsing into single points or lines. As a consequence, computing shape properties such as surface normals and curvatures becomes difficult and unreliable.\nIn this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap. Furthermore, this lets us reliably compute quantities such as surface normals and curvatures. We will demonstrate on several tasks that this yields more accurate surface reconstructions than the state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches.",
                        "Citation Paper Authors": "Authors:Jan Bednarik, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann, Pascal Fua"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". Such network learning has been further regularized\nby geometric constraints like the Eikonal equation [ 13,3,2] or using an intermediate meta-network\nfor faster convergence ",
                    "Citation Text": "LITTWIN , G., AND WOLF, L. Deep meta functionals for shape representation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (2019), pp. 1824\u20131833.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.06277",
                        "Citation Paper Title": "Title:Deep Meta Functionals for Shape Representation",
                        "Citation Paper Abstract": "Abstract:We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network \\textcolor{black}{parametrized by} these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code is available at: this https URL",
                        "Citation Paper Authors": "Authors:Gidi Littwin, Lior Wolf"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "Single-Scene Neural Shape Representations Over\ufb01tting networks represent one speci\ufb01c shape\nvia a single network by optimizing network weights. Such over\ufb01tted networks are useful for several\napplications including compression [ 9,36], adaptive network parameter allocation ",
                    "Citation Text": "MARTEL , J. N., L INDELL , D. B., L IN, C. Z., C HAN , E. R., M ONTEIRO , M., AND WETZSTEIN , G.\nAcorn: Adaptive coordinate networks for neural scene representation. arXiv preprint arXiv:2105.02788\n(2021).\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.02788",
                        "Citation Paper Title": "Title:ACORN: Adaptive Coordinate Networks for Neural Scene Representation",
                        "Citation Paper Abstract": "Abstract:Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",
                        "Citation Paper Authors": "Authors:Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.08813v1": {
            "Paper Title": "Audio Input Generates Continuous Frames to Synthesize Facial Video Using\n  Generative Adiversarial Networks",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "was \napproached with two branches, one learning the \nappearance and another improving the temporally \ncoherent performance.  In Dynamics Transfer GAN ",
                    "Citation Text": "W. J. Baddar, G. Gu, S. Lee, an d Y. M. J. a. \np. a. Ro, \"Dynamics transfer gan: Generating video \nby transferring arbitrary temporal dynamics from a \nsource video to a single target image,\" 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.03534",
                        "Citation Paper Title": "Title:Dynamics Transfer GAN: Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose Dynamics Transfer GAN; a new method for generating video sequences based on generative adversarial learning. The spatial constructs of a generated video sequence are acquired from the target image. The dynamics of the generated video sequence are imported from a source video sequence, with arbitrary motion, and imposed onto the target image. To preserve the spatial construct of the target image, the appearance of the source video sequence is suppressed and only the dynamics are obtained before being imposed onto the target image. That is achieved using the proposed appearance suppressed dynamics feature. Moreover, the spatial and temporal consistencies of the generated video sequence are verified via two discriminator networks. One discriminator validates the fidelity of the generated frames appearance, while the other validates the dynamic consistency of the generated video sequence. Experiments have been conducted to verify the quality of the video sequences generated by the proposed method. The results verified that Dynamics Transfer GAN successfully transferred arbitrary dynamics of the source video sequence onto a target image when generating the output video sequence. The experimental results also showed that Dynamics Transfer GAN maintained the spatial constructs (appearance) of the target image while generating spatially and temporally consistent video sequences.",
                        "Citation Paper Authors": "Authors:Wissam J. Baddar, Geonmo Gu, Sangmin Lee, Yong Man Ro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.07900v1": {
            "Paper Title": "Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", a framework that encodes a 3D location\nmap at the spatial location of each visible joint, which also focuses\non a person-centric problem. Zhen et al. ",
                    "Citation Text": "Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and\nXiaowei Zhou. 2020. Smap: Single-shot multi-person absolute 3d pose estimation.\nInEuropean Conference on Computer Vision . Springer, 550\u2013566.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.11469",
                        "Citation Paper Title": "Title:SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Recovering multi-person 3D poses with absolute scales from a single RGB image is a challenging problem due to the inherent depth and scale ambiguity from a single view. Addressing this ambiguity requires to aggregate various cues over the entire image, such as body sizes, scene layouts, and inter-person relationships. However, most previous methods adopt a top-down scheme that first performs 2D pose detection and then regresses the 3D pose and scale for each detected person individually, ignoring global contextual cues. In this paper, we propose a novel system that first regresses a set of 2.5D representations of body parts and then reconstructs the 3D absolute poses based on these 2.5D representations with a depth-aware part association algorithm. Such a single-shot bottom-up scheme allows the system to better learn and reason about the inter-person depth relationship, improving both 3D and 2D pose estimation. The experiments demonstrate that the proposed approach achieves the state-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is applicable to in-the-wild videos.",
                        "Citation Paper Authors": "Authors:Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", a representative work, which was the first to present the\npart-affinity field approach that links key points likely to lie in the\nsame person. Zhou et al. ",
                    "Citation Text": "Xingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. 2019. Objects as points.\narXiv preprint arXiv:1904.07850 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.07850",
                        "Citation Paper Title": "Title:Objects as Points",
                        "Citation Paper Abstract": "Abstract:Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.",
                        "Citation Paper Authors": "Authors:Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", we use Protocol 2 and sample every\n5th and 64th frames in videos for training and testing respectively.\nCOCO Dataset. The COCO dataset ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740\u2013755.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", were used to assess the frame-\nwork\u2019s ability to estimate multi-person 3D poses. MuCo-3DHP, the\ntraining set we used is a large-scale synthesized dataset, which\nwas generated from single person 3D pose estimation dataset MPI-\nINF-3DHP ",
                    "Citation Text": "Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko,\nWeipeng Xu, and Christian Theobalt. 2017. Monocular 3d human pose estimation\nin the wild using improved cnn supervision. In 2017 international conference on\n3D vision (3DV) . IEEE, 506\u2013516.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.09813",
                        "Citation Paper Title": "Title:Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision",
                        "Citation Paper Abstract": "Abstract:We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.",
                        "Citation Paper Authors": "Authors:Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.07695v1": {
            "Paper Title": "An Exact Bitwise Reversible Integrator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05385v2": {
            "Paper Title": "Controllable Shadow Generation Using Pixel Height Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.07104v1": {
            "Paper Title": "Relighting4D: Neural Relightable Human from Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.06793v1": {
            "Paper Title": "Neural apparent BRDF fields for multiview photometric stereo",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "introduces a system for 3D recon-\nstruction from in-the-wild photo collections. This was achieved by\nintroducing Generative Latent Optimization (GLO) ",
                    "Citation Text": "Bojanowski, P., Joulin, A., Lopez-Paz, D., Szlam, A.: Optimizing the latent space\nof generative networks. arXiv preprint arXiv:1707.05776 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05776",
                        "Citation Paper Title": "Title:Optimizing the Latent Space of Generative Networks",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed an algorithm by extending the\ntraditional plenoptic sampling theory for view synthesis from an\nirregular grid of sample views via MPI representations. Neural\nVolumes ",
                    "Citation Text": "Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.: Neural\nvolumes: Learning dynamic renderable volumes from images. arXiv preprint\narXiv:1906.07751 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07751",
                        "Citation Paper Title": "Title:Neural Volumes: Learning Dynamic Renderable Volumes from Images",
                        "Citation Paper Abstract": "Abstract:Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",
                        "Citation Paper Authors": "Authors:Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "synthesises multiplane images\n(MPI) from sparse camera viewpoints improving performance on\nobject boundaries, light reflection and thin structures. Local light\nfield fusion (LLFF) ",
                    "Citation Text": "Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R.,\nNg, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive\nsampling guidelines. ACM Transactions on Graphics (TOG) 38(4), 1\u201314 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00889",
                        "Citation Paper Title": "Title:Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines",
                        "Citation Paper Abstract": "Abstract:We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "provides trade-offs across fidelity, efficiency and compression\ncapabilities enabling high quality shape representation, interpo-\nlation and completion. Local Deep Implicit Functions (LDIF) ",
                    "Citation Text": "Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit func-\ntions for 3d shape. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 4857\u20134866 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06126",
                        "Citation Paper Title": "Title:Local Deep Implicit Functions for 3D Shape",
                        "Citation Paper Abstract": "Abstract:The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",
                        "Citation Paper Authors": "Authors:Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "trains an autoencoder to handle complex and diverse\nindoor scenes. Continuous Single Distance Function (DeepSDF) ",
                    "Citation Text": "Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning\ncontinuous signed distance functions for shape representation. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n165\u2013174 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "or circumvent factors \u2013 like in Texture\nFields \u2013 which limit the fidelity of highly textured surfaces ",
                    "Citation Text": "Oechsle, M., Mescheder, L., Niemeyer, M., Strauss, T., Geiger, A.: Texture fields:\nLearning texture representations in function space. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision. pp. 4531\u20134540 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07259",
                        "Citation Paper Title": "Title:Texture Fields: Learning Texture Representations in Function Space",
                        "Citation Paper Abstract": "Abstract:In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",
                        "Citation Paper Authors": "Authors:Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Previously, such MLP architectures have been\nused to represent natural textured materials which can be sampled\nover infinite domains ",
                    "Citation Text": "Henzler, P., Mitra, N.J., Ritschel, T.: Learning a neural 3d texture space from 2d\nexemplars. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 8356\u20138364 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04158",
                        "Citation Paper Title": "Title:Learning a Neural 3D Texture Space from 2D Exemplars",
                        "Citation Paper Abstract": "Abstract:We propose a generative model of 2D and 3D natural textures with diversity, visual fidelity and at high computational efficiency. This is enabled by a family of methods that extend ideas from classic stochastic procedural texturing (Perlin noise) to learned, deep, non-linearities. The key idea is a hard-coded, tunable and differentiable step that feeds multiple transformed random 2D or 3D fields into an MLP that can be sampled over infinite domains. Our model encodes all exemplars from a diverse set of textures without a need to be re-trained for each exemplar. Applications include texture interpolation, and learning 3D textures from 2D exemplars.",
                        "Citation Paper Authors": "Authors:Philipp Henzler, Niloy J. Mitra, Tobias Ritschel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.06403v1": {
            "Paper Title": "3D Concept Grounding on Neural Fields",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ". In this work, we utilize descriptors\nde\ufb01ned on such \ufb01elds ",
                    "Citation Text": "A. Simeonov, Y . Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V . Sitz-\nmann. Neural descriptor \ufb01elds: Se (3)-equivariant object representations for manipulation.\narXiv preprint arXiv:2112.05124 , 2021. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05124",
                        "Citation Paper Title": "Title:Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation",
                        "Citation Paper Abstract": "Abstract:We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: this https URL.",
                        "Citation Paper Authors": "Authors:Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "uses a text encoder to compute embeddings of descriptive input labels together with a transformer-\nbased image encoder that computes dense per-pixel embeddings of the input image. GroupViT ",
                    "Citation Text": "J. Xu, S. D. Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang. Groupvit: Semantic\nsegmentation emerges from text supervision. ArXiv , abs/2202.11094, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.11094",
                        "Citation Paper Title": "Title:GroupViT: Semantic Segmentation Emerges from Text Supervision",
                        "Citation Paper Abstract": "Abstract:Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision. We open-source our code at this https URL .",
                        "Citation Paper Authors": "Authors:Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "is an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or a question. LSeg ",
                    "Citation Text": "B. Li, K. Q. Weinberger, S. Belongie, V . Koltun, and R. Ranftl. Language-driven semantic\nsegmentation, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.03546",
                        "Citation Paper Title": "Title:Language-driven Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., \"grass\" or \"building\") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at this https URL.",
                        "Citation Paper Authors": "Authors:Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, Ren\u00e9 Ranftl"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "distills the knowledge from a pre-trained zero-shot image\nclassi\ufb01cation model into a two-stage detector. MDETR ",
                    "Citation Text": "A. Kamath, M. Singh, Y . LeCun, I. Misra, G. Synnaeve, and N. Carion. Mdetr - modulated\ndetection for end-to-end multi-modal understanding. 2021 IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 1760\u20131770, 2021. 3\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.12763",
                        "Citation Paper Title": "Title:MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding",
                        "Citation Paper Abstract": "Abstract:Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "to differentiably reason in 3D.\nLanguage-driven Segmentation. Recent works have been focused on leveraging language for\nsegmentation. Speci\ufb01cally, BiLD ",
                    "Citation Text": "X. Gu, T.-Y . Lin, W. Kuo, and Y . Cui. Zero-shot detection via vision and language knowledge\ndistillation. ArXiv , abs/2104.13921, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13921",
                        "Citation Paper Title": "Title:Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "Visual Reasoning. There have been different tasks focusing on learning visual concepts from\nnatural language, such as visually-grounded question answering [ 8,9] and text-image retrieval ",
                    "Citation Text": "I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. Order-embeddings of images and language.\nCoRR , abs/1511.06361, 2016. 2\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06361",
                        "Citation Paper Title": "Title:Order-Embeddings of Images and Language",
                        "Citation Paper Abstract": "Abstract:Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.",
                        "Citation Paper Authors": "Authors:Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.08532v2": {
            "Paper Title": "Dress Code: High-Resolution Multi-Category Virtual Try-On",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05618v1": {
            "Paper Title": "Htex: Per-Halfedge Texturing for Arbitrary Mesh Topologies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13326v5": {
            "Paper Title": "SHREC 2022: pothole and crack detection in the road pavement using\n  images and RGB-D data",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "and Cross Entropy with Online Hard Ex-\nample Mining (OhemCE) loss (also known as Boot-\nstrapping Cross Entropy Loss ",
                    "Citation Text": "Z. Wu, C. Shen, and A. van den Hengel.\nHigh-performance semantic segmentation using\nvery deep fully convolutional networks. CoRR ,\nabs/1604.04339, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.04339",
                        "Citation Paper Title": "Title:High-performance Semantic Segmentation Using Very Deep Fully Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:We propose a method for high-performance semantic image segmentation (or semantic pixel labelling) based on very deep residual networks, which achieves the state-of-the-art performance. A few design factors are carefully considered to this end.\nWe make the following contributions. (i) First, we evaluate different variations of a fully convolutional residual network so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. Our experiments show that further enlarging the field-of-view and increasing the resolution of feature maps are typically beneficial, which however inevitably leads to a higher demand for GPU memories. To walk around the limitation, we propose a new method to simulate a high resolution network with a low resolution network, which can be applied during training and/or testing. (ii) Second, we propose an online bootstrapping method for training. We demonstrate that online bootstrapping is critically important for achieving good accuracy. (iii) Third we apply the traditional dropout to some of the residual blocks, which further improves the performance. (iv) Finally, our method achieves the currently best mean intersection-over-union 78.3\\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset Cityscapes.",
                        "Citation Paper Authors": "Authors:Zifeng Wu, Chunhua Shen, Anton van den Hengel"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ". Fast.ai\nadds an additional layer of abstraction above Py-\ntorch ",
                    "Citation Text": "A. Paszke, S. Gross, F. Massa, A. Lerer,\nJ. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An\nimperative style, high-performance deep learn-\ning library. Advances in neural information pro-\ncessing systems , 32:8026{8037, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", following a similar ap-\nproach to what was presented in ",
                    "Citation Text": "R. Fan, H. Wang, M. J. Bocus, and M. Liu. We\nlearn better road pothole detection: From at-\ntention aggregation to adversarial domain adap-\ntation. In European Conference on Computer\nVision , pages 285{300. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06840",
                        "Citation Paper Title": "Title:We Learn Better Road Pothole Detection: from Attention Aggregation to Adversarial Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:Manual visual inspection performed by certified inspectors is still the main form of road pothole detection. This process is, however, not only tedious, time-consuming and costly, but also dangerous for the inspectors. Furthermore, the road pothole detection results are always subjective, because they depend entirely on the individual experience. Our recently introduced disparity (or inverse depth) transformation algorithm allows better discrimination between damaged and undamaged road areas, and it can be easily deployed to any semantic segmentation network for better road pothole detection results. To boost the performance, we propose a novel attention aggregation (AA) framework, which takes the advantages of different types of attention modules. In addition, we develop an effective training set augmentation technique based on adversarial domain adaptation, where the synthetic road RGB images and transformed road disparity (or inverse depth) images are generated to enhance the training of semantic segmentation networks. The experimental results demonstrate that, firstly, the transformed disparity (or inverse depth) images become more informative; secondly, AA-UNet and AA-RTFNet, our best performing implementations, respectively outperform all other state-of-the-art single-modal and data-fusion networks for road pothole detection; and finally, the training set augmentation technique based on adversarial domain adaptation not only improves the accuracy of the state-of-the-art semantic segmentation networks, but also accelerates their convergence.",
                        "Citation Paper Authors": "Authors:Rui Fan, Hengli Wang, Mohammud J. Bocus, Ming Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.05415v1": {
            "Paper Title": "Rendering along the Hilbert Curve",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09290v2": {
            "Paper Title": "PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer\n  Vision",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", existing games like GTA V [ 31,32,33], or game engines ",
                    "Citation Text": "Cesar Roberto de Souza, Adrien Gaidon, Yohann Cabon, and Antonio Manuel Lopez. Procedural\ngeneration of videos to train deep action recognition networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pages 4757\u20134767, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00881",
                        "Citation Paper Title": "Title:Procedural Generation of Videos to Train Deep Action Recognition Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for \"Procedural Human Action Videos\". It contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.",
                        "Citation Paper Authors": "Authors:C\u00e9sar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Antonio Manuel L\u00f3pez Pe\u00f1a"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "focus largely on embodied AI tasks. More generic tools\nfor object detection dataset generation include BlenderProc ",
                    "Citation Text": "Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry\nOle\ufb01r, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam. Blenderproc. arXiv preprint\narXiv:1911.01911 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01911",
                        "Citation Paper Title": "Title:BlenderProc",
                        "Citation Paper Abstract": "Abstract:BlenderProc is a modular procedural pipeline, which helps in generating real looking images for the training of convolutional neural networks. These can be used in a variety of use cases including segmentation, depth, normal and pose estimation and many others. A key feature of our extension of blender is the simple to use modular pipeline, which was designed to be easily extendable. By offering standard modules, which cover a variety of scenarios, we provide a starting point on which new modules can be created.",
                        "Citation Paper Authors": "Authors:Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "develop\nsimulators for indoor object detection. Robotic simulators include AI-2THOR ",
                    "Citation Text": "Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3D\nenvironment for visual AI. arXiv preprint arXiv:1712.05474 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05474",
                        "Citation Paper Title": "Title:AI2-THOR: An Interactive 3D Environment for Visual AI",
                        "Citation Paper Abstract": "Abstract:We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",
                        "Citation Paper Authors": "Authors:Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.05025v1": {
            "Paper Title": "PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of\n  Human-Centric Computer Vision Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.04945v1": {
            "Paper Title": "SHREC'22 Track: Sketch-Based 3D Shape Retrieval in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02974v2": {
            "Paper Title": "Generate and Edit Your Own Character in a Canonical View",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "from a scratch that is based on a 3D representation with a\nneural radiance \ufb01eld ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In ECCV , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "There has been remarkable progress in the \ufb01elds of im-\nage generation and editing via Generative Adversarial Net-\nworks (GANs) ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS ,\n2014. 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.04465v1": {
            "Paper Title": "Progressively-connected Light Field Network for Efficient View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03543v1": {
            "Paper Title": "Highlight Specular Reflection Separation based on Tensor Low-rank and\n  Sparse Decomposition Using Polarimetric Cues",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.02774v1": {
            "Paper Title": "Local Relighting of Real Scenes",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "propose a more forgiving metric, Local Mean Squared Er-\nror (LMSE), which sums the MSE over several windows.\nWe choose for the windows to be 20\u000220spaced 10pix-\nels apart. Lastly, Learned Perceptual Image Patch Simi-\nlarity (LPIPS) ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.00041v2": {
            "Paper Title": "A Recurrent Differentiable Engine for Modeling Tensegrity Robots\n  Trainable with Low-Frequency Data",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ", including\nby training in simulation. But simulated locomotion is hard\nto replicate on a real platform, even after hand-tuning, which\nemphasizes the importance of learning a transferable policy\nthat is the focus of this work. Domain randomization ",
                    "Citation Text": "Y . Chebotar, A. Handa, V . Makoviychuk, M. Macklin, J. Issac,\nN. Ratliff, and D. Fox, \u201cClosing the sim-to-real loop: Adapting simula-\ntion randomization with real world experience,\u201d in 2019 International\nConference on Robotics and Automation (ICRA) . IEEE, 2019, pp.\n8973\u20138979.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05687",
                        "Citation Paper Title": "Title:Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                        "Citation Paper Abstract": "Abstract:We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at this https URL",
                        "Citation Paper Authors": "Authors:Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.01413v1": {
            "Paper Title": "Disentangling Random and Cyclic Effects in Time-Lapse Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05249v5": {
            "Paper Title": "Breaking Good: Fracture Modes for Realtime Destruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.00896v1": {
            "Paper Title": "VocabulARy: Learning Vocabulary in AR Supported by Keyword\n  Visualisations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.05085v4": {
            "Paper Title": "Improved Direct Voxel Grid Optimization for Radiance Fields\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "with our ef\ufb01cient realization improves our quality\nand training speed.\nMethod Tr. time PSNR \u2191SSIM\u2191LPIPS (VGG)\u2193\nNeRF++ ",
                    "Citation Text": "Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\n\ufb01elds. arxiv CS.CV 2010.07492 , 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07492",
                        "Citation Paper Title": "Title:NeRF++: Analyzing and Improving Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The\ngrid resolution of Plenoxels is 6403for foreground and 2048\u00d7\n1024\u00d764for background, while ours is a single 3203grid shared\nby foreground and background.\nMethod Tr. time PSNR \u2191SSIM\u2191LPIPS (VGG)\u2193\nNeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In ECCV , 2020. 1,3,4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.15183v1": {
            "Paper Title": "Neural Network Assisted Depth Map Packing for Compression Using Standard\n  Hardware Video Codecs",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". However, these solutions require multiple incoming\nvideo streams, and the depth estimation process is computationally much too demanding to be\nperformed in real time for resource constrained mobile devices.\nThe light probe streaming solution presented in ",
                    "Citation Text": "Michael Stengel, Zander Majercik, Benjamin Boudaoud, and Morgan McGuire. 2021. A Distributed, Decoupled System\nfor Losslessly Streaming Dynamic Light Probes to Thin Clients. In Proceedings of the 12th ACM Multimedia Systems\nConference (MMSys \u201921) . Association for Computing Machinery, New York, NY, USA, 159\u2013172.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.05875",
                        "Citation Paper Title": "Title:A Distributed, Decoupled System for Losslessly Streaming Dynamic Light Probes to Thin Clients",
                        "Citation Paper Abstract": "Abstract:We present a networked, high performance graphics system that combines dynamic, high quality, ray traced global illumination computed on a server with direct illumination and primary visibility computed on a client. This approach provides many of the image quality benefits of real-time ray tracing on low-power and legacy hardware, while maintaining a low latency response and mobile form factor. Our system distributes the graphic pipeline over a network by computing diffuse global illumination on a remote machine. Global illumination is computed using a recent irradiance volume representation combined with a novel, lossless, HEVC-based, hardware-accelerated encoding, and a perceptually-motivated update scheme. Our experimental implementation streams thousands of irradiance probes per second and requires less than 50 Mbps of throughput, reducing the consumed bandwidth by 99.4% when streaming at 60 Hz compared to traditional lossless texture compression. This bandwidth reduction allows higher quality and lower latency graphics than state-of-the-art remote rendering via video streaming. In addition, our split-rendering solution decouples remote computation from local rendering and so does not limit local display update rate or resolution.",
                        "Citation Paper Authors": "Authors:Michael Stengel, Zander Majercik, Benjamin Boudaoud, Morgan McGuire"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.11703v2": {
            "Paper Title": "U-Attention to Textures: Hierarchical Hourglass Vision Transformer for\n  Universal Texture Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "as a way to solve\nthe problem of long-range dependency on input sequences\nand was later adapted to the Transformer architecture ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "proposed to\nsynthesize textures in the Fourier domain in which global\nchanges in image space are locally represented. Closer to\nour approach, Liu et al. ",
                    "Citation Text": "Guilin Liu, Rohan Taori, Ting-Chun Wang, Zhiding Yu,\nShiqiu Liu, Fitsum A Reda, Karan Sapra, Andrew Tao, and\nBryan Catanzaro. Transposer: Universal texture synthesis\nusing feature maps as transposed convolution \ufb01lter. arXiv\npreprint arXiv:2007.07243 , 2020. 1, 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.07243",
                        "Citation Paper Title": "Title:Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter",
                        "Citation Paper Abstract": "Abstract:Conventional CNNs for texture synthesis consist of a sequence of (de)-convolution and up/down-sampling layers, where each layer operates locally and lacks the ability to capture the long-term structural dependency required by texture synthesis. Thus, they often simply enlarge the input texture, rather than perform reasonable synthesis. As a compromise, many recent methods sacrifice generalizability by training and testing on the same single (or fixed set of) texture image(s), resulting in huge re-training time costs for unseen images. In this work, based on the discovery that the assembling/stitching operation in traditional texture synthesis is analogous to a transposed convolution operation, we propose a novel way of using transposed convolution operation. Specifically, we directly treat the whole encoded feature map of the input texture as transposed convolution filters and the features' self-similarity map, which captures the auto-correlation information, as input to the transposed convolution. Such a design allows our framework, once trained, to be generalizable to perform synthesis of unseen textures with a single forward pass in nearly real-time. Our method achieves state-of-the-art texture synthesis quality based on various metrics. While self-similarity helps preserve the input textures' regular structural patterns, our framework can also take random noise maps for irregular input textures instead of self-similarity maps as transposed convolution inputs. It allows to get more diverse results as well as generate arbitrarily large texture outputs by directly sampling large noise maps in a single pass as well.",
                        "Citation Paper Authors": "Authors:Guilin Liu, Rohan Taori, Ting-Chun Wang, Zhiding Yu, Shiqiu Liu, Fitsum A. Reda, Karan Sapra, Andrew Tao, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "uses a recurrent update of a low\ndimensional latent representation, and the Synthesizer ",
                    "Citation Text": "Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe\nZhao, and Che Zheng. Synthesizer: Rethinking self-attention\nfor transformer models. In International Conference on Ma-\nchine Learning , pages 10183\u201310192. PMLR, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00743",
                        "Citation Paper Title": "Title:Synthesizer: Rethinking Self-Attention in Transformer Models",
                        "Citation Paper Abstract": "Abstract:The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. In our experiments, we first show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we find that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only $60\\%$ faster but also improves perplexity by a relative $3.5\\%$. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.",
                        "Citation Paper Authors": "Authors:Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "lowers the rank of the attention ma-\ntrix, the Perceiver ",
                    "Citation Text": "Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\nserman, Oriol Vinyals, and Jo \u02dcao Carreira. Perceiver: General\nperception with iterative attention. In ICML , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03206",
                        "Citation Paper Title": "Title:Perceiver: General Perception with Iterative Attention",
                        "Citation Paper Abstract": "Abstract:Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
                        "Citation Paper Authors": "Authors:Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "2.1. Algorithmic texture synthesis\nTexture synthesis aims at generating a larger extent of\na given texture. This research area has been active for\nthe past decades, as described in the survey of Raad et\nal. ",
                    "Citation Text": "Lara Raad, Axel Davy, Agn `es Desolneux, and Jean-Michel\nMorel. A survey of exemplar-based texture synthesis. CoRR ,\nabs/1707.07184, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07184",
                        "Citation Paper Title": "Title:A survey of exemplar-based texture synthesis",
                        "Citation Paper Abstract": "Abstract:Exemplar-based texture synthesis is the process of generating, from an input sample, new texture images of arbitrary size and which are perceptually equivalent to the sample. The two main approaches are statistics-based methods and patch re-arrangement methods. In the first class, a texture is characterized by a statistical signature; then, a random sampling conditioned to this signature produces genuinely different texture images. The second class boils down to a clever \"copy-paste\" procedure, which stitches together large regions of the sample. Hybrid methods try to combine ideas from both approaches to avoid their hurdles. The recent approaches using convolutional neural networks fit to this classification, some being statistical and others performing patch re-arrangement in the feature space. They produce impressive synthesis on various kinds of textures. Nevertheless, we found that most real textures are organized at multiple scales, with global structures revealed at coarse scales and highly varying details at finer ones. Thus, when confronted with large natural images of textures the results of state-of-the-art methods degrade rapidly, and the problem of modeling them remains wide open.",
                        "Citation Paper Authors": "Authors:Lara Raad, Axel Davy, Agn\u00e8s Desolneux, Jean-Michel Morel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.14970v1": {
            "Paper Title": "Controlling Material Appearance by Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.14939v2": {
            "Paper Title": "Pupil-aware Holography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13502v1": {
            "Paper Title": "Programmatic Concept Learning for Human Motion Description and Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "to synthesize\nrealistic videos from the generated pose sequence for all\nmodels. We compute the PNSR, SSIM, LPIPS and MSE\nmetrics to judge the resulting video quality.\nBaselines. We compare against two methods: i) Lin et\nal. ",
                    "Citation Text": "Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning\nEfficient Point Cloud Generation for Dense 3D Object Recon-\nstruction. In AAAI , 2018. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.07036",
                        "Citation Paper Title": "Title:Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction",
                        "Citation Paper Abstract": "Abstract:Conventional methods of 3D object generative modeling learn volumetric predictions using deep networks with 3D convolutional operations, which are direct analogies to classical 2D ones. However, these methods are computationally wasteful in attempt to predict 3D shapes, where information is rich only on the surfaces. In this paper, we propose a novel 3D generative modeling framework to efficiently generate object shapes in the form of dense point clouds. We use 2D convolutional operations to predict the 3D structure from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. We introduce the pseudo-renderer, a differentiable module to approximate the true rendering operation, to synthesize novel depth maps for optimization. Experimental results for single-image 3D object reconstruction tasks show that we outperforms state-of-the-art methods in terms of shape similarity and prediction density.",
                        "Citation Paper Authors": "Authors:Chen-Hsuan Lin, Chen Kong, Simon Lucey"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Most relevant are\nworks that perform a) action-conditioned motion synthesis\nand b) controlled motion synthesis from descriptions.\nAction-conditioned motion synthesis is the problem of\ngenerating natural and diverse motion from a given action\nclass. Action2Motion ",
                    "Citation Text": "Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao\nSun, Annan Deng, Minglun Gong, and Li Cheng. Ac-\ntion2motion: Conditioned generation of 3d human motions.\nInProceedings of the 28th ACM International Conference on\nMultimedia , pages 2021\u20132029, 2020. 1, 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15240",
                        "Citation Paper Title": "Title:Action2Motion: Conditioned Generation of 3D Human Motions",
                        "Citation Paper Abstract": "Abstract:Action recognition is a relatively established task, where givenan input sequence of human motion, the goal is to predict its ac-tion category. This paper, on the other hand, considers a relativelynew problem, which could be thought of as an inverse of actionrecognition: given a prescribed action type, we aim to generateplausible human motion sequences in 3D. Importantly, the set ofgenerated motions are expected to maintain itsdiversityto be ableto explore the entire action-conditioned motion space; meanwhile,each sampled sequence faithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by these objectives, we followthe physics law of human kinematics by adopting the Lie Algebratheory to represent thenaturalhuman motions; we also propose atemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of the motion space. A new 3D human motion dataset, HumanAct12, is also constructed. Empirical experiments overthree distinct human motion datasets (including ours) demonstratethe effectiveness of our approach.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "and extend it to capture more expressive motion.\nLearning of visual concepts in deep networks has been\ndemonstrated in other domains [24, 30, 42, 48]. NS-CL ",
                    "Citation Text": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenen-\nbaum, and Jiajun Wu. The Neuro-Symbolic Concept Learner:\nInterpreting Scenes, Words, and Sentences from Natural Su-\npervision. In ICLR , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12584",
                        "Citation Paper Title": "Title:The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
                        "Citation Paper Abstract": "Abstract:We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
                        "Citation Paper Authors": "Authors:Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "proposes a V AE approach and\nuses Lie algebra to represent natural human motion. AC-\nTOR ",
                    "Citation Text": "Mathis Petrovich, Michael J Black, and G\u00fcl Varol. Action-\nconditioned 3d human motion synthesis with transformer vae.\narXiv preprint arXiv:2104.05670 , 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.05670",
                        "Citation Paper Title": "Title:Action-Conditioned 3D Human Motion Synthesis with Transformer VAE",
                        "Citation Paper Abstract": "Abstract:We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page.",
                        "Citation Paper Authors": "Authors:Mathis Petrovich, Michael J. Black, G\u00fcl Varol"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "involve variants of the Transformer architecture.\nMost relevant to us is work on weakly-supervised actionlocalization from video-level labels [18, 35, 52]. E-CTC ",
                    "Citation Text": "De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Con-\nnectionist temporal modeling for weakly supervised action\nlabeling. In European Conference on Computer Vision , pages\n137\u2013153. Springer, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08584",
                        "Citation Paper Title": "Title:Connectionist Temporal Modeling for Weakly Supervised Action Labeling",
                        "Citation Paper Abstract": "Abstract:We propose a weakly-supervised framework for action labeling in video, where only the order of occurring actions is required during training time. The key challenge is that the per-frame alignments between the input (video) and label (action) sequences are unknown during training. We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities. This protects the model from distractions of visually inconsistent or degenerated alignments without the need of temporal supervision. We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video. With less than 1% of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches.",
                        "Citation Paper Authors": "Authors:De-An Huang, Li Fei-Fei, Juan Carlos Niebles"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.13213v1": {
            "Paper Title": "Immersive and Interactive Visualization of 3D Spatio-Temporal Data using\n  a Space Time Hypercube",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02444v3": {
            "Paper Title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit\n  Surfaces via Range Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06199v2": {
            "Paper Title": "ABO: Dataset and Benchmarks for Real-World 3D Object Understanding",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "ex-\ntend the idea of self-augmentation to train with unlabeled\ndata, but their work is limited by the same constraints. ",
                    "Citation Text": "Valentin Deschaintre, Miika Aittala, Fredo Durand, George\nDrettakis, and Adrien Bousseau. Single-image svbrdf cap-\nture with a rendering-aware deep network. ACM Transac-\ntions on Graphics (TOG) , 37(4):128, 2018. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.09718",
                        "Citation Paper Title": "Title:Single-Image SVBRDF Capture with a Rendering-Aware Deep Network",
                        "Citation Paper Abstract": "Abstract:Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs.",
                        "Citation Paper Authors": "Authors:Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, Adrien Bousseau"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "also offers videos of common objects\nfrom 50 different categories, however they do not provide\nfull 3D mesh reconstructions.\nExisting 3D datasets typically assume very simplistic\ntexture models that are not physically realistic. To im-\nprove on this, PhotoShapes ",
                    "Citation Text": "Keunhong Park, Konstantinos Rematas, Ali Farhadi, and\nSteven M Seitz. Photoshape: Photorealistic materi-\nals for large-scale shape collections. arXiv preprint\narXiv:1809.09761 , 2018. 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.09761",
                        "Citation Paper Title": "Title:PhotoShape: Photorealistic Materials for Large-Scale Shape Collections",
                        "Citation Paper Abstract": "Abstract:Existing online 3D shape repositories contain thousands of 3D models but lack photorealistic appearance. We present an approach to automatically assign high-quality, realistic appearance models to large scale 3D shape collections. The key idea is to jointly leverage three types of online data -- shape collections, material collections, and photo collections, using the photos as reference to guide assignment of materials to shapes. By generating a large number of synthetic renderings, we train a convolutional neural network to classify materials in real photos, and employ 3D-2D alignment techniques to transfer materials to different parts of each shape model. Our system produces photorealistic, relightable, 3D shapes (PhotoShapes).",
                        "Citation Paper Authors": "Authors:Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M. Seitz"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "are both video\ndatasets that have the camera operator walk around various\nobjects, but are limited in the number of categories repre-\nsented. CO3D ",
                    "Citation Text": "Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation\nof real-life 3d category reconstruction. arXiv preprint\narXiv:2109.00512 , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.00512",
                        "Citation Paper Title": "Title:Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction",
                        "Citation Paper Abstract": "Abstract:Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale \"in-the-wild\" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at this https URL .",
                        "Citation Paper Authors": "Authors:Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "provide 2D-3D alignment for im-\nages and provide more instances and categories, howeverBenchmark Domain ClassesInstances ImagesStructure Recall@1train val test train val test-target test-query\nCUB-200-2011 Birds 200 - - - 5994 0 - 5794 15 parts 79.2% ",
                    "Citation Text": "HeeJae Jun, ByungSoo Ko, Youngjoon Kim, Insik Kim, and\nJongtack Kim. Combination of multiple global descriptors\nfor image retrieval. arXiv preprint arXiv:1903.10663 , 2019.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10663",
                        "Citation Paper Title": "Title:Combination of Multiple Global Descriptors for Image Retrieval",
                        "Citation Paper Abstract": "Abstract:Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement. However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory. In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner. The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties. In the benchmark evaluation, the proposed framework achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks. Our model implementations and pretrained models are publicly available.",
                        "Citation Paper Authors": "Authors:HeeJae Jun, Byungsoo Ko, Youngjoon Kim, Insik Kim, Jongtack Kim"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "In-Shop Clothes 25 3997 0 3985 25882 0 12612 14218 Landmarks, poses, masks 92.6% ",
                    "Citation Text": "Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\nProxy anchor loss for deep metric learning. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , June 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.13911",
                        "Citation Paper Title": "Title:Proxy Anchor Loss for Deep Metric Learning",
                        "Citation Paper Abstract": "Abstract:Existing metric learning losses can be categorized into two classes: pair-based and proxy-based losses. The former class can leverage fine-grained semantic relations between data points, but slows convergence in general due to its high training complexity. In contrast, the latter class enables fast and reliable convergence, but cannot consider the rich data-to-data relations. This paper presents a new proxy-based loss that takes advantages of both pair- and proxy-based methods and overcomes their limitations. Thanks to the use of proxies, our loss boosts the speed of convergence and is robust against noisy labels and outliers. At the same time, it allows embedding vectors of data to interact with each other in its gradients to exploit data-to-data relations. Our method is evaluated on four public benchmarks, where a standard network trained with our loss achieves state-of-the-art performance and most quickly converges.",
                        "Citation Paper Authors": "Authors:Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.11952v1": {
            "Paper Title": "UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural\n  Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ".\nLLFF A total of 5 complex forward facing camera scenes made available by the Local Light Field\nFusion paper ",
                    "Citation Text": "Mildenhall, B., Srinivasan, P.P., Cayon, R.O., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar,\nA.: Local light \ufb01eld fusion: Practical view synthesis with prescriptive sampling guidelines.\nCoRR abs/1905.00889 (2019), http://arxiv.org/abs/1905.00889",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00889",
                        "Citation Paper Title": "Title:Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines",
                        "Citation Paper Abstract": "Abstract:We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", we make use of skip connections which help pass high-frequency information\nthrough the spatial bottleneck. This mitigates the otherwise common bias of MLPs towards lower\nfrequency features ",
                    "Citation Text": "Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y ., Courville,\nA.: On the spectral bias of neural networks. In: Chaudhuri, K., Salakhutdinov, R. (eds.)\nProceedings of the 36th International Conference on Machine Learning. Proceedings of Machine\nLearning Research, vol. 97, pp. 5301\u20135310. PMLR (09\u201315 Jun 2019), https://proceedings.\nmlr.press/v97/rahaman19a.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08734",
                        "Citation Paper Title": "Title:On the Spectral Bias of Neural Networks",
                        "Citation Paper Abstract": "Abstract:Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100\\%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \\emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.",
                        "Citation Paper Authors": "Authors:Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "factorizes the MLP into a large\nset of very small MLPs. FastNeRF ",
                    "Citation Text": "Garbin, S.J., Kowalski, M., Johnson, M., Shotton, J., Valentin, J.P.C.: Fastnerf: High-\ufb01delity\nneural rendering at 200fps. 2021 IEEE/CVF International Conference on Computer Vision\n(ICCV) pp. 14326\u201314335 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10380",
                        "Citation Paper Title": "Title:FastNeRF: High-Fidelity Neural Rendering at 200FPS",
                        "Citation Paper Abstract": "Abstract:Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
                        "Citation Paper Authors": "Authors:Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", improving rendering time to 15 FPS at the cost of half of the visual quality of NeRF.\nAutoInt ",
                    "Citation Text": "Lindell, D.B., Martel, J.N., Wetzstein, G.: Autoint: Automatic integration for fast neural volume\nrendering. In: Proceedings of the conference on Computer Vision and Pattern Recognition\n(CVPR) (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.01714",
                        "Citation Paper Title": "Title:AutoInt: Automatic Integration for Fast Neural Volume Rendering",
                        "Citation Paper Abstract": "Abstract:Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",
                        "Citation Paper Authors": "Authors:David B. Lindell, Julien N. P. Martel, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.11759v1": {
            "Paper Title": "What makes you, you? Analyzing Recognition by Swapping Face Parts",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "Source - 70.7% 13.4% 0% 0%\nTarget 88.7% 13.4% 60.9% 80.5$ 81.7%\nSENet ",
                    "Citation Text": "J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition , 2018, pp. 7132\u20137141.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01507",
                        "Citation Paper Title": "Title:Squeeze-and-Excitation Networks",
                        "Citation Paper Abstract": "Abstract:The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "already investigatedTABLE III\nRANK @1 RECOGNITION RATES FOR INTER -SUBJECT SWAPPING .\nOriginal Full face Eyes Nose Mouth\nInceptionv1 ",
                    "Citation Text": "C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking\nthe inception architecture for computer vision,\u201d in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2016, pp.\n2818\u20132826.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.11456v1": {
            "Paper Title": "Metric Optimization in Penner Coordinates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12488v3": {
            "Paper Title": "Intuitive Shape Editing in Latent Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.05649v2": {
            "Paper Title": "TileGen: Tileable, Controllable Material Generation and Capture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08990v1": {
            "Paper Title": "Shadows Shed Light on 3D Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.07873v2": {
            "Paper Title": "HDR Lighting Dilation for Dynamic Range Reduction on Virtual Production\n  Stages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08698v1": {
            "Paper Title": "Towards computing complete parameter ranges in parametric modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08497v1": {
            "Paper Title": "Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape\n  Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08357v1": {
            "Paper Title": "Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "trained\non LSUN Cars, LSUN Horses, LSUN Cats, and FFHQ ",
                    "Citation Text": "Tero Karras, Samuli Laine, and Timo Aila. A style-\nbased generator architecture for generative adversarial\nnetworks. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2019. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.04948",
                        "Citation Paper Title": "Title:A Style-Based Generator Architecture for Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Timo Aila"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08343v1": {
            "Paper Title": "Realistic One-shot Mesh-based Head Avatars",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07798v1": {
            "Paper Title": "Gaussian Blue Noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07748v1": {
            "Paper Title": "Immersion Metrics for Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07707v1": {
            "Paper Title": "Variable Bitrate Neural Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.00948v3": {
            "Paper Title": "Eikonal Fields for Refractive Novel-View Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.04127v2": {
            "Paper Title": "HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular\n  Video",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "free-viewpoint videos of moving people.\nMost of these methods rely on multi-view videos \u2013 typi-\ncally expensive studio setups \u2013 while we are interested in a\nsimple monocular camera configuration.\nNeural radiance fields: NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. ECCV , 2020. 2, 3, 4, 5, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ".\n4.3. Loss and ray sampling\nLoss function: We employ both an MSE loss to match\npixel-wise appearance and a perceptual loss, LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. CVPR , 2018. 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". By do-\ning so, we can completely disable non-rigid motion opti-\nmization by setting \u03c4= 0 ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz. HyperNeRF: A higher-\ndimensional representation for topologically varying neural\nradiance fields. SIGGRAPH Asia , 2021. 1, 2, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.13228",
                        "Citation Paper Title": "Title:HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at this https URL.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ", increasing the window size as the opti-\nmization proceeds. Following Park et al. ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nICCV , 2021. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": ",\na method intended for rigidly animatable characters. In-\nstead, we focus on the free-viewpoint application and re-\ncovering pose-dependent, non-rigid deformation and out-\nperform them significantly for this application.\nConcurrent work: Xu et al. ",
                    "Citation Text": "Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.\nH-nerf: Neural radiance fields for rendering and temporal\nreconstruction of humans in motion. Advances in Neural In-\nformation Processing Systems , 34, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.13746",
                        "Citation Paper Title": "Title:H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion",
                        "Citation Paper Abstract": "Abstract:We present neural radiance fields for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance field with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject -- including both body and clothing -- and to regularize the radiance field to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities significantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.",
                        "Citation Paper Authors": "Authors:Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "trained a UNet to\nimprove the artifacts introduced by volumetric capture. The\nfollow-up work of Pandey et al. ",
                    "Citation Text": "Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pid-\nlypenskyi, Jonathan Taylor, Ricardo Martin-Brualla, Andrea\nTagliasacchi, George Papandreou, Philip Davidson, Cem Ke-\nskin, et al. V olumetric capture of humans with a single\nRGBD camera via semi-parametric learning. CVPR , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12162",
                        "Citation Paper Title": "Title:Volumetric Capture of Humans with a Single RGBD Camera via Semi-Parametric Learning",
                        "Citation Paper Abstract": "Abstract:Volumetric (4D) performance capture is fundamental for AR/VR content generation. Whereas previous work in 4D performance capture has shown impressive results in studio settings, the technology is still far from being accessible to a typical consumer who, at best, might own a single RGBD sensor. Thus, in this work, we propose a method to synthesize free viewpoint renderings using a single RGBD camera. The key insight is to leverage previously seen \"calibration\" images of a given user to extrapolate what should be rendered in a novel viewpoint from the data available in the sensor. Given these past observations from multiple viewpoints, and the current RGBD image from a fixed view, we propose an end-to-end framework that fuses both these data sources to generate novel renderings of the performer. We demonstrate that the method can produce high fidelity images, and handle extreme changes in subject pose and camera viewpoints. We also show that the system generalizes to performers not seen in the training data. We run exhaustive experiments demonstrating the effectiveness of the proposed semi-parametric model (i.e. calibration images available to the neural network) compared to other state of the art machine learned solutions. Further, we compare the method with more traditional pipelines that employ multi-view capture. We show that our framework is able to achieve compelling results, with substantially less infrastructure than previously required.",
                        "Citation Paper Authors": "Authors:Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Ricardo Martin-Brualla, Andrea Tagliasacchi, George Papandreou, Philip Davidson, Cem Keskin, Shahram Izadi, Sean Fanello"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "starts from a pre-captured body model and learns to\nmodel time-dependent dynamic textures and enforce tem-\nporal coherence. Martin-Brualla et al. ",
                    "Citation Text": "Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel\nPidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh\nKhamis, Philip Davidson, Anastasia Tkach, Peter Lincoln,\net al. LookinGood: Enhancing performance capture with\nreal-time neural re-rendering. ACM Transactions on Graph-\nics (TOG) , 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.05029",
                        "Citation Paper Title": "Title:LookinGood: Enhancing Performance Capture with Real-time Neural Re-Rendering",
                        "Citation Paper Abstract": "Abstract:Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in real-time performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach neural (re-)rendering, and our live system \"LookinGood\". Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation. We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a salient reweighing scheme of the loss function that is able to discard outliers. We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360 degree capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects. The supplementary video is available at this http URL.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan B Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.06100v1": {
            "Paper Title": "AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural\n  Images with Aperture Rendering Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "trained using stereo\npairs.6To measure the differences in depth, we used the\nscale-invariant depth error (SIDE) ",
                    "Citation Text": "David Eigen, Christian Puhrsch, and Rob Fergus. Depth\nmap prediction from a single image using a multi-scale\ndeep network. In NIPS , 2014. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2283",
                        "Citation Paper Title": "Title:Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
                        "Citation Paper Abstract": "Abstract:Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
                        "Citation Paper Authors": "Authors:David Eigen, Christian Puhrsch, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": ", which measures\nthe maximum mean discrepancy between real and generated\nimages within the inception model ",
                    "Citation Text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the Inception ar-\nchitecture for computer vision. In CVPR , 2016. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.01873v2": {
            "Paper Title": "NeuralMLS: Geometry-Aware Control Point Deformation",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", the control\npoints impose hard constraints on a distortion minimization ob-\njective. In KeypointDeformer (KPD) ",
                    "Citation Text": "Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu,\nNoah Snavely, and Angjoo Kanazawa. Keypointdeformer:\nUnsupervised 3d keypoint discovery for shape control. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11224",
                        "Citation Paper Title": "Title:KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control",
                        "Citation Paper Abstract": "Abstract:We introduce KeypointDeformer, a novel unsupervised method for shape control through automatically discovered 3D keypoints. We cast this as the problem of aligning a source 3D object to a target 3D object from the same object category. Our method analyzes the difference between the shapes of the two objects by comparing their latent representations. This latent representation is in the form of 3D keypoints that are learned in an unsupervised way. The difference between the 3D keypoints of the source and the target objects then informs the shape deformation algorithm that deforms the source object into the target object. The whole model is learned end-to-end and simultaneously discovers 3D keypoints while learning to use them for deforming object shapes. Our approach produces intuitive and semantically consistent control of shape deformations. Moreover, our discovered 3D keypoints are consistent across object category instances despite large shape variations. As our method is unsupervised, it can be readily deployed to new object categories without requiring annotations for 3D keypoints and deformations.",
                        "Citation Paper Authors": "Authors:Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", mesh reconstruction [ 13,\n36] and point cloud consolidation ",
                    "Citation Text": "Gal Metzer, Rana Hanocka, Raja Giryes, and Daniel Cohen-\nOr. Self-sampling for neural point cloud consolidation. arXiv\npreprint arXiv:2008.06471 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06471",
                        "Citation Paper Title": "Title:Self-Sampling for Neural Point Cloud Consolidation",
                        "Citation Paper Abstract": "Abstract:We introduce a novel technique for neural point cloud consolidation which learns from only the input point cloud. Unlike other point upsampling methods which analyze shapes via local patches, in this work, we learn from global subsets. We repeatedly self-sample the input point cloud with global subsets that are used to train a deep neural network. Specifically, we define source and target subsets according to the desired consolidation criteria (e.g., generating sharp points or points in sparse regions). The network learns a mapping from source to target subsets, and implicitly learns to consolidate the point cloud. During inference, the network is fed with random subsets of points from the input, which it displaces to synthesize a consolidated point set. We leverage the inductive bias of neural networks to eliminate noise and outliers, a notoriously difficult problem in point cloud consolidation. The shared weights of the network are optimized over the entire shape, learning non-local statistics and exploiting the recurrence of local-scale geometries. Specifically, the network encodes the distribution of the underlying shape surface within a fixed set of local kernels, which results in the best explanation of the underlying shape surface. We demonstrate the ability to consolidate point sets from a variety of shapes, while eliminating outliers and noise.",
                        "Citation Paper Authors": "Authors:Gal Metzer, Rana Hanocka, Raja Giryes, Daniel Cohen-Or"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.05344v1": {
            "Paper Title": "Differentiable Rendering of Neural SDFs through Reparameterization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04901v1": {
            "Paper Title": "NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "uses GAN mech-\nanism to maintain local and global consistency in the \ufb01-\nnal results. Nazeri et al. ",
                    "Citation Text": "Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,\nand Mehran Ebrahimi. Edgeconnect: Generative image\ninpainting with adversarial edge learning. arXiv preprint\narXiv:1901.00212 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00212",
                        "Citation Paper Title": "Title:EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning",
                        "Citation Paper Abstract": "Abstract:Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: this https URL",
                        "Citation Paper Authors": "Authors:Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "is pro-\nposed where the convolution is masked and re-normalized\nto utilize valid pixels only. Yu et al. ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 5505\u20135514,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07892",
                        "Citation Paper Title": "Title:Generative Image Inpainting with Contextual Attention",
                        "Citation Paper Abstract": "Abstract:Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ".\nHowever, these methods often focus on regular masks only.\nTo handle irregular masks, partial convolution ",
                    "Citation Text": "Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In Proceedings of\nthe European conference on computer vision (ECCV) , pages\n85\u2013100, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07723",
                        "Citation Paper Title": "Title:Image Inpainting for Irregular Holes Using Partial Convolutions",
                        "Citation Paper Abstract": "Abstract:Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.",
                        "Citation Paper Authors": "Authors:Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nMeanwhile, previous works try to reduce the training time\nby optimizing voxel grids of features [34, 41] and factoring\nthe radiance \ufb01eld ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance \ufb01elds. arXiv preprint\narXiv:2203.09517 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.09517",
                        "Citation Paper Title": "Title:TensoRF: Tensorial Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.05061v1": {
            "Paper Title": "Glyph from Icon -- Automated Generation of Metaphoric Glyphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04421v1": {
            "Paper Title": "Solid NURBS Conforming Scaffolding for Isogeometric Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.03809v2": {
            "Paper Title": "Music2Video: Automatic Generation of Music Video with fusion of audio\n  and text",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ".\n2.4 Visualizing Audio\nThe extension of CLIP which is originally a visual-\nlanguage model allows audio-based image generation. A\nmethod that is built on the pretrained Big-GAN model uti-\nlizes the intensity controlling property of the noise vec-\ntor ",
                    "Citation Text": "Andrew Brock, Jeff Donahue, and Karen Simonyan.\nLarge scale gan training for high \ufb01delity natural image\nsynthesis, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11096",
                        "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                        "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.06006v3": {
            "Paper Title": "ConTesse: Accurate Occluding Contours for Subdivision Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01473v3": {
            "Paper Title": "Neural Point Light Fields",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "and Neural Point Light Fields. All methods were trained on the same set of scenes from the Waymo Open Dataset ",
                    "Citation Text": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 2446\u20132454, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04838",
                        "Citation Paper Title": "Title:Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
                        "Citation Paper Abstract": "Abstract:The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at this http URL.",
                        "Citation Paper Authors": "Authors:Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, Dragomir Anguelov"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "use\na hybrid representation that stores implicit functions in a\nvoxel grid. NeRF++ proposes to separate background and\nforeground scene components ",
                    "Citation Text": "Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\n\ufb01elds. arXiv preprint arXiv:2010.07492 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07492",
                        "Citation Paper Title": "Title:NeRF++: Analyzing and Improving Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "showed that optimizing point locations from an ini-\ntial point cloud, together with their novel view synthesis\npipeline, can compensate for errors during reconstruction\nfrom MVS. These methods and similar ",
                    "Citation Text": "Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graph-\nics. pages 696\u2013712, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08240",
                        "Citation Paper Title": "Title:Neural Point-Based Graphics",
                        "Citation Paper Abstract": "Abstract:We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.",
                        "Citation Paper Authors": "Authors:Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "or offer a scaffold for learned fea-\ntures [34, 17]. Riegler and Koltun [34, 33] propose such\ngeometric scaffolds living on MVS-meshes. Kopanas et al. ",
                    "Citation Text": "Georgios Kopanas, Julien Philip, Thomas Leimk \u00a8uhler, and\nGeorge Drettakis. Point-based neural rendering with per-\nview optimization. In Computer Graphics Forum , vol-\nume 40, pages 29\u201343. Wiley Online Library, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.02369",
                        "Citation Paper Title": "Title:Point-Based Neural Rendering with Per-View Optimization",
                        "Citation Paper Abstract": "Abstract:There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel-view synthesis. A key element of our approach is our new differentiable point-based pipeline, based on bi-directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi-view harmonization and stylization in addition to novel-view synthesis.",
                        "Citation Paper Authors": "Authors:Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, George Drettakis"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "have tackled this\nissue at test time evaluation by either predicting the sam-pling regions [25, 4] or explicitly extracting proxy geometry ",
                    "Citation Text": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua,\nand Christian Theobalt. Neural sparse voxel \ufb01elds. arXiv\npreprint arXiv:2007.11571 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.03128v1": {
            "Paper Title": "Spatial-Temporal Adaptive Graph Convolution with Attention Network for\n  Traffic Forecasting",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "is an end-to-end solution for\ntraf\ufb01c forecasting that captures spatial, short-term, and long-\nterm periodical dependencies. ",
                    "Citation Text": "Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal\ngraph convolutional networks: A deep learning framework for\ntraf\ufb01c forecasting. In IJCAI, pages 3634\u20133640, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.04875",
                        "Citation Paper Title": "Title:Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting",
                        "Citation Paper Abstract": "Abstract:Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.",
                        "Citation Paper Authors": "Authors:Bing Yu, Haoteng Yin, Zhanxing Zhu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", which is an end-to-end solution for traf\ufb01c fore-\ncasting that captures spatial, short-term, and long-term\nperiodical dependencies.\n\u000fST-UNet. Spatial-Temporal U-Net ",
                    "Citation Text": "Yu, Bing, Haoteng Yin, and Zhanxing Zhu.\u201cSt-Unet: A Spatio-\nTemporal u-Network for Graph-Structured Time Series Model-\ning.\u201d ArXiv Preprint arXiv:1903.05631, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.05631",
                        "Citation Paper Title": "Title:ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling",
                        "Citation Paper Abstract": "Abstract:The spatio-temporal graph learning is becoming an increasingly important object of graph study. Many application domains involve highly dynamic graphs where temporal information is crucial, e.g. traffic networks and financial transaction graphs. Despite the constant progress made on learning structured data, there is still a lack of effective means to extract dynamic complex features from spatio-temporal structures. Particularly, conventional models such as convolutional networks or recurrent neural networks are incapable of revealing the temporal patterns in short or long terms and exploring the spatial properties in local or global scope from spatio-temporal graphs simultaneously. To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling, the unpooling (ST-Unpool) restores the original structure of spatio-temporal graphs and resumes regular intervals within graph sequences. Experiments on spatio-temporal prediction tasks demonstrate that our model effectively captures comprehensive features in multiple scales and achieves substantial improvements over mainstream methods on several real-world datasets.",
                        "Citation Paper Authors": "Authors:Bing Yu, Haoteng Yin, Zhanxing Zhu"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "combines\ngraph convolution networks with recurrent neural networks in\nan encoder-decoder manner. ",
                    "Citation Text": "Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal\ngraph convolutional networks: A deep learning framework for\ntraf\ufb01c forecasting. In IJCAI, pages 3634\u20133640, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.04875",
                        "Citation Paper Title": "Title:Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting",
                        "Citation Paper Abstract": "Abstract:Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.",
                        "Citation Paper Authors": "Authors:Bing Yu, Haoteng Yin, Zhanxing Zhu"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "takes one-hop neighbors\nas neighborhoods and de\ufb01nes the weighting function as various\naggregators over neighborhoods. Based on GCN, ",
                    "Citation Text": "Li, Yaguang, Rose Yu, Cyrus Shahabi, and Yan Liu. \u201cDiffusion\nConvolutional Recurrent Neural Network: Data-Driven Traf\ufb01c\nForecasting.\u201d ArXiv Preprint arXiv:1707.01926, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01926",
                        "Citation Paper Title": "Title:Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
                        "Citation Paper Abstract": "Abstract:Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Yaguang Li, Rose Yu, Cyrus Shahabi, Yan Liu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "introduced a Cheby-\nshev polynomial parametrization for the spectral \ufb01lter. ",
                    "Citation Text": "Hamilton, Will, Zhitao Ying, and Jure Leskovec. \u201cInductive\nRepresentation Learning on Large Graphs.\u201d Advances in Neural\nInformation Processing Systems 30 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.05762v2": {
            "Paper Title": "Nanomatrix: Scalable Construction of Crowded Biological Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08860v1": {
            "Paper Title": "Optimizing Indoor Navigation Policies For Spatial Distancing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02015v1": {
            "Paper Title": "APES: Articulated Part Extraction from Sprite Sheets",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "proposed an unsuper-\nvised part model to infer parts in a 3D motion sequence.\nMultiBodySync ",
                    "Citation Text": "Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Fed-\nerica Arrigoni, Shi-Min Hu, and Leonidas Guibas. Multi-\nbodysync: Multi-body segmentation and motion estimation\nvia 3d scan synchronization. In CVPR , 2021. 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06605",
                        "Citation Paper Title": "Title:MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization",
                        "Citation Paper Abstract": "Abstract:We present MultiBodySync, a novel, end-to-end trainable multi-body motion segmentation and rigid registration framework for multiple input 3D point clouds. The two non-trivial challenges posed by this multi-scan multibody setting that we investigate are: (i) guaranteeing correspondence and segmentation consistency across multiple input point clouds capturing different spatial arrangements of bodies or body parts; and (ii) obtaining robust motion-based rigid body segmentation applicable to novel object categories. We propose an approach to address these issues that incorporates spectral synchronization into an iterative deep declarative network, so as to simultaneously recover consistent correspondences as well as motion segmentation. At the same time, by explicitly disentangling the correspondence and motion segmentation estimation modules, we achieve strong generalizability across different object categories. Our extensive evaluations demonstrate that our method is effective on various datasets ranging from rigid parts in articulated objects to individually moving objects in a 3D scene, be it single-view or full point clouds.",
                        "Citation Paper Authors": "Authors:Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Federica Arrigoni, Shi-Min Hu, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ".\n3D mobility segmentation. Mobility-based segmentation\nfor 3D point clouds has also been investigated in recent\nworks [22, 23, 51, 53]. Yi et al. ",
                    "Citation Text": "Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao\nSu, and Leonidas Guibas. Deep part induction from articu-\nlated object pairs. ACM TOG , 37(6), 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.07417",
                        "Citation Paper Title": "Title:Deep Part Induction from Articulated Object Pairs",
                        "Citation Paper Abstract": "Abstract:Object functionality is often expressed through part articulation -- as when the two rigid parts of a scissor pivot against each other to perform the cutting function. Such articulations are often similar across objects within the same functional category. In this paper, we explore how the observation of different articulation states provides evidence for part structure and motion of 3D objects. Our method takes as input a pair of unsegmented shapes representing two different articulation states of two functionally related objects, and induces their common parts along with their underlying rigid motion. This is a challenging setting, as we assume no prior shape structure, no prior shape category information, no consistent shape orientation, the articulation states may belong to objects of different geometry, plus we allow inputs to be noisy and partial scans, or point clouds lifted from RGB images. Our method learns a neural network architecture with three modules that respectively propose correspondences, estimate 3D deformation flows, and perform segmentation. To achieve optimal performance, our architecture alternates between correspondence, deformation flow, and segmentation prediction iteratively in an ICP-like fashion. Our results demonstrate that our method significantly outperforms state-of-the-art techniques in the task of discovering articulated parts of objects. In addition, our part induction is object-class agnostic and successfully generalizes to new and unseen objects.",
                        "Citation Paper Authors": "Authors:Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.15848v1": {
            "Paper Title": "Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for\n  Multi-view Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". The radiance network is parameterized by a 4-layer MLP with 256 hidden\nunits. Positional encoding ",
                    "Citation Text": "Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Doso-\nvitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance \ufb01elds for unconstrained\nphoto collections. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 7210\u20137219, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02268",
                        "Citation Paper Title": "Title:NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "use the weight function that involved SDF during the rendering process to make colors and geometry\ncloser. UNISURF ",
                    "Citation Text": "Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit sur-\nfaces and radiance \ufb01elds for multi-view reconstruction. In Proceedings of the IEEE International\nConference on Computer Vision , pages 5589\u20135599, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10078",
                        "Citation Paper Title": "Title:UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction",
                        "Citation Paper Abstract": "Abstract:Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.",
                        "Citation Paper Authors": "Authors:Michael Oechsle, Songyou Peng, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "reconstructs surfaces with neural networks by representing\nthe geometry as the zero level set of an MLP that is considered to be an SDF. MVSDF ",
                    "Citation Text": "Jingyang Zhang, Yao Yao, and Long Quan. Learning signed distance \ufb01eld for multi-view\nsurface reconstruction. In Proceedings of the IEEE International Conference on Computer\nVision , pages 6525\u20136534, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09964",
                        "Citation Paper Title": "Title:Learning Signed Distance Field for Multi-view Surface Reconstruction",
                        "Citation Paper Abstract": "Abstract:Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.",
                        "Citation Paper Authors": "Authors:Jingyang Zhang, Yao Yao, Long Quan"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "represented by the network.\nNeural implicit surface reconstruction. Neural implicit \ufb01eld is a new way to represent the ge-\nometry of objects. With NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,\nand Ren Ng. Nerf: Representing scenes as neural radiance \ufb01elds for view synthesis. In\nProceedings of the European Conference on Computer Vision , pages 405\u2013421, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.15846v1": {
            "Paper Title": "SaccadeNet: Towards Real-time Saccade Prediction for Virtual Reality\n  Infinite Walking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.15768v1": {
            "Paper Title": "SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary\n  Image collections",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "requires training the neural volume twice while\nkeeping the previous camera parameter optimization. GNeRF ",
                    "Citation Text": "Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi\nYu. GNeRF: GAN-based Neural Radiance Field without Posed Camera. In IEEE International\nConference on Computer Vision (ICCV) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15606",
                        "Citation Paper Title": "Title:GNeRF: GAN-based Neural Radiance Field without Posed Camera",
                        "Citation Paper Abstract": "Abstract:We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",
                        "Citation Paper Authors": "Authors:Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "proposes a coarse to \ufb01ne optimization using a varying number of Fourier frequencies\nand requires rough camera poses and NeRF-- ",
                    "Citation Text": "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF \u2212\u2212:\nNeural radiance \ufb01elds without known camera parameters. ArXiv e-prints , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07064",
                        "Citation Paper Title": "Title:NeRF--: Neural Radiance Fields Without Known Camera Parameters",
                        "Citation Paper Abstract": "Abstract:Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (NeRF) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose NeRF$--$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with NeRF training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.",
                        "Citation Paper Authors": "Authors:Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.15572v1": {
            "Paper Title": "3PSDF: Three-Pole Signed Distance Function for Learning Surfaces with\n  Arbitrary Topologies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.02762v3": {
            "Paper Title": "DrawingInStyles: Portrait Image Generation and Editing with Spatially\n  Conditioned StyleGAN",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "improves\nthe performance of pix2pix and generates higher-resolution\nresults given condition images. Scribbler by Sangkloy et\nal. ",
                    "Citation Text": "P . Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays, \u201cScribbler: Con-\ntrolling deep image synthesis with sketch and color,\u201d in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) .\nIEEE, 2017, pp. 5400\u20135409.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00835",
                        "Citation Paper Title": "Title:Scribbler: Controlling Deep Image Synthesis with Sketch and Color",
                        "Citation Paper Abstract": "Abstract:Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.",
                        "Citation Paper Authors": "Authors:Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "also adopt\nthe idea of image inpainting and present high-quality face\nediting results with simple guiding sketches and colored\nstrokes within local regions. A similar idea is adopted by\nYu et al. ",
                    "Citation Text": "J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, \u201cFree-form\nimage inpainting with gated convolution,\u201d in IEEE International\nConference on Computer Vision (ICCV) . IEEE, 2019, pp. 4471\u20134480.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03589",
                        "Citation Paper Title": "Title:Free-Form Image Inpainting with Gated Convolution",
                        "Citation Paper Abstract": "Abstract:We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: this https URL",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "propose a spatially-adaptive normal-\nization layer for synthesizing photo-realistic images given\nan input semantic layout. Their system supports effective\nimage editing via changing the semantic map of a target\nimage. Gu et al. ",
                    "Citation Text": "S. Gu, J. Bao, H. Yang, D. Chen, F. Wen, and L. Yuan, \u201cMask-\nguided portrait editing with conditional gans,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) . IEEE, 2019,\npp. 3436\u20133445.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10346",
                        "Citation Paper Title": "Title:Mask-Guided Portrait Editing with Conditional GANs",
                        "Citation Paper Abstract": "Abstract:Portrait editing is a popular subject in photo manipulation. The Generative Adversarial Network (GAN) advances the generating of realistic faces and allows more face editing. In this paper, we argue about three issues in existing techniques: diversity, quality, and controllability for portrait synthesis and editing. To address these issues, we propose a novel end-to-end learning framework that leverages conditional GANs guided by provided face masks for generating faces. The framework learns feature embeddings for every face component (e.g., mouth, hair, eye), separately, contributing to better correspondences for image translation, and local face editing. With the mask, our network is available to many applications, like face synthesis driven by mask, face Swap+ (including hair in swapping), and local manipulation. It can also boost the performance of face parsing a bit as an option of data augmentation.",
                        "Citation Paper Authors": "Authors:Shuyang Gu, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, Lu Yuan"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "have been\nwidely adopted as generative models for image generation\nproblems. For example, pix2pix proposed by Isola et al. ",
                    "Citation Text": "P . Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-image trans-\nlation with conditional adversarial networks,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) . IEEE, 2017,\npp. 1125\u20131134.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07004",
                        "Citation Paper Title": "Title:Image-to-Image Translation with Conditional Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
                        "Citation Paper Authors": "Authors:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.13996v2": {
            "Paper Title": "Video2StyleGAN: Disentangling Local and Global Variations in a Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14929v1": {
            "Paper Title": "Neural Volumetric Object Selection",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "in Eq. (6). We verify the effectiveness of this pre-trained 3D\nU-Net by replacing it with a Kinetics ",
                    "Citation Text": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics\nhuman action video dataset. arXiv:1705.06950 , 2017. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06950",
                        "Citation Paper Title": "Title:The Kinetics Human Action Video Dataset",
                        "Citation Paper Abstract": "Abstract:We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
                        "Citation Paper Authors": "Authors:Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "looked at composing photo-\nrealistic scenes of captured objects. Yang et al. ",
                    "Citation Text": "Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learn-\ning object-compositional neural radiance field for editable\nscene rendering. In ICCV , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.01847",
                        "Citation Paper Title": "Title:Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering",
                        "Citation Paper Abstract": "Abstract:Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",
                        "Citation Paper Authors": "Authors:Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "introduced shape priors\nto incorporate more expressive object information.\nMore recently, more expressive object information has\nbeen incorporated via deep nets [23,26 \u201328,49]. For instance,\ngiven user input, Xu et al . ",
                    "Citation Text": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas\nHuang. Deep interactive object selection. In CVPR , 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04042",
                        "Citation Paper Title": "Title:Deep Interactive Object Selection",
                        "Citation Paper Abstract": "Abstract:Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep learning based algorithm which has a much better understanding of objectness and thus can reduce user interactions to just a few clicks. Our algorithm transforms user provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model user click patterns and use them to fine tune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN 8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects clearly demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.",
                        "Citation Paper Authors": "Authors:Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.14886v1": {
            "Paper Title": "Neural Shape Mating: Self-Supervised Object Assembly with Adversarial\n  Shape Priors",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", which are continuous, differentiable, and can be\nqueried at arbitrary resolution. DeepSDF ",
                    "Citation Text": "Jeong Joon Park, Peter Florence, Julian Straub, Richard New-\ncombe, and Steven Lovegrove. Deepsdf: Learning continuous\nsigned distance functions for shape representation. In CVPR ,\n2019. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". Even when the ground truth is\navailable and a unimodal correct output exists, adversarial\nlosses lead to enhanced detail and more realistic outputs, e.g.,\nfor super-resolution ",
                    "Citation Text": "Alice Lucas, Santiago Lopez-Tapia, Rafael Molina, and Agge-\nlos K Katsaggelos. Generative adversarial networks and per-\nceptual losses for video super-resolution. TIP, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05764",
                        "Citation Paper Title": "Title:Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution",
                        "Citation Paper Abstract": "Abstract:Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this work, we propose a Generative Adversarial Network(GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with a new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the Mean-Squared-Error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et al., 2018) to compare state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms current state-of-the-art SR models, both quantitatively and qualitatively.",
                        "Citation Paper Authors": "Authors:Alice Lucas, Santiago Lopez Tapia, Rafael Molina, Aggelos K. Katsaggelos"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", or\nwhen the ground truth is available but multiple plausible out-\nputs exist, as in MUNIT ",
                    "Citation Text": "Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.\nMultimodal unsupervised image-to-image translation. In\nECCV , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04732",
                        "Citation Paper Title": "Title:Multimodal Unsupervised Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "or semantic keypoints [ 46,53] or mappings to a normalized\ncoordinate space ",
                    "Citation Text": "He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,\nShuran Song, and Leonidas J Guibas. Normalized objectcoordinate space for category-level 6d object pose and size\nestimation. In CVPR , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02970",
                        "Citation Paper Title": "Title:Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation",
                        "Citation Paper Abstract": "Abstract:The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to \"instance-level\" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.",
                        "Citation Paper Authors": "Authors:He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "trains a single model that can generate\ncuboid primitives across all classes. ",
                    "Citation Text": "R Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang,\nEllen Jiang, Paul Guerrero, Niloy J Mitra, and Daniel Ritchie.\nShapeassembly: Learning to generate programs for 3d shape\nstructure synthesis. ACM TOG , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08026",
                        "Citation Paper Title": "Title:ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis",
                        "Citation Paper Abstract": "Abstract:Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly, a domain-specific \"assembly-language\" for 3D shape structures. ShapeAssembly programs construct shapes by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. Its functions are parameterized with free variables, so that one program structure is able to capture a family of related shapes. We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. The program captures the subset of variability that is interpretable and editable. The deep model captures correlations across shape collections that are hard to express procedurally. We evaluate our approach by comparing shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds.",
                        "Citation Paper Authors": "Authors:R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "trains per-class\nmodels that generate objects by assembling volumetric prim-\nitives (cuboids). ",
                    "Citation Text": "Salman H Khan, Yulan Guo, Munawar Hayat, and Nick\nBarnes. Unsupervised primitive discovery for improved 3d\ngenerative modeling. In CVPR , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03650",
                        "Citation Paper Title": "Title:Unsupervised Primitive Discovery for Improved 3D Generative Modeling",
                        "Citation Paper Abstract": "Abstract:3D shape generation is a challenging problem due to the high-dimensional output space and complex part configurations of real-world objects. As a result, existing algorithms experience difficulties in accurate generative modeling of 3D shapes. Here, we propose a novel factorized generative model for 3D shape generation that sequentially transitions from coarse to fine scale shape generation. To this end, we introduce an unsupervised primitive discovery algorithm based on a higher-order conditional random field model. Using the primitive parts for shapes as attributes, a parameterized 3D representation is modeled in the first stage. This representation is further refined in the next stage by adding fine scale details to shape. Our results demonstrate improved representation ability of the generative model and better quality samples of newly generated 3D shapes. Further, our primitive generation approach can accurately parse common objects into a simplified representation.",
                        "Citation Paper Authors": "Authors:Salman H. Khan, Yulan Guo, Munawar Hayat, Nick Barnes"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "3D shape assembly. A distinct, but related, line of work\ninvestigates generative models that learn to represent objects\nas concatenations of simple 3D shapes. ",
                    "Citation Text": "Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A.\nEfros, and Jitendra Malik. Learning shape abstractions by\nassembling volumetric primitives. In CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00404",
                        "Citation Paper Title": "Title:Learning Shape Abstractions by Assembling Volumetric Primitives",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives. In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data. We demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure. We also examine applications for image-based prediction as well as shape manipulation.",
                        "Citation Paper Authors": "Authors:Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, Jitendra Malik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.14657v1": {
            "Paper Title": "COFS: Controllable Furniture layout Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". One of the early attempts to use a truly permutation\ninvariant set transformer was in Set Transformer ",
                    "Citation Text": "Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer:\nA framework for attention-based permutation-invariant neural networks. In Proceedings of the 36th\nInternational Conference on Machine Learning , pages 3744\u20133753, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00825",
                        "Citation Paper Title": "Title:Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
                        "Citation Paper Abstract": "Abstract:Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.",
                        "Citation Paper Authors": "Authors:Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, Yee Whye Teh"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "shows that with a carefully designed masking schedule, high\nquality image samples can be generated from MLMs with parallel sampling which makes them much\nfaster than autoregressive models. Edi-BERT ",
                    "Citation Text": "Thibaut Issenhuth, Ugo Tanielian, J\u00e9r\u00e9mie Mary, and David Picard. Edibert, a generative model for image\nediting. arXiv preprint arXiv:2111.15264 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.15264",
                        "Citation Paper Title": "Title:EdiBERT, a generative model for image editing",
                        "Citation Paper Abstract": "Abstract:Advances in computer vision are pushing the limits of im-age manipulation, with generative models sampling detailed images on various tasks. However, a specialized model is often developed and trained for each specific task, even though many image edition tasks share similarities. In denoising, inpainting, or image compositing, one always aims at generating a realistic image from a low-quality one. In this paper, we aim at making a step towards a unified approach for image editing. To do so, we propose EdiBERT, a bi-directional transformer trained in the discrete latent space built by a vector-quantized auto-encoder. We argue that such a bidirectional model is suited for image manipulation since any patch can be re-sampled conditionally to the whole image. Using this unique and straightforward training objective, we show that the resulting model matches state-of-the-art performances on a wide variety of tasks: image denoising, image completion, and image composition.",
                        "Citation Paper Authors": "Authors:Thibaut Issenhuth, Ugo Tanielian, J\u00e9r\u00e9mie Mary, David Picard"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", which uses a Gibbs-sampling approach to sample from\na pre-trained BERT model. Follow up work in Mansimov et al. ",
                    "Citation Text": "Elman Mansimov, Alex Wang, and Kyunghyun Cho. A generalized framework of sequence generation\nwith application to undirected sequence models, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12790",
                        "Citation Paper Title": "Title:A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models",
                        "Citation Paper Abstract": "Abstract:Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs significantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample & Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-the-art on WMT'14 English-German translation.",
                        "Citation Paper Authors": "Authors:Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.12997v3": {
            "Paper Title": "Hierarchical Nearest Neighbor Graph Embedding for Efficient\n  Dimensionality Reduction",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", have\nfurther improved t-SNE so it can converge faster and scale\nbetter to larger datasets. Much of the followup work fol-\nlows the same direction. For example, LargeVis ",
                    "Citation Text": "Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Vi-\nsualizing large-scale and high-dimensional data. In Proceed-\nings of the 25th international conference on world wide web ,\npages 287\u2013297, 2016. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.00370",
                        "Citation Paper Title": "Title:Visualizing Large-scale and High-dimensional Data",
                        "Citation Paper Abstract": "Abstract:We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.",
                        "Citation Paper Authors": "Authors:Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.14573v1": {
            "Paper Title": "ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.02374v3": {
            "Paper Title": "As-Continuous-As-Possible Extrusion Fabrication of Surface Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.07425v4": {
            "Paper Title": "NeuralSound: Learning-based Modal Sound Synthesis With Acoustic Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06397v3": {
            "Paper Title": "N-Cloth: Predicting 3D Cloth Deformation with Mesh-Based Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02157v3": {
            "Paper Title": "A Web of Confocal Parabolas in a Grid of Hexagons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13150v1": {
            "Paper Title": "Semantically Supervised Appearance Decomposition for Virtual Staging\n  from a Single Panorama",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11951v2": {
            "Paper Title": "Diffuse Map Guiding Unsupervised Generative Adversarial Network for\n  SVBRDF Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06076v4": {
            "Paper Title": "PVT: Point-Voxel Transformer for Point Cloud Learning",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "62.60 94.31 98.42 79.13 0.00 26.71 55.21 66.21 83.32 86.83 47.64 68.32 56.41 52.12\nIAF-Net ",
                    "Citation Text": "M. Xu, Z. Zhou, J. Zhang, and Y . Qiao. Investigate in-\ndistinguishable points in semantic segmentation of 3d point\ncloud. Proceedings of the AAAI Conference on Arti\ufb01cial In-\ntelligence , abs/2103.10339, 2021. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10339",
                        "Citation Paper Title": "Title:Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud",
                        "Citation Paper Abstract": "Abstract:This paper investigates the indistinguishable points (difficult to predict label) in semantic segmentation for large-scale 3D point clouds. The indistinguishable points consist of those located in complex boundary, points with similar local textures but different categories, and points in isolate small hard areas, which largely harm the performance of 3D semantic segmentation. To address this challenge, we propose a novel Indistinguishable Area Focalization Network (IAF-Net), which selects indistinguishable points adaptively by utilizing the hierarchical semantic features and enhances fine-grained features for points especially those indistinguishable points. We also introduce multi-stage loss to improve the feature representation in a progressive way. Moreover, in order to analyze the segmentation performances of indistinguishable areas, we propose a new evaluation metric called Indistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the comparable results with state-of-the-art performance on several popular 3D point cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other methods on IPBM.",
                        "Citation Paper Authors": "Authors:Mingye Xu, Zhipeng Zhou, Junhao Zhang, Yu Qiao"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ". Moreover, PVT attains mIoU of 69.2%(a) Top row: features aggregated from the voxel branch.\n(b) Bottom row: features extracted from the point branch.Figure 4. We demonstrate the output features extracted from two branches using Open3D ",
                    "Citation Text": "Q.-Y . Zhou, J. Park, and V . Koltun. Open3D: A modern li-\nbrary for 3D data processing. arXiv:1801.09847 , 2018. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.09847",
                        "Citation Paper Title": "Title:Open3D: A Modern Library for 3D Data Processing",
                        "Citation Paper Abstract": "Abstract:Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.",
                        "Citation Paper Authors": "Authors:Qian-Yi Zhou, Jaesik Park, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", which includes\n273 million points from six indoor areas of three different\nbuildings. Each point is annotated with a semantic label\nfrom 13 classes\u2014e.g., beam, bookcase, chair, column, and\nwindow. We follow ",
                    "Citation Text": "L. Tchapmi, C. Choy, I. Armeni, J. Y . Gwak, and S. Savarese.\nSegcloud: Semantic segmentation of 3d point clouds. 2017\nInternational Conference on 3D Vision (3DV) , 2017. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07563",
                        "Citation Paper Title": "Title:SEGCloud: Semantic Segmentation of 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks (NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, this http URL), and show performance comparable or superior to the state-of-the-art on all datasets.",
                        "Citation Paper Authors": "Authors:Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "proposed PT1to extract global features by introduc-\ning the dot-product SA mechanism. ",
                    "Citation Text": "M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin,\nand S.-M. Hu. Pct: Point cloud transformer. Computational\nVisual Media , 7(2):187\u2013199, Apr 2021. 1, 2, 3, 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09688",
                        "Citation Paper Title": "Title:PCT: Point cloud transformer",
                        "Citation Paper Abstract": "Abstract:The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",
                        "Citation Paper Authors": "Authors:Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "recently introduced Swin Transformer to in-\ncorporate inductive bias for spatial locality, hierarchy and\ntranslation invariance.2.4. Transformers on point cloud\nRecently, plenty of researchers have attempted to explore\nTransformer-based architectures for point cloud learning. ",
                    "Citation Text": "N. Engel, V . Belagiannis, and K. Dietmayer. Point trans-\nformer. CoRR , abs/2011.00931, 2020. 1, 2, 3, 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.00931",
                        "Citation Paper Title": "Title:Point Transformer",
                        "Citation Paper Abstract": "Abstract:In this work, we present Point Transformer, a deep neural network that operates directly on unordered and unstructured point sets. We design Point Transformer to extract local and global features and relate both representations by introducing the local-global attention mechanism, which aims to capture spatial point relations and shape information. For that purpose, we propose SortNet, as part of the Point Transformer, which induces input permutation invariance by selecting points based on a learned score. The output of Point Transformer is a sorted and permutation invariant feature list that can directly be incorporated into common computer vision applications. We evaluate our approach on standard classification and part segmentation benchmarks to demonstrate competitive results compared to the prior work. Code is publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Nico Engel, Vasileios Belagiannis, Klaus Dietmayer"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "further\nproposed self-attention to visualize and interpret sentence\nembeddings. Subsequent works employed self-attention\nlayers to replace some or all the spatial convolution layers,\nsuch as Transformer for machine translation ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. Advances in Neural Information Processing Sys-\ntems (NeurIPS) , 2017. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "have attempted to explore Transformer-based\narchitectures for point cloud completion and achieved sig-\nni\ufb01cant performance on all completion tasks. Recently, ",
                    "Citation Text": "X. Yu, L. Tang, Y . Rao, T. Huang, J. Zhou, and J. Lu. Point-\nbert: Pre-training 3d point cloud transformers with masked\npoint modeling. CoRR , abs/2111.14819, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14819",
                        "Citation Paper Title": "Title:Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
                        "Citation Paper Abstract": "Abstract:We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "proposed the SE(3)-Transformer, a variant of the self-\nattention module for 3D point clouds and graphs, which\nis equivariant under continuous 3D roto-translations. Late, ",
                    "Citation Text": "X. Yu, Y . Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou. Pointr:\nDiverse point cloud completion with geometry-aware trans-\nformers. In 2021 IEEE/CVF International Conference on\nComputer Vision, ICCV 2021, Montreal, QC, Canada, Octo-\nber 10-17, 2021 , pages 12478\u201312487. IEEE, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08839",
                        "Citation Paper Title": "Title:PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers",
                        "Citation Paper Abstract": "Abstract:Point clouds captured in real-world applications are often incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new benchmarks and the existing ones. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, Jie Zhou"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "proposed a neural machine translation method with\nan attention mechanism, in which attention weight is com-\nputed through the hidden state of an RNN. Then ",
                    "Citation Text": "Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou,\nand Y . Bengio. A structured self-attentive sentence embed-\nding. in ICLR , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.03130",
                        "Citation Paper Title": "Title:A Structured Self-attentive Sentence Embedding",
                        "Citation Paper Abstract": "Abstract:This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
                        "Citation Paper Authors": "Authors:Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.12183v2": {
            "Paper Title": "StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D\n  Mutual Learning",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": ", we measure the short and long term consistency\nusing the warped LPIPS metric ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recognition\n(CVPR) , pages 586\u2013595, 2018. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "dy-\nnamically adjusts inter-channel distributions based on relax-\nation and regularization.\nThese 2D-based methods lack a spatial consistency con-\nstraint and 3D scene perception, thus do not have the ability\nto maintain long-term consistency in our task. Huang et\nal. ",
                    "Citation Text": "Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh\nSingh, and Ming-Hsuan Yang. Learning to stylize novel\nviews. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 13869\u201313878,\n2021. 2, 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.13509",
                        "Citation Paper Title": "Title:Learning to Stylize Novel Views",
                        "Citation Paper Abstract": "Abstract:We tackle a 3D scene stylization problem - generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to results that are blurry or not consistent across different views. We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Second, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches.",
                        "Citation Paper Authors": "Authors:Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, Ming-Hsuan Yang"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "extend stylization to 3D scenes where a 3D scene is\nrepresented as a point cloud. It ensures consistency even in\n360\u000eunbounded scenes. ",
                    "Citation Text": "Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-\nSheng Lai, and Wei-Chen Chiu. Stylizing 3D scene via im-\nplicit representation and HyperNetwork. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision (WACV) , pages 1475\u20131484, 2022. 2, 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.13016",
                        "Citation Paper Title": "Title:Stylizing 3D Scene via Implicit Representation and HyperNetwork",
                        "Citation Paper Abstract": "Abstract:In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high-quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance fields model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance fields model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.",
                        "Citation Paper Authors": "Authors:Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "proposed a method that em-\nbeds whitening and coloring transformation (WCT) to gen-\nerate high resolution stylized images. Methods based on it\nsprung up like PhotoWCT ",
                    "Citation Text": "Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and\nJan Kautz. A closed-form solution to photorealistic imagestylization. In Proceedings of the European Conference on\nComputer Vision (ECCV) , pages 453\u2013468, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06474",
                        "Citation Paper Title": "Title:A Closed-form Solution to Photorealistic Image Stylization",
                        "Citation Paper Abstract": "Abstract:Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at this https URL .",
                        "Citation Paper Authors": "Authors:Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "is a pio-neering work in this \ufb01eld focusing on an optimization-based\nscheme. For faster stylization, follow-up works turn to\nleverage feed-forward neural networks, such as Avatar ",
                    "Citation Text": "Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-\nnet: Multi-scale zero-shot style transfer by feature decora-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 8242\u2013\n8250, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03857",
                        "Citation Paper Title": "Title:Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration",
                        "Citation Paper Abstract": "Abstract:Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efficiency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efficient yet effective Avatar-Net that enables visually plausible multi-scale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multi-scale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efficiency of the proposed method in generating high-quality stylized images, with a series of applications include multiple style integration, video stylization and etc.",
                        "Citation Paper Authors": "Authors:Lu Sheng, Ziyi Lin, Jing Shao, Xiaogang Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "further models the radiance \ufb01elds\nof the scene as particles emitting and blocking lights. The\nfollowing works extend NeRF to octree structure ",
                    "Citation Text": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel \ufb01elds. In Advances\nin Neural Information Processing Systems (NeurIPS) , 2020.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.12231v1": {
            "Paper Title": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High\n  Resolutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11880v1": {
            "Paper Title": "Hierarchical Vectorization for Portrait Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11733v1": {
            "Paper Title": "Single-View View Synthesis in the Wild with Learned Adaptive Multiplane\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11659v1": {
            "Paper Title": "Fast GPU bounding boxes on tree-structured scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.01605v2": {
            "Paper Title": "Toward Modeling Creative Processes for Algorithmic Painting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10326v3": {
            "Paper Title": "ShapeFormer: Transformer-based Shape Completion via Sparse\n  Representation",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ". And to evaluate completion quality for high am-\nbiguity setting, we follow prior work ",
                    "Citation Text": "Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3d\npoint cloud generative adversarial network based on tree\nstructured graph convolutions, 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06292",
                        "Citation Paper Title": "Title:3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel generative adversarial network (GAN) for 3D point clouds generation, which is called tree-GAN. To achieve state-of-the-art performance for multi-class 3D point cloud generation, a tree-structured graph convolution network (TreeGCN) is introduced as a generator for tree-GAN. Because TreeGCN performs graph convolutions within a tree, it can use ancestor information to boost the representation power for features. To evaluate GANs for 3D point clouds accurately, we develop a novel evaluation metric called Frechet point cloud distance (FPD). Experimental results demonstrate that the proposed tree-GAN outperforms state-of-the-art GANs in terms of both conventional metrics and FPD, and can generate point clouds for different semantic parts without prior knowledge.",
                        "Citation Paper Authors": "Authors:Dong Wook Shu, Sung Woo Park, Junseok Kwon"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". For ShapeNet, following prior works [13, 15,\n44,53], we use 13 classes of the ShapeNet with train/val/test\nsplit from 3D-R2N2 ",
                    "Citation Text": "Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A uni\ufb01ed approach\nfor single and multi-view 3d object reconstruction. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , 2016. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.00449",
                        "Citation Paper Title": "Title:3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction",
                        "Citation Paper Abstract": "Abstract:Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).",
                        "Citation Paper Authors": "Authors:Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "trains Transformers to complete and genera-\ntion shapes with dense grid. And Point-BERT ",
                    "Citation Text": "Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-bert: Pre-training 3d point\ncloud transformers with masked point modeling. ArXiv ,\nabs/2111.14819, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14819",
                        "Citation Paper Title": "Title:Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
                        "Citation Paper Abstract": "Abstract:We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "training objective. In the 3D domain, autoregressive models\nhave been used to learn the distribution of point clouds [60,\n69] and meshes ",
                    "Citation Text": "Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter\nBattaglia. Polygen: An autoregressive generative model of\n3d meshes. In International conference on machine learning ,\npages 7220\u20137229. PMLR, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10880",
                        "Citation Paper Title": "Title:PolyGen: An Autoregressive Generative Model of 3D Meshes",
                        "Citation Paper Abstract": "Abstract:Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.",
                        "Citation Paper Authors": "Authors:Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "that\nare sharp in masked regions by adopting the BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", known for\ntheir ability to model long-range dependencies through self-\nattentions, have shown the power of autoregressive models\nin natural languages [8,54], image generation [11,52]. Con-\ntrary to deterministic masked auto-encoders ",
                    "Citation Text": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00e1r, and Ross Girshick. Masked autoencoders are scalable\nvision learners, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06377",
                        "Citation Paper Title": "Title:Masked Autoencoders Are Scalable Vision Learners",
                        "Citation Paper Abstract": "Abstract:This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ". Using neural networks to pa-\nrameterize the conditional distribution has been proved to\nbe effective [29,63] in general, and more speci\ufb01cally to im-\nage generation [12, 50, 65]. Transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.10688v1": {
            "Paper Title": "Co-design of Embodied Neural Intelligence via Constrained Evolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.06267v2": {
            "Paper Title": "Topologically-Aware Deformation Fields for Single-View 3D Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.08028v1": {
            "Paper Title": "Browser-based Hyperbolic Visualization of Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.05822v2": {
            "Paper Title": "CLIPasso: Semantically-Aware Object Sketching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.05076v2": {
            "Paper Title": "Reduce Information Loss in Transformers for Pluralistic Image Inpainting",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ", five pairs\nper input image are generated. Meanwhile, the Fr \u00b4echet In-\nception Distance (FID) ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems ,\n30, 2017. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "10.442 23.946 15.363 19.309 33.510 23.331 23.889 54.327 32.624\nPUTall(Ours) 11.221 19.934 13.248 19.776 38.206 24.605 19.411 43.239 26.223\nPIC (CVPR, 2019) ",
                    "Citation Text": "Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic\nimage completion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n1438\u20131447, 2019. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.04227",
                        "Citation Paper Title": "Title:Pluralistic Image Completion",
                        "Citation Paper Abstract": "Abstract:Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for \\textbf{pluralistic image completion} -- the task of generating multiple and diverse plausible solutions for image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one given ground truth to get prior distribution of missing parts and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebA-HQ), and natural images (ImageNet), our method not only generated higher-quality completion results, but also with multiple and diverse plausible outputs.",
                        "Citation Paper Authors": "Authors:Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "12.949 26.217 16.961 20.180 34.965 23.206 27.821 63.768 39.199\nMED (ECCV , 2020) ",
                    "Citation Text": "Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao\nYang. Rethinking image inpainting via a mutual encoder-\ndecoder with feature equalizations. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part II 16 , pages 725\u2013741.\nSpringer, 2020. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06929",
                        "Citation Paper Title": "Title:Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature Equalizations",
                        "Citation Paper Abstract": "Abstract:Deep encoder-decoder based CNNs have advanced image inpainting methods for hole filling. While existing methods recover structures and textures step-by-step in the hole regions, they typically use two encoder-decoders for separate recovery. The CNN features of each encoder are learned to capture either missing structures or textures without considering them as a whole. The insufficient utilization of these encoder features limit the performance of recovering both structures and textures. In this paper, we propose a mutual encoder-decoder CNN for joint recovery of both. We use CNN features from the deep and shallow layers of the encoder to represent structures and textures of an input image, respectively. The deep layer features are sent to a structure branch and the shallow layer features are sent to a texture branch. In each branch, we fill holes in multiple scales of the CNN features. The filled CNN features from both branches are concatenated and then equalized. During feature equalization, we reweigh channel attentions first and propose a bilateral propagation activation function to enable spatial equalization. To this end, the filled CNN features of structure and texture mutually benefit each other to represent image content at all feature levels. We use the equalized feature to supplement decoder features for output image generation through skip connections. Experiments on the benchmark datasets show the proposed method is effective to recover structures and textures and performs favorably against state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, Chao Yang"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "Mask Ratio (%) 20-40 40-60 10-60 20-40 40-60 10-60 20-40 40-60 10-60\nFID\u2193DFv2 (ICCV , 2019) ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 4471\u20134480, 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03589",
                        "Citation Paper Title": "Title:Free-Form Image Inpainting with Gated Convolution",
                        "Citation Paper Abstract": "Abstract:We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: this https URL",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ", only\n1K images are randomly chosen from the test split of Ima-\ngeNet for evaluation and the irregular masks provided by\nPConv ",
                    "Citation Text": "Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for\nirregular holes using partial convolutions. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 85\u2013100, 2018. 1, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07723",
                        "Citation Paper Title": "Title:Image Inpainting for Irregular Holes Using Partial Convolutions",
                        "Citation Paper Abstract": "Abstract:Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.",
                        "Citation Paper Authors": "Authors:Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "trains a\nperceptual vision tokenizer for vision transformer BERT-\npretraining ",
                    "Citation Text": "Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen,\nXiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou,\nand Lu Yuan. Bevt: Bert pretraining of video transformers.\nInIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2022) , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01529",
                        "Citation Paper Title": "Title:BEVT: BERT Pretraining of Video Transformers",
                        "Citation Paper Abstract": "Abstract:This paper studies the BERT pretraining of video transformers. It is a straightforward but worth-studying extension given the recent success from BERT pretraining of image transformers. We introduce BEVT which decouples video representation learning into spatial representation learning and temporal dynamics learning. In particular, BEVT first performs masked image modeling on image data, and then conducts masked image modeling jointly with masked video modeling on video data. This design is motivated by two observations: 1) transformers learned on image datasets provide decent spatial priors that can ease the learning of video transformers, which are often times computationally-intensive if trained from scratch; 2) discriminative clues, i.e., spatial and temporal information, needed to make correct predictions vary among different videos due to large intra-class and inter-class variations. We conduct extensive experiments on three challenging video benchmarks where BEVT achieves very promising results. On Kinetics 400, for which recognition mostly relies on discriminative spatial representations, BEVT achieves comparable results to strong supervised baselines. On Something-Something-V2 and Diving 48, which contain videos relying on temporal dynamics, BEVT outperforms by clear margins all alternative baselines and achieves state-of-the-art performance with a 71.4\\% and 87.2\\% Top-1 accuracy respectively. Code will be made available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, Lu Yuan"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Recently, based on the simi-\nlar quantization mechanism with VQ-V AE, V AGAN ",
                    "Citation Text": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 12873\u201312883, 2021. 1, 2, 3,\n4, 8, 13, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09841",
                        "Citation Paper Title": "Title:Taming Transformers for High-Resolution Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .",
                        "Citation Paper Authors": "Authors:Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "is proposed for discrete representation learning to cir-\ncumvent issues of \u201cposterior collaps\u201d, and further devel-\noped by VQ-V AE-2 ",
                    "Citation Text": "Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-\ning diverse high-fidelity images with vq-vae-2. In Advances\nin neural information processing systems , pages 14866\u2013\n14876, 2019. 2, 3, 4, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00446",
                        "Citation Paper Title": "Title:Generating Diverse High-Fidelity Images with VQ-VAE-2",
                        "Citation Paper Abstract": "Abstract:We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
                        "Citation Paper Authors": "Authors:Ali Razavi, Aaron van den Oord, Oriol Vinyals"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.07301v1": {
            "Paper Title": "Conditional Vector Graphics Generation for Music Cover Images",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "architecture and capable of generating real-\nistic images based on provided text condition. At the same\ntime, OpenAI has released a Contrastive Language\u2013Image\nPre-training (CLIP) model ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable vi-\nsual models from natural language supervision. CoRR ,\nabs/2103.00020, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "also uses the attention. However, its\nbasic principle of image generation is to recognize and cre-\nate individual objects from a given text description. Mirror-\nGAN paper ",
                    "Citation Text": "Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.\nMirrorgan: Learning text-to-image generation by redescrip-\ntion. CoRR , abs/1903.05854, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.05854",
                        "Citation Paper Title": "Title:MirrorGAN: Learning Text-to-image Generation by Redescription",
                        "Citation Paper Abstract": "Abstract:Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning text-to-image generation by redescription and consists of three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word- and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "as a learning factor, which allows selecting words\nto generate image fragments. Due to modi\ufb01cations, this net-\nwork shows signi\ufb01cantly better results than traditional GAN\nsystems. ObjGAN ",
                    "Citation Text": "Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang,\nXiaodong He, Siwei Lyu, and Jianfeng Gao. Object-\ndriven text-to-image synthesis via adversarial training. 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 12166\u201312174, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10740",
                        "Citation Paper Title": "Title:Object-driven Text-to-Image Synthesis via Adversarial Training",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose Object-driven Attentive Generative Adversarial Newtorks (Obj-GANs) that allow object-centered text-to-image synthesis for complex scenes. Following the two-step (layout-image) generation process, a novel object-driven attentive image generator is proposed to synthesize salient objects by paying attention to the most relevant words in the text description and the pre-generated semantic layout. In addition, a new Fast R-CNN based object-wise discriminator is proposed to provide rich object-wise discrimination signals on whether the synthesized object matches the text description and the pre-generated layout. The proposed Obj-GAN significantly outperforms the previous state of the art in various metrics on the large-scale COCO benchmark, increasing the Inception score by 27% and decreasing the FID score by 11%. A thorough comparison between the traditional grid attention and the new object-driven attention is provided through analyzing their mechanisms and visualizing their attention layers, showing insights of how the proposed model generates complex scenes in high quality.",
                        "Citation Paper Authors": "Authors:Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, Jianfeng Gao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.02551v3": {
            "Paper Title": "Exploring the Dynamics of the Circumcenter Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.07014v1": {
            "Paper Title": "SaiNet: Stereo aware inpainting behind objects with generative networks",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "added a contextual attention layer to aid the\nmodelling of long-term correlations between the distant in-\nformation and the hole regions.\nTraditional vanilla convolutions depend on the hole ini-\ntialisation values, which usually leads to visual artefacts.\nLiuet al . ",
                    "Citation Text": "Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image Inpainting for\nIrregular Holes Using Partial Convolutions. Proceedings of\nthe European Conference on Computer Vision (ECCV) , Apr.\n2018. 1, 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07723",
                        "Citation Paper Title": "Title:Image Inpainting for Irregular Holes Using Partial Convolutions",
                        "Citation Paper Abstract": "Abstract:Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.",
                        "Citation Paper Authors": "Authors:Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", CNN architectures were able to ex-\ntract meaningful semantics from images and generate novel\ncontent. Pathak et al. ",
                    "Citation Text": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A. Efros. Context Encoders: Feature\nLearning by Inpainting. Proceedings of the IEEE Computer\nSociety Conference on Computer Vision and Pattern Recog-\nnition , Apr. 2016. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.07379",
                        "Citation Paper Title": "Title:Context Encoders: Feature Learning by Inpainting",
                        "Citation Paper Abstract": "Abstract:We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
                        "Citation Paper Authors": "Authors:Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "create a dictionary of\npatches from multiple available viewpoints that are then co-\nherently selected and combined to recover the missing re-\ngion.\nThe \ufb01rst stereo inpainting approach using deep learning\nwas made by Luo et al. ",
                    "Citation Text": "Xuan Luo, Yanmeng Kong, Jason Lawrence, Ricardo\nMartin-Brualla, and Steven M. Seitz. KeystoneDepth: His-\ntory in 3D. In 2020 International Conference on 3D Vision\n(3DV) , Nov. 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07732",
                        "Citation Paper Title": "Title:KeystoneDepth: Visualizing History in 3D",
                        "Citation Paper Abstract": "Abstract:This paper introduces the largest and most diverse collection of rectified stereo image pairs to the research community, KeystoneDepth, consisting of tens of thousands of stereographs of historical people, events, objects, and scenes between 1860 and 1963. Leveraging the Keystone-Mast raw scans from the California Museum of Photography, we apply multiple processing steps to produce clean stereo image pairs, complete with calibration data, rectification transforms, and depthmaps. A second contribution is a novel approach for view synthesis that runs at real-time rates on a mobile device, simulating the experience of looking through an open window into these historical scenes. We produce results for thousands of antique stereographs, capturing many important historical moments.",
                        "Citation Paper Authors": "Authors:Xuan Luo, Yanmeng Kong, Jason Lawrence, Ricardo Martin-Brualla, Steve Seitz"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed the use of Partial Convolutions :\nmasked and re-normalised convolutional \ufb01lters conditioned\nonly on valid pixels. Yu et al . ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas Huang. Free-Form Image Inpainting With Gated\nConvolution. In IEEE/CVF International Conference on\nComputer Vision (ICCV) , Oct. 2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.03589",
                        "Citation Paper Title": "Title:Free-Form Image Inpainting with Gated Convolution",
                        "Citation Paper Abstract": "Abstract:We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: this https URL",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "proposed using both local and global context dis-\ncriminators, which helped the local consistency of gener-\nated patches and still held image coherence in the whole.\nYuet al. ",
                    "Citation Text": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S. Huang. Generative Image Inpainting with Con-\ntextual Attention. Proceedings of the IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition ,\nJan. 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07892",
                        "Citation Paper Title": "Title:Generative Image Inpainting with Contextual Attention",
                        "Citation Paper Abstract": "Abstract:Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". Yet all these methods were only applicable\nto tiny and thin masks and lacked semantic understanding\nof the scenes. With the addition of Generative Adversarial\nNetworks (GANs) ",
                    "Citation Text": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Proceedings\nof the 27th International Conference on Neural Information\nProcessing Systems - Volume 2 , Dec. 2014. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.05740v2": {
            "Paper Title": "Surface Representation for Point Clouds",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "1k pnts 93.6 - - - 2.38M 1.16G - -\nKPConv ",
                    "Citation Text": "Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Franc \u00b8ois Goulette, and Leonidas J\nGuibas. Kpconv: Flexible and deformable convolution for\npoint clouds. In Proceedings of the IEEE International Con-\nference on Computer Vision , pages 6411\u20136420, 2019. 1, 2,\n6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08889",
                        "Citation Paper Title": "Title:KPConv: Flexible and Deformable Convolution for Point Clouds",
                        "Citation Paper Abstract": "Abstract:We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.",
                        "Citation Paper Authors": "Authors:Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "1k pnts\u221793.1 91.3 - - - - - 42.20ms\nPointTrans. ",
                    "Citation Text": "Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 16259\u201316268, 2021. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09164",
                        "Citation Paper Title": "Title:Point Transformer",
                        "Citation Paper Abstract": "Abstract:Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",
                        "Citation Paper Authors": "Authors:Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "1k pnts 93.9 - - - 2.44M 1.68G - -\nRPNet ",
                    "Citation Text": "Haoxi Ran, Wei Zhuo, Jun Liu, and Li Lu. Learning inner-\ngroup relations on point clouds. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 15477\u201315487, 2021. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.12468",
                        "Citation Paper Title": "Title:Learning Inner-Group Relations on Point Clouds",
                        "Citation Paper Abstract": "Abstract:The prevalence of relation networks in computer vision is in stark contrast to underexplored point-based methods. In this paper, we explore the possibilities of local relation operators and survey their feasibility. We propose a scalable and efficient module, called group relation aggregator. The module computes a feature of a group based on the aggregation of the features of the inner-group points weighted by geometric relations and semantic relations. We adopt this module to design our RPNet. We further verify the expandability of RPNet, in terms of both depth and width, on the tasks of classification and segmentation. Surprisingly, empirical results show that wider RPNet fits for classification, while deeper RPNet works better on segmentation. RPNet achieves state-of-the-art for classification and segmentation on challenging benchmarks. We also compare our local aggregator with PointNet++, with around 30% parameters and 50% computation saving. Finally, we conduct experiments to reveal the robustness of RPNet with regard to rigid transformation and noises.",
                        "Citation Paper Authors": "Authors:Haoxi Ran, Wei Zhuo, Jun Liu, Li Lu"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "for further im-\nprovement. Though the results on ModelNet40 tend to be\nsaturated, our RepSurf-U achieves 94.7%, surpassing Cur-\nveNet ",
                    "Citation Text": "Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, and\nWeidong Cai. Walk in the cloud: Learning curves for point\nclouds shape analysis. arXiv preprint arXiv:2105.01288 ,\n2021. 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01288",
                        "Citation Paper Title": "Title:Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis",
                        "Citation Paper Abstract": "Abstract:Discrete point cloud objects lack sufficient shape descriptors of 3D geometries. In this paper, we present a novel method for aggregating hypothetical curves in point clouds. Sequences of connected points (curves) are initially grouped by taking guided walks in the point clouds, and then subsequently aggregated back to augment their point-wise features. We provide an effective implementation of the proposed aggregation strategy including a novel curve grouping operator followed by a curve aggregation operator. Our method was benchmarked on several point cloud analysis tasks where we achieved the state-of-the-art classification accuracy of 94.2% on the ModelNet40 classification task, instance IoU of 86.8 on the ShapeNetPart segmentation task, and cosine error of 0.11 on the ModelNet40 normal estimation task.",
                        "Citation Paper Authors": "Authors:Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, Weidong Cai"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "\u223c7k pnts 92.9 - - - 14.3M - 218.7ms 543.7ms\nPointASNL ",
                    "Citation Text": "Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and\nShuguang Cui. Pointasnl: Robust point clouds processing\nusing nonlocal neural networks with adaptive sampling. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 5589\u20135598, 2020. 2,\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00492",
                        "Citation Paper Title": "Title:PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling",
                        "Citation Paper Abstract": "Abstract:Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. In this paper, we present a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. The key component in our approach is the adaptive sampling (AS) module. It first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. Our AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. To further capture the neighbor and long-range dependencies of the sampled point, we proposed a local-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL module enables the learning process insensitive to noise. Extensive experiments verify the robustness and superiority of our approach in point clouds processing tasks regardless of synthesis data, indoor data, and outdoor data with or without noise. Specifically, PointASNL achieves state-of-the-art robust performance for classification and segmentation tasks on all datasets, and significantly outperforms previous methods on real-world outdoor SemanticKITTI dataset with considerate noise. Our code is released through this https URL.",
                        "Citation Paper Authors": "Authors:Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, Shuguang Cui"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "applies traditional convolution on point clouds after trans-\nforming neighboring points to the canonical order. RS-\nCNN ",
                    "Citation Text": "Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong\nPan. Relation-shape convolutional neural network for point\ncloud analysis. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 8895\u2013\n8904, 2019. 1, 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.07601",
                        "Citation Paper Title": "Title:Relation-Shape Convolutional Neural Network for Point Cloud Analysis",
                        "Citation Paper Abstract": "Abstract:Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.",
                        "Citation Paper Authors": "Authors:Yongcheng Liu, Bin Fan, Shiming Xiang, Chunhong Pan"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "1k pnts\u221793.2 - - - 10.1M 1.80G - -\nGrid-GCN ",
                    "Citation Text": "Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang,\nand Ulrich Neumann. Grid-gcn for fast and scalable point\ncloud learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 5661\u2013\n5670, 2020. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02984",
                        "Citation Paper Title": "Title:Grid-GCN for Fast and Scalable Point Cloud Learning",
                        "Citation Paper Abstract": "Abstract:Due to the sparsity and irregularity of the point cloud data, methods that directly consume points have become popular. Among all point-based models, graph convolutional networks (GCN) lead to notable performance by fully preserving the data granularity and exploiting point interrelation. However, point-based networks spend a significant amount of time on data structuring (e.g., Farthest Point Sampling (FPS) and neighbor points querying), which limit the speed and scalability. In this paper, we present a method, named Grid-GCN, for fast and scalable point cloud learning. Grid-GCN uses a novel data structuring strategy, Coverage-Aware Grid Query (CAGQ). By leveraging the efficiency of grid space, CAGQ improves spatial coverage while reducing the theoretical time complexity. Compared with popular sampling methods such as Farthest Point Sampling (FPS) and Ball Query, CAGQ achieves up to 50X speed-up. With a Grid Context Aggregation (GCA) module, Grid-GCN achieves state-of-the-art performance on major point cloud classification and segmentation benchmarks with significantly faster runtime than previous studies. Remarkably, Grid-GCN achieves the inference speed of 50fps on ScanNet using 81920 points per scene as input.",
                        "Citation Paper Authors": "Authors:Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, Ulrich Neumann"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "predefines geometric relations between points\nand their neighbors for local aggregation. DGCNN ",
                    "Citation Text": "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. Acm Transactions\nOn Graphics (tog) , 38(5):1\u201312, 2019. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07829",
                        "Citation Paper Title": "Title:Dynamic Graph CNN for Learning on Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.",
                        "Citation Paper Authors": "Authors:Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "removes the hand-crafted opera-\ntion of grouping by introducing Transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems , pages 5998\u20136008, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.12212v2": {
            "Paper Title": "M\u00f6bius Convolutions for Spherical CNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.05678v1": {
            "Paper Title": "RISP: Rendering-Invariant State Predictor with Differentiable Simulation\n  and Rendering for Cross-Domain Parameter Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08345v2": {
            "Paper Title": "Learning Smooth Neural Functions via Lipschitz Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.04978v1": {
            "Paper Title": "NeRF-Editing: Geometry Editing of Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "and Peak Signal-to-Noise Ra-\ntio (PSNR) as the metrics to evaluate the performance of\nour approach. We also evaluate the Fr \u00b4echet Inception Dis-\ntance ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. In Advances in Neural Information Processing Sys-\ntems, volume 30, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". In this work, we fo-\ncus on geometry editing/deformation for NeRF. As men-\ntioned before, EditingNeRF ",
                    "Citation Text": "Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance \ufb01elds. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 5773\u20135783,\n2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.06466",
                        "Citation Paper Title": "Title:Editing Conditional Radiance Fields",
                        "Citation Paper Abstract": "Abstract:A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.",
                        "Citation Paper Authors": "Authors:Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.04175v1": {
            "Paper Title": "NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image\n  Using Implicit Neural Representations",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "learns a per-pixel implicit representation from the\npixel-aligned feature with the global context, and could pre-\nserve local details while producing high-\ufb01delity reconstruc-\ntion. Similarly, Jiang et al. ",
                    "Citation Text": "Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\nHuang, Matthias Nie\u00dfner, Thomas Funkhouser, et al. Local\nimplicit grid representations for 3d scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 6001\u20136010, 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08981",
                        "Citation Paper Title": "Title:Local Implicit Grid Representations for 3D Scenes",
                        "Citation Paper Abstract": "Abstract:Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale -- i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing significantly better results than alternative approaches.",
                        "Citation Paper Authors": "Authors:Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.04018v1": {
            "Paper Title": "Photo-to-Shape Material Transfer for Diverse Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.03532v1": {
            "Paper Title": "Factory: Fast Contact for Robotic Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02904v1": {
            "Paper Title": "Neural Jacobian Fields: Learning Intrinsic Mappings of Arbitrary Meshes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02875v1": {
            "Paper Title": "Metaversal Learning Environments: Measuring, predicting and improving\n  interpersonal effectiveness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02834v1": {
            "Paper Title": "Fixing Malfunctional Objects With Learned Physical Simulation and\n  Functional Prediction",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "adds a temporal dimension to Point-\nNet++ to process 4D points and use chain-\ufb02ow grouping.\n\u2022PSTNet ",
                    "Citation Text": "Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, and Mohan\nKankanhalli. Pstnet: Point spatio-temporal convolution on\npoint cloud sequences. In International Conference on\nLearning Representations , 2020. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.13713",
                        "Citation Paper Title": "Title:PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences",
                        "Citation Paper Abstract": "Abstract:Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences. Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension. Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner. Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.",
                        "Citation Paper Authors": "Authors:Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, Mohan Kankanhalli"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "works on the single frame point cloud,\nwhich is used to examine whether the dynamics data in\nvideo assists in \ufb01nding the correct \ufb01x.\n\u2022MeteorNet ",
                    "Citation Text": "Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Meteor-\nnet: Deep learning on dynamic 3d point cloud sequences. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 9246\u20139255, 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.09165",
                        "Citation Paper Title": "Title:MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences",
                        "Citation Paper Abstract": "Abstract:Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called $MeteorNet$ for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.",
                        "Citation Paper Authors": "Authors:Xingyu Liu, Mengyuan Yan, Jeannette Bohg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.02367v1": {
            "Paper Title": "Time-multiplexed Neural Holography: A flexible framework for holographic\n  near-eye displays with fast heavily-quantized spatial light modulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01455v2": {
            "Paper Title": "Zero-Shot Text-Guided Object Generation with Dream Fields",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "was restricted to \ufb01xed\ngeometry and only optimized texture. Together these ad-\nvances enable a fundamentally new capability: open-ended\ntext-guided generation of object geometry and texture.\nConcurrently to Dream Fields, a few early works have\nused CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable vi-\nsual models from natural language supervision. CoRR ,\nabs/2103.00020, 2021. 2, 3, 4, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "trained con-\ntrastively on a large dataset of 400 Mcaptioned 2242images.\nWe also use a baseline Locked Image-Text Tuning (LiT)\nViT B/32 model from ",
                    "Citation Text": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit:\nZero-shot transfer with locked-image text tuning, 2021. 2, 3,\n4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.07991",
                        "Citation Paper Title": "Title:LiT: Zero-Shot Transfer with Locked-image text Tuning",
                        "Citation Paper Abstract": "Abstract:This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.",
                        "Citation Paper Authors": "Authors:Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". For \ufb01gures, we train with a 50 %higher\nresolution of 2522.\nWe collect an object-centric caption dataset with 153\ncaptions as a subset of the Common Objects in Context\n(COCO) dataset ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C. Lawrence\nZitnick. Microsoft COCO: Common objects in context.\nECCV , 2014. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "and includes noisy captions.\nFigure 1 shows a high-level overview of our method.\nDietNeRF ",
                    "Citation Text": "Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nICCV , 2021. 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00677",
                        "Citation Paper Title": "Title:Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360\u00b0 scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",
                        "Citation Paper Authors": "Authors:Ajay Jain, Matthew Tancik, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "and NeRF do not have ground truth\nmodels, but compare renderings to pixel-aligned ground\ntruth images from held-out poses with PSNR or LPIPS, a\ndeep perceptual metric ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. CVPR , 2018. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "trained via the same procedure\nas CLIP on a larger dataset of billions of higher-resolution\n(2882) captioned images. The LiT training set was collected\nfollowing a simpli\ufb01ed version of the ALIGN web alt-text\ndataset collection process ",
                    "Citation Text": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. ICML , 2021. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.05918",
                        "Citation Paper Title": "Title:Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
                        "Citation Paper Abstract": "Abstract:Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",
                        "Citation Paper Authors": "Authors:Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "have shown how\nCLIP can be used to guide GAN models like StyleGAN ",
                    "Citation Text": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. CVPR , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ". Recent work from\nMario Klingemann ( @quasimondo ) and ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. ICCV , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.09130v3": {
            "Paper Title": "Ensembling Off-the-shelf Models for GAN Training",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", and SwA V-FID [48, 61] using feature space of\nSwA V ",
                    "Citation Text": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS , 2020. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09882",
                        "Citation Paper Title": "Title:Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
                        "Citation Paper Abstract": "Abstract:Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",
                        "Citation Paper Authors": "Authors:Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". We compute the com-\nmonly used Fr \u00b4echet Inception Distance (FID) metric ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nInNeurIPS , 2017. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": "to reduce over\ufb01tting, the discriminator still tends\nto over\ufb01t, failing to perform well on a validation set. In addi-\ntion, the discriminator can potentially focus on artifacts that\nare indiscernible to humans but obvious for machines ",
                    "Citation Text": "Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\nOwens, and Alexei A Efros. Cnn-generated images are\nsurprisingly easy to spot... for now. In CVPR , 2020. 3, 17",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11035",
                        "Citation Paper Title": "Title:CNN-generated images are surprisingly easy to spot... for now",
                        "Citation Paper Abstract": "Abstract:In this work we ask whether it is possible to create a \"universal\" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis. Code and pre-trained networks are available at this https URL .",
                        "Citation Paper Authors": "Authors:Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": ", and extracting semantic edit-\ning regions with pretrained segmentation networks ",
                    "Citation Text": "Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.\nBarbershop: Gan-based image compositing using segmenta-\ntion masks, 2021. 2\n13Appendix\nWe show more visualizations and quantitative results to\nshow the ef\ufb01cacy of our method. Namely, Section A shows\nqualitative comparisons between our method and leading\nmethods for GAN training i.e. StyleGAN2-ADA",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.01505",
                        "Citation Paper Title": "Title:Barbershop: GAN-based Image Compositing using Segmentation Masks",
                        "Citation Paper Abstract": "Abstract:Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time.",
                        "Citation Paper Authors": "Authors:Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "improves the recall\nof generated samples by enforcing each real image to be\nclose to a generated image in deep feature space. Shocher\net al. ",
                    "Citation Text": "Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal\nYarom, Michal Irani, William T Freeman, and Tali Dekel.\nSemantic pyramid for image generation. In CVPR , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06221",
                        "Citation Paper Title": "Title:Semantic Pyramid for Image Generation",
                        "Citation Paper Abstract": "Abstract:We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.",
                        "Citation Paper Authors": "Authors:Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal Yarom, Michal Irani, William T. Freeman, Tali Dekel"
                    }
                },
                {
                    "Sentence ID": 99,
                    "Sentence": "uses\ndeep features to get synthetic clustering labels for condi-\ntional GAN training. InclusiveGAN ",
                    "Citation Text": "Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis,\nand Mario Fritz. Inclusive gan: Improving data and minority\ncoverage in generative models. In ECCV , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03355",
                        "Citation Paper Title": "Title:Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub.",
                        "Citation Paper Authors": "Authors:Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "us-\ning random projection and achieve better and faster GAN\ntraining.\nLoosely related to our work, other works have used pre-\ntrained models for clustering, encoding, and nearest neigh-\nbor search during their model training. Logo-GAN ",
                    "Citation Text": "Alexander Sage, Eirikur Agustsson, Radu Timofte, and Luc\nVan Gool. Logo synthesis and manipulation with clustered\ngenerative adversarial networks. In CVPR , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.04407",
                        "Citation Paper Title": "Title:Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training. We are able to generate a high diversity of plausible logos and we demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. Moreover, we validate the proposed clustered GAN training on CIFAR 10, achieving state-of-the-art Inception scores when using synthetic labels obtained via clustering the features of an ImageNet classifier. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models will be made publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 85,
                    "Sentence": "show that\ndeep features can indeed match the human perception of\nimage similarity better than classic metrics. Sungatullina et\nal. ",
                    "Citation Text": "Diana Sungatullina, Egor Zakharov, Dmitry Ulyanov, and\nVictor Lempitsky. Image manipulation with perceptual dis-\ncriminators. In ECCV , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01396",
                        "Citation Paper Title": "Title:Image Manipulation with Perceptual Discriminators",
                        "Citation Paper Abstract": "Abstract:Systems that perform image manipulation using deep convolutional networks have achieved remarkable realism. Perceptual losses and losses based on adversarial discriminators are the two main classes of learning objectives behind these advances. In this work, we show how these two ideas can be combined in a principled and non-additive manner for unaligned image translation tasks. This is accomplished through a special architecture of the discriminator network inside generative adversarial learning framework. The new architecture, that we call a perceptual discriminator, embeds the convolutional parts of a pre-trained deep classification network inside the discriminator network. The resulting architecture can be trained on unaligned image datasets while benefiting from the robustness and efficiency of perceptual losses. We demonstrate the merits of the new architecture in a series of qualitative and quantitative comparisons with baseline approaches and state-of-the-art frameworks for unaligned image translation.",
                        "Citation Paper Authors": "Authors:Diana Sungatullina, Egor Zakharov, Dmitry Ulyanov, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "propose a perceptual discriminator to combine per-\nceptual loss and adversarial loss for unpaired image-to-image\ntranslation. This idea was recently used by a concurrent work\non CG2real ",
                    "Citation Text": "Stephan R Richter, Hassan Abu AlHaija, and Vladlen Koltun.\nEnhancing photorealism enhancement. arXiv preprint\narXiv:2105.04619 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04619",
                        "Citation Paper Title": "Title:Enhancing Photorealism Enhancement",
                        "Citation Paper Abstract": "Abstract:We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Hassan Abu AlHaija, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.01624v2": {
            "Paper Title": "Practical Saccade Prediction for Head-Mounted Displays: Towards a\n  Comprehensive Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.01758v1": {
            "Paper Title": "Differentiable Simulation of Soft Multi-body Systems",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "use source code transformation to differentiate the dynamics,\nbut their contact model does not follow Coulomb\u2019s law. Geilinger et al. ",
                    "Citation Text": "Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz B\u00e4cher, Bernhard Thomaszewski, and Stelian\nCoros. ADD: Analytically differentiable dynamics for multi-body systems with frictional contact. ACM\nTransactions on Graphics (TOG) , 39(6), 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00987",
                        "Citation Paper Title": "Title:ADD: Analytically Differentiable Dynamics for Multi-Body Systems with Frictional Contact",
                        "Citation Paper Abstract": "Abstract:We present a differentiable dynamics solver that is able to handle frictional contact for rigid and deformable objects within a unified framework. Through a principled mollification of normal and tangential contact forces, our method circumvents the main difficulties inherent to the non-smooth nature of frictional contact. We combine this new contact model with fully-implicit time integration to obtain a robust and efficient dynamics solver that is analytically differentiable. In conjunction with adjoint sensitivity analysis, our formulation enables gradient-based optimization with adaptive trade-offs between simulation accuracy and smoothness of objective function landscapes. We thoroughly analyse our approach on a set of simulation examples involving rigid bodies, visco-elastic materials, and coupled multi-body systems. We furthermore showcase applications of our differentiable simulator to parameter estimation for deformable objects, motion planning for robotic manipulation, trajectory optimization for compliant walking robots, as well as efficient self-supervised learning of control policies.",
                        "Citation Paper Authors": "Authors:Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz B\u00e4cher, Bernhard Thomaszewski, Stelian Coros"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", and design [ 14,49]. For differentiable soft-body dynamics, Du et al. ",
                    "Citation Text": "Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela Rus, and Wojciech Matusik.\nDiffPD: Differentiable projective dynamics with contact. arXiv:2101.05917 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05917",
                        "Citation Paper Title": "Title:DiffPD: Differentiable Projective Dynamics",
                        "Citation Paper Abstract": "Abstract:We present a novel, fast differentiable simulator for soft-body learning and control applications. Existing differentiable soft-body simulators can be classified into two categories based on their time integration methods: Simulators using explicit time-stepping schemes require tiny time steps to avoid numerical instabilities in gradient computation, and simulators using implicit time integration typically compute gradients by employing the adjoint method and solving the expensive linearized dynamics. Inspired by Projective Dynamics (PD), we present Differentiable Projective Dynamics (DiffPD), an efficient differentiable soft-body simulator based on PD with implicit time integration. The key idea in DiffPD is to speed up backpropagation by exploiting the prefactorized Cholesky decomposition in forward PD simulation. In terms of contact handling, DiffPD supports two types of contacts: a penalty-based model describing contact and friction forces and a complementarity-based model enforcing non-penetration conditions and static friction. We evaluate the performance of DiffPD and observe it is 4-19 times faster compared with the standard Newton's method in various applications including system identification, inverse design problems, trajectory optimization, and closed-loop control. We also apply DiffPD in a reality-to-simulation (real-to-sim) example with contact and collisions and show its capability of reconstructing a digital twin of real-world scenes.",
                        "Citation Paper Authors": "Authors:Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela Rus, Wojciech Matusik"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "propose a\nsystem for FEM simulation represented by volume mesh. This system has been applied to robot\ndesign ",
                    "Citation Text": "Pingchuan Ma, Tao Du, John Z Zhang, Kui Wu, Andrew Spielberg, Robert K Katzschmann, and Wojciech\nMatusik. DiffAqua: A differentiable computational design pipeline for soft underwater swimmers with\nshape interpolation. In SIGGRAPH , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00837",
                        "Citation Paper Title": "Title:DiffAqua: A Differentiable Computational Design Pipeline for Soft Underwater Swimmers with Shape Interpolation",
                        "Citation Paper Abstract": "Abstract:The computational design of soft underwater swimmers is challenging because of the high degrees of freedom in soft-body modeling. In this paper, we present a differentiable pipeline for co-designing a soft swimmer's geometry and controller. Our pipeline unlocks gradient-based algorithms for discovering novel swimmer designs more efficiently than traditional gradient-free solutions. We propose Wasserstein barycenters as a basis for the geometric design of soft underwater swimmers since it is differentiable and can naturally interpolate between bio-inspired base shapes via optimal transport. By combining this design space with differentiable simulation and control, we can efficiently optimize a soft underwater swimmer's performance with fewer simulations than baseline methods. We demonstrate the efficacy of our method on various design problems such as fast, stable, and energy-efficient swimming and demonstrate applicability to multi-objective design.",
                        "Citation Paper Authors": "Authors:Pingchuan Ma, Tao Du, John Z. Zhang, Kui Wu, Andrew Spielberg, Robert K. Katzschmann, Wojciech Matusik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06635v3": {
            "Paper Title": "ADOP: Approximate Differentiable One-Pixel Point Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02597v2": {
            "Paper Title": "Neural 3D Video Synthesis from Multi-view Video",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "employs video depth estimation\nto supervise a space-time radiance \ufb01eld. ",
                    "Citation Text": "Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nProceedings of the IEEE International Conference on Com-\nputer Vision , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.06468",
                        "Citation Paper Title": "Title:Dynamic View Synthesis from Dynamic Monocular Video",
                        "Citation Paper Abstract": "Abstract:We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.",
                        "Citation Paper Authors": "Authors:Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "uses a model-based step for merging the\nestimated depth maps to a uni\ufb01ed representation that can be\nrendered from novel views. Neural Scene Flow Fields ",
                    "Citation Text": "Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene \ufb02ow \ufb01elds for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6498\u2013\n6508, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13084",
                        "Citation Paper Title": "Title:Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",
                        "Citation Paper Authors": "Authors:Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "uses a neural network for space-time and illumination in-\nterpolation. ",
                    "Citation Text": "Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park,\nand Jan Kautz. Novel view synthesis of dynamic scenes with\nglobally coherent depths from a monocular camera. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 5336\u20135345, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.01294",
                        "Citation Paper Title": "Title:Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera",
                        "Citation Paper Abstract": "Abstract:This paper presents a new method to synthesize an image from arbitrary views and times given a collection of images of a dynamic scene. A key challenge for the novel view synthesis arises from dynamic scene reconstruction where epipolar geometry does not apply to the local motion of dynamic contents. To address this challenge, we propose to combine the depth from single view (DSV) and the depth from multi-view stereo (DMV), where DSV is complete, i.e., a depth is assigned to every pixel, yet view-variant in its scale, while DMV is view-invariant yet incomplete. Our insight is that although its scale and quality are inconsistent with other views, the depth estimation from a single view can be used to reason about the globally coherent geometry of dynamic contents. We cast this problem as learning to correct the scale of DSV, and to refine each depth with locally consistent motions between views to form a coherent depth estimation. We integrate these tasks into a depth fusion network in a self-supervised fashion. Given the fused depth maps, we synthesize a photorealistic virtual view in a specific location and time with our deep blending network that completes the scene and renders the virtual view. We evaluate our method of depth estimation and view synthesis on diverse real-world dynamic scenes and show the outstanding performance over existing methods.",
                        "Citation Paper Authors": "Authors:Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "focus on\nmore general scenes. They decompose them into a static\nand dynamic component, re-project information based on\nestimated coarse depth, and employ a U-Net in screen space\nto convert the intermediate result to realistic imagery. ",
                    "Citation Text": "Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel,\nand Tobias Ritschel. X-\ufb01elds: implicit neural view-, light-\nand time-image interpolation. ACM Transactions on Graph-\nics (TOG) , 39(6):1\u201315, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.00450",
                        "Citation Paper Title": "Title:X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation",
                        "Citation Paper Abstract": "Abstract:We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the \"basic tricks\" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.",
                        "Citation Paper Authors": "Authors:Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "achieve volumetric video capture for hu-\nman performances from sparse camera views. ",
                    "Citation Text": "Aayush Bansal, Minh V o, Yaser Sheikh, Deva Ramanan, and\nSrinivasa Narasimhan. 4d visualization of dynamic events\nfrom unconstrained multi-view videos. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5366\u20135375, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13532",
                        "Citation Paper Title": "Title:4D Visualization of Dynamic Events from Unconstrained Multi-View Videos",
                        "Citation Paper Abstract": "Abstract:We present a data-driven approach for 4D space-time visualization of dynamic events from videos captured by hand-held multiple cameras. Key to our approach is the use of self-supervised neural networks specific to the scene to compose static and dynamic aspects of an event. Though captured from discrete viewpoints, this model enables us to move around the space-time of the event continuously. This model allows us to create virtual cameras that facilitate: (1) freezing the time and exploring views; (2) freezing a view and moving through time; and (3) simultaneously changing both time and view. We can also edit the videos and reveal occluded objects for a given view if it is visible in any of the other views. We validate our approach on challenging in-the-wild events captured using up to 15 mobile cameras.",
                        "Citation Paper Authors": "Authors:Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "push this further and encode the scene appearance in\na differentiable sphere-based representation. ",
                    "Citation Text": "Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 2437\u20132446, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.01024",
                        "Citation Paper Title": "Title:DeepVoxels: Learning Persistent 3D Feature Embeddings",
                        "Citation Paper Abstract": "Abstract:In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, Michael Zollh\u00f6fer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.00923v1": {
            "Paper Title": "Recording and replaying psychomotor user actions in VR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.00804v1": {
            "Paper Title": "Seeding Diversity into AI Art",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.00587v1": {
            "Paper Title": "A Position-Free Path Integral for Homogeneous Slabs and Multiple\n  Scattering on Smith Microfacets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02791v2": {
            "Paper Title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "over its training images to estimate cameras and collect\nsparse keypoints for depth supervision.\nRedwood-3dscan (Redwood) ",
                    "Citation Text": "Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen\nKoltun. A large dataset of object scans. arXiv:1602.02481 ,\n2016. 2, 4, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02481",
                        "Citation Paper Title": "Title:A Large Dataset of Object Scans",
                        "Citation Paper Abstract": "Abstract:We have created a dataset of more than ten thousand 3D scans of real objects. To create the dataset, we recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. We worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was irrevocably placed in the public domain and is available freely at this http URL .",
                        "Citation Paper Authors": "Authors:Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ".\nSimilar to our insight that the primary dif\ufb01culty in \ufb01tting\nfew view NeRF is correctly modeling 3D geometry, MVS-\nNeRF ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generaliz-\nable radiance \ufb01eld reconstruction from multi-view stereo. In\nIEEE International Conference on Computer Vision (ICCV) ,\n2021. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15595",
                        "Citation Paper Title": "Title:MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo",
                        "Citation Paper Abstract": "Abstract:We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.13812v1": {
            "Paper Title": "Visualization and Optimization Techniques for High Dimensional Parameter\n  Spaces",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": ".\nFurther, purely data driven measures for feature selection can\nignore the semantic relationships between the variables. Metrics\nlike Word2Vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Dis-\ntributed representations of words and phrases and their composition-\nality. In Advances in neural information processing systems , pp.\n3111\u20133119, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07945v2": {
            "Paper Title": "Efficient Geometry-aware 3D Generative Adversarial Networks",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", our method works naturally for\nsteep camera angles and in 360\u25e6viewing conditions.\nThe concurrently developed 3D-aware GANs StyleN-\neRF ",
                    "Citation Text": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\ning the image quality of StyleGAN. In IEEE Conference on\nComputer Vision andPattern Recognition (CVPR), 2020. 5,\n6, 7, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ", we also employ\n2D CNN-based upsampling after neural rendering, but our\nmethod introduces dual discrimination to avoid view incon-\nsistencies introduced by the upsampling layers. Unlike ex-\nisting StyleGAN2-based 2.5D GANs, which generate im-\nages and depth maps ",
                    "Citation Text": "Yichun Shi, Divyansh Aggarwal, and Anil K Jain. Lifting 2D\nstylegan for 3D-aware face generation. In IEEE Conference\nonComputer Vision andPattern Recognition (CVPR), 2021.\n3, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13126",
                        "Citation Paper Title": "Title:Lifting 2D StyleGAN for 3D-Aware Face Generation",
                        "Citation Paper Abstract": "Abstract:We propose a framework, called LiftedGAN, that disentangles and lifts a pre-trained StyleGAN2 for 3D-aware face generation. Our model is \"3D-aware\" in the sense that it is able to (1) disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, lighting and (2) generate 3D components for rendering synthetic images. Unlike most previous methods, our method is completely self-supervised, i.e. it neither requires any manual annotation nor 3DMM model for training. Instead, it learns to generate images as well as their 3D components by distilling the prior knowledge in StyleGAN2 with a differentiable renderer. The proposed model is able to output both the 3D shape and texture, allowing explicit pose and lighting control over generated images. Qualitative and quantitative results show the superiority of our approach over existing methods on 3D-controllable GANs in content controllability while generating realistic high quality images.",
                        "Citation Paper Authors": "Authors:Yichun Shi, Divyansh Aggarwal, Anil K. Jain"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ".\n5.1. Comparisons\nBaselines. We compare our methods against three state-\nof-the-art methods for 3D-aware image synthesis: \u03c0-\nGAN ",
                    "Citation Text": "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high \ufb01delity natural image synthe-\nsis. In International Conference onLearning Representations\n(ICLR), 2019. 5, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11096",
                        "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                        "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". We assess multi-view\nfacial identity consistency (ID) by calculating the mean Ar-\ncface ",
                    "Citation Text": "Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong. Accurate 3d face reconstruction with\nweakly-supervised learning: From single image to image\nset. In IEEE Computer Vision and Pattern Recognition\nWorkshops, 2019. 1, 2, 11, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.08527",
                        "Citation Paper Title": "Title:Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set",
                        "Citation Paper Abstract": "Abstract:Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency.However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce. In this paper, we propose a novel deep 3D face reconstruction approach that 1) leverages a robust, hybrid loss function for weakly-supervised learning which takes into account both low-level and perception-level information for supervision, and 2) performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation. Our method is fast, accurate, and robust to occlusion and large pose. We provide comprehensive experiments on three datasets, systematically comparing our method with fifteen recent methods and demonstrating its state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ". Fig. 3\nand Tab. 1 demonstrate that the tri-plane representation is\ncapable of representing this complex scene, albeit without\nview-dependent effects, outperforming dense feature vol-\nume representations [38, 61] and fully implicit represen-\ntations ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance \ufb01elds for view\nsynthesis. In European Conference onComputer Vision\n(ECCV), 2020. 1, 2, 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "(Fig. 3). In this exper-\niment, we use feature planes of resolution N= 512 and\nchannelsC= 48 , paired with an MLP of four layers of\n128 hidden units each and a Fourier feature encoding ",
                    "Citation Text": "Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-\ntures let networks learn high frequency functions in low di-\nmensional domains. In Advances inNeural Information\nProcessing Systems (NeurIPS), 2020. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.10739",
                        "Citation Paper Title": "Title:Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
                        "Citation Paper Abstract": "Abstract:We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.13100v1": {
            "Paper Title": "Few-Shot Head Swapping in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.10746v2": {
            "Paper Title": "Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic\n  Images in Facial Capture Pipelines",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "performs dense\nmarker driven mesh deformations, etc.). Markerless meth-\nods, previously more commonly utilized in democratized\napproaches, have in recent years also become an active area\nof research for high-end applications, with inverse rendering\ntechnology (e.g. ",
                    "Citation Text": "N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y . Lo,\nJ. Johnson, and G. Gkioxari. Accelerating 3d deep learning\nwith pytorch3d. arXiv:2007.08501, 2020. 2, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08501",
                        "Citation Paper Title": "Title:Accelerating 3D Deep Learning with PyTorch3D",
                        "Citation Paper Abstract": "Abstract:Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.",
                        "Citation Paper Authors": "Authors:Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "have also emerged.Both the PCA and deep-learning approaches typically pa-\nrameterize both appearance and expression ",
                    "Citation Text": "B. Egger, W. A. Smith, A. Tewari, S. Wuhrer, M. Zollhoefer,\nT. Beeler, F. Bernard, T. Bolkart, A. Kortylewski, S. Romd-\nhani, et al. 3d morphable face models\u2014past, present, and\nfuture. ACM Transactions onGraphics (TOG) , 39(5):1\u201338,\n2020. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.01815",
                        "Citation Paper Title": "Title:3D Morphable Face Models -- Past, Present and Future",
                        "Citation Paper Abstract": "Abstract:In this paper, we provide a detailed survey of 3D Morphable Face Models over the 20 years since they were first proposed. The challenges in building and applying these models, namely capture, modeling, image formation, and image analysis, are still active research topics, and we review the state-of-the-art in each of these areas. We also look ahead, identifying unsolved challenges, proposing directions for future research and highlighting the broad range of current and future applications.",
                        "Citation Paper Authors": "Authors:Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, Thomas Vetter"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ") or other sparse features as\nloss contraints for training regressors that predict geome-\ntry parameters from monocular images (e.g. ",
                    "Citation Text": "S. Sanyal, T. Bolkart, H. Feng, and M. J. Black. Learning to\nregress 3d face shape and expression from an image without\n3d supervision. In Proceedings oftheIEEE/CVF Conference\nonComputer Vision andPattern Recognition , pages 7763\u2013\n7772, 2019. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06817",
                        "Citation Paper Title": "Title:Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual's face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces `not quite in-the-wild' (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes at this http URL.",
                        "Citation Paper Authors": "Authors:Soubhik Sanyal, Timo Bolkart, Haiwen Feng, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "). Our use of inverse rendering for of\ufb02ine\nmotion capture also bears similarity to ",
                    "Citation Text": "B. Gecer, S. Ploumpis, I. Kotsia, and S. Zafeiriou. Gan\ufb01t:\nGenerative adversarial network \ufb01tting for high \ufb01delity 3d face\nreconstruction. In Proceedings oftheIEEE/CVF conference\noncomputer vision and pattern recognition , pages 1155\u2013\n1164, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05978",
                        "Citation Paper Title": "Title:GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction",
                        "Citation Paper Abstract": "Abstract:In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.",
                        "Citation Paper Authors": "Authors:Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". These methods typically use monolithic\nmulti-identity neural networks, although ",
                    "Citation Text": "A. Jourabloo, F. D. la Torre, J. M. Saragih, S. Wei, T. Wang,\nS. Lombardi, D. Belko, A. Trimble, and H. Badino. Robust\negocentric photo-realistic facial expression transfer for virtual\nreality. CoRR, abs/2104.04794, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.04794",
                        "Citation Paper Title": "Title:Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality",
                        "Citation Paper Abstract": "Abstract:Social presence, the feeling of being there with a real person, will fuel the next generation of communication systems driven by digital humans in virtual reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny effect rely on person-specific (PS) models. However, these PS models are time-consuming to build and are typically trained with limited data variability, which results in poor generalization and robustness. Major sources of variability that affects the accuracy of facial expression transfer algorithms include using different VR headsets (e.g., camera configuration, slop of the headset), facial appearance changes over time (e.g., beard, make-up), and environmental factors (e.g., lighting, backgrounds). This is a major drawback for the scalability of these models in VR. This paper makes progress in overcoming these limitations by proposing an end-to-end multi-identity architecture (MIA) trained with specialized augmentation strategies. MIA drives the shape component of the avatar from three cameras in the VR headset (two eyes, one mouth), in untrained subjects, using minimal personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS texture decoder is available, MIA is able to drive the full avatar (shape+texture) robustly outperforming PS models in challenging scenarios. Our key contribution to improve robustness and generalization, is that our method implicitly decouples, in an unsupervised manner, the facial expression from nuisance factors (e.g., headset, environment, facial appearance). We demonstrate the superior performance and robustness of the proposed method versus state-of-the-art PS approaches in a variety of experiments.",
                        "Citation Paper Authors": "Authors:Amin Jourabloo, Baris Gecer, Fernando De la Torre, Jason Saragih, Shih-En Wei, Te-Li Wang, Stephen Lombardi, Danielle Belko, Autumn Trimble, Hernan Badino"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ".\nAlternatively, monolithic deep-learning regressors can be\ntrained via sparse landmark constraints to directly output\nparameters from images ",
                    "Citation Text": "Y . Liu, A. Jourabloo, W. Ren, and X. Liu. Dense face align-\nment. In Proceedings oftheIEEE International Conference\nonComputer Vision Workshops, pages 1619\u20131628, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01442",
                        "Citation Paper Title": "Title:Dense Face Alignment",
                        "Citation Paper Abstract": "Abstract:Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on the challenging datasets. Our model can run at real time during testing.",
                        "Citation Paper Authors": "Authors:Yaojie Liu, Amin Jourabloo, William Ren, Xiaoming Liu"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". More re-\ncently, deep-learning approaches (which generate nonlinear\nparametrizations) have gained popularity (e.g. ",
                    "Citation Text": "Q. Tan, L. Gao, Y . Lai, and S. Xia. Variational autoencoders\nfor deforming 3d mesh models. CoRR , abs/1709.04307, 2017.\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.04307",
                        "Citation Paper Title": "Title:Variational Autoencoders for Deforming 3D Mesh Models",
                        "Citation Paper Abstract": "Abstract:3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are flexible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows flexibly adjusting the significance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Qingyang Tan, Lin Gao, Yu-Kun Lai, Shihong Xia"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ") being used to solve for animation\nparameters directly from image data (e.g. ",
                    "Citation Text": "M. Bao, M. Cong, S. Grabli, and R. Fedkiw. High-quality face\ncapture using anatomical muscles. CoRR , abs/1812.02836,\n2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02836",
                        "Citation Paper Title": "Title:High-Quality Face Capture Using Anatomical Muscles",
                        "Citation Paper Abstract": "Abstract:Muscle-based systems have the potential to provide both anatomical accuracy and semantic interpretability as compared to blendshape models; however, a lack of expressivity and differentiability has limited their impact. Thus, we propose modifying a recently developed rather expressive muscle-based system in order to make it fully-differentiable; in fact, our proposed modifications allow this physically robust and anatomically accurate muscle model to conveniently be driven by an underlying blendshape basis. Our formulation is intuitive, natural, as well as monolithically and fully coupled such that one can differentiate the model from end to end, which makes it viable for both optimization and learning-based approaches for a variety of applications. We illustrate this with a number of examples including both shape matching of three-dimensional geometry as as well as the automatic determination of a three-dimensional facial pose from a single two-dimensional RGB image without using markers or depth information.",
                        "Citation Paper Authors": "Authors:Michael Bao, Matthew Cong, St\u00e9phane Grabli, Ronald Fedkiw"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.12805v1": {
            "Paper Title": "A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D\n  Shape Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.12797v1": {
            "Paper Title": "Towards Quantum Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.10072v2": {
            "Paper Title": "Detecting Topology Attacks against Graph Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "jointly learn the graph structure and the graph neural networks. Other\nrelated defenses include adversarial training ",
                    "Citation Text": "Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack\nand defense for graph neural networks: an optimization perspective. In IJCAI , pages 3961\u20133967, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04214",
                        "Citation Paper Title": "Title:Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
                        "Citation Paper Abstract": "Abstract:Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph.",
                        "Citation Paper Authors": "Authors:Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, Xue Lin"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "use Gaussian distributions as the hidden node representations so that\nattacking effect can be absorbed; Jin et al. ",
                    "Citation Text": "Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for\nrobust graph neural networks. In KDD , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.10203",
                        "Citation Paper Title": "Title:Graph Structure Learning for Robust Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: this https URL). The specific experimental settings to reproduce our results can be found in this https URL.",
                        "Citation Paper Authors": "Authors:Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "study black-box targeted attacks via reinforcement learning. For the latter, DICE ",
                    "Citation Text": "Marcin Waniek, Tomasz P Michalak, Michael J Wooldridge, and Talal Rahwan. Hiding individuals and commu-\nnities in a social network. Nature Human Behaviour , 2(2):139\u2013147, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00375",
                        "Citation Paper Title": "Title:Hiding Individuals and Communities in a Social Network",
                        "Citation Paper Abstract": "Abstract:The Internet and social media have fueled enormous interest in social network analysis. New tools continue to be developed and used to analyse our personal connections, with particular emphasis on detecting communities or identifying key individuals in a social network. This raises privacy concerns that are likely to exacerbate in the future. With this in mind, we ask the question: Can individuals or groups actively manage their connections to evade social network analysis tools?\nBy addressing this question, the general public may better protect their privacy, oppressed activist groups may better conceal their existence, and security agencies may better understand how terrorists escape detection. We first study how an individual can evade \"network centrality\" analysis without compromising his or her influence within the network. We prove that an optimal solution to this problem is hard to compute. Despite this hardness, we demonstrate that even a simple heuristic, whereby attention is restricted to the individual's immediate neighbourhood, can be surprisingly effective in practice. For instance, it could disguise Mohamed Atta's leading position within the WTC terrorist network, and that is by rewiring a strikingly-small number of connections. Next, we study how a community can increase the likelihood of being overlooked by community-detection algorithms. We propose a measure of concealment, expressing how well a community is hidden, and use it to demonstrate the effectiveness of a simple heuristic, whereby members of the community either \"unfriend\" certain other members, or \"befriend\" some non-members, in a coordinated effort to camouflage their community.",
                        "Citation Paper Authors": "Authors:Marcin Waniek, Tomasz Michalak, Talal Rahwan, Michael Wooldridge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.11918v1": {
            "Paper Title": "Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household\n  Items",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "dataset contains 8K high-\nquality textured CAD objects in 98 categories, along with\nreference images; this dataset, along with GSO, was used to\nbenchmark a differentiable stereopsis shape reconstruction\napproach ",
                    "Citation Text": "Shubham Goel, Georgia Gkioxari, and Jitendra Malik. Differentiablestereopsis: Meshes from multiple views using differentiable rendering,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.05472",
                        "Citation Paper Title": "Title:Differentiable Stereopsis: Meshes from multiple views using differentiable rendering",
                        "Citation Paper Abstract": "Abstract:We propose Differentiable Stereopsis, a multi-view stereo approach that reconstructs shape and texture from few input views and noisy cameras. We pair traditional stereopsis and modern differentiable rendering to build an end-to-end model which predicts textured 3D meshes of objects with varying topologies and shape. We frame stereopsis as an optimization problem and simultaneously update shape and cameras via simple gradient descent. We run an extensive quantitative analysis and compare to traditional multi-view stereo techniques and state-of-the-art learning based methods. We show compelling reconstructions on challenging real-world scenes and for an abundance of object types with complex shape, topology and texture. Project webpage: this https URL",
                        "Citation Paper Authors": "Authors:Shubham Goel, Georgia Gkioxari, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". ShapeNet is\ndesigned for vision and does not contain mass or friction,\nbut GraspGAN ",
                    "Citation Text": "Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai,\nMatthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz,\nPeter Pastor, Kurt Konolige, Sergey Levine, and Vincent Vanhoucke.\nUsing simulation and domain adaptation to improve ef\ufb01ciency of deep\nrobotic grasping. In 2018 IEEE International Conference on Robotics\nand Automation (ICRA) , pages 4243\u20134250, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07857",
                        "Citation Paper Title": "Title:Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping",
                        "Citation Paper Abstract": "Abstract:Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.",
                        "Citation Paper Authors": "Authors:Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "and as a foundation for further\ndataset creation, such as the IKEA ASM furniture assembly\ndataset ",
                    "Citation Text": "Yizhak Ben-Shabat, Xin Yu, Fatemeh Saleh, Dylan Campbell, Cristian\nRodriguez-Opazo, Hongdong Li, and Stephen Gould. The ikea asm\ndataset: Understanding people assembling furniture through actions,\nobjects and pose. In Proc. of the IEEE/CVF Winter Conference on\nApplications of Computer Vision , pages 847\u2013859, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00394",
                        "Citation Paper Title": "Title:The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose",
                        "Citation Paper Abstract": "Abstract:The availability of a large labeled dataset is a key requirement for applying deep learning methods to solve various computer vision tasks. In the context of understanding human activities, existing public datasets, while large in size, are often limited to a single RGB camera and provide only per-frame or per-clip action annotations. To enable richer analysis and understanding of human activities, we introduce IKEA ASM -- a three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. Additionally, we benchmark prominent methods for video action recognition, object segmentation and human pose estimation tasks on this challenging dataset. The dataset enables the development of holistic methods, which integrate multi-modal and multi-view data to better perform on these tasks.",
                        "Citation Paper Authors": "Authors:Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "used several techniques\nto create furniture models for a virtual environment building\nwalkthrough. New neural rendering techniques, such as neu-\nral radiance \ufb01elds ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.\nBarron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes\nas neural radiance \ufb01elds for view synthesis. CoRR , abs/2003.08934,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.04533v2": {
            "Paper Title": "Learning from Shader Program Traces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12772v2": {
            "Paper Title": "JoinABLe: Learning Bottom-up Assembly of Parametric CAD Joints",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Moreover, while training on set object classes\ngreatly improves within-class performance, generalization\nto unseen categories is an ongoing area of research ",
                    "Citation Text": "Songfang Han, Jiayuan Gu, Kaichun Mo, Li Yi, Siyu\nHu, Xuejin Chen, and Hao Su. Compositionally gen-\neralizable 3d structure prediction. arXiv:2012.02493 ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02493",
                        "Citation Paper Title": "Title:Compositionally Generalizable 3D Structure Prediction",
                        "Citation Paper Abstract": "Abstract:Single-image 3D shape reconstruction is an important and long-standing problem in computer vision. A plethora of existing works is constantly pushing the state-of-the-art performance in the deep learning era. However, there remains a much more difficult and under-explored issue on how to generalize the learned skills over unseen object categories that have very different shape geometry distributions. In this paper, we bring in the concept of compositional generalizability and propose a novel framework that could better generalize to these unseen categories. We factorize the 3D shape reconstruction problem into proper sub-problems, each of which is tackled by a carefully designed neural sub-module with generalizability concerns. The intuition behind our formulation is that object parts (slates and cylindrical parts), their relationships (adjacency and translation symmetry), and shape substructures (T-junctions and a symmetric group of parts) are mostly shared across object categories, even though object geometries may look very different (e.g. chairs and cabinets). Experiments on PartNet show that we achieve superior performance than state-of-the-art. This validates our problem factorization and network designs.",
                        "Citation Paper Authors": "Authors:Songfang Han, Jiayuan Gu, Kaichun Mo, Li Yi, Siyu Hu, Xuejin Chen, Hao Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.10170v1": {
            "Paper Title": "Data Parallel Path Tracing in Object Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09996v1": {
            "Paper Title": "Computational Design of Kinesthetic Garments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09783v1": {
            "Paper Title": "TopoEmbedding, a web tool for the interactive analysis of persistent\n  homology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09341v1": {
            "Paper Title": "OutCast: Outdoor Single-image Relighting with Cast Shadows",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09138v1": {
            "Paper Title": "RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02713v2": {
            "Paper Title": "Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08601v1": {
            "Paper Title": "A Tour of Visualization Techniques for Computer Vision Datasets",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": "objects, their sizes, and locations.\nNevertheless, as noted by Wang et al. ",
                    "Citation Text": "Angelina Wang, Arvind Narayanan, and Olga Russakovsky.\nREVISE: A tool for measuring and mitigating bias in vi-\nsual datasets. In European Conference on Computer Vision\n(ECCV) , 2020. 1, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07999",
                        "Citation Paper Title": "Title:REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets",
                        "Citation Paper Abstract": "Abstract:Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. Our work tackles this issue and enables the preemptive analysis of large-scale datasets. REVISE (REvealing VIsual biaSEs) is a tool that assists in the investigation of a visual dataset, surfacing potential biases along three dimensions: (1) object-based, (2) person-based, and (3) geography-based. Object-based biases relate to the size, context, or diversity of the depicted objects. Person-based metrics focus on analyzing the portrayal of people within the dataset. Geography-based analyses consider the representation of different geographic locations. These three dimensions are deeply intertwined in how they interact to bias a dataset, and REVISE sheds light on this; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at this https URL",
                        "Citation Paper Authors": "Authors:Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, Olga Russakovsky"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". Average images can also be used to com-\npare subsets of images of the same nature and semantics,\ne.g., to trace how portrait photos change over time ",
                    "Citation Text": "Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, and\nAlexei A Efros. A century of portraits: A visual historical\nrecord of american high school yearbooks. In IEEE Inter-\nnational Conference on Computer Vision Workshops , pages\n1\u20137, 2015. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.02575",
                        "Citation Paper Title": "Title:A Century of Portraits: A Visual Historical Record of American High School Yearbooks",
                        "Citation Paper Abstract": "Abstract:Imagery offers a rich description of our world and communicates a volume and type of information that cannot be captured by text alone. Since the invention of the camera, an ever-increasing number of photographs document our \"visual culture\" complementing historical texts. But currently, this treasure trove of knowledge can only be analyzed manually by historians, and only at small scale. In this paper we perform automated analysis on a large-scale historical image dataset. Our main contributions are: 1) A publicly-available dataset of 168,055 (37,921 frontal-facing) American high school yearbook portraits. 2) Weakly-supervised data-driven techniques to discover historical visual trends in fashion and identify date-specific visual patterns. 3) A classifier to predict when a portrait was taken, with median error of 4 years for women and 6 for men. 4) A new method for discovering and displaying the visual elements used by the CNN-based date-prediction model to date portraits, finding that they correspond to the tell-tale fashions of each era. Project page can be found at: this http URL .",
                        "Citation Paper Authors": "Authors:Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee, Philipp Krahenbuhl, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.16020v3": {
            "Paper Title": "Machine learning method for light field refocusing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.01628v1": {
            "Paper Title": "SynopSet: Multiscale Visual Abstraction Set for Explanatory Analysis of\n  DNA Nanotechnology Simulations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08472v1": {
            "Paper Title": "Simultaneous Multiple-Prompt Guided Generation Using Differentiable\n  Optimal Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.06675v1": {
            "Paper Title": "Geometric Understanding of Sketches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.06504v1": {
            "Paper Title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific\n  Visualization",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "(gradient penalty)\ndistance for probability distributions over metric spaces. As\nan alternative to traditional GAN training, WGAN ",
                    "Citation Text": "M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative\nadversarial networks. In Proceedings of International Conference on\nMachine Learning , pages 214\u2013223, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.07875",
                        "Citation Paper Title": "Title:Wasserstein GAN",
                        "Citation Paper Abstract": "Abstract:We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.",
                        "Citation Paper Authors": "Authors:Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou"
                    }
                },
                {
                    "Sentence ID": 152,
                    "Sentence": "(GCN) \u2014 \u2014\ngeneration [ ], visualization generation [ ]), and encoder\nonly covers the prediction task [ ]. Many networks are either\nvariants of CNNs or based on CNNs, including deformable\nCNN ",
                    "Citation Text": "X. Wang, K. C. K. Chan, K. Yu, C. Dong, and C. C. Loy. EDVR:\nVideo restoration with enhanced deformable convolutional net-\nworks. In Proceedings of IEEE Conference on Computer Vision and\nPattern Recognition Workshops , pages 1954\u20131963, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02716",
                        "Citation Paper Title": "Title:EDVR: Video Restoration with Enhanced Deformable Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, Chen Change Loy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.06180v1": {
            "Paper Title": "Dynamic Neural Textures: Generating Talking-Face Videos with\n  Continuously Controllable Expressions",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "uses an auto-encoder to disentangle subject-related information\nand speech-related information. ",
                    "Citation Text": "K. R. Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C. V. Jawahar.\n2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In the\nWild. In The 28th ACM International Conference on Multimedia (MM) . 484\u2013492.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10010",
                        "Citation Paper Title": "Title:A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild",
                        "Citation Paper Abstract": "Abstract:In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: \\url{this http URL}. The code and models are released at this GitHub repository: \\url{this http URL}. You can also try out the interactive demo at this link: \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposes a conditional recurrent generation\nnetwork that takes an audio and an image as input. ",
                    "Citation Text": "Joon Son Chung, Amir Jamaludin, and Andrew Zisserman. 2017. You said that?.\nInBritish Machine Vision Conference (BMVC) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02966",
                        "Citation Paper Title": "Title:You said that?",
                        "Citation Paper Abstract": "Abstract:We present a method for generating a video of a talking face. The method takes as inputs: (i) still images of the target face, and (ii) an audio speech segment; and outputs a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time.\nTo achieve this we propose an encoder-decoder CNN model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on tens of hours of unlabelled videos.\nWe also show results of re-dubbing videos using speech from a different person.",
                        "Citation Paper Authors": "Authors:Joon Son Chung, Amir Jamaludin, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.05218v1": {
            "Paper Title": "An Optimal Experimental Design Approach for Light Configurations in\n  Photometric Stereo",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.15399v2": {
            "Paper Title": "InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". LPIPS\nestimates normalized features distance between image pair\nwhile FID and KID compute the distance in Inception rep-\nresentations ",
                    "Citation Text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR , 2016. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ".\nWe also use perceptual metrics, learned perceptual image\npatch similarity (LPIPS) ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "achieves photo-realistic rendering results by applying multi-\nlayer perceptrons to differentiable volume rendering suc-\ncessfully. The following works attempt to extend NeRF in\nvarious aspects, such as dynamic view synthesis ",
                    "Citation Text": "Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13084",
                        "Citation Paper Title": "Title:Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",
                        "Citation Paper Authors": "Authors:Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.04455v1": {
            "Paper Title": "Noise-based Enhancement for Foveated Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10520v3": {
            "Paper Title": "StylePart: Image-based Shape Part Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03237v1": {
            "Paper Title": "Rule-based Procedural Tree Modeling Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03105v1": {
            "Paper Title": "AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". However, since\ndiscretization is in 3D rather than in 2D (pixels), these ap-\nproaches either cannot scale up, or cannot predict the color\nefficiently due to the irregularity of the representation.\nTexture fields ",
                    "Citation Text": "Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo\nStrauss, and Andreas Geiger. Texture fields: Learning tex-\nture representations in function space. In ICCV , 2019. 1, 2,\n7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07259",
                        "Citation Paper Title": "Title:Texture Fields: Learning Texture Representations in Function Space",
                        "Citation Paper Abstract": "Abstract:In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",
                        "Citation Paper Authors": "Authors:Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "predict the color for each 3D point in\na continuous 3D space. The NeRF family ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV , 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.15234v2": {
            "Paper Title": "NeRFReN: Neural Radiance Fields with Reflections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.00854v2": {
            "Paper Title": "Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic\n  Crowd Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11426v4": {
            "Paper Title": "Neural Fields in Visual Computing and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02411v1": {
            "Paper Title": "Texturify: Generating Textures on 3D Shape Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02289v1": {
            "Paper Title": "Neural Convolutional Surfaces",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", or forcing the sur-\nface to agree with an implicit function ",
                    "Citation Text": "Omid Poursaeed, Matthew Fisher, Noam Aigerman, and\nVladimir G. Kim. Coupling explicit and implicit surface rep-\nresentations for generative 3d modeling. ECCV , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.10294",
                        "Citation Paper Title": "Title:Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling",
                        "Citation Paper Abstract": "Abstract:We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.",
                        "Citation Paper Authors": "Authors:Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.02219v1": {
            "Paper Title": "SNUG: Self-Supervised Neural Dynamic Garments",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": ". Instead of relying on losses that evaluate pre-\ndiction error based on the difference with respect to ground-\ntruth samples, self-supervised methods use implicit proper-\nties of the training data (or domain) as a supervision sig-\nnal ",
                    "Citation Text": "Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsoure-\nlakis, and Paris Perdikaris. Physics-constrained deep learn-\ning for high-dimensional surrogate modeling and uncertainty\nquanti\ufb01cation without labeled data. Journal of Computa-\ntional Physics , 394:56\u201381, 2019. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.06314",
                        "Citation Paper Title": "Title:Physics-Constrained Deep Learning for High-dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data",
                        "Citation Paper Abstract": "Abstract:Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.",
                        "Citation Paper Authors": "Authors:Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, Paris Perdikaris"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "enforce\nincompressibility constraints to learn to solve the system of\nequations required in physics-based \ufb02uid simulation, Xie etal. ",
                    "Citation Text": "You Xie, Erik Franz, Mengyu Chu, and Nils Thuerey.\ntempoGAN: A Temporally Coherent, V olumetric GAN for\nSuper-resolution Fluid Flow. ACM Transactions on Graph-\nics (Proc. SIGGRAPH) , 37(4), 2018. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.09710",
                        "Citation Paper Title": "Title:tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow",
                        "Citation Paper Abstract": "Abstract:We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate advected quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.",
                        "Citation Paper Authors": "Authors:You Xie, Erik Franz, Mengyu Chu, Nils Thuerey"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": ", pose-and-shape [5, 45], design [31, 39, 55], or\ngarment size ",
                    "Citation Text": "Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger-\nard Pons-Moll. SIZER: A Dataset and Model for Parsing 3D\nClothing and Learning Size Sensitive 3D Clothing. In Proc.\nof European Conference on Computer Vision (ECCV) , 2020.\n1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11610",
                        "Citation Paper Title": "Title:SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing",
                        "Citation Paper Abstract": "Abstract:While models of 3D clothing learned from real data exist, no method can predict clothing deformation as a function of garment size. In this paper, we introduce SizerNet to predict 3D clothing conditioned on human body shape and garment size parameters, and ParserNet to infer garment meshes and shape under clothing with personal details in a single pass from an input mesh. SizerNet allows to estimate and visualize the dressing effect of a garment in various sizes, and ParserNet allows to edit clothing of an input mesh directly, removing the need for scan segmentation, which is a challenging problem in itself. To learn these models, we introduce the SIZER dataset of clothing size variation which includes $100$ different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. Our experiments show better parsing accuracy and size prediction than baseline methods trained on SIZER. The code, model and dataset will be released for research purposes.",
                        "Citation Paper Authors": "Authors:Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ", a common strat-\negy is to learn parametric garment deformations, which are\nadded to a mesh template, as a function of pose [16, 56],\nshape ",
                    "Citation Text": "Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan\nCasas. Fully Convolutional Graph Neural Networks for Para-\nmetric Virtual Try-On. Computer Graphics Forum (Proc.\nSCA) , 39(8), 2020. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.04592",
                        "Citation Paper Title": "Title:Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On",
                        "Citation Paper Abstract": "Abstract:We present a learning-based approach for virtual try-on applications based on a fully convolutional graph neural network. In contrast to existing data-driven models, which are trained for a specific garment or mesh topology, our fully convolutional model can cope with a large family of garments, represented as parametric predefined 2D panels with arbitrary mesh topology, including long dresses, shirts, and tight tops. Under the hood, our novel geometric deep learning approach learns to drape 3D garments by decoupling the three different sources of deformations that condition the fit of clothing: garment type, target body shape, and material. Specifically, we first learn a regressor that predicts the 3D drape of the input parametric garment when worn by a mean body shape. Then, after a mesh topology optimization step where we generate a sufficient level of detail for the input garment type, we further deform the mesh to reproduce deformations caused by the target body shape. Finally, we predict fine-scale details such as wrinkles that depend mostly on the garment material. We qualitatively and quantitatively demonstrate that our fully convolutional approach outperforms existing methods in terms of generalization capabilities and memory requirements, and therefore it opens the door to more general learning-based models for virtual try-on applications.",
                        "Citation Paper Authors": "Authors:Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". Recent efforts also include\nthe design of differentiable physics simulators ",
                    "Citation Text": "Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan\nCarr, Jonathan Ragan-Kelley, and Fr \u00b4edo Durand. DiffTaichi:\nDifferentiable Programming for Physical Simulation. ICLR ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.00935",
                        "Citation Paper Title": "Title:DiffTaichi: Differentiable Programming for Physical Simulation",
                        "Citation Paper Abstract": "Abstract:We present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using source code transformations that preserve arithmetic intensity and parallelism. A light-weight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation. We demonstrate the performance and productivity of our language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in our language is 4.2x shorter than the hand-engineered CUDA version yet runs as fast, and is 188x faster than the TensorFlow implementation. Using our differentiable programs, neural network controllers are typically optimized within only tens of iterations.",
                        "Citation Paper Authors": "Authors:Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, Fr\u00e9do Durand"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.12792v2": {
            "Paper Title": "SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "to re\ufb01ne the details. Finally, a consistency\nloss is designed to match both representations.\nFor a self-rotating video with Nframes, we adopt the\nmethod described in VideoAvatar ",
                    "Citation Text": "Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Video based reconstruc-\ntion of 3d people models. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2018. 1, 2, 3,\n6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04758",
                        "Citation Paper Title": "Title:Video Based Reconstruction of 3D People Models",
                        "Citation Paper Abstract": "Abstract:This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.",
                        "Citation Paper Authors": "Authors:Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "aims at\nbinocular images, utilizes volume alignment feature and\npredicted high-precision depth to guide implicit function\nprediction, can effectively alleviate the depth ambiguity and\nrestore absolute scale information. PIFuHD ",
                    "Citation Text": "Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 84\u201393, 2020. 1, 2, 3, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00452",
                        "Citation Paper Title": "Title:PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization",
                        "Citation Paper Abstract": "Abstract:Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",
                        "Citation Paper Authors": "Authors:Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "utilizes\nhigher resolution features and predicted normal information\nto re\ufb01ne the geometric details of PIFu. PaMIR ",
                    "Citation Text": "Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.\nPamir: Parametric model-conditioned implicit representa-\ntion for image-based human reconstruction. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence , 2021. 1,\n2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.03858",
                        "Citation Paper Title": "Title:PaMIR: Parametric Model-Conditioned Implicit Representation for Image-based Human Reconstruction",
                        "Citation Paper Abstract": "Abstract:Modeling 3D humans accurately and robustly from a single image is very challenging, and the key for such an ill-posed problem is the 3D representation of the human models. To overcome the limitations of regular 3D representations, we propose Parametric Model-Conditioned Implicit Representation (PaMIR), which combines the parametric body model with the free-form deep implicit function. In our PaMIR-based reconstruction framework, a novel deep neural network is proposed to regularize the free-form deep implicit function using the semantic features of the parametric model, which improves the generalization ability under the scenarios of challenging poses and various clothing topologies. Moreover, a novel depth-ambiguity-aware training loss is further integrated to resolve depth ambiguities and enable successful surface detail reconstruction with imperfect body reference. Finally, we propose a body reference optimization method to improve the parametric model estimation accuracy and to enhance the consistency between the parametric model and the implicit function. With the PaMIR representation, our framework can be easily extended to multi-image input scenarios without the need of multi-camera calibration and pose synchronization. Experimental results demonstrate that our method achieves state-of-the-art performance for image-based 3D human reconstruction in the cases of challenging poses and clothing types.",
                        "Citation Paper Authors": "Authors:Zerong Zheng, Tao Yu, Yebin Liu, Qionghai Dai"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ".\nRigidity Loss. We require the \ufb01rst deformation \ufb01eld d\nto be as rigid as possible to avoid distortion. Following Park\net al. ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, So\ufb01en\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Ner\ufb01es: Deformable neural radiance \ufb01elds.\nICCV , 2021. 1, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "optimizes the deformation of\ntemplate mesh to match 2D cues. LiveCap ",
                    "Citation Text": "Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-\nard Pons-Moll, and Christian Theobalt. Livecap: Real-time\nhuman performance capture from monocular video. ACM\nTransactions On Graphics (TOG) , 38(2):1\u201317, 2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.02648",
                        "Citation Paper Title": "Title:LiveCap: Real-time Human Performance Capture from Monocular Video",
                        "Citation Paper Abstract": "Abstract:We present the first real-time human performance capture approach that reconstructs dense, space-time coherent deforming geometry of entire humans in general everyday clothing from just a single RGB video. We propose a novel two-stage analysis-by-synthesis optimization whose formulation and implementation are designed for high performance. In the first stage, a skinned template model is jointly fitted to background subtracted input video, 2D and 3D skeleton joint positions found using a deep neural network, and a set of sparse facial landmark detections. In the second stage, dense non-rigid 3D deformations of skin and even loose apparel are captured based on a novel real-time capable algorithm for non-rigid tracking using dense photometric and silhouette constraints. Our novel energy formulation leverages automatically identified material regions on the template to model the differing non-rigid deformation behavior of skin and apparel. The two resulting non-linear optimization problems per-frame are solved with specially-tailored data-parallel Gauss-Newton solvers. In order to achieve real-time performance of over 25Hz, we design a pipelined parallel architecture using the CPU and two commodity GPUs. Our method is the first real-time monocular approach for full-body performance capture. Our method yields comparable accuracy with off-line performance capture techniques, while being orders of magnitude faster.",
                        "Citation Paper Authors": "Authors:Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "combines implicit signed distance \ufb01eld and differential neu-\nral rendering to generate high-quality rigid reconstruction\nfrom multi-view images. Concurrent IMAvatar ",
                    "Citation Text": "Yufeng Zheng, Victoria Fern \u00b4andez Abrevaya, Xu Chen, Mar-\ncel C B \u00a8uhler, Michael J Black, and Otmar Hilliges. Im\navatar: Implicit morphable head avatars from videos. arXiv\npreprint arXiv:2112.07471 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07471",
                        "Citation Paper Title": "Title:I M Avatar: Implicit Morphable Head Avatars from Videos",
                        "Citation Paper Abstract": "Abstract:Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Yufeng Zheng, Victoria Fern\u00e1ndez Abrevaya, Marcel C. B\u00fchler, Xu Chen, Michael J. Black, Otmar Hilliges"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "\ufb01eld condi-\ntioned at body structured latent codes and utilizes the NeRF\n\ufb01eld to synthesize new images. However, the extracted ge-\nometry from NeRF suffers from noise. H-NeRF ",
                    "Citation Text": "Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.\nH-nerf: Neural radiance \ufb01elds for rendering and temporal\nreconstruction of humans in motion. Advances in Neural In-\nformation Processing Systems , 34, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.13746",
                        "Citation Paper Title": "Title:H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion",
                        "Citation Paper Abstract": "Abstract:We present neural radiance fields for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance field with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject -- including both body and clothing -- and to regularize the radiance field to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities significantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.",
                        "Citation Paper Authors": "Authors:Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "proposes an end-to-end trainable framework that turns raw\n3D scans of a clothed human into an animatable avatar.SNARF ",
                    "Citation Text": "Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes. In Interna-\ntional Conference on Computer Vision (ICCV) , 2021. 1, 2,\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03953",
                        "Citation Paper Title": "Title:SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes",
                        "Citation Paper Abstract": "Abstract:Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.",
                        "Citation Paper Authors": "Authors:Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.07732v2": {
            "Paper Title": "S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular\n  Image",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "to capture fine geometry details. How-\never these methods may not generalize well and are compu-\ntationally expensive.\nSome deep-based methods rely partially on synthetic\ndata ([20, 47, 22]) or a mix of labeled and unlabeled data ",
                    "Citation Text": "Victoria Fern \u00b4andez Abrevaya, Adnane Boukhayma,\nPhilip HS Torr, and Edmond Boyer. Cross-modal deep face\nnormals with deactivable skip connections. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 4979\u20134989, 2020. 2, 3, 8, 9, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09691",
                        "Citation Paper Title": "Title:Cross-modal Deep Face Normals with Deactivable Skip Connections",
                        "Citation Paper Abstract": "Abstract:We present an approach for estimating surface normals from in-the-wild color images of faces. While data-driven strategies have been proposed for single face images, limited available ground truth data makes this problem difficult. To alleviate this issue, we propose a method that can leverage all available image and normal data, whether paired or not, thanks to a novel cross-modal learning architecture. In particular, we enable additional training with single modality data, either color or normal, by using two encoder-decoder networks with a shared latent space. The proposed architecture also enables face details to be transferred between the image and normal domains, given paired data, through skip connections between the image encoder and normal decoder. Core to our approach is a novel module that we call deactivable skip connections, which allows integrating both the auto-encoded and image-to-normal branches within the same architecture that can be trained end-to-end. This allows learning of a rich latent space that can accurately capture the normal information. We compare against state-of-the-art methods and show that our approach can achieve significant improvements, both quantitative and qualitative, with natural face images.",
                        "Citation Paper Authors": "Authors:Victoria Fernandez Abrevaya, Adnane Boukhayma, Philip H. S. Torr, Edmond Boyer"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "uses\nsynthetic data to train their network which estimates a nor-\nmal map and skin reflectance (limited to pure-lambertian\nBRDF) but does not capture high frequency geometry de-\ntails. More recently, Feng et al. ",
                    "Citation Text": "Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.\nLearning an animatable detailed 3d face model from in-\nthe-wild images. ACM Transactions on Graphics (TOG) ,\n40(4):1\u201313, 2021. 2, 3, 8, 9, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04012",
                        "Citation Paper Title": "Title:Learning an Animatable Detailed 3D Face Model from In-The-Wild Images",
                        "Citation Paper Abstract": "Abstract:While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Yao Feng, Haiwen Feng, Michael J. Black, Timo Bolkart"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", we use nine spherical har-\nmonics (SH) bands to model light. Dib et al. ",
                    "Citation Text": "Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, C \u00b4edric\nTh\u00b4ebault, Philippe-Henri Gosselin, Marco Romeo, and\nLouis Chevallier. Practical face reconstruction via differen-\ntiable ray tracing. Computer Graphics Forum , 2021. 1, 2, 3,\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05356",
                        "Citation Paper Title": "Title:Practical Face Reconstruction via Differentiable Ray Tracing",
                        "Citation Paper Abstract": "Abstract:We present a differentiable ray-tracing based novel face reconstruction approach where scene attributes - 3D geometry, reflectance (diffuse, specular and roughness), pose, camera parameters, and scene illumination - are estimated from unconstrained monocular images. The proposed method models scene illumination via a novel, parameterized virtual light stage, which in-conjunction with differentiable ray-tracing, introduces a coarse-to-fine optimization formulation for face reconstruction. Our method can not only handle unconstrained illumination and self-shadows conditions, but also estimates diffuse and specular albedos. To estimate the face attributes consistently and with practical semantics, a two-stage optimization strategy systematically uses a subset of parametric attributes, where subsequent attribute estimations factor those previously estimated. For example, self-shadows estimated during the first stage, later prevent its baking into the personalized diffuse and specular albedos in the second stage. We show the efficacy of our approach in several real-world scenarios, where face attributes can be estimated even under extreme illumination conditions. Ablation studies, analyses and comparisons against several recent state-of-the-art methods show improved accuracy and versatility of our approach. With consistent face attributes reconstruction, our method leads to several style -- illumination, albedo, self-shadow -- edit and transfer applications, as discussed in the paper.",
                        "Citation Paper Authors": "Authors:Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, C\u00e9dric Th\u00e9bault, Philippe-Henri Gosselin, Marco Romeo, Louis Chevallier"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "proposed an ef-\nficient vertex-based differentiable rendering that can only\nhandle pure Lambertian BRDF and cannot capture self-\nshadows. Works such as ",
                    "Citation Text": "Linjie Lyu, Marc Habermann, Lingjie Liu, Ayush Tewari,\nChristian Theobalt, et al. Efficient and differentiable\nshadow computation for inverse problems. arXiv preprint\narXiv:2104.00359 , 2021. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00359",
                        "Citation Paper Title": "Title:Efficient and Differentiable Shadow Computation for Inverse Problems",
                        "Citation Paper Abstract": "Abstract:Differentiable rendering has received increasing interest for image-based inverse problems. It can benefit traditional optimization-based solutions to inverse problems, but also allows for self-supervision of learning-based approaches for which training data with ground truth annotation is hard to obtain. However, existing differentiable renderers either do not model visibility of the light sources from the different points in the scene, responsible for shadows in the images, or are too slow for being used to train deep architectures over thousands of iterations. To this end, we propose an accurate yet efficient approach for differentiable visibility and soft shadow computation. Our approach is based on the spherical harmonics approximations of the scene illumination and visibility, where the occluding surface is approximated with spheres. This allows for a significantly more efficient shadow computation compared to methods based on ray tracing. As our formulation is differentiable, it can be used to solve inverse problems such as texture, illumination, rigid pose, and geometric deformation recovery from images using analysis-by-synthesis optimization.",
                        "Citation Paper Authors": "Authors:Linjie Lyu, Marc Habermann, Lingjie Liu, Mallikarjun B R, Ayush Tewari, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "to capture fine detailed geometry. The bias introduced\nby these methods may impede the resulting precision due\nto the mismatch with real data distribution. They also do\nnot estimate face reflectance. Sengupta et al. ",
                    "Citation Text": "Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo,\nand David W Jacobs. Sfsnet: Learning shape, reflectance\nand illuminance of facesin the wild\u2019. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 6296\u20136305, 2018. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01261",
                        "Citation Paper Title": "Title:SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild",
                        "Citation Paper Abstract": "Abstract:We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real world images. This allows the network to capture low frequency variations from synthetic and high frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation.",
                        "Citation Paper Authors": "Authors:Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, David Jacobs"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.01823v1": {
            "Paper Title": "Sensitive vPSA -- Exploring Sensitivity in Visual Parameter Space\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01386v1": {
            "Paper Title": "Dressi: A Hardware-Agnostic Differentiable Renderer with Reactive Shader\n  Packing and Soft Rasterization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01287v1": {
            "Paper Title": "Software Rasterization of 2 Billion Points in Real Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04481v2": {
            "Paper Title": "What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D\n  Scene Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": ". Existing scene-level work, e.g., [36, 37, 45, 61]\ntrains on synthetic datasets with pre-segmented, watertight\nobjects like SunCG ",
                    "Citation Text": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-\nlis Savva, and Thomas Funkhouser. Semantic scene com-\npletion from a single depth image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1746\u20131754, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08974",
                        "Citation Paper Title": "Title:Semantic Scene Completion from a Single Depth Image",
                        "Citation Paper Abstract": "Abstract:This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.",
                        "Citation Paper Authors": "Authors:Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "or\nimages that have been aligned with synthetic ground-truth\n3D ",
                    "Citation Text": "Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong\nZhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum,\nand William T Freeman. Pix3d: Dataset and methods for\nsingle-image 3d shape modeling. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2974\u20132983, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04610",
                        "Citation Paper Title": "Title:Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling",
                        "Citation Paper Abstract": "Abstract:We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.",
                        "Citation Paper Authors": "Authors:Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B. Tenenbaum, William T. Freeman"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "and meshes [21, 22]. These approaches\nare often trained with synthetic data, e.g., ShapeNet ",
                    "Citation Text": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al. Shapenet:\nAn information-rich 3d model repository. arXiv preprint\narXiv:1512.03012 , 2015. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". Our work in-\nstead aims to infer the full 3D of the scene, including invis-\nible parts. Most work on invisible surfaces focuses on sin-\ngle objects with voxels [10, 20, 23], point-clouds [16, 38],\nCAD models ",
                    "Citation Text": "Hamid Izadinia, Qi Shan, and Steven M Seitz. Im2cad. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 5134\u20135143, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.05137",
                        "Citation Paper Title": "Title:IM2CAD",
                        "Citation Paper Abstract": "Abstract:Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database. We present a completely automatic system to address this IM2CAD problem that produces high quality results on challenging imagery from interior home design and remodeling websites. Our approach iteratively optimizes the placement and scale of objects in the room to best match scene renderings to the input photo, using image comparison metrics trained via deep convolutional neural nets. By operating jointly on the full scene at once, we account for inter-object occlusions. We also show the applicability of our method in standard scene understanding benchmarks where we obtain significant improvement.",
                        "Citation Paper Authors": "Authors:Hamid Izadinia, Qi Shan, Steven M. Seitz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.01117v1": {
            "Paper Title": "UrbanFlow: Designing Comfortable Outdoor Areas",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.13998v1": {
            "Paper Title": "Learning High-DOF Reaching-and-Grasping via Dynamic Representation of\n  Gripper-Object Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.00567v1": {
            "Paper Title": "Multi-Agent Path Planning with Asymmetric Interactions In Tight Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.17276v1": {
            "Paper Title": "Bringing Old Films Back to Life",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "for\n\ufb02ow estimation, whose parameters are \ufb01xed during the \ufb01rstMethod PSNR\"SSIM\"LPIPS#Ewarp#\nInput 19.982 0.699 0.456 0.0167\nOld Photo+TS [23, 40] 21.962 0.768 0.315 0.0041\nBasicVSR ",
                    "Citation Text": "Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and\nChen Change Loy. Basicvsr: The search for essential compo-\nnents in video super-resolution and beyond. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 4947\u20134956, 2021. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02181",
                        "Citation Paper Title": "Title:BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond",
                        "Citation Paper Abstract": "Abstract:Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.",
                        "Citation Paper Authors": "Authors:Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "17.9062 51.2813\nOurs 15.4254 42.1422\nTable 3. Quantitative restoration comparisons on real old \ufb01lms.\nand the ground truth for synthetic data. 2) To better match the\njudgment of human perception, we also calculate the learned\nperceptual image patch similarity (LPIPS) ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.17275v1": {
            "Paper Title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable\n  Object Manipulations with Tools",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16626v1": {
            "Paper Title": "DDNeRF: Depth Distribution Neural Radiance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05849v2": {
            "Paper Title": "Advances in Neural Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05381v2": {
            "Paper Title": "UNIST: Unpaired Neural Implicit Shape Translation Network",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "that employs a conditional GAN\nwith a reconstruction loss. Unpaired translation is more\ncomplex and usually requires additional loss functions such\nas cycle consistency [26, 29]. While most methods [26, 29]\noperate in image space, works such as ",
                    "Citation Text": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised\nimage-to-image translation networks. In NeurIPS , 2017. 1,\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00848",
                        "Citation Paper Title": "Title:Unsupervised Image-to-Image Translation Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL .",
                        "Citation Paper Authors": "Authors:Ming-Yu Liu, Thomas Breuel, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "Image translation. Image-to-image translation can be done\nunder a paired or unpaired setting. An example of a paired\ntranslation is pix2pix ",
                    "Citation Text": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07004",
                        "Citation Paper Title": "Title:Image-to-Image Translation with Conditional Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
                        "Citation Paper Authors": "Authors:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01524v2": {
            "Paper Title": "GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "). In particular, we use\nthe estimated camera parameters to convert estimated mo-\ntions from the camera coordinates to the global coordinates.\nFor motion in\ufb01lling, we use (1) linear interpolation, (2) lastMethod(All)\nG-MPJPE(All)\nG-PVE(Invisible)\nFID(Invisible)\nPA-MPJPE(Visible)\nPA-MPJPE(All)\nAccel\nKAMA ",
                    "Citation Text": "Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece,\nRemo Ziegler, and Otmar Hilliges. Convolutional autoen-\ncoders for human motion in\ufb01lling. In 3DV, 2020. 3, 5, 7,\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11531",
                        "Citation Paper Title": "Title:Convolutional Autoencoders for Human Motion Infilling",
                        "Citation Paper Abstract": "Abstract:In this paper we propose a convolutional autoencoder to address the problem of motion infilling for 3D human motion data. Given a start and end sequence, motion infilling aims to complete the missing gap in between, such that the filled in poses plausibly forecast the start sequence and naturally transition into the end sequence. To this end, we propose a single, end-to-end trainable convolutional autoencoder. We show that a single model can be used to create natural transitions between different types of activities. Furthermore, our method is not only able to fill in entire missing frames, but it can also be used to complete gaps where partial poses are available (e.g. from end effectors), or to clean up other forms of noise (e.g. Gaussian). Also, the model can fill in an arbitrary number of gaps that potentially vary in length. In addition, no further post-processing on the model's outputs is necessary such as smoothing or closing discontinuities at the end of the gap. At the heart of our approach lies the idea to cast motion infilling as an inpainting problem and to train a convolutional de-noising autoencoder on image-like representations of motion sequences. At training time, blocks of columns are removed from such images and we ask the model to fill in the gaps. We demonstrate the versatility of the approach via a number of complex motion sequences and report on thorough evaluations performed to better understand the capabilities and limitations of the proposed approach.",
                        "Citation Paper Authors": "Authors:Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler, Otmar Hilliges"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "handle unsynchronized\nmoving cameras but assume multi-view input and rely on\naudio stream for synchronization. More recently, Dong et\nal. ",
                    "Citation Text": "Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xi-\naowei Zhou, and Hujun Bao. Motion capture from internet\nvideos. In ECCV , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07931",
                        "Citation Paper Title": "Title:Motion Capture from Internet Videos",
                        "Citation Paper Abstract": "Abstract:Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a celebrity performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular motion capture methods.",
                        "Citation Paper Authors": "Authors:Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, Hujun Bao"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "exploit a limb-\nlength constraint to recover the absolute translation of the\nperson using a 2.5D representation. Some approaches ap-\nproximate the depth of the person using the bounding box\nsize [40, 71, 118]. HybrIK ",
                    "Citation Text": "Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin\nYang, and Cewu Lu. Hybrik: A hybrid analytical-neural\ninverse kinematics solution for 3d human pose and shape\nestimation. In CVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.14672",
                        "Citation Paper Title": "Title:HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation",
                        "Citation Paper Abstract": "Abstract:Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh for the human body by estimating several parameters. However, learning the abstract parameters is a highly non-linear process and suffers from image-model misalignment, leading to mediocre model performance. In contrast, 3D keypoint estimation methods combine deep CNN network with the volumetric representation to achieve pixel-level localization accuracy but may predict unrealistic body structure. In this paper, we address the above issues by bridging the gap between body mesh estimation and 3D keypoint estimation. We propose a novel hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms accurate 3D joints to relative body-part rotations for 3D body mesh reconstruction, via the twist-and-swing decomposition. The swing rotation is analytically solved with 3D joints, and the twist rotation is derived from the visual cues through the neural network. We show that HybrIK preserves both the accuracy of 3D pose and the realistic body structure of the parametric human model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than the pure 3D keypoint estimation methods. Without bells and whistles, the proposed method surpasses the state-of-the-art methods by a large margin on various 3D human pose and shape benchmarks. As an illustrative example, HybrIK outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW dataset. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.16393v1": {
            "Paper Title": "Online Motion Style Transfer for Interactive Character Control",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "),\nand also extracted the style with a TCN encoder \ud835\udc6c\ud835\udc94. Then, they\napplied AdaIN on a TCN decoder \ud835\udc6eto generate unseen content-\nstyle combination, with Generative Adversarial Loss ",
                    "Citation Text": "Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial\nnetworks. arXiv preprint arXiv:1406.2661 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04312v3": {
            "Paper Title": "Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural\n  Human Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11427v2": {
            "Paper Title": "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": ". Inconsistencies in\nbackground might decrease photorealism (right column).\nground as suggested in NeRF++ ",
                    "Citation Text": "Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\n\ufb01elds. arXiv preprint arXiv:2010.07492 , 2020. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07492",
                        "Citation Paper Title": "Title:NeRF++: Analyzing and Improving Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "datasets. FFHQ contains 70,000 images of di-\nverse human faces at 1024\u00021024 resolution, which arecentered and aligned according to the procedure introduced\nin Karras et al. ",
                    "Citation Text": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of GANs for improved quality, stability,\nand variation. In ICLR , 2018. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10196",
                        "Citation Paper Title": "Title:Progressive Growing of GANs for Improved Quality, Stability, and Variation",
                        "Citation Paper Abstract": "Abstract:We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
                        "Citation Paper Authors": "Authors:Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "In this section, we review related approaches in 2D im-\nage synthesis, 3D generative modeling, and 3D-aware im-\nage synthesis.\nGenerative Adversarial Networks: State-of-the-art Gen-\nerative Adversarial Networks ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS ,\npages 2672\u20132680, 2014. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.15926v1": {
            "Paper Title": "Disentangled3D: Learning a 3D Generative Model with Disentangled\n  Geometry and Appearance from Monocular Images",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "learn\nto generate explicit 3D voxels and meshes respectively, but\nproduce shapes and images with limited quality. Recently,\nthere has been a surge of interest in adopting coordinate-\nbased neural volumetric representations ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In ECCV , 2020. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "learn a canonical representa-\ntion of the entire scene from which the other frames can be\nobtained by learning deformations to the canonical space.\nThese methods also propose a number of regularizers to\ncontrol the deformation space. Li et al. ",
                    "Citation Text": "Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene \ufb02ow \ufb01elds for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6498\u2013\n6508, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13084",
                        "Citation Paper Title": "Title:Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",
                        "Citation Paper Authors": "Authors:Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.05023v2": {
            "Paper Title": "Stereo Magnification with Multi-Layer Images",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "proposed representing scenes with multiple fronto-\nparallel semitransparent layers and acquiring such represen-\ntations through stereo-matching of a pair of input views.\nTwenty years later, several approaches [8, 21, 30] starting\nfrom ",
                    "Citation Text": "Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magni\ufb01cation: Learning view syn-\nthesis using multiplane images. In Proc. ACM SIGGRAPH ,\n2018. 1, 2, 3, 4, 5, 6, 7, 9, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.09817",
                        "Citation Paper Title": "Title:Stereo Magnification: Learning View Synthesis using Multiplane Images",
                        "Citation Paper Abstract": "Abstract:The view synthesis problem--generating novel views of a scene from known imagery--has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including VR cameras and now-widespread dual-lens camera phones. We call this problem stereo magnification, and propose a learning framework that leverages a new layered representation that we call multiplane images (MPIs). Our method also uses a massive new data source for learning view extrapolation: online videos on YouTube. Using data mined from such videos, we train a deep network that predicts an MPI from an input stereo image pair. This inferred MPI can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.",
                        "Citation Paper Authors": "Authors:Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "only for the RAW\nscheme on the predicted images ^In. The main goal of ad-\nversarial loss is to reduce unnatural artefacts such as ghost-\ning and duplications. To regularize the discriminator, R1\npenalty ",
                    "Citation Text": "Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\nWhich Training Methods for GANs do actually Converge?\nInProc. ICML , 2018. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.04406",
                        "Citation Paper Title": "Title:Which Training Methods for GANs do actually Converge?",
                        "Citation Paper Abstract": "Abstract:Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Andreas Geiger, Sebastian Nowozin"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "or from monocular depth estima-\ntion [28, 38]. In this class, the 3D layered inpainting ap-\nproach ",
                    "Citation Text": "M. L. Shih, S. Y . Su, J. Kopf, and J. B. Huang. 3d photogra-\nphy using context-aware layered depth inpainting. In Proc.\nCVPR , 2020. 1, 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04727",
                        "Citation Paper Title": "Title:3D Photography using Context-aware Layered Depth Inpainting",
                        "Citation Paper Abstract": "Abstract:We propose a method for converting a single RGB-D input image into a 3D photo - a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner. The resulting 3D photos can be efficiently rendered with motion parallax using standard graphics engines. We validate the effectiveness of our method on a wide range of challenging everyday scenes and show fewer artifacts compared with the state of the arts.",
                        "Citation Paper Authors": "Authors:Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13152v3": {
            "Paper Title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis\n  Through Set-Latent Scene Representations",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "). Recent work has demonstrated\nthat novel views can be synthesized using deep networks\ntrained to compute radiance values for a given 3D position\nand direction. In particular, NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan\nBarron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing\nScenes as Neural Radiance Fields for View Synthesis. In\nECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "There is a long history of prior work on novel view syn-\nthesis from multiple RGB images based on neural represen-\ntations (see Tab. 1 and ",
                    "Citation Text": "Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-\nvasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,\nVincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-\nbardi, et al. Advances in neural rendering. arXiv preprint\narXiv:2111.05849 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05849",
                        "Citation Paper Title": "Title:Advances in Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.15258v1": {
            "Paper Title": "Efficient Reflectance Capture with a Deep Gated Mixture-of-Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.14755v2": {
            "Paper Title": "FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.15233v1": {
            "Paper Title": "AutoPoly: Predicting a Polygonal Mesh Construction Sequence from a\n  Silhouette Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.12691v3": {
            "Paper Title": "Learning to generate line drawings that convey geometry and semantics",
            "Sentences": [
                {
                    "Sentence ID": 87,
                    "Sentence": "uses an attention mod-\nule and auxiliary classifier and cycle consistency. 4) ACL-\nGAN ",
                    "Citation Text": "Yihao Zhao, Ruihai Wu, and Hao Dong. Unpaired image-\nto-image translation using adversarial consistency loss. In\nEuropean Conference on Computer Vision , pages 800\u2013815.\nSpringer, 2020. 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.04858",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Adversarial Consistency Loss",
                        "Citation Paper Abstract": "Abstract:Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform geometric changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarial-consistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfie-to-anime translation.",
                        "Citation Paper Authors": "Authors:Yihao Zhao, Ruihai Wu, Hao Dong"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". The loss for each\ndomain using the LSGAN setup ",
                    "Citation Text": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen\nWang, and Stephen Paul Smolley. Least squares genera-\ntive adversarial networks. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 2794\u20132802,\n2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04076",
                        "Citation Paper Title": "Title:Least Squares Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",
                        "Citation Paper Authors": "Authors:Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". Although these methods successfully generate\nline drawings from 3D models, they cannot be applied to\narbitrary photographs with unavailable 3D geometry. Fur-\nthermore, most methods draw lines in only one style, al-\nthough Neural Strokes ",
                    "Citation Text": "Difan Liu, Matthew Fisher, Aaron Hertzmann, and Evange-\nlos Kalogerakis. Neural strokes: Stylized line drawing of\n3d shapes. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.03900",
                        "Citation Paper Title": "Title:Neural Strokes: Stylized Line Drawing of 3D Shapes",
                        "Citation Paper Abstract": "Abstract:This paper introduces a model for producing stylized line drawings from 3D shapes. The model takes a 3D shape and a viewpoint as input, and outputs a drawing with textured strokes, with variations in stroke thickness, deformation, and color learned from an artist's style. The model is fully differentiable. We train its parameters from a single training drawing of another 3D shape. We show that, in contrast to previous image-based methods, the use of a geometric representation of 3D shape and 2D strokes allows the model to transfer important aspects of shape and texture style while preserving contours. Our method outputs the resulting drawing in a vector representation, enabling richer downstream analysis or editing in interactive applications.",
                        "Citation Paper Authors": "Authors:Difan Liu, Matthew Fisher, Aaron Hertzmann, Evangelos Kalogerakis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.03609v2": {
            "Paper Title": "Human-Aware Object Placement for Visual Environment Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "to reconstruct a 3D scene from labeled\nor detected 2D instance segmentation masks ",
                    "Citation Text": "Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\nshick. PointRend: Image segmentation as rendering. In\nComputer Vision and Pattern Recognition (CVPR) , pages\n9799\u20139808, 2020. 2, 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.08193",
                        "Citation Paper Title": "Title:PointRend: Image Segmentation as Rendering",
                        "Citation Paper Abstract": "Abstract:We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at this https URL.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "on all scene-reconstruction metrics and different\ndatasets, as shown in Tab. 2 and Tab. 3.\nFurthermore, we evaluate the error of the camera orienta-\ntion and ground plane penetration ",
                    "Citation Text": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J. Guibas. HuMoR: 3D hu-\nman motion model for robust pose estimation. In Inter-national Conference on Computer Vision (ICCV) , pages\n11488\u201311499, 2021. 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04668",
                        "Citation Paper Title": "Title:HuMoR: 3D Human Motion Model for Robust Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos.",
                        "Citation Paper Authors": "Authors:Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", estimating the 3D human poses and shape [ 45,66], and extracting the expected contact\nvertices on the estimated bodies using POSA ",
                    "Citation Text": "Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios\nTzionas, and Michael J. Black. Populating 3D scenes by\nlearning human-scene interaction. In Computer Vision and\n9Pattern Recognition (CVPR) , pages 14708\u201314718, 2021. 2,\n3, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11581",
                        "Citation Paper Title": "Title:Populating 3D Scenes by Learning Human-Scene Interaction",
                        "Citation Paper Abstract": "Abstract:Humans live within a 3D space and constantly interact with it to perform tasks. Such interactions involve physical contact between surfaces that is semantically meaningful. Our goal is to learn how humans interact with scenes and leverage this to enable virtual characters to do the same. To that end, we introduce a novel Human-Scene Interaction (HSI) model that encodes proximal relationships, called POSA for \"Pose with prOximitieS and contActs\". The representation of interaction is body-centric, which enables it to generalize to new scenes. Specifically, POSA augments the SMPL-X parametric human body model such that, for every mesh vertex, it encodes (a) the contact probability with the scene surface and (b) the corresponding semantic scene label. We learn POSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX dataset, which contains SMPL-X meshes of people interacting with 3D scenes, and the corresponding scene semantics from the PROX-E dataset. We demonstrate the value of POSA with two applications. First, we automatically place 3D scans of people in scenes. We use a SMPL-X model fit to the scan as a proxy and then find its most likely placement in 3D. POSA provides an effective representation to search for \"affordances\" in the scene that match the likely contact relationships for that pose. We perform a perceptual study that shows significant improvement over the state of the art on this task. Second, we show that POSA's learned representation of body-scene interaction supports monocular human pose estimation that is consistent with a 3D scene, improving on the state of the art. Our model and code are available for research purposes at this https URL.",
                        "Citation Paper Authors": "Authors:Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "retrieve individual CAD models for the detected objects in\nthe scene. Bansal et al. ",
                    "Citation Text": "Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr\nrevisited: 2D-3D alignment via surface normal prediction.\nInComputer Vision and Pattern Recognition (CVPR) , pages\n5965\u20135974, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01347",
                        "Citation Paper Title": "Title:Marr Revisited: 2D-3D Alignment via Surface Normal Prediction",
                        "Citation Paper Abstract": "Abstract:We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.",
                        "Citation Paper Authors": "Authors:Aayush Bansal, Bryan Russell, Abhinav Gupta"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "detects the objects in an RGB image, and pre-\ndicts geometry for each object individually. Instead of a gen-\nerative mesh model, Izadinia et al. ",
                    "Citation Text": "Hamid Izadinia, Qi Shan, and Steven M Seitz. IM2CAD.\nInComputer Vision and Pattern Recognition (CVPR) , pages\n5134\u20135143, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.05137",
                        "Citation Paper Title": "Title:IM2CAD",
                        "Citation Paper Abstract": "Abstract:Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database. We present a completely automatic system to address this IM2CAD problem that produces high quality results on challenging imagery from interior home design and remodeling websites. Our approach iteratively optimizes the placement and scale of objects in the room to best match scene renderings to the input photo, using image comparison metrics trained via deep convolutional neural nets. By operating jointly on the full scene at once, we account for inter-object occlusions. We also show the applicability of our method in standard scene understanding benchmarks where we obtain significant improvement.",
                        "Citation Paper Authors": "Authors:Hamid Izadinia, Qi Shan, Steven M. Seitz"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "and implicit surface func-\ntions [ 55,65], to explicit surface representations like trian-\ngular meshes [ 22,85]. To reconstruct scenes, single objects\ncan be detected ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In International Conference on Com-\nputer Vision (ICCV) , pages 2961\u20132969, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.10703v2": {
            "Paper Title": "Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual\n  Fly-Throughs",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "condition NeRF on\npredicted image features while Tancik et al. ",
                    "Citation Text": "Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\nSchmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren\nNg. Learned initializations for optimizing coordinate-based\nneural representations. In CVPR , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02189",
                        "Citation Paper Title": "Title:Learned Initializations for Optimizing Coordinate-Based Neural Representations",
                        "Citation Paper Abstract": "Abstract:Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "dataset. We\nadopt similar appearance embeddings for Mega-NeRF and\nquantify its impact in Sec. 4.2.\nConcurrent to us, Urban Radiance Fields ",
                    "Citation Text": "Konstantinos Rematas, Andrew Liu, Pratul P. Srinivasan,\nJonathan T. Barron, Andrea Tagliasacchi, Tom Funkhouser,\nand Vittorio Ferrari. Urban radiance fields. CVPR , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14643",
                        "Citation Paper Title": "Title:Urban Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The goal of this work is to perform 3D reconstruction and novel view synthesis from data captured by scanning platforms commonly deployed for world mapping in urban outdoor environments (e.g., Street View). Given a sequence of posed RGB images and lidar sweeps acquired by cameras and scanners moving through an outdoor scene, we produce a model from which 3D surfaces can be extracted and novel RGB images can be synthesized. Our approach extends Neural Radiance Fields, which has been demonstrated to synthesize realistic novel images for small scenes in controlled settings, with new methods for leveraging asynchronously captured lidar data, for addressing exposure variation between captured images, and for leveraging predicted image segmentations to supervise densities on rays pointing at the sky. Each of these three extensions provides significant performance improvements in experiments on Street View data. Our system produces state-of-the-art 3D surface reconstructions and synthesizes higher quality novel views in comparison to both traditional methods (e.g.~COLMAP) and recent neural representations (e.g.~Mip-NeRF).",
                        "Citation Paper Authors": "Authors:Konstantinos Rematas, Andrew Liu, Pratul P. Srinivasan, Jonathan T. Barron, Andrea Tagliasacchi, Thomas Funkhouser, Vittorio Ferrari"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "decomposes the scene into multiple cells via\nspatial V oronoi partitioning. Each cell is independently ren-\ndered using a smaller MLP, accelerating rendering by 3x\nover NeRF. KiloNeRF ",
                    "Citation Text": "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 14335\u2013\n14345, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13744",
                        "Citation Paper Title": "Title:KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
                        "Citation Paper Abstract": "Abstract:NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
                        "Citation Paper Authors": "Authors:Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.09127v2": {
            "Paper Title": "ICON: Implicit Clothed humans Obtained from Normals",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ", and consisting of residual blocks\nwith4down-sampling layers. The image encoder for PIFu\u0003,\nPaMIR\u0003, and ICON encis a stacked hourglass ",
                    "Citation Text": "Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation. In European\nConference on Computer Vision (ECCV) , volume 9912, pages\n483\u2013499, 2016. 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06937",
                        "Citation Paper Title": "Title:Stacked Hourglass Networks for Human Pose Estimation",
                        "Citation Paper Abstract": "Abstract:This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a \"stacked hourglass\" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.",
                        "Citation Paper Authors": "Authors:Alejandro Newell, Kaiyu Yang, Jia Deng"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "introduce deep implicit functions for clothed 3D\nhuman reconstruction from RGB images and, later ",
                    "Citation Text": "Shunsuke Saito, Tomas Simon, Jason M. Saragih, and Han-\nbyul Joo. PIFuHD: Multi-level pixel-aligned implicit function\nfor high-resolution 3D human digitization. In Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 81\u201390, 2020. 2,\n3, 5, 6, 7, 9, 10, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00452",
                        "Citation Paper Title": "Title:PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization",
                        "Citation Paper Abstract": "Abstract:Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",
                        "Citation Paper Authors": "Authors:Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "infers an occupancy \ufb01eld with body/clothing lay-\ners, registers SMPL to the body layer with inferred body-part\nsegmentation, and captures clothing as offsets from SMPL\nto the point cloud. Given an RGB image of a clothed per-\nson, ARCH ",
                    "Citation Text": "Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony\nTung. ARCH: Animatable reconstruction of clothed humans.\nInComputer Vision and Pattern Recognition (CVPR) , pages\n3090\u20133099, 2020. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04572",
                        "Citation Paper Title": "Title:ARCH: Animatable Reconstruction of Clothed Humans",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",
                        "Citation Paper Authors": "Authors:Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ".\nUnder a weak-perspective camera model, with scale\ns2Rand translation t2R3, we use the PyTorch3D ",
                    "Citation Text": "Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari. Accelerating 3D deep learning with PyTorch3D.\narXiv:2007.08501 , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08501",
                        "Citation Paper Title": "Title:Accelerating 3D Deep Learning with PyTorch3D",
                        "Citation Paper Abstract": "Abstract:Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.",
                        "Citation Paper Authors": "Authors:Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "due to its better mesh-to-image alignment compared to other\nmethods. SMPL is parameterized by shape, \f2R10, and\npose,\u00122R3\u0002K, whereN= 6;890vertices and K= 24\njoints. ICON is also compatible with SMPL-X ",
                    "Citation Text": "Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo\nBolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3D hands, face,\nand body from a single image. In Computer Vision and Pat-\ntern Recognition (CVPR) , pages 10975\u201310985, 2019. 1, 2,\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05866",
                        "Citation Paper Title": "Title:Expressive Body Capture: 3D Hands, Face, and Body from a Single Image",
                        "Citation Paper Abstract": "Abstract:To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at this https URL.",
                        "Citation Paper Authors": "Authors:Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "\u201cbody-under-clothing\u201d mesh to\nreduce ambiguities and guide front and (especially) back\nclothed-body normal prediction. To estimate the SMPL\nmeshM(\f;\u0012)2RN\u00023from imageI, we use PyMAF ",
                    "Citation Text": "Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human\npose and shape regression with pyramidal mesh alignment\nfeedback loop. In International Conference on Computer\nVision (ICCV) , pages 11446\u201311456, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.16507",
                        "Citation Paper Title": "Title:PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop",
                        "Citation Paper Abstract": "Abstract:Regression-based methods have recently shown promising results in reconstructing human meshes from monocular images. By directly mapping raw pixels to model parameters, these methods can produce parametric models in a feed-forward manner via neural networks. However, minor deviation in parameters may lead to noticeable misalignment between the estimated meshes and image evidences. To address this issue, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status in our deep regressor. In PyMAF, given the currently predicted parameters, mesh-aligned evidences will be extracted from finer-resolution features accordingly and fed back for parameter rectification. To reduce noise and enhance the reliability of these evidences, an auxiliary pixel-wise supervision is imposed on the feature encoder, which provides mesh-image correspondence guidance for our network to preserve the most related information in spatial features. The efficacy of our approach is validated on several benchmarks, including Human3.6M, 3DPW, LSP, and COCO, where experimental results show that our approach consistently improves the mesh-image alignment of the reconstruction. The project page with code and video results can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, Zhenan Sun"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "infer an embedded\ndeformation graph to manipulate implicit functions, while\nYang et al. ",
                    "Citation Text": "Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng\nHuang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel\nUrtasun. S3: Neural shape, skeleton, and skinning \ufb01eldsfor 3D human modeling. In Computer Vision and Pattern\nRecognition (CVPR) , pages 13284\u201313293, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06571",
                        "Citation Paper Title": "Title:S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling",
                        "Citation Paper Abstract": "Abstract:Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.",
                        "Citation Paper Authors": "Authors:Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01554v2": {
            "Paper Title": "Neural Head Avatars from Monocular RGB Videos",
            "Sentences": [
                {
                    "Sentence ID": 92,
                    "Sentence": ". Even though these meth-\nods produce high-quality results and even allow for real-\ntime synthesis ",
                    "Citation Text": "Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya,\nand Victor Lempitsky. Fast bi-layer neural synthesis of one-\nshot realistic head avatars. In European Conference of Com-\nputer vision (ECCV) , August 2020. 1, 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10174",
                        "Citation Paper Title": "Title:Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars",
                        "Citation Paper Abstract": "Abstract:We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.",
                        "Citation Paper Authors": "Authors:Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": "are generalized across subjects and can\ngenerate novel views, based on single or multiple input im-\nages. Even though solving geometric and temporal incon-\nsistencies, the proposed methods either fail to disentangle\npose and expression ",
                    "Citation Text": "Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot\nfree-view neural talking-head synthesis for video conferenc-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 10039\u201310049,\n2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.15126",
                        "Citation Paper Title": "Title:One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing",
                        "Citation Paper Abstract": "Abstract:We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person's appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representation, where the identity-specific and motion-related information is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing system that achieves the same visual quality as the commercial H.264 standard while only using one-tenth of the bandwidth. Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences.",
                        "Citation Paper Authors": "Authors:Ting-Chun Wang, Arun Mallya, Ming-Yu Liu"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ", neu-\nral radiance \ufb01elds (NeRF) in combination with volumet-\nric rendering ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In European conference on computer vision , pages\n405\u2013421. Springer, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13316v2": {
            "Paper Title": "Immersive Visual Analysis of Cello Bow Movements",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "creates overviews of instructional videos for\nguitar players and extracts the played notes from audio through\nCREPE ",
                    "Citation Text": "Jong W. Kim, Justin Salamon, Peter Li, and Juan P. Bello. 2018. CREPE: A Con-\nvolutional Representation for Pitch Estimation. In International Conf. Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 161\u2013165. https://doi.org/10.1109/\nICASSP.2018.8461329",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06182",
                        "Citation Paper Title": "Title:CREPE: A Convolutional Representation for Pitch Estimation",
                        "Citation Paper Abstract": "Abstract:The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.",
                        "Citation Paper Authors": "Authors:Jong Wook Kim, Justin Salamon, Peter Li, Juan Pablo Bello"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "teaches users to play guitar chords and uses audio to check for cor-\nrectness. Soloist ",
                    "Citation Text": "Bryan Wang, Meng Yu Yang, and Tovi Grossman. 2021. Soloist: Generating\nMixed-Initiative Tutorials from Existing Guitar Instructional Videos Through\nAudio Processing. In Proc. 2021 CHI Conf. Human Factors in Computing Systems\n(CHI) . ACM, Article 98, 14 pages. https://doi.org/10.1145/3411764.3445162",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08846",
                        "Citation Paper Title": "Title:Soloist: Generating Mixed-Initiative Tutorials from Existing Guitar Instructional Videos Through Audio Processing",
                        "Citation Paper Abstract": "Abstract:Learning musical instruments using online instructional videos has become increasingly prevalent. However, pre-recorded videos lack the instantaneous feedback and personal tailoring that human tutors provide. In addition, existing video navigations are not optimized for instrument learning, making the learning experience encumbered. Guided by our formative interviews with guitar players and prior literature, we designed Soloist, a mixed-initiative learning framework that automatically generates customizable curriculums from off-the-shelf guitar video lessons. Soloist takes raw videos as input and leverages deep-learning based audio processing to extract musical information. This back-end processing is used to provide an interactive visualization to support effective video navigation and real-time feedback on the user's performance, creating a guided learning experience. We demonstrate the capabilities and specific use-cases of Soloist within the domain of learning electric guitar solos using instructional YouTube videos. A remote user study, conducted to gather feedback from guitar players, shows encouraging results as the users unanimously preferred learning with Soloist over unconverted instructional videos.",
                        "Citation Paper Authors": "Authors:Bryan Wang, Mengyu Yang, Tovi Grossman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.13421v2": {
            "Paper Title": "Neural Rays for Occlusion-aware Image-based Rendering",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "22.65 0.808 0.202 19.40 0.463 0.447 18.66 0.588 0.463\nMVSNeRF ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. MVSNeRF: fast gener-\nalizable radiance field reconstruction from multi-view stereo.\nInICCV , 2021. 2, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15595",
                        "Citation Paper Title": "Title:MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo",
                        "Citation Paper Abstract": "Abstract:We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "-style\ndensity in the appendix.Synthetic Object NeRF Real Object DTU Real Forward-facing LLFF\nSettings Method PSNR \u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193\nGeneralizationPixelNeRF ",
                    "Citation Text": "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelNeRF: Neural radiance fields from one or few images.\nInCVPR , 2021. 1, 2, 3, 4, 5, 6, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02190",
                        "Citation Paper Title": "Title:pixelNeRF: Neural Radiance Fields from One or Few Images",
                        "Citation Paper Abstract": "Abstract:We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "as the metrics.\nObject dataset . The object dataset includes the NeRF\nsynthetic dataset ",
                    "Citation Text": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nCVPR , 2020. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.07372",
                        "Citation Paper Title": "Title:Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "renders photo-realistic im-\nages by volume rendering on a radiance field. Many fol-\nlowing works [9, 25, 26, 31, 35, 37, 41, 52, 59, 64] have at-\ntempted to improve NeRF in various aspects. Among these,\nNeRV ",
                    "Citation Text": "Pratul P Srinivasan, Boyang Deng, Xiuming Zhang,\nMatthew Tancik, Ben Mildenhall, and Jonathan T Barron.\nNeRV: Neural reflectance and visibility fields for relighting\nand view synthesis. In CVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03927",
                        "Citation Paper Title": "Title:NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",
                        "Citation Paper Authors": "Authors:Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04645v2": {
            "Paper Title": "BACON: Band-limited Coordinate Networks for Multiscale Scene\n  Representation",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "have also been proposed. Recent 2D GANs have\nanalyzed the bandwidth of convolutional layers for image\ngeneration ",
                    "Citation Text": "Tero Karras, Miika Aittala, Samuli Laine, Erik H \u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In Proc. NeurIPS , 2021. 2,\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.12423",
                        "Citation Paper Title": "Title:Alias-Free Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
                        "Citation Paper Authors": "Authors:Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". See supplemental for additional details and\nresults.\n4.2. Neural Radiance Fields\nNeural radiance fields (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proc. ECCV , 2020. 1, 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ", which is scale-dependent.\nWe initialize all networks with 4 hidden layers, 256 hid-\nden features, and we train on the 256 \u00d7256 resolution im-\nage for 5000 iterations using PyTorch ",
                    "Citation Text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, et al. Pytorch: An imperative style, high-\nperformance deep learning library. In Proc. NeurIPS , 2019.\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": ",\nhave tunable parameters that bias the network to fitting low-\nor high-frequency signals ",
                    "Citation Text": "Wang Yifan, Lukas Rahmann, and Olga Sorkine-Hornung.\nGeometry-consistent neural shape representation with im-\nplicit displacement fields. In Proc. ICLR , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05187",
                        "Citation Paper Title": "Title:Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields",
                        "Citation Paper Abstract": "Abstract:We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.",
                        "Citation Paper Authors": "Authors:Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.14292v2": {
            "Paper Title": "Deblur-NeRF: Neural Radiance Fields from Blurry Images",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "( PVD + NeRF ). For defocus\nblur, we compare to the KPAC ",
                    "Citation Text": "Hyeongseok Son, Junyong Lee, Sunghyun Cho, and Seungy-\nong Lee. Single image defocus deblurring using kernel-\nsharing parallel atrous convolutions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2642\u20132650, 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09108",
                        "Citation Paper Title": "Title:Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions",
                        "Citation Paper Abstract": "Abstract:This paper proposes a novel deep learning approach for single image defocus deblurring based on inverse kernels. In a defocused image, the blur shapes are similar among pixels although the blur sizes can spatially vary. To utilize the property with inverse kernels, we exploit the observation that when only the size of a defocus blur changes while keeping the shape, the shape of the corresponding inverse kernel remains the same and only the scale changes. Based on the observation, we propose a kernel-sharing parallel atrous convolutional (KPAC) block specifically designed by incorporating the property of inverse kernels for single image defocus deblurring. To effectively simulate the invariant shapes of inverse kernels with different scales, KPAC shares the same convolutional weights among multiple atrous convolution layers. To efficiently simulate the varying scales of inverse kernels, KPAC consists of only a few atrous convolution layers with different dilations and learns per-pixel scale attentions to aggregate the outputs of the layers. KPAC also utilizes the shape attention to combine the outputs of multiple convolution filters in each atrous convolution layer, to deal with defocus blur with a slightly varying shape. We demonstrate that our approach achieves state-of-the-art performance with a much smaller number of parameters than previous methods.",
                        "Citation Paper Authors": "Authors:Hyeongseok Son, Junyong Lee, Sunghyun Cho, Seungyong Lee"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "ex-\ntends the optical \ufb02ow to feature correlation volume, which\ngreatly improves the performance. Similarly, Son et al. ",
                    "Citation Text": "Hyeongseok Son, Junyong Lee, Jonghyeop Lee, Sunghyun\nCho, and Seungyong Lee. Recurrent video deblurring with\nblur-invariant motion estimation and pixel volumes. ACM\nTransactions on Graphics (TOG) , 40(5):1\u201318, 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09982",
                        "Citation Paper Title": "Title:Recurrent Video Deblurring with Blur-Invariant Motion Estimation and Pixel Volumes",
                        "Citation Paper Abstract": "Abstract:For the success of video deblurring, it is essential to utilize information from neighboring frames. Most state-of-the-art video deblurring methods adopt motion compensation between video frames to aggregate information from multiple frames that can help deblur a target frame. However, the motion compensation methods adopted by previous deblurring methods are not blur-invariant, and consequently, their accuracy is limited for blurry frames with different blur amounts. To alleviate this problem, we propose two novel approaches to deblur videos by effectively aggregating information from multiple video frames. First, we present blur-invariant motion estimation learning to improve motion estimation accuracy between blurry frames. Second, for motion compensation, instead of aligning frames by warping with estimated motions, we use a pixel volume that contains candidate sharp pixels to resolve motion estimation errors. We combine these two processes to propose an effective recurrent video deblurring network that fully exploits deblurred previous frames. Experiments show that our method achieves the state-of-the-art performance both quantitatively and qualitatively compared to recent methods that use deep learning.",
                        "Citation Paper Authors": "Authors:Hyeongseok Son, Junyong Lee, Jonghyeop Lee, Sunghyun Cho, Seungyong Lee"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "reconstructs a neural volume\nwith only one or few images. Moreover, Jonathan et al .\nproposes Mip-NeRF ",
                    "Citation Text": "Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance \ufb01elds. arXiv preprint arXiv:2103.13415 , 2021.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "intro-\nduces several extensions to NeRF that successfully model\nthe inconsistent appearance variations and transient objects\nacross views. PixelNeRF ",
                    "Citation Text": "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance \ufb01elds from one or few images.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 4578\u20134587, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02190",
                        "Citation Paper Title": "Title:pixelNeRF: Neural Radiance Fields from One or Few Images",
                        "Citation Paper Abstract": "Abstract:We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "focuses on jointly calibrating a more complex non-linear\ncamera model. To address the NeRF training under un-\ncontrolled, in-the-wild photographs, NeRF-W ",
                    "Citation Text": "Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance \ufb01elds for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 7210\u20137219, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02268",
                        "Citation Paper Title": "Title:NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12077v3": {
            "Paper Title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ", SSIM=0.522\nFigure 7. (a) A test-set image from our dataset\u2019s stump scene, with (b) our model\u2019s rendered image and depth map (median ray termination\ndistance ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, So\ufb01en\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Ner\ufb01es: Deformable Neural Radiance\nFields. ICCV , 2021. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.12948",
                        "Citation Paper Title": "Title:Nerfies: Deformable Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "across the test images\nin our dataset. For all NeRF-like models, we report train\ntimes from a TPU v2 with 32 cores ",
                    "Citation Text": "Norman P Jouppi, Cliff Young, Nishant Patil, David Patter-\nson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh\nBhatia, Nan Boden, Al Borchers, et al. In-datacenter per-\nformance analysis of a tensor processing unit. International\nSymposium on Computer Architecture , 2017. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04760",
                        "Citation Paper Title": "Title:In-Datacenter Performance Analysis of a Tensor Processing Unit",
                        "Citation Paper Abstract": "Abstract:Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.",
                        "Citation Paper Authors": "Authors:Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, Doe Hyun Yoon"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ", here we distill the structure of\nthe outputs predicted by the NeRF MLP into the proposal\nMLP \u201conline\u201d by training both networks simultaneously.\nNeRV ",
                    "Citation Text": "Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang,\nMatthew Tancik, Ben Mildenhall, and Jonathan T. Barron.\nNeRV: Neural re\ufb02ectance and visibility \ufb01elds for relighting\nand view synthesis. CVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03927",
                        "Citation Paper Title": "Title:NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",
                        "Citation Paper Authors": "Authors:Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13817v1": {
            "Paper Title": "AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02789v3": {
            "Paper Title": "HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "generates blur texture results due to the reliance on im-\nplicit texture representation, while the method ",
                    "Citation Text": "Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye\nWu, Kaiwen Guo, and Lan Xu. Neuralhumanfvv: Real-time\nneural volumetric human performance rendering using rgb\ncameras. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR) . IEEE, jun 2021. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07700",
                        "Citation Paper Title": "Title:NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras",
                        "Citation Paper Abstract": "Abstract:4D reconstruction and rendering of human activities is critical for immersive VR/AR experience.Recent advances still fail to recover fine geometry and texture results with the level of detail present in the input images from sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a real-time neural human performance capture and rendering system to generate both high-quality geometry and photo-realistic texture of human activities in arbitrary novel views. We propose a neural geometry generation scheme with a hierarchical sampling strategy for real-time implicit geometry inference, as well as a novel neural blending scheme to generate high resolution (e.g., 1k) and photo-realistic texture results in the novel views. Furthermore, we adopt neural normal blending to enhance geometry details and formulate our neural geometry and texture rendering into a multi-task learning framework. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality geometry and photo-realistic free view-point reconstruction for challenging human performances.",
                        "Citation Paper Authors": "Authors:Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Kaiwen Guo, Minye Wu, Lan Xu"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "both qualitatively\nand quantitatively. Furthermore, we also compare our\nmethod with generalizable methods, i.e., IBRNet ",
                    "Citation Text": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-\nvasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-\nBrualla, Noah Snavely, and Thomas Funkhouser. Ibrnet:\nLearning multi-view image-based rendering. In CVPR , 2021.\n1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.13090",
                        "Citation Paper Title": "Title:IBRNet: Learning Multi-View Image-Based Rendering",
                        "Citation Paper Abstract": "Abstract:We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "suffers\nfrom geometric discontinuity due to the lack of temporal\ninformation. Recently, Kwon et al. ",
                    "Citation Text": "Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry\nFuchs. Neural human performer: Learning generalizable ra-\ndiance fields for human performance rendering. Advances in\nNeural Information Processing Systems , 34, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.07448",
                        "Citation Paper Title": "Title:Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering",
                        "Citation Paper Abstract": "Abstract:In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at this https URL.",
                        "Citation Paper Authors": "Authors:Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13458v1": {
            "Paper Title": "PANDORA: Polarization-Aided Neural Decomposition Of Radiance",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ", as a build block in PANDORA.\nNeRF models the net outgoing radiance from a scene point in which both the material properties and the\nlighting are mixed. Several approaches such as NeRV ",
                    "Citation Text": "P. P. Srinivasan, B. Deng, X. Zhang, M. Tancik, B. Mildenhall, and J. T. Barron. Nerv: Neural re\ufb02ectance\nand visibility \ufb01elds for relighting and view synthesis. In CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03927",
                        "Citation Paper Title": "Title:NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",
                        "Citation Paper Authors": "Authors:Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ", which showed that modelling radiance using implicit representations\nleads to high-quality novel view synthesis.\nSince the advent of NeRF, several works have exploited neural implicit representations for inverse rendering\napplications. IDR ",
                    "Citation Text": "L. Yariv, Y. Kasten, D. Moran, M. Galun, M. Atzmon, B. Ronen, and Y. Lipman. Multiview neural surface\nreconstruction by disentangling geometry and appearance. Advances in Neural Information Processing\nSystems , 33:2492\u20132502, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09852",
                        "Citation Paper Title": "Title:Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance",
                        "Citation Paper Abstract": "Abstract:In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",
                        "Citation Paper Authors": "Authors:Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "has led to an\nexplosion of interest in neural inverse rendering ",
                    "Citation Text": "A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, Y. Wang, C. Lassner, V. Sitzmann, R. Martin-\nBrualla, S. Lombardi, et al. Advances in neural rendering. arXiv preprint arXiv:2111.05849 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05849",
                        "Citation Paper Title": "Title:Advances in Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": ".\nNeural Inverse Rendering . Recent emergence of neural implicit representations ",
                    "Citation Text": "Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann, and\nS. Sridhar. Neural \ufb01elds in visual computing and beyond. arXiv preprint arXiv:2111.11426 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13441v1": {
            "Paper Title": "3D GAN Inversion for Controllable Portrait Image Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.13438v1": {
            "Paper Title": "Microstructure Surface Reconstruction from SEM Images: An Alternative to\n  Digital Image Correlation (DIC)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.13320v1": {
            "Paper Title": "Data-Driven Visual Reflection on Music Instrument Practice",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "makes use of existing material\nby creating overviews of instructional videos for guitar and extract-\ning the played notes from audio through CREPE ",
                    "Citation Text": "Jong W. Kim, Justin Salamon, Peter Li, and Juan P. Bello. 2018. CREPE: A Con-\nvolutional Representation for Pitch Estimation. In International Conf. Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 161\u2013165. https://doi.org/10.1109/\nICASSP.2018.8461329",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06182",
                        "Citation Paper Title": "Title:CREPE: A Convolutional Representation for Pitch Estimation",
                        "Citation Paper Abstract": "Abstract:The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.",
                        "Citation Paper Authors": "Authors:Jong Wook Kim, Justin Salamon, Peter Li, Juan Pablo Bello"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13215v1": {
            "Paper Title": "Neural Neighbor Style Transfer",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "add a soft con-\nstraint on the matches found by nearest neighbors that each\nstyle vector may only be used at most ktimes. Nevertheless,\nthis constraint can be overly restrictive and lead to content\ndistortion artifacts. Liao et al. ",
                    "Citation Text": "Liao, J., Yao, Y ., Yuan, L., Hua, G., Kang, S.B.: Visual\nattribute transfer through deep image analogy. ACM\nTransactions on Graphics 36(4), 120 (2017) 2, 3, 5,\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.01088",
                        "Citation Paper Title": "Title:Visual Attribute Transfer through Deep Image Analogy",
                        "Citation Paper Abstract": "Abstract:We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene.\nOur technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of \"image analogy\" with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique Deep Image Analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse.",
                        "Citation Paper Authors": "Authors:Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, Sing Bing Kang"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "in their \u2018A Neural Algorithm of Artistic Style\u2019. This work\ncontributed in two key ideas. Firstly, it shows how to\nleverage features extracted by a convolutional neural net-\nwork pre-trained for image classi\ufb01cation tasks (typically\nVGG ",
                    "Citation Text": "Simonyan, K., Zisserman, A.: Very deep convolu-\ntional networks for large-scale image recognition. In:\nProceedings of International Conference on Learning\nRepresentations (2014) 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.1556",
                        "Citation Paper Title": "Title:Very Deep Convolutional Networks for Large-Scale Image Recognition",
                        "Citation Paper Abstract": "Abstract:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "uses\nself-attention to improve content preservation in perceptu-\nally important regions and Park et al. ",
                    "Citation Text": "Park, D.Y ., Lee, K.H.: Arbitrary style transfer with\nstyle-attentional networks. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition. pp. 5880\u20135888 (2019) 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02342",
                        "Citation Paper Title": "Title:Arbitrary Style Transfer with Style-Attentional Networks",
                        "Citation Paper Abstract": "Abstract:Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before. Recent arbitrary style transfer algorithms find it challenging to balance the content structure and the style patterns. Moreover, simultaneously maintaining the global and local style patterns is difficult due to the patch-based mechanism. In this paper, we introduce a novel style-attentional network (SANet) that efficiently and flexibly integrates the local style patterns according to the semantic spatial distribution of the content image. A new identity loss function and multi-level feature embeddings enable our SANet and decoder to preserve the content structure as much as possible while enriching the style patterns. Experimental results demonstrate that our algorithm synthesizes stylized images in real-time that are higher in quality than those produced by the state-of-the-art algorithms.",
                        "Citation Paper Authors": "Authors:Dae Young Park, Kwang Hee Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.13131v1": {
            "Paper Title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": "used diffusion models conditioned on\nimages. Inspired by the high-quality unconditional images\ngeneration model, GLIDE employed guided inference withand without a classi\ufb01er network to generate high-\ufb01delity\nimages. LAFITE ",
                    "Citation Text": "Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nTong Sun. La\ufb01te: Towards language-free training for text-to-\nimage generation. arXiv preprint arXiv:2111.13792 , 2021.\n4, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.13792",
                        "Citation Paper Title": "Title:LAFITE: Towards Language-Free Training for Text-to-Image Generation",
                        "Citation Paper Abstract": "Abstract:One of the major challenges in training text-to-image generation models is the need of a large number of high-quality image-text pairs. While image samples are often easily accessible, the associated text descriptions typically require careful human captioning, which is particularly time- and cost-consuming. In this paper, we propose the first work to train text-to-image generation models without any text data. Our method leverages the well-aligned multi-modal semantic space of the powerful pre-trained CLIP model: the requirement of text-conditioning is seamlessly alleviated via generating text features from image features. Extensive experiments are conducted to illustrate the effectiveness of the proposed method. We obtain state-of-the-art results in the standard text-to-image generation tasks. Importantly, the proposed language-free model outperforms most existing models trained with full image-text pairs. Furthermore, our method can be applied in fine-tuning pre-trained models, which saves both training time and cost in training text-to-image generation models. Our pre-trained model obtains competitive results in zero-shot text-to-image generation on the MS-COCO dataset, yet with around only 1% of the model size and training data size relative to the recently proposed large DALL-E model.",
                        "Citation Paper Authors": "Authors:Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": ", or down-sample the image res-\nolution [11, 21]. Our method is based on autoregressive\nmodeling of discrete image representation.\n2XMC-GAN ",
                    "Citation Text": "Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang. Cross-modal contrastive learning for text-to-\nimage generation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n833\u2013842, 2021. 3, 4, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.04702",
                        "Citation Paper Title": "Title:Cross-Modal Contrastive Learning for Text-to-Image Generation",
                        "Citation Paper Abstract": "Abstract:The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1 for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.",
                        "Citation Paper Authors": "Authors:Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "employed a fusion block, fusing\ntext information into image features. Contrastive learning\nfurther improved the results of DM-GAN ",
                    "Citation Text": "Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderra-\nman, and Shihao Ji. Improving text-to-image synthesis us-\ning contrastive learning. arXiv preprint arXiv:2107.02423 ,\n2021. 4, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.02423",
                        "Citation Paper Title": "Title:Improving Text-to-Image Synthesis Using Contrastive Learning",
                        "Citation Paper Abstract": "Abstract:The goal of text-to-image synthesis is to generate a visually realistic image that matches a given text description. In practice, the captions annotated by humans for the same image have large variance in terms of contents and the choice of words. The linguistic discrepancy between the captions of the identical image leads to the synthetic images deviating from the ground truth. To address this issue, we propose a contrastive learning approach to improve the quality and enhance the semantic consistency of synthetic images. In the pretraining stage, we utilize the contrastive learning approach to learn the consistent textual representations for the captions corresponding to the same image. Furthermore, in the following stage of GAN training, we employ the contrastive learning method to enhance the consistency between the generated images from the captions related to the same image. We evaluate our approach over two popular text-to-image synthesis models, AttnGAN and DM-GAN, on datasets CUB and COCO, respectively. Experimental results have shown that our approach can effectively improve the quality of synthetic images in terms of three metrics: IS, FID and R-precision. Especially, on the challenging COCO dataset, our approach boosts the FID signifcantly by 29.60% over AttnGAN and by 21.96% over DM-GAN.",
                        "Citation Paper Authors": "Authors:Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, Shihao Ji"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "introduced an attention component, allow-\ning the generator network to attend to relevant words in the\ntext. DM-GAN ",
                    "Citation Text": "Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:\nDynamic memory generative adversarial networks for text-\nto-image synthesis. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition , pages\n5802\u20135810, 2019. 4, 8, 9\n12A. Additional implementation details\nA.1. VQ-SEG\nVQ-SEG is trained for 600kiterations, with a batch size\nof48, dictionary size of 1024 . The number of segmentation\ncategories per-group are mp= 133 for the panoptic seg-\nmentation,mh= 20 for the human parsing, and mf= 5\nfor the face parsing. The per-category weight function fol-\nlows the notation:\n\u000bcat=(\n20;if cat2[154;:::;158]\n1;otherwise;(4)\nwhere cat2[154;:::;158] are the face-parts categories eye-\nbrows, eyes, nose, outer-mouth, and inner-mouth.\nA.2. VQ-IMG\nVQ-IMG 256andVQ-IMG 512are trained for 800kand\n940kiterations respectively, with a batch size of 192and\n128, a channel multiplier of [1;1;2;4]and[1;1;2;4;4],\nwhile both are trained with a dictionary size of 8192 .\nThe per-layer normalizing hyperparameter for the face-\naware loss is \u000bl\nf= [\u000bf1;\u000bf2\u00020:01;\u000bf2\u00020:1;\u000bf2\u0002\n0:2;\u000bf2\u00020:02]corresponding to the last layer of each block\nof size 1\u00021;7\u00027;28\u000228;56\u000256;128\u0002128, where\u000bf1=\n0:1and\u000bf2= 0:25. We experimented with two settings,\nthe \ufb01rst where \u000bf1=\u000bf2= 1:0, and the second, which\nwas used to train the \ufb01nal models, where \u000bf1= 0:1;\u000bf2=\n0:25. The remaining face-loss values were taken from the\nwork of",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01310",
                        "Citation Paper Title": "Title:DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis",
                        "Citation Paper Abstract": "Abstract:In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "introduced a\nspatially-adaptive normalization layer which elevated infor-\nmation lost in normalization layers. ",
                    "Citation Text": "Oran Gafni, Oron Ashual, and Lior Wolf. Single-\nshot freestyle dance reenactment. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 882\u2013891, 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.01158",
                        "Citation Paper Title": "Title:Single-Shot Freestyle Dance Reenactment",
                        "Citation Paper Abstract": "Abstract:The task of motion transfer between a source dancer and a target person is a special case of the pose transfer problem, in which the target person changes their pose in accordance with the motions of the dancer.\nIn this work, we propose a novel method that can reanimate a single image by arbitrary video sequences, unseen during training. The method combines three networks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering network, and (iii) a face refinement network. By separating this task into three stages, we are able to attain a novel sequence of realistic frames, capturing natural motion and appearance. Our method obtains significantly better visual quality than previous methods and is able to animate diverse body types and appearances, which are captured in challenging poses, as shown in the experiments and supplementary video.",
                        "Citation Paper Authors": "Authors:Oran Gafni, Oron Ashual, Lior Wolf"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "improved the\nlatter by increasing output image resolution thanks to im-\nproved network architecture. SPADE ",
                    "Citation Text": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 2337\u20132346,\n2019. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07291",
                        "Citation Paper Title": "Title:Semantic Image Synthesis with Spatially-Adaptive Normalization",
                        "Citation Paper Abstract": "Abstract:We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "utilized conditional GANs together with a su-\npervised reconstruction loss. pix2pixHD ",
                    "Citation Text": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\nthesis and semantic manipulation with conditional gans. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 8798\u20138807, 2018. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.11585",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
                        "Citation Paper Abstract": "Abstract:We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",
                        "Citation Paper Authors": "Authors:Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "projected two different do-\nmains into a shared latent space and used a per-domain\ndecoder to re-synthesize images in the desired domain.\nBoth methods do not require supervision between domains.\npix2pix ",
                    "Citation Text": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1125\u20131134,\n2017. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07004",
                        "Citation Paper Title": "Title:Image-to-Image Translation with Conditional Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
                        "Citation Paper Authors": "Authors:Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "learns\nto extract a discrete latent representation by performing on-\nline clustering. VQ-V AE-2 ",
                    "Citation Text": "Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-\ufb01delity images with vq-vae-2. Advances\nin neural information processing systems , 32, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00446",
                        "Citation Paper Title": "Title:Generating Diverse High-Fidelity Images with VQ-VAE-2",
                        "Citation Paper Abstract": "Abstract:We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",
                        "Citation Paper Authors": "Authors:Ali Razavi, Aaron van den Oord, Oriol Vinyals"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "2.1. Image generation\nRecent advancements in deep generative models\nhave enabled algorithms to generate high-quality and\nnatural-looking images. Generative Adversarial Net-\nworks (GANs) ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.12919v1": {
            "Paper Title": "Learning Dense Correspondence from Synthetic Environments",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "model that \ufb01ts a human shape\nmodel to humans in 2D images. ",
                    "Citation Text": "Matteo Fabbri, Guillem Bras \u00b4o, Gianluca Maugeri, Or-\ncun Cetintas, Riccardo Gasparini, Aljo \u02c7sa O\u02c7sep, Simone\nCalderara, Laura Leal-Taix \u00b4e, and Rita Cucchiara, \u201cMot-\nsynth: How can synthetic data help pedestrian detection\nand tracking?,\u201d in Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , 2021, pp.\n10849\u201310859.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09518",
                        "Citation Paper Title": "Title:MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?",
                        "Citation Paper Abstract": "Abstract:Deep learning-based methods for video pedestrian detection and tracking require large volumes of training data to achieve good performance. However, data acquisition in crowded public environments raises data privacy concerns -- we are not allowed to simply record and store data without the explicit consent of all participants. Furthermore, the annotation of such data for computer vision applications usually requires a substantial amount of manual effort, especially in the video domain. Labeling instances of pedestrians in highly crowded scenarios can be challenging even for human annotators and may introduce errors in the training data. In this paper, we study how we can advance different aspects of multi-person tracking using solely synthetic data. To this end, we generate MOTSynth, a large, highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking.",
                        "Citation Paper Authors": "Authors:Matteo Fabbri, Guillem Braso, Gianluca Maugeri, Orcun Cetintas, Riccardo Gasparini, Aljosa Osep, Simone Calderara, Laura Leal-Taixe, Rita Cucchiara"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "intro-\nduced a synthetic human dataset for regression analysis con-\ntaining 3D human scans placed in scenes that performed well\non \ufb01ne tuning the SMPL (Skinned Multi-Person Linear) oPti-\nmization IN the loop (SPIN) ",
                    "Citation Text": "Nikos Kolotouros, Georgios Pavlakos, Michael J Black,\nand Kostas Daniilidis, \u201cLearning to reconstruct 3d hu-\nman pose and shape via model-\ufb01tting in the loop,\u201d in\nProceedings of the IEEE/CVF International Conference\non Computer Vision , 2019, pp. 2252\u20132261.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.12828",
                        "Citation Paper Title": "Title:Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop",
                        "Citation Paper Abstract": "Abstract:Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, Kostas Daniilidis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12448v5": {
            "Paper Title": "3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch\n  Feature Swapping for Bodies and Faces",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "is tailored for the evaluation of the disen-\ntanglement of style and content information, while ",
                    "Citation Text": "Sunny Duan, Loic Matthey, Andre Saraiva, Nicholas Wat-\nters, Christopher P Burgess, Alexander Lerchner, and\nIrina Higgins. Unsupervised model selection for varia-\ntional disentangled representation learning. arXiv preprint\narXiv:1905.12614 , 2019. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12614",
                        "Citation Paper Title": "Title:Unsupervised Model Selection for Variational Disentangled Representation Learning",
                        "Citation Paper Abstract": "Abstract:Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages the recent theoretical results that explain why variational autoencoders disentangle (Rolinek et al, 2019), to quantify the quality of disentanglement by performing pairwise comparisons between trained model representations. We show that our approach performs comparably to the existing supervised alternatives across 5,400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",
                        "Citation Paper Authors": "Authors:Sunny Duan, Loic Matthey, Andre Saraiva, Nicholas Watters, Christopher P. Burgess, Alexander Lerchner, Irina Higgins"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "created a two level architecture\ncombining a point-cloud AE with a V AE where the latentspace is successfully partitioned by relying on multiple ge-\nometric losses and disentanglement penalties. ",
                    "Citation Text": "Luca Cosmo, Antonio Norelli, Oshri Halimi, Ron Kimmel,\nand Emanuele Rodola. Limp: Learning latent shape rep-\nresentations with metric preservation priors. arXiv preprint\narXiv:2003.12283 , 2, 2020. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12283",
                        "Citation Paper Title": "Title:LIMP: Learning Latent Shape Representations with Metric Preservation Priors",
                        "Citation Paper Abstract": "Abstract:In this paper, we advocate the adoption of metric preservation as a powerful prior for learning latent representations of deformable 3D shapes. Key to our construction is the introduction of a geometric distortion criterion, defined directly on the decoded shapes, translating the preservation of the metric on the decoding to the formation of linear paths in the underlying latent space. Our rationale lies in the observation that training samples alone are often insufficient to endow generative models with high fidelity, motivating the need for large training datasets. In contrast, metric preservation provides a rigorous way to control the amount of geometric distortion incurring in the construction of the latent space, leading in turn to synthetic samples of higher quality. We further demonstrate, for the first time, the adoption of differentiable intrinsic distances in the backpropagation of a geodesic loss. Our geometric priors are particularly relevant in the presence of scarce training data, where learning any meaningful latent structure can be especially challenging. The effectiveness and potential of our generative model is showcased in applications of style transfer, content generation, and shape completion.",
                        "Citation Paper Authors": "Authors:Luca Cosmo, Antonio Norelli, Oshri Halimi, Ron Kimmel, Emanuele Rodol\u00e0"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11113v2": {
            "Paper Title": "No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static\n  Models by Fitting Feature-level Space-time Surfaces",
            "Sentences": [
                {
                    "Sentence ID": 60,
                    "Sentence": ". All experi-\nments are conducted on the NVIDIA DGX-1 stations with\nTesla V100 GPUs. In most experiments, PointNet++ ",
                    "Citation Text": "Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. PointNet++: Deep hierarchical feature learning on\npoint sets in a metric space. In NeurIPS , 2017. 2, 5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ") for gesture\nrecognition or action classification. The proposed frame-\nwork is implemented with TensorFlow ",
                    "Citation Text": "Mart \u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Joze-\nfowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man \u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris\nOlah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-\nvan, Fernanda Vi \u00b4egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow:\nLarge-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "to solve the normals, which is unfriendly to back-\npropagation ",
                    "Citation Text": "Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua, and Math-\nieu Salzmann. Backpropagation-friendly eigendecomposi-\ntion. In NeurIPS , 2019. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.09023",
                        "Citation Paper Title": "Title:Backpropagation-Friendly Eigendecomposition",
                        "Citation Paper Abstract": "Abstract:Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full. In this paper, we introduce a numerically stable and differentiable approach to leveraging eigenvectors in deep networks. It can handle large matrices without requiring to split them. We demonstrate the better robustness of our approach over standard ED and PI for ZCA whitening, an alternative to batch normalization, and for PCA denoising, which we introduce as a new normalization strategy for deep networks, aiming to further denoise the network's features.",
                        "Citation Paper Authors": "Authors:Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua, Mathieu Salzmann"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "introduced correlations to continuous feature maps. For\nthe purpose of acceleration, temporal shift modules ",
                    "Citation Text": "Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift\nmodule for efficient video understanding. In ICCV , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08383",
                        "Citation Paper Title": "Title:TSM: Temporal Shift Module for Efficient Video Understanding",
                        "Citation Paper Abstract": "Abstract:The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Ji Lin, Chuang Gan, Song Han"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "adopted\nself-attentional structures along with the popularity of video\ntransformers ",
                    "Citation Text": "Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+d: A large scale dataset for 3d human activity analysis. In\nCVPR , 2016. 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.02808",
                        "Citation Paper Title": "Title:NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis",
                        "Citation Paper Abstract": "Abstract:Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.",
                        "Citation Paper Authors": "Authors:Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.12399v1": {
            "Paper Title": "PRTT: Precomputed Radiance Transfer Textures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.12339v1": {
            "Paper Title": "Real-time Rendering and Editing of Scattering Effects for Translucent\n  Objects",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ", etc.\n4. Using physically-based process to guide the generation\nof data for single image re\ufb02ection removal ",
                    "Citation Text": "S. Kim, Y . Huo, and S.-E. Yoon. Single image re\ufb02ec-\ntion removal with physically-based training images. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 5164\u2013\n5173, 2020. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11934",
                        "Citation Paper Title": "Title:Single Image Reflection Removal with Physically-Based Training Images",
                        "Citation Paper Abstract": "Abstract:Recently, deep learning-based single image reflection separation methods have been exploited widely. To benefit the learning approach, a large number of training image pairs (i.e., with and without reflections) were synthesized in various ways, yet they are away from a physically-based direction. In this paper, physically based rendering is used for faithfully synthesizing the required training images, and a corresponding network structure and loss term are proposed. We utilize existing RGBD/RGB images to estimate meshes, then physically simulate the light transportation between meshes, glass, and lens with path tracing to synthesize training data, which successfully reproduce the spatially variant anisotropic visual effect of glass reflection. For guiding the separation better, we additionally consider a module, backtrack network (BT-net) for backtracking the reflections, which removes complicated ghosting, attenuation, blurred and defocused effect of glass/lens. This enables obtaining a priori information before having the distortion. The proposed method considering additional a priori information with physically simulated training data is validated with various real reflection images and shows visually pleasant and numerical advantages compared with state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Soomin Kim, Yuchi Huo, Sung-Eui Yoon"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", use\nweight sharing to quickly predict the rendering kernel\nto speed up reconstruction ",
                    "Citation Text": "H. Fan, R. Wang, Y . Huo, and H. Bao. Real-time\nmonte carlo denoising with weight sharing kernel pre-\ndiction network. In Computer Graphics Forum , vol-\nume 40, pages 15\u201327. Wiley Online Library, 2021. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05977",
                        "Citation Paper Title": "Title:Real-time Monte Carlo Denoising with Weight Sharing Kernel Prediction Network",
                        "Citation Paper Abstract": "Abstract:Real-time Monte Carlo denoising aims at removing severe noise under low samples per pixel (spp) in a strict time budget. Recently, kernel-prediction methods use a neural network to predict each pixel's filtering kernel and have shown a great potential to remove Monte Carlo noise. However, the heavy computation overhead blocks these methods from real-time applications. This paper expands the kernel-prediction method and proposes a novel approach to denoise very low spp (e.g., 1-spp) Monte Carlo path traced images at real-time frame rates. Instead of using the neural network to directly predict the kernel map, i.e., the complete weights of each per-pixel filtering kernel, we predict an encoding of the kernel map, followed by a high-efficiency decoder with unfolding operations for a high-quality reconstruction of the filtering kernels. The kernel map encoding yields a compact single-channel representation of the kernel map, which can significantly reduce the kernel-prediction network's throughput. In addition, we adopt a scalable kernel fusion module to improve denoising quality. The proposed approach preserves kernel prediction methods' denoising quality while roughly halving its denoising time for 1-spp noisy inputs. In addition, compared with the recent neural bilateral grid-based real-time denoiser, our approach benefits from the high parallelism of kernel-based reconstruction and produces better denoising results at equal time.",
                        "Citation Paper Authors": "Authors:Hangming Fan, Rui Wang, Yuchi Huo, Hujun Bao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11938v1": {
            "Paper Title": "\u03c6-SfT: Shape-from-Template with a Physics-Based Deformation Model",
            "Sentences": [
                {
                    "Sentence ID": 60,
                    "Sentence": "to video input and es-\ntimate a temporally consistent coarse mesh reconstruction\nfor weakly articulated objects. LASR ",
                    "Citation Text": "Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,\nForrester Cole, Huiwen Chang, Deva Ramanan, William T.\nFreeman, and Ce Liu. Lasr: Learning articulated shape re-\nconstruction from a monocular video. In Computer Vision\nand Pattern Recognition (CVPR) , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.02976",
                        "Citation Paper Title": "Title:LASR: Learning Articulated Shape Reconstruction from a Monocular Video",
                        "Citation Paper Abstract": "Abstract:Remarkable progress has been made in 3D reconstruction of rigid structures from a video or a collection of images. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as parametric shape models, have achieved great success in modeling the \"closed world\" of known object categories, they cannot well handle the \"open-world\" of novel object categories or outlier shapes. In this work, we introduce a template-free approach to learn 3D shapes from a single video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical flow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parameters. Without using a category-specific shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes. Code will be available at this http URL .",
                        "Citation Paper Authors": "Authors:Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William T. Freeman, Ce Liu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "show that almost-rigid object\ncategories, like dolphins, can be reconstructed from image\ncollections. Kanazawa et al. ",
                    "Citation Text": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and\nJitendra Malik. Learning category-speci\ufb01c mesh reconstruc-\ntion from image collections. In European Conference on\nComputer Vision (ECCV) , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07549",
                        "Citation Paper Title": "Title:Learning Category-Specific Mesh Reconstruction from Image Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "run on unknown data. Some 2D keypoint lifting approaches\nfor 3D human pose estimation, such as Chen et al. ",
                    "Citation Text": "Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dy-\nlan Drover, Rohith MV , Stefan Stojanov, and James M.\nRehg. Unsupervised 3d pose estimation with geometric self-\nsupervision. In Computer Vision and Pattern Recognition\n(CVPR) , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.04812",
                        "Citation Paper Title": "Title:Unsupervised 3D Pose Estimation with Geometric Self-Supervision",
                        "Citation Paper Abstract": "Abstract:We present an unsupervised learning approach to recover 3D human pose from 2D skeletal joints extracted from a single image. Our method does not require any multi-view image data, 3D skeletons, correspondences between 2D-3D points, or use previously learned 3D priors during training. A lifting network accepts 2D landmarks as inputs and generates a corresponding 3D skeleton estimate. During training, the recovered 3D skeleton is reprojected on random camera viewpoints to generate new \"synthetic\" 2D poses. By lifting the synthetic 2D poses back to 3D and re-projecting them in the original camera view, we can define self-consistency loss both in 3D and in 2D. The training can thus be self supervised by exploiting the geometric self-consistency of the lift-reproject-lift process. We show that self-consistency alone is not sufficient to generate realistic skeletons, however adding a 2D pose discriminator enables the lifter to output valid 3D poses. Additionally, to learn from 2D poses \"in the wild\", we train an unsupervised 2D domain adapter network to allow for an expansion of 2D data. This improves results and demonstrates the usefulness of 2D pose data for unsupervised 3D lifting. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach improves upon the previous unsupervised methods by 30% and outperforms many weakly supervised approaches that explicitly use 3D data.",
                        "Citation Paper Authors": "Authors:Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, Rohith MV, Stefan Stojanov, James M. Rehg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11484v1": {
            "Paper Title": "A Virtual Point Light Generation Method in Close-Range Area",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", etc.\n4. Using physically-based process to guide the generation\nof data for single image re\ufb02ection removal ",
                    "Citation Text": "S. Kim, Y . Huo, and S.-E. Yoon. Single image re\ufb02ec-\ntion removal with physically-based training images. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 5164\u2013\n5173, 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11934",
                        "Citation Paper Title": "Title:Single Image Reflection Removal with Physically-Based Training Images",
                        "Citation Paper Abstract": "Abstract:Recently, deep learning-based single image reflection separation methods have been exploited widely. To benefit the learning approach, a large number of training image pairs (i.e., with and without reflections) were synthesized in various ways, yet they are away from a physically-based direction. In this paper, physically based rendering is used for faithfully synthesizing the required training images, and a corresponding network structure and loss term are proposed. We utilize existing RGBD/RGB images to estimate meshes, then physically simulate the light transportation between meshes, glass, and lens with path tracing to synthesize training data, which successfully reproduce the spatially variant anisotropic visual effect of glass reflection. For guiding the separation better, we additionally consider a module, backtrack network (BT-net) for backtracking the reflections, which removes complicated ghosting, attenuation, blurred and defocused effect of glass/lens. This enables obtaining a priori information before having the distortion. The proposed method considering additional a priori information with physically simulated training data is validated with various real reflection images and shows visually pleasant and numerical advantages compared with state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Soomin Kim, Yuchi Huo, Sung-Eui Yoon"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", use\nweight sharing to quickly predict the rendering kernel\nto speed up reconstruction ",
                    "Citation Text": "H. Fan, R. Wang, Y . Huo, and H. Bao. Real-time\nmonte carlo denoising with weight sharing kernel pre-\ndiction network. In Computer Graphics Forum , vol-\nume 40, pages 15\u201327. Wiley Online Library, 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05977",
                        "Citation Paper Title": "Title:Real-time Monte Carlo Denoising with Weight Sharing Kernel Prediction Network",
                        "Citation Paper Abstract": "Abstract:Real-time Monte Carlo denoising aims at removing severe noise under low samples per pixel (spp) in a strict time budget. Recently, kernel-prediction methods use a neural network to predict each pixel's filtering kernel and have shown a great potential to remove Monte Carlo noise. However, the heavy computation overhead blocks these methods from real-time applications. This paper expands the kernel-prediction method and proposes a novel approach to denoise very low spp (e.g., 1-spp) Monte Carlo path traced images at real-time frame rates. Instead of using the neural network to directly predict the kernel map, i.e., the complete weights of each per-pixel filtering kernel, we predict an encoding of the kernel map, followed by a high-efficiency decoder with unfolding operations for a high-quality reconstruction of the filtering kernels. The kernel map encoding yields a compact single-channel representation of the kernel map, which can significantly reduce the kernel-prediction network's throughput. In addition, we adopt a scalable kernel fusion module to improve denoising quality. The proposed approach preserves kernel prediction methods' denoising quality while roughly halving its denoising time for 1-spp noisy inputs. In addition, compared with the recent neural bilateral grid-based real-time denoiser, our approach benefits from the high parallelism of kernel-based reconstruction and produces better denoising results at equal time.",
                        "Citation Paper Authors": "Authors:Hangming Fan, Rui Wang, Yuchi Huo, Hujun Bao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11452v1": {
            "Paper Title": "Improved characterization of Lagrangian coherent structures through\n  time-scale analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.11398v1": {
            "Paper Title": "A Hybrid Lagrangian-Eulerian Model for the Structural Analysis of\n  Multifield Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09383v2": {
            "Paper Title": "DeepCurrents: Learning Implicit Representations of Shapes with\n  Boundaries",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": ",\nwhich learns unsigned distance functions rather than SDFs,\nand ",
                    "Citation Text": "R. Venkatesh, T. Karmali, S. Sharma, A. Ghosh, R. V . Babu,\nL. A. Jeni, and M. Singh. Deep implicit surface point predic-\ntion networks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , pages 12653\u201312662,\n2021-10. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05779",
                        "Citation Paper Title": "Title:Deep Implicit Surface Point Prediction Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural representations of 3D shapes as implicit functions have been shown to produce high fidelity models surpassing the resolution-memory trade-off faced by the explicit representations using meshes and point clouds. However, most such approaches focus on representing closed shapes. Unsigned distance function (UDF) based approaches have been proposed recently as a promising alternative to represent both open and closed shapes. However, since the gradients of UDFs vanish on the surface, it is challenging to estimate local (differential) geometric properties like the normals and tangent planes which are needed for many downstream applications in vision and graphics. There are additional challenges in computing these properties efficiently with a low-memory footprint. This paper presents a novel approach that models such surfaces using a new class of implicit representations called the closest surface-point (CSP) representation. We show that CSP allows us to represent complex surfaces of any topology (open or closed) with high fidelity. It also allows for accurate and efficient computation of local geometric properties. We further demonstrate that it leads to efficient implementation of downstream algorithms like sphere-tracing for rendering the 3D surface as well as to create explicit mesh-based representations. Extensive experimental evaluation on the ShapeNet dataset validate the above contributions with results surpassing the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Rahul Venkatesh, Tejan Karmali, Sarthak Sharma, Aurobrata Ghosh, R. Venkatesh Babu, L\u00e1szl\u00f3 A. Jeni, Maneesh Singh"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "restrict the class of\nlearned implicit surfaces to half-spaces and convex hulls,\nrespectively. ",
                    "Citation Text": "S.-L. Liu, H.-X. Guo, H. Pan, P. Wang, X. Tong, and Y . Liu.\nDeep implicit moving least-squares functions for 3D recon-\nstruction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12266",
                        "Citation Paper Title": "Title:Deep Implicit Moving Least-Squares Functions for 3D Reconstruction",
                        "Citation Paper Abstract": "Abstract:Point set is a flexible and lightweight representation widely used for 3D deep learning. However, their discrete nature prevents them from representing continuous and fine geometry, posing a major issue for learning-based shape generation. In this work, we turn the discrete point sets into smooth surfaces by introducing the well-known implicit moving least-squares (IMLS) surface formulation, which naturally defines locally implicit functions on point sets. We incorporate IMLS surface generation into deep neural networks for inheriting both the flexibility of point sets and the high quality of implicit surfaces. Our IMLSNet predicts an octree structure as a scaffold for generating MLS points where needed and characterizes shape geometry with learned local priors. Furthermore, our implicit function evaluation is independent of the neural network once the MLS points are predicted, thus enabling fast runtime evaluation. Our experiments on 3D object reconstruction demonstrate that IMLSNets outperform state-of-the-art learning-based methods in terms of reconstruction quality and computational efficiency. Extensive ablation tests also validate our network design and loss functions.",
                        "Citation Paper Authors": "Authors:Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ",\nmanipulations can be applied to learned implicit shapes by\nmaking changes to corresponding explicit geometric prim-\nitives. BSP-Net ",
                    "Citation Text": "Z. Chen, A. Tagliasacchi, and H. Zhang. BSP-Net: Gen-\nerating compact meshes via binary space partitioning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.06971",
                        "Citation Paper Title": "Title:BSP-Net: Generating Compact Meshes via Binary Space Partitioning",
                        "Citation Paper Abstract": "Abstract:Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "reconstruct shapes by\nlearning multiple implicit representations arranged accord-\ning to a learned template con\ufb01guration. In DualSDF ",
                    "Citation Text": "Z. Hao, H. Averbuch-Elor, N. Snavely, and S. Belongie. Du-\nalSDF: Semantic shape manipulation using a two-level rep-\nresentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n7631\u20137641, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.02869",
                        "Citation Paper Title": "Title:DualSDF: Semantic Shape Manipulation using a Two-Level Representation",
                        "Citation Paper Abstract": "Abstract:We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",
                        "Citation Paper Authors": "Authors:Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", ap-\nplying targeted manipulations and deformations to learned\nshapes remains nontrivial. Several papers propose hybrid\nrepresentations , combining the expressive power of neural\nimplicit representations with the control afforded by explicit\nrepresentations. Genova et al. ",
                    "Citation Text": "K. Genova, F. Cole, A. Sud, A. Sarna, and T. Funkhouser.\nLocal deep implicit functions for 3D shape. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 4857\u20134866, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06126",
                        "Citation Paper Title": "Title:Local Deep Implicit Functions for 3D Shape",
                        "Citation Paper Abstract": "Abstract:The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",
                        "Citation Paper Authors": "Authors:Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "learn a \ufb01eld that approximates signed distance to the\ntarget geometry, while Mescheder et al. ",
                    "Citation Text": "L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and\nA. Geiger. Occupancy networks: Learning 3D reconstruction\nin function space. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03828",
                        "Citation Paper Title": "Title:Occupancy Networks: Learning 3D Reconstruction in Function Space",
                        "Citation Paper Abstract": "Abstract:With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11283v1": {
            "Paper Title": "NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". This can be potentially addressed\nin the future by doing per-view reconstruction in disparity\nspace or applying spherical coordinates for regions at long\ndistances (similar to ",
                    "Citation Text": "Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\n\ufb01elds. arXiv preprint arXiv:2010.07492 , 2020. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.07492",
                        "Citation Paper Title": "Title:NeRF++: Analyzing and Improving Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.10521v1": {
            "Paper Title": "Variational Hierarchical Directed Bounding Box Construction for Solid\n  Mesh Models",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ", anti-aliasing [ ?], etc.\n4. Using physically-based process to guide the generation\nof data for single image re\ufb02ection removal ",
                    "Citation Text": "S. Kim, Y . Huo, and S.-E. Yoon. Single image re\ufb02ec-\ntion removal with physically-based training images. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 5164\u2013\n5173, 2020. 2, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11934",
                        "Citation Paper Title": "Title:Single Image Reflection Removal with Physically-Based Training Images",
                        "Citation Paper Abstract": "Abstract:Recently, deep learning-based single image reflection separation methods have been exploited widely. To benefit the learning approach, a large number of training image pairs (i.e., with and without reflections) were synthesized in various ways, yet they are away from a physically-based direction. In this paper, physically based rendering is used for faithfully synthesizing the required training images, and a corresponding network structure and loss term are proposed. We utilize existing RGBD/RGB images to estimate meshes, then physically simulate the light transportation between meshes, glass, and lens with path tracing to synthesize training data, which successfully reproduce the spatially variant anisotropic visual effect of glass reflection. For guiding the separation better, we additionally consider a module, backtrack network (BT-net) for backtracking the reflections, which removes complicated ghosting, attenuation, blurred and defocused effect of glass/lens. This enables obtaining a priori information before having the distortion. The proposed method considering additional a priori information with physically simulated training data is validated with various real reflection images and shows visually pleasant and numerical advantages compared with state-of-the-art techniques.",
                        "Citation Paper Authors": "Authors:Soomin Kim, Yuchi Huo, Sung-Eui Yoon"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", use\nweight sharing to quickly predict the rendering kernel\n9to speed up reconstruction ",
                    "Citation Text": "H. Fan, R. Wang, Y . Huo, and H. Bao. Real-time\nmonte carlo denoising with weight sharing kernel pre-\ndiction network. In Computer Graphics Forum , vol-\nume 40, pages 15\u201327. Wiley Online Library, 2021. 2,\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05977",
                        "Citation Paper Title": "Title:Real-time Monte Carlo Denoising with Weight Sharing Kernel Prediction Network",
                        "Citation Paper Abstract": "Abstract:Real-time Monte Carlo denoising aims at removing severe noise under low samples per pixel (spp) in a strict time budget. Recently, kernel-prediction methods use a neural network to predict each pixel's filtering kernel and have shown a great potential to remove Monte Carlo noise. However, the heavy computation overhead blocks these methods from real-time applications. This paper expands the kernel-prediction method and proposes a novel approach to denoise very low spp (e.g., 1-spp) Monte Carlo path traced images at real-time frame rates. Instead of using the neural network to directly predict the kernel map, i.e., the complete weights of each per-pixel filtering kernel, we predict an encoding of the kernel map, followed by a high-efficiency decoder with unfolding operations for a high-quality reconstruction of the filtering kernels. The kernel map encoding yields a compact single-channel representation of the kernel map, which can significantly reduce the kernel-prediction network's throughput. In addition, we adopt a scalable kernel fusion module to improve denoising quality. The proposed approach preserves kernel prediction methods' denoising quality while roughly halving its denoising time for 1-spp noisy inputs. In addition, compared with the recent neural bilateral grid-based real-time denoiser, our approach benefits from the high parallelism of kernel-based reconstruction and produces better denoising results at equal time.",
                        "Citation Paper Authors": "Authors:Hangming Fan, Rui Wang, Yuchi Huo, Hujun Bao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.12671v1": {
            "Paper Title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of\n  Academic Performance",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "visualized the paper based on\nits reference and the papers citing it. This visualization aims\nto demonstrate the impact of individual papers instead of\nthe entire network. Shin et al. ",
                    "Citation Text": "M. Shin, A. Soen, B. T. Readshaw, S. M. Blackburn, M. Whitelaw,\nand L. Xie. In\ufb02uence \ufb02owers of academic entities. In Proceedings\nof IEEE Conference on Visual Analytics Science and Technology , pages\n1\u201310, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.12748",
                        "Citation Paper Title": "Title:Influence Flowers of Academic Entities",
                        "Citation Paper Abstract": "Abstract:We present the Influence Flower, a new visual metaphor for the influence profile of academic entities, including people, projects, institutions, conferences, and journals. While many tools quantify influence, we aim to expose the flow of influence between entities. The Influence Flower is an ego-centric graph, with a query entity placed in the centre. The petals are styled to reflect the strength of influence to and from other entities of the same or different type. For example, one can break down the incoming and outgoing influences of a research lab by research topics. The Influence Flower uses a recent snapshot of Microsoft Academic Graph, consisting of 212million authors, their 176 million publications, and 1.2 billion citations. An interactive web app, Influence Map, is constructed around this central metaphor for searching and curating visualisations. We also propose a visual comparison method that highlights change in influence patterns over time. We demonstrate through several case studies that the Influence Flower supports data-driven inquiries about the following: researchers' careers over time; paper(s) and projects, including those with delayed recognition; the interdisciplinary profile of a research institution; and the shifting topical trends in conferences. We also use this tool on influence data beyond academic citations, by contrasting the academic and Twitter activities of a researcher.",
                        "Citation Paper Authors": "Authors:Minjeong Shin, Alexander Soen, Benjamin T. Readshaw, Stephen M. Blackburn, Mitchell Whitelaw, Lexing Xie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.10213v1": {
            "Paper Title": "Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.16748v3": {
            "Paper Title": "Dual Contrastive Loss and Attention for GANs",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": "compresses input tensor to a set of 1D\nfeature vectors, interprets them as semantic tokens, and\nleverages language transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 3,\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.08528v2": {
            "Paper Title": "Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion\n  Tracking from Sparse Inertial Sensors",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "pro-\npose a weakly-supervised learning framework for dynamics\nestimation from human motion. Shimada et al. ",
                    "Citation Text": "Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick\nP\u00b4erez, and Christian Theobalt. Neural monocular 3d human\nmotion capture with physical awareness. ACM Transactions\non Graphics , 40, aug 2021. 2, 3, 5, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01057",
                        "Citation Paper Title": "Title:Neural Monocular 3D Human Motion Capture with Physical Awareness",
                        "Citation Paper Abstract": "Abstract:We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub physionical, is aware of physical and environmental constraints. It combines in a fully differentiable way several key innovations, i.e., 1. a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2. an explicit rigid body dynamics model and 3. a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters -- both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are available at this http URL",
                        "Citation Paper Authors": "Authors:Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick P\u00e9rez, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "use sparse orientation measurements to perform\nperson-specific pose estimation. To improve the accuracy,\nrecent works [12, 20, 44, 66, 73] leverage both acceleration\nand orientation measurements. Marcard et al. ",
                    "Citation Text": "Timo von Marcard, Bodo Rosenhahn, Michael Black, and\nGerard Pons-Moll. Sparse inertial poser: Automatic 3d hu-\nman pose estimation from sparse imus. Computer Graph-\nics Forum 36(2), Proceedings of the 38th Annual Conference\nof the European Association for Computer Graphics (Euro-\ngraphics) , 2017. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.08014",
                        "Citation Paper Title": "Title:Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs",
                        "Citation Paper Abstract": "Abstract:We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.",
                        "Citation Paper Authors": "Authors:Timo von Marcard, Bodo Rosenhahn, Michael J. Black, Gerard Pons-Moll"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05329v4": {
            "Paper Title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ", it requires large\namounts of high-\ufb01delity 3D facial data to ensure the anima-\ntion quality and the generalization to unseen identities.\n2.2. Transformers in Vision and Graphics\nTransformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems , pages 5998\u20136008, 2017. 2,\n3, 4, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": ", etc. In\ncomputer graphics, transformers have been exploited for\n3D point cloud representations and 3D mesh, such as Point\nTransformer ",
                    "Citation Text": "Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164 , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.09164",
                        "Citation Paper Title": "Title:Point Transformer",
                        "Citation Paper Abstract": "Abstract:Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",
                        "Citation Paper Authors": "Authors:Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "leverages the linear\npredictive coding method to encode audio and designs a la-tent code to disambiguate the variations in facial expres-\nsion. Zhou et al . ",
                    "Citation Text": "Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kaloger-\nakis, Subhransu Maji, and Karan Singh. Visemenet: Audio-\ndriven animator-centric speech animation. ACM Transac-\ntions on Graphics , 37(4):1\u201310, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.09488",
                        "Citation Paper Title": "Title:VisemeNet: Audio-Driven Animator-Centric Speech Animation",
                        "Citation Paper Abstract": "Abstract:We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.",
                        "Citation Paper Authors": "Authors:Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, Karan Singh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.07858v2": {
            "Paper Title": "A Survey of Non-Rigid 3D Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08612v1": {
            "Paper Title": "CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08272v1": {
            "Paper Title": "Active Exploration for Neural Global Illumination of Variable Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08063v1": {
            "Paper Title": "MotionCLIP: Exposing Human Motion Generation to CLIP Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.12914v2": {
            "Paper Title": "SILT: Self-supervised Lighting Transfer Using Implicit Image\n  Decomposition",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "was created and used for\nrelighting challenges [9, 10]. It features indoor and outdoor scenes lit from 8 directions and\ncaptured under 5 temperature settings. The applications of VIDIT include ",
                    "Citation Text": "Li-Wen Wang, Wan-Chi Siu, Zhi-Song Liu, Chu-Tak Li, and Daniel P. K. Lun. Deep\nRelighting Networks for Image Light Source Manipulation. In Proceedings of the\nEuropean Conference on Computer Vision Workshops (ECCVW) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.08298",
                        "Citation Paper Title": "Title:Deep Relighting Networks for Image Light Source Manipulation",
                        "Citation Paper Abstract": "Abstract:Manipulating the light source of given images is an interesting task and useful in various applications, including photography and cinematography. Existing methods usually require additional information like the geometric structure of the scene, which may not be available for most images. In this paper, we formulate the single image relighting task and propose a novel Deep Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to reveal the primary scene structure through a deep auto-encoder network, 2) shadow prior estimation, to predict light effect from the new light direction through adversarial learning, and 3) re-renderer, to combine the primary structure with the reconstructed shadow view to form the required estimation under the target light source. Experimental results show that the proposed method outperforms other possible methods, both qualitatively and quantitatively. Specifically, the proposed DRN has achieved the best PSNR in the \"AIM2020 - Any to one relighting challenge\" of the 2020 ECCV conference.",
                        "Citation Paper Authors": "Authors:Li-Wen Wang, Wan-Chi Siu, Zhi-Song Liu, Chu-Tak Li, Daniel P.K. Lun"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "train their model to convert all input directions and temperatures of\nthe training set to 4500K-E, and then during inference convert from 6500K-N to 4500K-E\n(as in the AIM2020 challenge ",
                    "Citation Text": "Majed El Helou, Ruofan Zhou, Sabine S\u00fcsstrunk, Radu Timofte, et al. AIM 2020:\nScene relighting and illumination estimation challenge. In Proceedings of the European\nConference on Computer Vision Workshops (ECCVW) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.12798",
                        "Citation Paper Title": "Title:AIM 2020: Scene Relighting and Illumination Estimation Challenge",
                        "Citation Paper Abstract": "Abstract:We review the AIM 2020 challenge on virtual image relighting and illumination estimation. This paper presents the novel VIDIT dataset used in the challenge and the different proposed solutions and final evaluation results over the 3 challenge tracks. The first track considered one-to-one relighting; the objective was to relight an input photo of a scene with a different color temperature and illuminant orientation (i.e., light source position). The goal of the second track was to estimate illumination settings, namely the color temperature and orientation, from a given image. Lastly, the third track dealt with any-to-any relighting, thus a generalization of the first track. The target color temperature and orientation, rather than being pre-determined, are instead given by a guide image. Participants were allowed to make use of their track 1 and 2 solutions for track 3. The tracks had 94, 52, and 56 registered participants, respectively, leading to 20 confirmed submissions in the final competition stage.",
                        "Citation Paper Authors": "Authors:Majed El Helou, Ruofan Zhou, Sabine S\u00fcsstrunk, Radu Timofte, Mahmoud Afifi, Michael S. Brown, Kele Xu, Hengxing Cai, Yuzhong Liu, Li-Wen Wang, Zhi-Song Liu, Chu-Tak Li, Sourya Dipta Das, Nisarg A. Shah, Akashdeep Jassal, Tongtong Zhao, Shanshan Zhao, Sabari Nathan, M. Parisa Beham, R. Suganya, Qing Wang, Zhongyun Hu, Xin Huang, Yaning Li, Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan, Densen Puthussery, Hrishikesh P S, Melvin Kuriakose, Jiji C V, Yu Zhu, Liping Dong, Zhuolong Jiang, Chenghua Li, Cong Leng, Jian Cheng"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "architecture by adding light direction estimation layers, and train\n8 network branches that can relight any image to 1 of 8 illumination directions covered by\nVIDIT. Puthussery et al. ",
                    "Citation Text": "Densen Puthussery, Hrishikesh P. S., Melvin Kuriakose, and Jiji C. V . WDRN : A\nWavelet Decomposed RelightNet for Image Relighting. In Proceedings of the Euro-\npean Conference on Computer Vision Workshops (ECCVW) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.06678",
                        "Citation Paper Title": "Title:WDRN : A Wavelet Decomposed RelightNet for Image Relighting",
                        "Citation Paper Abstract": "Abstract:The task of recalibrating the illumination settings in an image to a target configuration is known as relighting. Relighting techniques have potential applications in digital photography, gaming industry and in augmented reality. In this paper, we address the one-to-one relighting problem where an image at a target illumination settings is predicted given an input image with specific illumination conditions. To this end, we propose a wavelet decomposed RelightNet called WDRN which is a novel encoder-decoder network employing wavelet based decomposition followed by convolution layers under a muti-resolution framework. We also propose a novel loss function called gray loss that ensures efficient learning of gradient in illumination along different directions of the ground truth image giving rise to visually superior relit images. The proposed solution won the first position in the relighting challenge event in advances in image manipulation (AIM) 2020 workshop which proves its effectiveness measured in terms of a Mean Perceptual Score which in turn is measured using SSIM and a Learned Perceptual Image Patch Similarity score.",
                        "Citation Paper Authors": "Authors:Densen Puthussery, Hrishikesh P.S., Melvin Kuriakose, Jiji C.V"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Das et al. stack two\nU-Nets to achieve good quality reconstructions while keeping the computational cost very\nlow. They expand on this idea in ",
                    "Citation Text": "Sourya Dipta Das, Nisarg A. Shah, Saikat Dutta, and Himanshu Kumar. DSRN: an\nEf\ufb01cient Deep Network for Image Relighting. arXiv preprint 2102.09242 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.09242",
                        "Citation Paper Title": "Title:DSRN: an Efficient Deep Network for Image Relighting",
                        "Citation Paper Abstract": "Abstract:Custom and natural lighting conditions can be emulated in images of the scene during post-editing. Extraordinary capabilities of the deep learning framework can be utilized for such purpose. Deep image relighting allows automatic photo enhancement by illumination-specific retouching. Most of the state-of-the-art methods for relighting are run-time intensive and memory inefficient. In this paper, we propose an efficient, real-time framework Deep Stacked Relighting Network (DSRN) for image relighting by utilizing the aggregated features from input image at different scales. Our model is very lightweight with total size of about 42 MB and has an average inference time of about 0.0116s for image of resolution $1024 \\times 1024$ which is faster as compared to other multi-scale models. Our solution is quite robust for translating image color temperature from input image to target image and also performs moderately for light gradient generation with respect to the target image. Additionally, we show that if images illuminated from opposite directions are used as input, the qualitative results improve over using a single input image.",
                        "Citation Paper Authors": "Authors:Sourya Dipta Das, Nisarg A. Shah, Saikat Dutta, Himanshu Kumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.07293v1": {
            "Paper Title": "InsetGAN for Full-Body Image Generation",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "and the explicit modeling\nof structure [19, 34]. Though these two papers specialize\nin human bodies, we find that the GAN architecture Co-\nModGAN ",
                    "Citation Text": "Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image com-\npletion via Co-Modulated generative adversarial networks.\nInInternational Conference on Learning Representations\n(ICLR) , 2021. 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10428",
                        "Citation Paper Title": "Title:Large Scale Image Completion via Co-Modulated Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "is of-\nten the architecture of choice. In our work, we are build-\ning on StyleGAN2-ADA, since this architecture yields bet-\nter FID ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. Advances in neural information processing systems ,\n30, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.02740v2": {
            "Paper Title": "WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud\n  Registration",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "s have lever-\naged contrastive learning, such as with the triplet ",
                    "Citation Text": "Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, \u201cThe perfect\nmatch: 3D point cloud matching with smoothed densities,\u201d in\nCVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.06879",
                        "Citation Paper Title": "Title:The Perfect Match: 3D Point Cloud Matching with Smoothed Densities",
                        "Citation Paper Abstract": "Abstract:We propose 3DSmoothNet, a full workflow to match 3D point clouds with a siamese deep learning architecture and fully convolutional layers using a voxelized smoothed density value (SDV) representation. The latter is computed per interest point and aligned to the local reference frame (LRF) to achieve rotation invariance. Our compact, learned, rotation invariant 3D point cloud descriptor achieves 94.9% average recall on the 3DMatch benchmark data set, outperforming the state-of-the-art by more than 20 percent points with only 32 output dimensions. This very low output dimension allows for near realtime correspondence search with 0.1 ms per feature point on a standard PC. Our approach is sensor- and sceneagnostic because of SDV, LRF and learning highly descriptive features with fully convolutional layers. We show that 3DSmoothNet trained only on RGB-D indoor scenes of buildings achieves 79.0% average recall on laser scans of outdoor vegetation, more than double the performance of our closest, learning-based competitors. Code, data and pre-trained models are available online at this https URL.",
                        "Citation Paper Authors": "Authors:Zan Gojcic, Caifa Zhou, Jan D. Wegner, Andreas Wieser"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "used in\nPPFNet. To avoid the issues of ground-truth labeling, ex-\nisting literature has further investigated unsupervised de-\nscriptor learning typically by taking auto-encoders ",
                    "Citation Text": "Y. Yang, C. Feng, Y. Shen, and D. Tian, \u201cFoldingNet: Point cloud\nauto-encoder via deep grid deformation,\u201d in CVPR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.07262",
                        "Citation Paper Title": "Title:FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation",
                        "Citation Paper Abstract": "Abstract:Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at this http URL",
                        "Citation Paper Authors": "Authors:Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12865v2": {
            "Paper Title": "Sparsity-Specific Code Optimization using Expression Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.05965v1": {
            "Paper Title": "Human-Like Navigation Behavior: A Statistical Evaluation Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.01366v2": {
            "Paper Title": "RTNN: Accelerating Neighbor Search Using Hardware Ray Tracing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.04930v1": {
            "Paper Title": "Triangular Character Animation Sampling with Motion, Emotion, and\n  Relation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.04889v1": {
            "Paper Title": "Low-light Image and Video Enhancement via Selective Manipulation of\n  Chromaticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01266v2": {
            "Paper Title": "Sound-to-Imagination: An Exploratory Study on Unsupervised Crossmodal\n  Translation Using Diverse Audiovisual Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.12567v2": {
            "Paper Title": "Sparse Sampling and Completion for Light Transport in VPL-based\n  Rendering",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ". In this paper, we chose an nonnegative factor-\nization method ",
                    "Citation Text": "Y . Xu, W. Yin, Z. Wen, and Y . Zhang. An alternating\ndirection algorithm for matrix completion with non-\nnegative factors. CoRR , abs/1103.1168, 2011. 1,3,5,\n8\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.1168",
                        "Citation Paper Title": "Title:An Alternating Direction Algorithm for Matrix Completion with Nonnegative Factors",
                        "Citation Paper Abstract": "Abstract:This paper introduces an algorithm for the nonnegative matrix factorization-and-completion problem, which aims to find nonnegative low-rank matrices X and Y so that the product XY approximates a nonnegative data matrix M whose elements are partially known (to a certain accuracy). This problem aggregates two existing problems: (i) nonnegative matrix factorization where all entries of M are given, and (ii) low-rank matrix completion where nonnegativity is not required. By taking the advantages of both nonnegativity and low-rankness, one can generally obtain superior results than those of just using one of the two properties. We propose to solve the non-convex constrained least-squares problem using an algorithm based on the classic alternating direction augmented Lagrangian method. Preliminary convergence properties of the algorithm and numerical simulation results are presented. Compared to a recent algorithm for nonnegative matrix factorization, the proposed algorithm produces factorizations of similar quality using only about half of the matrix entries. On tasks of recovering incomplete grayscale and hyperspectral images, the proposed algorithm yields overall better qualities than those produced by two recent matrix-completion algorithms that do not exploit nonnegativity.",
                        "Citation Paper Authors": "Authors:Yangyang Xu, Wotao Yin, Zaiwen Wen, Yin Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.10741v3": {
            "Paper Title": "GLIDE: Towards Photorealistic Image Generation and Editing with\n  Text-Guided Diffusion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03870v1": {
            "Paper Title": "Morphological Anti-Aliasing Method for Boundary Slope Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03570v1": {
            "Paper Title": "Kubric: A scalable dataset generator",
            "Sentences": [
                {
                    "Sentence ID": 63,
                    "Sentence": "have led to\nstate-of-the-art results in novel-view synthesis tasks on real-\nworld datasets ",
                    "Citation Text": "Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj-\njadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel\nDuckworth. NeRF in the Wild: Neural Radiance Fields for\nUnconstrained Photo Collections. In CVPR , 2021. 16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02268",
                        "Citation Paper Title": "Title:NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ".\nFigure 9 (bottom) shows that this simple pilot experiment\nalready halves the gap between random pre-training and pre-\ntraining on ImageNet, suggesting that this is a promising\napproach.\n4.6. Robust NeRF\nNeural Radiance Fields ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Ravi\nRamamoorthi, and Ren Ng. Nerf: Representing scenes as\nneural radiance \ufb01elds for view synthesis. In ECCV , 2020. 2,\n9, 16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". There are many large synthetic datasets such as\nCLEVR ",
                    "Citation Text": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten,\nLi Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR:\nA diagnostic dataset for compositional language and elemen-\ntary visual reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , 2017. 2,\n6, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.06890",
                        "Citation Paper Title": "Title:CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                        "Citation Paper Abstract": "Abstract:When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 120,
                    "Sentence": "(a proprietary\ndata set with more diverse poses). As in Simpose ",
                    "Citation Text": "Tyler Zhu, Per Karlsson, and Christoph Bregler. Simpose:\nEffectively learning densepose and surface normals of peo-\nple from simulated data. In European Conference on Com-\nputer Vision , pages 225\u2013242. Springer, 2020. 8\n21",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15506",
                        "Citation Paper Title": "Title:SimPose: Effectively Learning DensePose and Surface Normals of People from Simulated Data",
                        "Citation Paper Abstract": "Abstract:With a proliferation of generic domain-adaptation approaches, we report a simple yet effective technique for learning difficult per-pixel 2.5D and 3D regression representations of articulated people. We obtained strong sim-to-real domain generalization for the 2.5D DensePose estimation task and the 3D human surface normal estimation task. On the multi-person DensePose MSCOCO benchmark, our approach outperforms the state-of-the-art methods which are trained on real images that are densely labelled. This is an important result since obtaining human manifold's intrinsic uv coordinates on real images is time consuming and prone to labeling noise. Additionally, we present our model's 3D surface normal predictions on the MSCOCO dataset that lacks any real 3D surface normal labels. The key to our approach is to mitigate the \"Inter-domain Covariate Shift\" with a carefully selected training batch from a mixture of domain samples, a deep batch-normalized residual network, and a modified multi-task learning objective. Our approach is complementary to existing domain-adaptation techniques and can be applied to other dense per-pixel pose estimation problems.",
                        "Citation Paper Authors": "Authors:Tyler Zhu, Per Karlsson, Christoph Bregler"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": ". However, FlyingChairs lacks photo-\nrealism, uses synthetic chairs as the only foreground ob-\njects, and does not have general 3D motion. AutoFlow ",
                    "Citation Text": "Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun\nJampani, Michael Krainin, Huiwen Chang, Ramin Zabih,\nWilliam T Freeman, and Ce Liu. Auto\ufb02ow: Learning a\nbet @articleMayer2016-xo, title = \u201dA large dataset to train\nconvolutional networks for disparity, optical \ufb02ow, and scene\n\ufb02ow estimation\u201d, author = \u201dMayer, N and Ilg, E and Hausser,\nP and Fischer, P and others\u201d, journal = \u201dProceedings of the\u201d,\npublisher = \u201dopenaccess.thecvf.com\u201d, year = 2016 ter train-\ning set for optical \ufb02ow. In CVPR , 2021. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.14544",
                        "Citation Paper Title": "Title:AutoFlow: Learning a Better Training Set for Optical Flow",
                        "Citation Paper Abstract": "Abstract:Synthetic datasets play a critical role in pre-training CNN models for optical flow, but they are painstaking to generate and hard to adapt to new applications. To automate the process, we present AutoFlow, a simple and effective method to render training data for optical flow that optimizes the performance of a model on a target dataset. AutoFlow takes a layered approach to render synthetic data, where the motion, shape, and appearance of each layer are controlled by learnable hyperparameters. Experimental results show that AutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and RAFT. Our code and data are available at this https URL .",
                        "Citation Paper Authors": "Authors:Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin, Huiwen Chang, Ramin Zabih, William T. Freeman, Ce Liu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "creates high quality room\nscenes, but has many manual steps including pose align-\nment and material assignment. ",
                    "Citation Text": "A Eftekhar, A Sax, and others. Omnidata: A scalable\npipeline for making multi-task mid-level vision datasets\nfrom 3d scans. arXiv:2110.04994 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.04994",
                        "Citation Paper Title": "Title:Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans",
                        "Citation Paper Abstract": "Abstract:This paper introduces a pipeline to parametrically sample and render multi-task vision datasets from comprehensive 3D scans from the real world. Changing the sampling parameters allows one to \"steer\" the generated datasets to emphasize specific information. In addition to enabling interesting lines of research, we show the tooling and generated data suffice to train robust vision models.\nCommon architectures trained on a generated starter dataset reached state-of-the-art performance on multiple common vision tasks and benchmarks, despite having seen no benchmark or non-pipeline data. The depth estimation network outperforms MiDaS and the surface normal estimation network is the first to achieve human-level performance for in-the-wild surface normal estimation -- at least according to one metric on the OASIS benchmark.\nThe Dockerized pipeline with CLI, the (mostly python) code, PyTorch dataloaders for the generated data, the generated starter dataset, download scripts and other utilities are available through our project website, https://omnidata.vision.",
                        "Citation Paper Authors": "Authors:Ainaz Eftekhar, Alexander Sax, Roman Bachmann, Jitendra Malik, Amir Zamir"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Synthetic data provides high quality labels for many im-\nage tasks such as semantic ",
                    "Citation Text": "Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.\nLearning semantic segmentation from synthetic data: A\ngeometrically guided input-output adaptation approach. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05040",
                        "Citation Paper Title": "Title:Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach",
                        "Citation Paper Abstract": "Abstract:Recently, increasing attention has been drawn to training semantic segmentation models using synthetic data and computer-generated annotation. However, domain gap remains a major barrier and prevents models learned from synthetic data from generalizing well to real-world applications. In this work, we take the advantage of additional geometric information from synthetic data, a powerful yet largely neglected cue, to bridge the domain gap. Such geometric information can be generated easily from synthetic data, and is proven to be closely coupled with semantic information. With the geometric information, we propose a model to reduce domain shift on two levels: on the input level, we augment the traditional image translation network with the additional geometric information to translate synthetic images into realistic styles; on the output level, we build a task network which simultaneously performs depth estimation and semantic segmentation on the synthetic data. Meanwhile, we encourage the network to preserve correlation between depth and semantics by adversarial training on the output space. We then validate our method on two pairs of synthetic to real dataset: Virtual KITTI to KITTI, and SYNTHIA to Cityscapes, where we achieve a significant performance gain compared to the non-adapt baseline and methods using only semantic label. This demonstrates the usefulness of geometric information from synthetic data for cross-domain semantic segmentation.",
                        "Citation Paper Authors": "Authors:Yuhua Chen, Wen Li, Xiaoran Chen, Luc Van Gool"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.03238v1": {
            "Paper Title": "Semantic Segmentation in Art Paintings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.02750v1": {
            "Paper Title": "On Rotation Gains Within and Beyond Perceptual Limitations for Seated VR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.02554v1": {
            "Paper Title": "Building 3D Generative Models from Minimal Data",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "and previous work on regressing 3DMM parameters\ndirectly from images ",
                    "Citation Text": "Tuan Tran, A., Hassner, T., Masi, I., Medioni, G.: Regressing robust and discrim-\ninative 3d morphable models with a very deep neural network. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n5163\u20135172 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.04904",
                        "Citation Paper Title": "Title:Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network",
                        "Citation Paper Abstract": "Abstract:The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied \"in the wild\", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",
                        "Citation Paper Authors": "Authors:Anh Tuan Tran, Tal Hassner, Iacopo Masi, Gerard Medioni"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.02252v1": {
            "Paper Title": "Parametric/direct CAD integration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05139v3": {
            "Paper Title": "CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "directly rather than a conditional NeRF? We\n\ufb01rst evaluate our designed CLIP loss on appearance editing\nin Fig. 14. Given a pre-trained NeRF model on the LLFF\ndataset ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light \ufb01eld fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG) , 38(4):1\u201314, 2019. 9, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00889",
                        "Citation Paper Title": "Title:Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines",
                        "Citation Paper Abstract": "Abstract:We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "NeRF and NeRF Editing. The past few years have wit-\nnessed tremendous progress in the implicit representation\nof 3D models with neural networks [28, 24, 35, 14, 9, 16].\nAmong them, NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In European conference on computer vision , pages\n405\u2013421. Springer, 2020. 1, 2, 5, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "designs a CLIP seman-\ntic consistency loss to improve few-shot NeRF and presents\nimpressive results, and GRAF ",
                    "Citation Text": "Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance \ufb01elds for 3d-aware image\nsynthesis. arXiv preprint arXiv:2007.02442 , 2020. 1, 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02442",
                        "Citation Paper Title": "Title:GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
                        "Citation Paper Abstract": "Abstract:While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",
                        "Citation Paper Authors": "Authors:Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "combines CLIP and StyleGAN [18, 17] to synthesize im-\nages by optimizing the latent code of a pre-trained Style-\nGAN according to a textual condition de\ufb01ned in the CLIP\nspace. Instead of generating images from scratch, Style-\nCLIP ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 2085\u20132094,\n2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05142v2": {
            "Paper Title": "HairCLIP: Design Your Hair by Text and Reference Image",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "performs a two-stage\noptimization in the W+space and noise space of Style-\nGANv2 ",
                    "Citation Text": "Tero Karras, S. Laine, Miika Aittala, Janne Hellsten, J.\nLehtinen, and Timo Aila. Analyzing and improving the\nimage quality of stylegan. 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n8107\u20138116, 2020. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "Generative Adversarial Networks. Since being proposed\nby Goodfellow et al. ",
                    "Citation Text": "Ian J. Goodfellow, Jean Pouget-Abadie, M. Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron C. Courville, and\nYoshua Bengio. Generative adversarial nets. In NIPS , 2014.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.01152v1": {
            "Paper Title": "DisARM: Displacement Aware Relation Module for 3D Detection",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "for 3D object detection and achieves better perfor-\nmance. V oteNet ",
                    "Citation Text": "Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 9277\u20139286, 2019. 2, 4,\n5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09664",
                        "Citation Paper Title": "Title:Deep Hough Voting for 3D Object Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "58.6 33.5\nV oteNet* 63.8 44.2\nV oteNet*+DisARM 66.1 \"49.7\"\nBRNet ",
                    "Citation Text": "Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and\nDong Xu. Back-tracing representative points for voting-\nbased 3d object detection in point clouds. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8963\u20138972, 2021. 4, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06114",
                        "Citation Paper Title": "Title:Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:3D object detection in point clouds is a challenging vision task that benefits various applications for understanding the 3D visual world. Lots of recent research focuses on how to exploit end-to-end trainable Hough voting for generating object proposals. However, the current voting strategy can only receive partial votes from the surfaces of potential objects together with severe outlier votes from the cluttered backgrounds, which hampers full utilization of the information from the input point clouds. Inspired by the back-tracing strategy in the conventional Hough voting methods, in this work, we introduce a new 3D object detection method, named as Back-tracing Representative Points Network (BRNet), which generatively back-traces the representative points from the vote centers and also revisits complementary seed points around these generated points, so as to better capture the fine local structural features surrounding the potential objects from the raw point clouds. Therefore, this bottom-up and then top-down strategy in our BRNet enforces mutual consistency between the predicted vote centers and the raw surface points and thus achieves more reliable and flexible object localization and class prediction results. Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in terms of mAP@0.50), while it is still lightweight and efficient. Code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, Dong Xu"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "groups ponts according to their voted\ncenters and extract object features from grouped points by\nthe PointNet. Some follow-up works further imporve the\npoint group generation procedure ",
                    "Citation Text": "Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang.\nH3dnet: 3d object detection using hybrid geometric primi-\ntives. In European Conference on Computer Vision , pages\n311\u2013329. Springer, 2020. 2, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05682",
                        "Citation Paper Title": "Title:H3DNet: 3D Object Detection Using Hybrid Geometric Primitives",
                        "Citation Paper Abstract": "Abstract:We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D.",
                        "Citation Paper Authors": "Authors:Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "reasons about the local depen-\ndencies of regions with considering the inner interactions\nof point sets by modeling their geometrical and locational\nrelations, which is not suitable for large indoor scenes un-\nderstanding. MLCVNet ",
                    "Citation Text": "Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming\nZhang, Kai Xu, and Jun Wang. Mlcvnet: Multi-level con-\ntext votenet for 3d object detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10447\u201310456, 2020. 1, 3, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05679",
                        "Citation Paper Title": "Title:MLCVNet: Multi-Level Context VoteNet for 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the 3D object detection task by capturing multi-level contextual information with the self-attention mechanism and multi-scale feature fusion. Most existing 3D object detection methods recognize objects individually, without giving any consideration on contextual information between these objects. Comparatively, we propose Multi-Level Context VoteNet (MLCVNet) to recognize 3D objects correlatively, building on the state-of-the-art VoteNet. We introduce three context modules into the voting and classifying stages of VoteNet to encode contextual information at different levels. Specifically, a Patch-to-Patch Context (PPC) module is employed to capture contextual information between the point patches, before voting for their corresponding object centroid points. Subsequently, an Object-to-Object Context (OOC) module is incorporated before the proposal and classification stage, to capture the contextual information between object candidates. Finally, a Global Scene Context (GSC) module is designed to learn the global scene context. We demonstrate these by capturing contextual information at patch, object and scene levels. Our method is an effective way to promote detection accuracy, achieving new state-of-the-art detection performance on challenging 3D object detection datasets, i.e., SUN RGBD and ScanNet. We also release our code at this https URL.",
                        "Citation Paper Authors": "Authors:Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", where the existing 2D object detec-\ntion or segmentation methods based on regular image coor-\ndinates can be effortlessly adapted. Other approaches also\nstudied how to exploit discriminative [17, 21] or generative\nshape templates ",
                    "Citation Text": "Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J\nGuibas. Gspn: Generative shape proposal network for 3d\ninstance segmentation in point cloud. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 3947\u20133956, 2019. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03320",
                        "Citation Paper Title": "Title:GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud",
                        "Citation Paper Abstract": "Abstract:We introduce a novel 3D object proposal approach named Generative Shape Proposal Network (GSPN) for instance segmentation in point cloud data. Instead of treating object proposal as a direct bounding box regression problem, we take an analysis-by-synthesis strategy and generate proposals by reconstructing shapes from noisy observations in a scene. We incorporate GSPN into a novel 3D instance segmentation framework named Region-based PointNet (R-PointNet) which allows flexible proposal refinement and instance segmentation generation. We achieve state-of-the-art performance on several 3D instance segmentation tasks. The success of GSPN largely comes from its emphasis on geometric understandings during object proposal, which greatly reducing proposals with low objectness.",
                        "Citation Paper Authors": "Authors:Li Yi, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "addresses the 3D object de-\ntection task by incorporating into V oteNet multi-level con-\ntextual information with the self-attention mechanism and\nmulti-scale feature fusion by considering relations of all\nobjects which results in information redundancy. While\nMonoPair ",
                    "Citation Text": "Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li.\nMonopair: Monocular 3d object detection using pairwise\nspatial relationships. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n12093\u201312102, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00504",
                        "Citation Paper Title": "Title:MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships",
                        "Citation Paper Abstract": "Abstract:Monocular 3D object detection is an essential component in autonomous driving while challenging to solve, especially for those occluded samples which are only partially visible. Most detectors consider each 3D object as an independent training target, inevitably resulting in a lack of useful information for occluded samples. To this end, we propose a novel method to improve the monocular 3D object detection by considering the relationship of paired samples. This allows us to encode spatial constraints for partially-occluded objects from their adjacent neighbors. Specifically, the proposed detector computes uncertainty-aware predictions for object locations and 3D distances for the adjacent object pairs, which are subsequently jointly optimized by nonlinear least squares. Finally, the one-stage uncertainty-aware prediction structure and the post-optimization module are dedicatedly integrated for ensuring the run-time efficiency. Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins, especially for the hard samples.",
                        "Citation Paper Authors": "Authors:Yongjian Chen, Lei Tai, Kai Sun, Mingyang Li"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "de\ufb01nes \ufb01ve\ntypes of relations for modeling the graph structure of fur-\nniture in indoor scenes, which, however, is time-consuming\nfor relations like facing. ",
                    "Citation Text": "Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Liang\nZhang, and Ajmal Mian. Relation graph network for 3d ob-\nject detection in point clouds. IEEE Transactions on Image\nProcessing , 30:92\u2013107, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00202",
                        "Citation Paper Title": "Title:Relation Graph Network for 3D Object Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) have emerged as a powerful strategy for most object detection tasks on 2D images. However, their power has not been fully realised for detecting 3D objects in point clouds directly without converting them to regular grids. Existing state-of-art 3D object detection methods aim to recognize 3D objects individually without exploiting their relationships during learning or inference. In this paper, we first propose a strategy that associates the predictions of direction vectors and pseudo geometric centers together leading to a win-win solution for 3D bounding box candidates regression. Secondly, we propose point attention pooling to extract uniform appearance features for each 3D object proposal, benefiting from the learned direction features, semantic features and spatial coordinates of the object points. Finally, the appearance features are used together with the position features to build 3D object-object relationship graphs for all proposals to model their co-existence. We explore the effect of relation graphs on proposals' appearance features enhancement under supervised and unsupervised settings. The proposed relation graph network consists of a 3D object proposal generation module and a 3D relation module, makes it an end-to-end trainable network for detecting 3D object in point clouds. Experiments on challenging benchmarks ( SunRGB-Dand ScanNet datasets ) of 3D point clouds show that our algorithm can perform better than the existing state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Liang Zhang, Ajmal Mian"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "directly computes 3D box proposals, where the\npoints within this 3D box are used for object feature extrac-\ntion. PV-RCNN ",
                    "Citation Text": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 10529\u201310538, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.13192",
                        "Citation Paper Title": "Title:PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "2.1. 3D object detection on point clouds\nObject detection from 3D point clouds is challenging\ndue to the irregular, sparse and orderless characteristics of\n3D points. Earlier attempts usually relied on projections\nonto regular grids such as multi-view images ",
                    "Citation Text": "Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition , pages 1907\u20131915, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07759",
                        "Citation Paper Title": "Title:Multi-View 3D Object Detection Network for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.",
                        "Citation Paper Authors": "Authors:Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.01323v1": {
            "Paper Title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor\n  Perturbation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.00051v1": {
            "Paper Title": "ERF: Explicit Radiance Field Reconstruction From Scratch",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.14020v1": {
            "Paper Title": "State-of-the-Art in the Architecture, Methods and Applications of\n  StyleGAN",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11228v3": {
            "Paper Title": "Cross-Domain and Disentangled Face Manipulation with 3D Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.13162v1": {
            "Paper Title": "Pix2NeRF: Unsupervised Conditional $\u03c0$-GAN for Single Image to Neural\n  Radiance Fields Translation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "could potentially achieve better visual quality.\nAdditionally, architecture search, especially with respect to\nthe encoder remains a challenging problem. Utilizing more\nmature encoder architectures from 2D GAN feed-forward\ninversion literature, e.g. pixel2style2pixel ",
                    "Citation Text": "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding\nin style: a stylegan encoder for image-to-image translation.\nInIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , June 2021. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.00951",
                        "Citation Paper Title": "Title:Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.",
                        "Citation Paper Authors": "Authors:Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", which can generalize to un-\nseen categories, multi-instance, and even real-world scenes.\nBeing a general framework, Pix2NeRF is not limited to using\n\u0019-GAN as its backbone. Newer generative NeRF models,\ne.g. EG3D ",
                    "Citation Text": "Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas\nGuibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and\nGordon Wetzstein. Ef\ufb01cient geometry-aware 3D generative\nadversarial networks. In arXiv , 2021. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07945",
                        "Citation Paper Title": "Title:Efficient Geometry-aware 3D Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.",
                        "Citation Paper Authors": "Authors:Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "are less consistent due to dataset noise (back-\nground, geometry, pose noise, artifacts, etc.), encouraging\nGANs to converge towards the mean as a trade-off to varia-\ntions. These observations can be related to manifold learn-\ning ",
                    "Citation Text": "Yilun Du, Katherine M. Collins, Joshua B. Tenenbaum, and\nVincent Sitzmann. Learning signal-agnostic manifolds of\nneural \ufb01elds, 2021. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06387",
                        "Citation Paper Title": "Title:Learning Signal-Agnostic Manifolds of Neural Fields",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains. Code and additional results are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yilun Du, Katherine M. Collins, Joshua B. Tenenbaum, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "shares similar ideas. The key differences are a\ndifferent backbone network (HoloGAN ",
                    "Citation Text": "Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. Hologan: Unsupervised\nlearning of 3d representations from natural images. In The\nIEEE International Conference on Computer Vision (ICCV) ,\nNov 2019. 2, 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01326",
                        "Citation Paper Title": "Title:HoloGAN: Unsupervised learning of 3D representations from natural images",
                        "Citation Paper Abstract": "Abstract:We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",
                        "Citation Paper Authors": "Authors:Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ".\nTechnical details . We choose the latent code prior distribu-\ntionpzas a multivariate uniform on [\u00001;1]. We build our\nmodel on top of the \u0019-GAN implementation in PyTorch ",
                    "Citation Text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K \u00a8opf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. Pytorch: An imper-\native style, high-performance deep learning library. CoRR ,\nabs/1912.01703, 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", from which we use the \u201cchairs\u201d\nsplit for the comparison with prior multi-view methods. The\ndataset contains 50 rendered views from ShapeNet ",
                    "Citation Text": "Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas,\nPat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li\nYi, and Fisher Yu. Shapenet: An information-rich 3d model\nrepository. CoRR , abs/1512.03012, 2015. 6, 7, 1, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "layers instead of simple\nMLPs. More recently, several works improved synthesis\nquality with high resolutions ",
                    "Citation Text": "Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStylenerf: A style-based 3d-aware generator for high-\nresolution image synthesis, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.08985",
                        "Citation Paper Title": "Title:StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis",
                        "Citation Paper Abstract": "Abstract:We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including zoom-in and-out, style mixing, inversion, and semantic editing.",
                        "Citation Paper Authors": "Authors:Jiatao Gu, Lingjie Liu, Peng Wang, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "Our work can be classi\ufb01ed as a category-speci\ufb01c 3D-\naware neural novel view synthesis method, which is strongly\nbased on NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view synthe-\nsis. In ECCV , 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.07845v4": {
            "Paper Title": "ARCH++: Animation-Ready Clothed Human Reconstruction Revisited",
            "Sentences": [
                {
                    "Sentence ID": 80,
                    "Sentence": ", 192 scans from\nAXYZ dataset, 26 scans from BUFF dataset ",
                    "Citation Text": "Chao Zhang, Sergi Pujades, Michael Black, and Gerard\nPons-Moll. Detailed, accurate, human shape estimation from\nclothed 3D scan sequences. In IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2017. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04454",
                        "Citation Paper Title": "Title:Detailed, accurate, human shape estimation from clothed 3D scan sequences",
                        "Citation Paper Abstract": "Abstract:We address the problem of estimating human pose and body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited body models produce smooth shapes lacking personalized details. We contribute a new approach to recover a personalized shape of the person. The estimated shape deviates from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available BUFF, a new 4D dataset that enables quantitative evaluation (this http URL). Our method outperforms the state of the art in both pose estimation and shape estimation, qualitatively and quantitatively.",
                        "Citation Paper Authors": "Authors:Chao Zhang, Sergi Pujades, Michael Black, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". For\nsingle-view clothed human reconstruction, direct regression\nmethods demonstrate promising results, supporting various\nclothing types with a wide range of shape representations\nincluding voxels [67, 30], two-way depth maps [19, 64],\nvisual hull ",
                    "Citation Text": "Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen,\nChongyang Ma, Hao Li, and Shigeo Morishima. Siclope:\nSilhouette-based clothed people. In IEEE Conference on\nComputer Vision and Pattern Recognition , 2019. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00049",
                        "Citation Paper Title": "Title:SiCloPe: Silhouette-Based Clothed People",
                        "Citation Paper Abstract": "Abstract:We introduce a new silhouette-based representation for modeling clothed human bodies using deep generative models. Our method can reconstruct a complete and textured 3D model of a person wearing clothes from a single input picture. Inspired by the visual hull algorithm, our implicit representation uses 2D silhouettes and 3D joints of a body pose to describe the immense shape complexity and variations of clothed people. Given a segmented 2D silhouette of a person and its inferred 3D joints from the input picture, we first synthesize consistent silhouettes from novel view points around the subject. The synthesized silhouettes which are the most consistent with the input segmentation are fed into a deep visual hull algorithm for robust 3D shape prediction. We then infer the texture of the subject's back view using the frontal image and segmentation mask as input to a conditional generative adversarial network. Our experiments demonstrate that our silhouette-based model is an effective representation and the appearance of the back view can be predicted reliably using an image-to-image translation network. While classic methods based on parametric models often fail for single-view images of subjects with challenging clothing, our approach can still produce successful results, which are comparable to those obtained from multi-view input.",
                        "Citation Paper Authors": "Authors:Ryota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li, Shigeo Morishima"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "shows that using SMPL model\nas guidance significantly improves robustness of non-rigid\nfusion from RGB-D inputs. For single-view human recon-\nstruction, Zheng et al. first introduce a hybrid approach of\na template-model (SMPL) and a non-parametric shape rep-\nresentation (voxel ",
                    "Citation Text": "Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and\nYebin Liu. Deephuman: 3d human reconstruction from a\nsingle image. In IEEE International Conference on Com-\nputer Vision , 2019. 1, 2, 5Supplementary\nA. Implementation Details\nIn this section, we provide the implementation details of\nour proposed method.\nA.1. Input Preprocessing\nDuring both training and test time, the input images to\nthe network are normalized with regard to the human body\nscale. In particular, we re-scale the image based on the 3D\nskeleton estimation of the subject. The image is resized then\ncentered, such that the pelvis of the person is aligned with\nthe center of the image. Each pixel represents 1cm length\nusing an orthographic scene projection. In this way we en-\nsure proper scaling of the body parts, which allows us to\ncapture the variations of different heights of people.\nA.2. Network Architectures\nSemantic-Aware Geometry Encoder is based on Point-\nNet++ [59, 58], which consists of 3 Set Abstraction (SA)\nlayers. The configurations of each layer are SA(2048,\n0.1, 16, 3, [16,16,32]), SA(512, 0.2, 32, 32, [32,32,64]),\nSA(128, 0.4, 64, 64, [64,64,128]). The explanation of each\nargument is (furthest point sampling size, point neighbor-\nhood radius, point neighborhood size limit, input feature\nchannel, MLP output channels list). Namely, the multi-\nscale point set sizes of Eq. (1) in the main paper are:\nN1= 2048 , N2= 512 , N3= 128 . When extracting\nspatially-aligned features for any given query point, we\nleverage the point Feature Propagation (FP) layers defined\nat the aforementioned 3 different point set scales: FP(32,\n[32,32]), FP(64, [32,32]), FP(128, [32,32]). The explana-\ntion of each argument is (input feature channel, MLP output\nchannels list). Therefore, the dimensions of our spatially-\naligned geometry features fgin Eq. (3) of the main paper\nare96 = 32 \u22173. Please refer to",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06473",
                        "Citation Paper Title": "Title:DeepHuman: 3D Human Reconstruction from a Single Image",
                        "Citation Paper Abstract": "Abstract:We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D human reconstruction from a single RGB image. To reduce the ambiguities associated with the surface geometry reconstruction, even for the reconstruction of invisible areas, we propose and leverage a dense semantic representation generated from SMPL model as an additional input. One key feature of our network is that it fuses different scales of image features into the 3D space through volumetric feature transformation, which helps to recover accurate surface geometry. The visible surface details are further refined through a normal refinement network, which can be concatenated with the volume generation network using our proposed volumetric normal projection layer. We also contribute THuman, a 3D real-world human model dataset containing about 7000 models. The network is trained using training data generated from the dataset. Overall, due to the specific design of our network and the diversity in our dataset, our method enables 3D human model estimation given only a single image and outperforms state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.12972v1": {
            "Paper Title": "FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment",
            "Sentences": [
                {
                    "Sentence ID": 77,
                    "Sentence": "which is\nmore stable than previous quality metrics such as the structural\nsimilarity index method (SSIM) or the Inception score ",
                    "Citation Text": "T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and\nX. Chen, \u201cImproved techniques for training gans,\u201d in Advances in Neural\nInformation Processing Systems , 2016, pp. 2234\u20132242. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ", is applied to capture the \ufb01ne details of the face texture. A\nfeature maps of a pretrained VGG network is used to compare\nthe high frequency details using a Euclidean distance. Following\nothers ",
                    "Citation Text": "R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe\nunreasonable effectiveness of deep features as a perceptual metric,\u201d in\nProc. Conf. Comput. Vision Pattern Recognition , 2018, pp. 586\u2013595. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ", utilizing a coarse-to-\ufb01ne\ngenerator, and a multi-scale discriminator.For the global generator, we use a U-Net with bottleneck\nblocks ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proc. Conf. Comput. Vision Pattern Recognition , 2016,\npp. 770\u2013778. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "proposed a face swapping\nmethod at megapixel resolution using an encoder-decoder with\ncontrast and light-preserving blending and landmark stabilization.\nZakharov et al. ",
                    "Citation Text": "E. Zakharov, A. Ivakhnenko, A. Shysheya, and V . Lempitsky, \u201cFast bi-\nlayer neural synthesis of one-shot realistic head avatars,\u201d in European\nConf. Comput. Vision . Springer, 2020, pp. 524\u2013540. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10174",
                        "Citation Paper Title": "Title:Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars",
                        "Citation Paper Abstract": "Abstract:We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.",
                        "Citation Paper Authors": "Authors:Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "proposed a latent space which separates the identity and geometric\nattributes, such as facial pose and expression.\nZakharov et al. ",
                    "Citation Text": "E. Zakharov, A. Shysheya, E. Burkov, and V . Lempitsky, \u201cFew-shot\nadversarial learning of realistic neural talking head models,\u201d arXiv , 2019.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08233",
                        "Citation Paper Title": "Title:Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
                        "Citation Paper Abstract": "Abstract:Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.",
                        "Citation Paper Authors": "Authors:Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.12722v1": {
            "Paper Title": "Integrating Immersive Technologies for Algorithmic Design in\n  Architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04362v2": {
            "Paper Title": "Physics-based Mesh Deformation with Haptic Feedback and Material\n  Anisotropy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.11249v1": {
            "Paper Title": "Virtual, Augmented, and Mixed Reality for Human-Robot Interaction: A\n  Survey and Virtual Design Element Taxonomy",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ", C: Virtual Control Room and External\nSensor Numerical Readings ",
                    "Citation Text": "Jeffrey I Lipton, Aidan J Fay, and Daniela Rus. 2017. Baxter\u2019s homunculus: Virtual reality spaces for teleoperation in manufacturing. IEEE Robotics\nand Automation Letters 3, 1 (2017), 179\u2013186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01270",
                        "Citation Paper Title": "Title:Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing",
                        "Citation Paper Abstract": "Abstract:Expensive specialized systems have hampered development of telerobotic systems for manufacturing systems. In this paper we demonstrate a telerobotic system which can reduce the cost of such system by leveraging commercial virtual reality(VR) technology and integrating it with existing robotics control software. The system runs on a commercial gaming engine using off the shelf VR hardware. This system can be deployed on multiple network architectures from a wired local network to a wireless network connection over the Internet. The system is based on the homunculus model of mind wherein we embed the user in a virtual reality control room. The control room allows for multiple sensor display, dynamic mapping between the user and robot, does not require the production of duals for the robot, or its environment. The control room is mapped to a space inside the robot to provide a sense of co-location within the robot. We compared our system with state of the art automation algorithms for assembly tasks, showing a 100% success rate for our system compared with a 66% success rate for automated systems. We demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",
                        "Citation Paper Authors": "Authors:Jeffrey I Lipton, Aidan J Fay, Daniela Rus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.08752v2": {
            "Paper Title": "OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ".\n360\u000eview synthesis techniques have been used to create motion\nparallax for VR viewing [2 \u20134, 34, 47]. Attal et al. develop MatryO-\nDShka ",
                    "Citation Text": "B. Attal, S. Ling, A. Gokaslan, C. Richardt, and J. Tompkin. MatryOD-\nShka: Real-time 6DoF video view synthesis using multi-sphere images.\nInEuropean Conference on Computer Vision (ECCV) , Aug. 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06534",
                        "Citation Paper Title": "Title:MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere Images",
                        "Citation Paper Abstract": "Abstract:We introduce a method to convert stereo 360\u00b0 (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360\u00b0 imagery can be captured from multi-camera systems for virtual reality (VR), but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and disocclusions via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium.",
                        "Citation Paper Authors": "Authors:Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian Richardt, James Tompkin"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". Our experiments are conducted\non synthetic outdoor scenes from CARLA ",
                    "Citation Text": "A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V . Koltun. CARLA:\nAn open urban driving simulator. In Proceedings of the 1st Annual Con-\nference on Robot Learning , pp. 1\u201316, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", we input the vertical index as an\nadditional channel for each convolutional layer in our depth prediction\nnetwork, a technique also known as CoordConv ",
                    "Citation Text": "R. Liu, J. Lehman, P. Molino, F. Petroski Such, E. Frank, A. Sergeev, and\nJ. Yosinski. An intriguing failing of convolutional neural networks and the\ncoordconv solution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, eds., Advances in Neural Information\nProcessing Systems 31 , pp. 9605\u20139616. Curran Associates, Inc., 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03247",
                        "Citation Paper Title": "Title:An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution",
                        "Citation Paper Abstract": "Abstract:Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.",
                        "Citation Paper Authors": "Authors:Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "with 5\ndownsampling blocks and 3 upsampling blocks for the feature encoder,\na 3D UNet with 3 downsampling and 3 upsampling blocks for the cost\nvolume re\ufb01nement network, and 2 convolutional blocks for the depth\ndecoder. Similar to 360SD-Net ",
                    "Citation Text": "N. H. Wang, B. Solarte, Y . H. Tsai, W. C. Chiu, and M. Sun. 360sd-\nnet: 360 stereo depth estimation with learnable cost volume. In 2020\nIEEE International Conference on Robotics and Automation (ICRA) , pp.\n582\u2013588, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04460",
                        "Citation Paper Title": "Title:360SD-Net: 360\u00b0 Stereo Depth Estimation with Learnable Cost Volume",
                        "Citation Paper Abstract": "Abstract:Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360\u00b0 images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360\u00b0 camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360\u00b0 stereo data, we collect two 360\u00b0 stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras.",
                        "Citation Paper Authors": "Authors:Ning-Hsu Wang, Bolivar Solarte, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "develop and end-to-end pipeline for\nmore suburban and rural areas by predicting multi-plane images (MPIs)\nfrom satellite imagery. Li et al. ",
                    "Citation Text": "Z. Li, Z. Cui, M. R. Oswald, and M. Pollefeys. Street-view panoramic\nvideo synthesis from a single satellite image, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.06628",
                        "Citation Paper Title": "Title:Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite Image",
                        "Citation Paper Abstract": "Abstract:We present a novel method for synthesizing both temporally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory. Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consistency, our approach explicitly creates a 3D point cloud representation of the scene and maintains dense 3D-2D correspondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for synthesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an upsampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene structure and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results compared to other state-of-the-art synthesis approaches that either lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to synthesize cross-view images to video.",
                        "Citation Paper Authors": "Authors:Zuoyue Li, Zhenqiang Li, Zhaopeng Cui, Rongjun Qin, Marc Pollefeys, Martin R. Oswald"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "synthesize street\nview images for urban areas by predicting the depth and semantics of\nsatelite images. Shi et al. ",
                    "Citation Text": "Y . Shi, D. Campbell, X. Yu, and H. Li. Geometry-guided street-view\npanorama synthesis from satellite imagery, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01623",
                        "Citation Paper Title": "Title:Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery",
                        "Citation Paper Abstract": "Abstract:This paper presents a new approach for synthesizing a novel street-view panorama given an overhead satellite image. Taking a small satellite image patch as input, our method generates a Google's omnidirectional street-view type panorama, as if it is captured from the same geographical location as the center of the satellite patch. Existing works tackle this task as an image generation problem which adopts generative adversarial networks to implicitly learn the cross-view transformations, while ignoring the domain relevance. In this paper, we propose to explicitly establish the geometric correspondences between the two-view images so as to facilitate the cross-view transformation learning. Specifically, we observe that when a 3D point in the real world is visible in both views, there is a deterministic mapping between the projected points in the two-view images given the height information of this 3D point. Motivated by this, we develop a novel Satellite to Street-view image Projection (S2SP) module which explicitly establishes such geometric correspondences and projects the satellite images to the street viewpoint. With these projected satellite images as network input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates images that better respect the scene geometry than existing approaches.",
                        "Citation Paper Authors": "Authors:Yujiao Shi, Dylan Campbell, Xin Yu, Hongdong Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.08614v2": {
            "Paper Title": "Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "extends MPIs to encode spherical basis func-\ntions that enable view-dependent rendering effects in real-\ntime. [9, 12, 37] also distill NeRFs to enable real-time ren-\ndering. ",
                    "Citation Text": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance \ufb01elds. In ICCV , 2021. 1, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14024",
                        "Citation Paper Title": "Title:PlenOctrees for Real-time Rendering of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "modi\ufb01es the architec-\nture of the NeRF so that inference requires fewer samples\nbut produces lower quality results.\nNeX ",
                    "Citation Text": "Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. NeX: Real-time\nview synthesis with neural basis expansion. In 2021 . 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.05606",
                        "Citation Paper Title": "Title:NeX: Real-time View Synthesis with Neural Basis Expansion",
                        "Citation Paper Abstract": "Abstract:We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\\times$ faster rendering time than the state of the art. For real-time demos, visit this https URL",
                        "Citation Paper Authors": "Authors:Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "learns\nsparse voxel grids of features that are input into a NeRF-like\nmodel. The sparse voxel grid allows the renderer to skip\nover empty regions when tracing a ray which improves the\nrendering time\u001810x. AutoInt ",
                    "Citation Text": "David B Lindell, Julien NP Martel, and Gordon Wetzstein.\nAutoint: Automatic integration for fast neural volume ren-\ndering. In CVPR , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.01714",
                        "Citation Paper Title": "Title:AutoInt: Automatic Integration for Fast Neural Volume Rendering",
                        "Citation Paper Abstract": "Abstract:Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",
                        "Citation Paper Authors": "Authors:David B. Lindell, Julien N. P. Martel, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.10551v1": {
            "Paper Title": "Geometry-Aware Planar Embedding of Treelike Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08956v2": {
            "Paper Title": "GNN-Surrogate: A Hierarchical and Adaptive Graph Neural Network for\n  Parameter Space Exploration of Unstructured-Mesh Ocean Simulations",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "to stabilize our GNN-Surrogate training.\nSecond, training a deep generative model for large-scale data is\nmemory costly and slow. Thus, we train GNN-Surrogate with mixed\nprecision ",
                    "Citation Text": "P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh et al. , \u201cMixed\nPrecision Training,\u201d in Proc. International Conference on Learning\nRepresentations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.03740",
                        "Citation Paper Title": "Title:Mixed Precision Training",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.",
                        "Citation Paper Authors": "Authors:Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "in parameter space\nexploration and graph neural networks on unstructured data.\nGNN-Surrogate is also categorized as deep learning for scienti\ufb01c\nvisualization work. Readers can check the supplementary material\nor one survey paper ",
                    "Citation Text": "Q. Wang, Z. Chen, Y . Wang, and H. Qu, \u201cA Survey on ML4VIS: Applying\nMachine Learning Advances to Data Visualization,\u201d IEEE Transactions\non Visualization and Computer Graphics , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.00467",
                        "Citation Paper Title": "Title:A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization",
                        "Citation Paper Abstract": "Abstract:Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: \"what visualization processes can be assisted by ML?\" and \"how ML techniques can be used to solve visualization problems?\" This survey reveals seven main processes where the employment of ML techniques can benefit visualizations:Data Processing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations.Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at this https URL",
                        "Citation Paper Authors": "Authors:Qianwen Wang, Zhutian Chen, Yong Wang, Huamin Qu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.10272v1": {
            "Paper Title": "Computational Pattern Making from 3D Garment Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14548v2": {
            "Paper Title": "Z2P: Instant Visualization of Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.05975v1": {
            "Paper Title": "FExGAN-Meta: Facial Expression Generation with Meta Humans",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "another CNN based architecture was proposed and evaluated on images of\nfaces and identities were compared using pre-trained face model ",
                    "Citation Text": "Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. VGGFace2: A dataset for recognising\nfaces across pose and age. arXiv:1710.08092 [cs] , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.08092",
                        "Citation Paper Title": "Title:VGGFace2: A dataset for recognising faces across pose and age",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.",
                        "Citation Paper Authors": "Authors:Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04283v3": {
            "Paper Title": "Adverse Weather Image Translation with Asymmetric and Uncertainty-aware\n  GAN",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "conduct multi-domain translation by adopting a target vector of desired\nattributes as an additional input. UNIT ",
                    "Citation Text": "Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image trans-\nlation networks. In Advances in Neural Information Processing Systems (NeurIPS) ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00848",
                        "Citation Paper Title": "Title:Unsupervised Image-to-Image Translation Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL .",
                        "Citation Paper Authors": "Authors:Ming-Yu Liu, Thomas Breuel, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ", there have been numerous studies on Image-to-image translation (I2I) which\naims to transfer an image of the source domain to that of the desired target domain. Cycle-\nGAN ",
                    "Citation Text": "Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-\nImage Translation Using Cycle-Consistent Adversarial Networks. In International\nConference on Computer Vision (ICCV) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.10593",
                        "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                        "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "(D.Resblk). Therefore, an input image, i.e., xA;B2R256\u0002512\u00023, is converted to\nencoded feature with the output size in R64\u0002128\u0002256. In addition, we utilize Instance Nor-\nmalization ",
                    "Citation Text": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The\nmissing ingredient for fast stylization. arXiv preprint arXiv:1607.08022 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08022",
                        "Citation Paper Title": "Title:Instance Normalization: The Missing Ingredient for Fast Stylization",
                        "Citation Paper Abstract": "Abstract:It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at this https URL. Full paper can be found at arXiv:1701.02096.",
                        "Citation Paper Authors": "Authors:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "that captures heteroscedastic noise inherent in the observations.\nRecently, the heteroscedastic regression is exploited in several vision tasks such as depth\nestimation ",
                    "Citation Text": "Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, and Michael Persson.\nUncertainty-aware cnns for depth completion: Uncertainty from beginning to end. In\nConference on Computer Vision and Pattern Recognition (CVPR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.03349",
                        "Citation Paper Title": "Title:Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End",
                        "Citation Paper Abstract": "Abstract:The focus in deep learning research has been mostly to push the limits of prediction accuracy. However, this was often achieved at the cost of increased complexity, raising concerns about the interpretability and the reliability of deep networks. Recently, an increasing attention has been given to untangling the complexity of deep networks and quantifying their uncertainty for different computer vision tasks. Differently, the task of depth completion has not received enough attention despite the inherent noisy nature of depth sensors. In this work, we thus focus on modeling the uncertainty of depth data in depth completion starting from the sparse noisy input all the way to the final prediction.\nWe propose a novel approach to identify disturbed measurements in the input by learning an input confidence estimator in a self-supervised manner based on the normalized convolutional neural networks (NCNNs). Further, we propose a probabilistic version of NCNNs that produces a statistically meaningful uncertainty measure for the final prediction. When we evaluate our approach on the KITTI dataset for depth completion, we outperform all the existing Bayesian Deep Learning approaches in terms of prediction accuracy, quality of the uncertainty measure, and the computational efficiency. Moreover, our small network with 670k parameters performs on-par with conventional approaches with millions of parameters. These results give strong evidence that separating the network into parallel uncertainty and prediction streams leads to state-of-the-art performance with accurate uncertainty estimates.",
                        "Citation Paper Authors": "Authors:Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, Mikael Persson"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "brings the concept of the shared latent space of\ntwo generators via weight sharing. To produce diverse outputs, MUNIT ",
                    "Citation Text": "Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised\nimage-to-image translation. In European Conference on Computer Vision (ECCV) ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.04732",
                        "Citation Paper Title": "Title:Multimodal Unsupervised Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposed cycle-consistency loss for translation between unsupervised source and\ntarget domain. This concept has in\ufb02uenced several unsupervised domain translation tasks\nsuch as face attribute editing [5, 11, 19, 24] or domain adaptation [23, 29, 34]. StarGAN ",
                    "Citation Text": "Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul\nChoo. StarGAN: Uni\ufb01ed generative adversarial networks for multi-domain image-to-\nimage translation. In Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.09020",
                        "Citation Paper Title": "Title:StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",
                        "Citation Paper Authors": "Authors:Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Unsupervised image-to-image translation Since the introduction of GAN by Goodfel-\nlowet al. ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In\nAdvances in Neural Information Processing Systems (NeurIPS) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.06330v1": {
            "Paper Title": "Optimal lofted B-spline surface interpolation based on serial closed\n  contours",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06252v1": {
            "Paper Title": "Application of Color Block Code in Image Scaling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06088v1": {
            "Paper Title": "NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06079v1": {
            "Paper Title": "Text and Image Guided 3D Avatar Generation and Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ", pro-\nposes a contrastive learning approach to \ufb01nd unsupervised\ndirections that are transferable to different classes. More-\nover, StyleCLIP ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. arXiv preprint arXiv:2103.17249 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "represent shapes and textures as 2D positional\nmaps that can be projected back into 3D space, and leverage\nrecent advances in 2D imaging ",
                    "Citation Text": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation, 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10196",
                        "Citation Paper Title": "Title:Progressive Growing of GANs for Improved Quality, Stability, and Variation",
                        "Citation Paper Abstract": "Abstract:We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
                        "Citation Paper Authors": "Authors:Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "or\noccupancy \ufb01elds [7, 37] to create a lightweight, continuous\nshape representation. However, a major drawback of im-\nplicit representations is that they require aggressive sam-\npling and querying of 3D coordinates to construct surfaces.\nFinally, works such as UV-GAN ",
                    "Citation Text": "Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang\nZhou, and Stefanos Zafeiriou. Uv-gan: Adversarial fa-\ncial uv map completion for pose-invariant face recognition.\n2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 7093\u20137102, 2018. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.04695",
                        "Citation Paper Title": "Title:UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, $94.05\\%$, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.05522v1": {
            "Paper Title": "Unsupervised HDR Imaging: What Can Be Learned from a Single 8-bit Video?",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "extended this work for temporal stabil-\nity via training regularization. Marnerides et al. ",
                    "Citation Text": "Demetris Marnerides, Thomas Bashford-Rogers, Jonathan\nHatchett, and Kurt Debattista. Expandnet: A deep convo-\nlutional neural network for high dynamic range expansion\nfrom low dynamic range content. Comput. Graph. Forum ,\n37(2):37\u201349, 2018. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02266",
                        "Citation Paper Title": "Title:ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content",
                        "Citation Paper Abstract": "Abstract:High dynamic range (HDR) imaging provides the capability of handling real world lighting as opposed to the traditional low dynamic range (LDR) which struggles to accurately represent images with higher dynamic range. However, most imaging content is still available only in LDR. This paper presents a method for generating HDR content from LDR content based on deep Convolutional Neural Networks (CNNs) termed ExpandNet. ExpandNet accepts LDR images as input and generates images with an expanded range in an end-to-end fashion. The model attempts to reconstruct missing information that was lost from the original signal due to quantization, clipping, tone mapping or gamma correction. The added information is reconstructed from learned features, as the network is trained in a supervised fashion using a dataset of HDR images. The approach is fully automatic and data driven; it does not require any heuristics or human expertise. ExpandNet uses a multiscale architecture which avoids the use of upsampling layers to improve image quality. The method performs well compared to expansion/inverse tone mapping operators quantitatively on multiple metrics, even for badly exposed inputs.",
                        "Citation Paper Authors": "Authors:Demetris Marnerides, Thomas Bashford-Rogers, Jonathan Hatchett, Kurt Debattista"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "masked out well-exposed re-\ngions which were reconstructed by a linear operator, and\noverexposed regions which were reconstructed by a UNet.\nEilerstein et al. ",
                    "Citation Text": "Gabriel Eilertsen, Rafa\u0142 K. Mantiuk, and Jonas Unger.\nSingle-frame regularization for temporally stable cnns. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 ,\npages 11176\u201311185. Computer Vision Foundation / IEEE,\n2019. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10424",
                        "Citation Paper Title": "Title:Single-frame Regularization for Temporally Stable CNNs",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks (CNNs) can model complicated non-linear relations between images. However, they are notoriously sensitive to small changes in the input. Most CNNs trained to describe image-to-image mappings generate temporally unstable results when applied to video sequences, leading to flickering artifacts and other inconsistencies over time. In order to use CNNs for video material, previous methods have relied on estimating dense frame-to-frame motion information (optical flow) in the training and/or the inference phase, or by exploring recurrent learning structures. We take a different approach to the problem, posing temporal stability as a regularization of the cost function. The regularization is formulated to account for different types of motion that can occur between frames, so that temporally stable CNNs can be trained without the need for video material or expensive motion estimation. The training can be performed as a fine-tuning operation, without architectural modifications of the CNN. Our evaluation shows that the training strategy leads to large improvements in temporal smoothness. Moreover, for small datasets the regularization can help in boosting the generalization performance to a much larger extent than what is possible with na\u00efve augmentation strategies.",
                        "Citation Paper Authors": "Authors:Gabriel Eilertsen, Rafa\u0142 K. Mantiuk, Jonas Unger"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "presented several supervised HDR reconstruction\nmethods which proposed a range of network architectures\nand datasets for the evaluation of static images, although\nthese methods are not compared with the state-of-the-art. In\nterms of video, Kim et al. ",
                    "Citation Text": "Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Deep sr-itm:\nJoint learning of super-resolution and inverse tone-mapping\nfor 4k uhd hdr applications. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 3116\u2013\n3125. IEEE, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11176",
                        "Citation Paper Title": "Title:Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping for 4K UHD HDR Applications",
                        "Citation Paper Abstract": "Abstract:Recent modern displays are now able to render high dynamic range (HDR), high resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently, UHD HDR broadcasting and streaming have emerged as high quality premium services. However, due to the lack of original UHD HDR video content, appropriate conversion technologies are urgently needed to transform the legacy low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions. In this paper, we propose a joint super-resolution (SR) and inverse tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an intricate task, where high frequency details must be restored for SR, jointly with the local contrast, for ITM. Our network is able to restore fine details by decomposing the input image and focusing on the separate base (low frequency) and detail (high frequency) layers. Moreover, the proposed modulation blocks apply location-variant operations to enhance local contrast. The Deep SR-ITM shows good subjective quality with increased contrast and details, outperforming the previous joint SR-ITM method.",
                        "Citation Paper Authors": "Authors:Soo Ye Kim, Jihyong Oh, Munchurl Kim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.05263v1": {
            "Paper Title": "Block-NeRF: Scalable Large Scene Neural View Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ", which improves aliasing issues that hurt NeRF\u2019s\nperformance in scenes where the input images observe the\nscene from many different distances. We incorporate tech-\nniques from NeRF in the Wild (NeRF-W) ",
                    "Citation Text": "Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance \ufb01elds for uncon-\nstrained photo collections. CVPR , 2021. 1, 3, 4, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02268",
                        "Citation Paper Title": "Title:NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ") or pri-\noritize visual diversity over repeated observations of a target\narea ( e.g., NuScenes ",
                    "Citation Text": "Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\ndataset for autonomous driving. CVPR , 2020. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11027",
                        "Citation Paper Title": "Title:nuScenes: A multimodal dataset for autonomous driving",
                        "Citation Paper Abstract": "Abstract:Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
                        "Citation Paper Authors": "Authors:Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ".\nWe build our Block-NeRF implementation on top of mip-\nNeRF ",
                    "Citation Text": "Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-NeRF: A multiscale representation for anti-aliasing neu-\nral radiance \ufb01elds. ICCV , 2021. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "that target larger-baseline view synthesis run\na global optimization over all input images to reconstruct\nevery new scene, similar to traditional bundle adjustment.\nNeural Radiance Fields (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view synthe-\nsis.ECCV , 2020. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "proposed to minimize the scene-\nlevel distribution shift from rendered outputs to real camera\nsensor data through a learned scenario generation frame-\nwork. Richter et al. ",
                    "Citation Text": "Stephan R Richter, Hassan Abu AlHaija, and Vladlen Koltun.\nEnhancing photorealism enhancement. arXiv:2105.04619 ,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04619",
                        "Citation Paper Title": "Title:Enhancing Photorealism Enhancement",
                        "Citation Paper Abstract": "Abstract:We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Hassan Abu AlHaija, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ".\nThis approach has also been applied to compress detailed\n3D shapes into neural signed distance functions ",
                    "Citation Text": "Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis,\nCharles Loop, Derek Nowrouzezahrai, Alec Jacobson, Mor-\ngan McGuire, and Sanja Fidler. Neural geometric level of\ndetail: Real-time rendering with implicit 3D shapes. CVPR ,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.10994",
                        "Citation Paper Title": "Title:Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
                        "Citation Paper Abstract": "Abstract:Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
                        "Citation Paper Authors": "Authors:Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.04040v1": {
            "Paper Title": "Self-Conditioned Generative Adversarial Networks for Image Editing",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", enabling accurate\nand highly editable reconstructions of real images.\n3.5. Training Details\nWe train our models using the StyleGAN2-Pytorch im-\nplementation. We use the of\ufb01cial FFHQ ",
                    "Citation Text": "Tero Karras, Samuli Laine, and Timo Aila. A style-\nbased generator architecture for generative adversarialnetworks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 4401\u2013\n4410, 2019. 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.04948",
                        "Citation Paper Title": "Title:A Style-Based Generator Architecture for Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ") using pairs of pre- and post-editing images.\nWhen considering continuous attributes, we ensure a simi-\nlar magnitude of change by employing pre-trained pose ",
                    "Citation Text": "Nataniel Ruiz, Eunji Chong, and James M. Rehg. Fine-\ngrained head pose estimation without keypoints. In\nThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops , June 2018. 7\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00925",
                        "Citation Paper Title": "Title:Fine-Grained Head Pose Estimation Without Keypoints",
                        "Citation Paper Abstract": "Abstract:Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.",
                        "Citation Paper Authors": "Authors:Nataniel Ruiz, Eunji Chong, James M. Rehg"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "and measure the cosine similarity between\nthe embeddings of a pre-trained identity recognition network\n(ArcFace ",
                    "Citation Text": "Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. Arcface: Additive angular margin loss for\ndeep face recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR) , June 2019. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07698",
                        "Citation Paper Title": "Title:ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "or traverse this\nmanifold by \ufb01nding a new local-basis at every step ",
                    "Citation Text": "Jaewoong Choi, Junho Lee, Changyeon Yoon, Jung Ho\nPark, Geonho Hwang, and Myungjoo Kang. Do not\nescape from the manifold: Discovering the local coor-\ndinates on the latent space of gans, 2021. 2, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.06959",
                        "Citation Paper Title": "Title:Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs",
                        "Citation Paper Abstract": "Abstract:The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called Local Basis, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially W-space of StyleGAN2. We show that W-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it.",
                        "Citation Paper Authors": "Authors:Jaewoong Choi, Junho Lee, Changyeon Yoon, Jung Ho Park, Geonho Hwang, Myungjoo Kang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "directions in W+.\nArmed with these editing directions, we next turn to label-\ning our training data. We invert all training set images into\nthe latent space of the network using a pre-trained e4e ",
                    "Citation Text": "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patash-\nnik, and Daniel Cohen-Or. Designing an encoder\nfor stylegan image manipulation. arXiv preprint\narXiv:2102.02766 , 2021. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.02766",
                        "Citation Paper Title": "Title:Designing an Encoder for StyleGAN Image Manipulation",
                        "Citation Paper Abstract": "Abstract:Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.",
                        "Citation Paper Authors": "Authors:Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "or enable a user to re-write\nthe synthesis rules of the network ",
                    "Citation Text": "David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu,\nand Antonio Torralba. Rewriting a deep generative\n8model. In European Conference on Computer Vision ,\npages 351\u2013369. Springer, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15646",
                        "Citation Paper Title": "Title:Rewriting a Deep Generative Model",
                        "Citation Paper Abstract": "Abstract:A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.",
                        "Citation Paper Authors": "Authors:David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ".\nThe intuition is that brief \ufb01ne-tuning sessions, whether\nadversarial ",
                    "Citation Text": "Zongze Wu, Yotam Nitzan, Eli Shechtman, and Dani\nLischinski. Stylealign: Analysis and applications of\naligned stylegan models, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.11323",
                        "Citation Paper Title": "Title:StyleAlign: Analysis and Applications of Aligned StyleGAN Models",
                        "Citation Paper Abstract": "Abstract:In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion.",
                        "Citation Paper Authors": "Authors:Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". Typi-\ncally these methods train a network to perform local manip-\nulations on a given latent code [2, 20, 43]. Others suggest to\nmodel the warped manifold of the GAN ",
                    "Citation Text": "Christos Tzelepis, Georgios Tzimiropoulos, and Ioan-\nnis Patras. WarpedGANSpace: Finding non-linear rbf\npaths in GAN latent space. In Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion (ICCV) , pages 6393\u20136402, October 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.13357",
                        "Citation Paper Title": "Title:WarpedGANSpace: Finding non-linear RBF paths in GAN latent space",
                        "Citation Paper Abstract": "Abstract:This work addresses the problem of discovering, in an unsupervised manner, interpretable paths in the latent space of pretrained GANs, so as to provide an intuitive and easy way of controlling the underlying generative factors. In doing so, it addresses some of the limitations of the state-of-the-art works, namely, a) that they discover directions that are independent of the latent code, i.e., paths that are linear, and b) that their evaluation relies either on visual inspection or on laborious human labeling. More specifically, we propose to learn non-linear warpings on the latent space, each one parametrized by a set of RBF-based latent space warping functions, and where each warping gives rise to a family of non-linear paths via the gradient of the function. Building on the work of Voynov and Babenko, that discovers linear paths, we optimize the trainable parameters of the set of RBFs, so as that images that are generated by codes along different paths, are easily distinguishable by a discriminator network. This leads to easily distinguishable image transformations, such as pose and facial expressions in facial images. We show that linear paths can be derived as a special case of our method, and show experimentally that non-linear paths in the latent space lead to steeper, more disentangled and interpretable changes in the image space than in state-of-the art methods, both qualitatively and quantitatively. We make the code and the pretrained models publicly available at: this https URL.",
                        "Citation Paper Authors": "Authors:Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". Linear editing directions, how-\never, typically suffer from entanglement or rapidly deterio-\nrating performance when applying large changes. Recently,\nit has been suggested that these shortcomings can be tackled\nby discovering non-linear paths in the latent space ",
                    "Citation Text": "Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Only\na matter of style: Age transformation using a style-\nbased regression model. ACM Trans. Graph. , 40(4),\n2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.02754",
                        "Citation Paper Title": "Title:Only a Matter of Style: Age Transformation Using a Style-Based Regression Model",
                        "Citation Paper Abstract": "Abstract:The task of age transformation illustrates the change of an individual's appearance over time. Accurately modeling this complex transformation over an input facial image is extremely challenging as it requires making convincing, possibly large changes to facial features and head shape, while still preserving the input identity. In this work, we present an image-to-image translation method that learns to directly encode real facial images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a given aging shift. We employ a pre-trained age regression network to explicitly guide the encoder in generating the latent codes corresponding to the desired age. In this formulation, our method approaches the continuous aging process as a regression task between the input age and desired target age, providing fine-grained control over the generated image. Moreover, unlike approaches that operate solely in the latent space using a prior on the path controlling age, our method learns a more disentangled, non-linear path. Finally, we demonstrate that the end-to-end nature of our approach, coupled with the rich semantic latent space of StyleGAN, allows for further editing of the generated images. Qualitative and quantitative evaluations show the advantages of our method compared to state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Yuval Alaluf, Or Patashnik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". Others have proposed ways to identify such seman-\ntic directions in an entirely unsupervised manner [16, 33] or\nin a zero-shot manner by leveraging models ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-\nguage supervision. arXiv preprint arXiv:2103.00020 ,\n2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "or mitigate biases without training through better\nsampling of latent codes ",
                    "Citation Text": "Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving\nthe fairness of deep generative models without retrain-\ning. arXiv preprint arXiv:2012.04842 , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04842",
                        "Citation Paper Title": "Title:Improving the Fairness of Deep Generative Models without Retraining",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) advance face synthesis through learning the underlying distribution of observed data. Despite the high-quality generated faces, some minority groups can be rarely generated from the trained models due to a biased image generation process. To study the issue, we first conduct an empirical study on a pre-trained face synthesis model. We observe that after training the GAN model not only carries the biases in the training data but also amplifies them to some degree in the image generation process. To further improve the fairness of image generation, we propose an interpretable baseline method to balance the output facial attributes without retraining. The proposed method shifts the interpretable semantic distribution in the latent space for a more balanced image generation while preserving the sample diversity. Besides producing more balanced data regarding a particular attribute (e.g., race, gender, etc.), our method is generalizable to handle more than one attribute at a time and synthesize samples of fine-grained subgroups. We further show the positive applicability of the balanced data sampled from GANs to quantify the biases in other face recognition systems, like commercial face attribute classifiers and face super-resolution algorithms.",
                        "Citation Paper Authors": "Authors:Shuhan Tan, Yujun Shen, Bolei Zhou"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ", or train a\ngenerator conditioned on the biased attributes [30, 42]. Oth-\ners analyze the role of inductive biases in the generative pro-\ncess ",
                    "Citation Text": "Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming\nSong, Noah Goodman, and Stefano Ermon. Bias and\ngeneralization in deep generative models: An empirical\nstudy. arXiv preprint arXiv:1811.03259 , 2018. 2\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03259",
                        "Citation Paper Title": "Title:Bias and Generalization in Deep Generative Models: An Empirical Study",
                        "Citation Paper Abstract": "Abstract:In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.",
                        "Citation Paper Authors": "Authors:Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, Stefano Ermon"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.09061v2": {
            "Paper Title": "Explore the Expression: Facial Expression Generation using Auxiliary\n  Classifier Generative Adversarial Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.04644v1": {
            "Paper Title": "On-the-fly 3D metrology of volumetric additive manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02390v1": {
            "Paper Title": "Condensation Jacobian with Adaptivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02309v1": {
            "Paper Title": "Neural Collision Detection for Deformable Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02171v1": {
            "Paper Title": "NeAT: Neural Adaptive Tomography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.01537v1": {
            "Paper Title": "Bending Graphs: Hierarchical Shape Matching using Gated Optimal\n  Transport",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "propose\na two-stage method to estimate an optimal linear transfor-\nmation by the use of an invariant embedding network com-\nbined with a probe function network. Trappolini et al. ",
                    "Citation Text": "Giovanni Trappolini, Luca Cosmo, Luca Moschella, Ric-\ncardo Marin, Simone Melzi, and Emanuele Rodol `a. Shape\nregistration in the time of transformers. arXiv preprint\narXiv:2106.13679 , 2021. 1, 3, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.13679",
                        "Citation Paper Title": "Title:Shape registration in the time of transformers",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a transformer-based procedure for the efficient registration of non-rigid 3D point clouds. The proposed approach is data-driven and adopts for the first time the transformer architecture in the registration task. Our method is general and applies to different settings. Given a fixed template with some desired properties (e.g. skinning weights or other animation cues), we can register raw acquired data to it, thereby transferring all the template properties to the input geometry. Alternatively, given a pair of shapes, our method can register the first onto the second (or vice-versa), obtaining a high-quality dense correspondence between the two. In both contexts, the quality of our results enables us to target real applications such as texture transfer and shape interpolation. Furthermore, we also show that including an estimation of the underlying density of the surface eases the learning process. By exploiting the potential of this architecture, we can train our model requiring only a sparse set of ground truth correspondences ($10\\sim20\\%$ of the total points). The proposed model and the analysis that we perform pave the way for future exploration of transformer-based architectures for registration and matching applications. Qualitative and quantitative evaluations demonstrate that our pipeline outperforms state-of-the-art methods for deformable and unordered 3D data registration on different datasets and scenarios.",
                        "Citation Paper Authors": "Authors:Giovanni Trappolini, Luca Cosmo, Luca Moschella, Riccardo Marin, Simone Melzi, Emanuele Rodol\u00e0"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ". Unlike previous works that rely on axiomatic\ndescriptors [39, 48], recent methods focus on learning an\noptimal descriptor to have a better functional map estima-\ntion [13, 15, 24, 31, 34, 49, 54, 59]. Marin et al. ",
                    "Citation Text": "Riccardo Marin, Marie-Julie Rakotosaona, Simone Melzi,\nand Maks Ovsjanikov. Correspondence learning via linearly-\ninvariant embedding. NeurIPS , 33, 2020. 3, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.13136",
                        "Citation Paper Title": "Title:Correspondence Learning via Linearly-invariant Embedding",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a fully differentiable pipeline for estimating accurate dense correspondences between 3D point clouds. The proposed pipeline is an extension and a generalization of the functional maps framework. However, instead of using the Laplace-Beltrami eigenfunctions as done in virtually all previous works in this domain, we demonstrate that learning the basis from data can both improve robustness and lead to better accuracy in challenging settings. We interpret the basis as a learned embedding into a higher dimensional space. Following the functional map paradigm the optimal transformation in this embedding space must be linear and we propose a separate architecture aimed at estimating the transformation by learning optimal descriptor functions. This leads to the first end-to-end trainable functional map-based correspondence approach in which both the basis and the descriptors are learned from data. Interestingly, we also observe that learning a \\emph{canonical} embedding leads to worse results, suggesting that leaving an extra linear degree of freedom to the embedding network gives it more robustness, thereby also shedding light onto the success of previous methods. Finally, we demonstrate that our approach achieves state-of-the-art results in challenging non-rigid 3D point cloud correspondence applications.",
                        "Citation Paper Authors": "Authors:Riccardo Marin, Marie-Julie Rakotosaona, Simone Melzi, Maks Ovsjanikov"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ", and a method that\ndirectly learns optimal descriptors with the functional map\nframework, i.e. DGFM ",
                    "Citation Text": "Nicolas Donati, Abhishek Sharma, and Maks Ovsjanikov.\nDeep geometric functional maps: Robust feature learning for\nshape correspondence. In CVPR , pages 8592\u20138601, 2020. 3,\n6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14286",
                        "Citation Paper Title": "Title:Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence",
                        "Citation Paper Abstract": "Abstract:We present a novel learning-based approach for computing correspondences between non-rigid 3D shapes. Unlike previous methods that either require extensive training data or operate on handcrafted input descriptors and thus generalize poorly across diverse datasets, our approach is both accurate and robust to changes in shape structure. Key to our method is a feature-extraction network that learns directly from raw shape geometry, combined with a novel regularized map extraction layer and loss, based on the functional map representation. We demonstrate through extensive experiments in challenging shape matching scenarios that our method can learn from less training data than existing supervised approaches and generalizes significantly better than current descriptor-based learning methods. Our source code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Donati, Abhishek Sharma, Maks Ovsjanikov"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". The absolute\ninput position vishould be encoded to a feature of size d.\nTo capture the \ufb01ne details and inspired by NeRF positional\nencoding ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. In European conference on computer vision , pages\n405\u2013421. Springer, 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "or to\nlearn a more robust feature description directly from point\nclouds [14, 46]. PointNet ",
                    "Citation Text": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classi\ufb01cation\nand segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 652\u2013660,\n2017. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "de\ufb01nes 3D local \ufb01lters with a set of kernel\npoints which allow ef\ufb01cient and \ufb02exible point description.\nGraphite ",
                    "Citation Text": "Mahdi Saleh, Shervin Dehghani, Benjamin Busam, Nassir\nNavab, and Federico Tombari. Graphite: Graph-induced\nfeature extraction for point cloud registration. In 2020 In-\nternational Conference on 3D Vision (3DV) , pages 241\u2013251.\nIEEE, 2020. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09079",
                        "Citation Paper Title": "Title:Graphite: GRAPH-Induced feaTure Extraction for Point Cloud Registration",
                        "Citation Paper Abstract": "Abstract:3D Point clouds are a rich source of information that enjoy growing popularity in the vision community. However, due to the sparsity of their representation, learning models based on large point clouds is still a challenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure Extraction pipeline, a simple yet powerful feature transform and keypoint detector. Graphite enables intensive down-sampling of point clouds with keypoint detection accompanied by a descriptor. We construct a generic graph-based learning scheme to describe point cloud regions and extract salient points. To this end, we take advantage of 6D pose information and metric learning to learn robust descriptions and keypoints across different scans. We Reformulate the 3D keypoint pipeline with graph neural networks which allow efficient processing of the point set while boosting its descriptive power which ultimately results in more accurate 3D registrations. We demonstrate our lightweight descriptor on common 3D descriptor matching and point cloud registration benchmarks and achieve comparable results with the state of the art. Describing 100 patches of a point cloud and detecting their keypoints takes only ~0.018 seconds with our proposed network.",
                        "Citation Paper Authors": "Authors:Mahdi Saleh, Shervin Dehghani, Benjamin Busam, Nassir Navab, Federico Tombari"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "propose to estimate the transformation between two input\npoint clouds with an auto-encoder architecture and a trans-\nformer network ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems , pages 5998\u20136008, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "is the \ufb01rst approach that di-\nrectly outputs feature description using points with a per-\nmutation invariant pooling, but it fails to capture geometric\ndetails and ignores neighbor information. PointNet++ ",
                    "Citation Text": "Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++ deep hierarchical feature learning on point sets in a\nmetric space. In Proceedings of the 31st International Con-\nference on Neural Information Processing Systems , pages\n5105\u20135114, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". For non rigid meshes, spec-\ntral descriptors are often used [3, 8, 12] given their invari-\nance to (near-)isometric deformations. Later, data-driven\napproaches are proposed to compress hand-crafted features\ninto a compact yet informative representation ",
                    "Citation Text": "Marc Khoury, Qian-Yi Zhou, and Vladlen Koltun. Learning\ncompact geometric features. In ICCV , pages 153\u2013161, 2017.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.05056",
                        "Citation Paper Title": "Title:Learning Compact Geometric Features",
                        "Citation Paper Abstract": "Abstract:We present an approach to learning features that represent the local geometry around a point in an unstructured point cloud. Such features play a central role in geometric registration, which supports diverse applications in robotics and 3D vision. Current state-of-the-art local features for unstructured point clouds have been manually crafted and none combines the desirable properties of precision, compactness, and robustness. We show that features with these properties can be learned from data, by optimizing deep networks that map high-dimensional histograms into low-dimensional Euclidean spaces. The presented approach yields a family of features, parameterized by dimension, that are both more compact and more accurate than existing descriptors.",
                        "Citation Paper Authors": "Authors:Marc Khoury, Qian-Yi Zhou, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.00183v2": {
            "Paper Title": "Mixed Variational Finite Elements for Implicit, General-Purpose\n  Simulation of Deformables",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.00659v1": {
            "Paper Title": "Stay Positive: Non-Negative Image Synthesis for Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09170v3": {
            "Paper Title": "Continuation of Famous Art with AI: A Conditional Adversarial Network\n  Inpainting Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10022v2": {
            "Paper Title": "Affine Body Dynamics: Fast, Stable & Intersection-free Simulation of\n  Stiff Materials",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13240v1": {
            "Paper Title": "Grid-Free Monte Carlo for PDEs with Spatially Varying Coefficients",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13190v1": {
            "Paper Title": "Differentiable Neural Radiosity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13168v1": {
            "Paper Title": "SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.12044v1": {
            "Paper Title": "Generative GaitNet",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11287v1": {
            "Paper Title": "Sketch-based 3D Shape Modeling from Sparse Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11284v1": {
            "Paper Title": "Interactive 3D Character Modeling from 2D Orthogonal Drawings with\n  Annotations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10887v1": {
            "Paper Title": "An Attempt of Adaptive Heightfield Rendering with Complex Interpolants\n  Using Ray Casting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00965v3": {
            "Paper Title": "Coverage Axis: Inner Point Selection for 3D Shape Skeletonization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10695v1": {
            "Paper Title": "Estimation of Spectral Biophysical Skin Properties from Captured RGB\n  Albedo",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10423v1": {
            "Paper Title": "Rayleigh EigenDirections (REDs): GAN latent space traversals for\n  multidimensional features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00979v3": {
            "Paper Title": "Parabola-Inscribed Poncelet Polygons Derived from the Bicentric Family",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.10122v1": {
            "Paper Title": "Analytically Integratable Zero-restlength Springs for Capturing Dynamic\n  Modes unrepresented by Quasistatic Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05430v2": {
            "Paper Title": "Loci of Poncelet Triangles in the General Closure Case",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Remarkably, the locus\nof the mittenpunkt2is stationary at the common center ",
                    "Citation Text": "Reznik, D., Garcia, R., Koiller, J. (2020). Can the elliptic billiard still surprise us? Math\nIntelligencer , 42: 6\u201317. rdcu.be/b2cg1 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01515",
                        "Citation Paper Title": "Title:Can the Elliptic Billiard Still Surprise Us?",
                        "Citation Paper Abstract": "Abstract:Can any secrets still be shed by that much studied, uniquely integrable, Elliptic Billiard? Starting by examining the family of 3-periodic trajectories and the loci of their Triangular Centers, one obtains a beautiful and variegated gallery of curves: ellipses, quartics, sextics, circles, and even a stationary point. Secondly, one notices this family conserves an intriguing ratio: Inradius-to-Circumradius. In turn this implies three conservation corollaries: (i) the sum of bounce angle cosines, (ii) the product of excentral cosines, and (iii) the ratio of excentral-to-orbit areas. Monge's Orthoptic Circle's close relation to 4-periodic Billiard trajectories is well-known. Its geometry provided clues with which to generalize 3-periodic invariants to trajectories of an arbitrary number of edges. This was quite unexpected. Indeed, the Elliptic Billiard did surprise us!",
                        "Citation Paper Authors": "Authors:Dan Reznik, Ronaldo Garcia, Jair Koiller"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nExperiments suggests that only in the confocal pair can the locus of the incenter\nbe a conic ",
                    "Citation Text": "Helman, M., Laurain, D., Garcia, R., Reznik, D. (2021). Center power and loci of poncelet\ntriangles. arXiv:2102.09438.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.09438",
                        "Citation Paper Title": "Title:Invariant Center Power and Elliptic Loci of Poncelet Triangles",
                        "Citation Paper Abstract": "Abstract:We study center power with respect to circles derived from Poncelet 3-periodics (triangles) in a generic pair of ellipses as well as loci of their triangle centers. We show that (i) for any concentric pair, the power of the center with respect to either circumcircle or Euler's circle is invariant, and (ii) if a triangle center of a 3-periodic in a generic nested pair is a fixed affine combination of barycenter and circumcenter, its locus over the family is an ellipse.",
                        "Citation Paper Authors": "Authors:Mark Helman, Dominique Laurain, Ronaldo Garcia, Dan Reznik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.08361v2": {
            "Paper Title": "Stitch it in Time: GAN-Based Facial Editing of Real Videos",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "de\ufb01ne this as the distortion-editabilitytrade-off. They suggest that the two aspects can be balanced\nby designing an encoder that predicts codes in W+which\nreside close toW. More recently, Roich et al. ",
                    "Citation Text": "Daniel Roich, Ron Mokady, Amit H Bermano, and\nDaniel Cohen-Or. Pivotal tuning for latent-based edit-\ning of real images. arXiv preprint arXiv:2106.05744 ,\n2021. 2, 3, 4, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.05744",
                        "Citation Paper Title": "Title:Pivotal Tuning for Latent-based Editing of Real Images",
                        "Citation Paper Abstract": "Abstract:Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". As e4e is a deep neural network with many\nmillions of parameters, it is inherently biased towards learn-\ning lower frequency representations ",
                    "Citation Text": "Nasim Rahaman, Aristide Baratin, Devansh Arpit, Fe-\nlix Draxler, Min Lin, Fred Hamprecht, Yoshua Ben-\ngio, and Aaron Courville. On the spectral bias of neu-\nral networks. In International Conference on Machine\nLearning , pages 5301\u20135310. PMLR, 2019. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08734",
                        "Citation Paper Title": "Title:On the Spectral Bias of Neural Networks",
                        "Citation Paper Abstract": "Abstract:Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100\\%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \\emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.",
                        "Citation Paper Authors": "Authors:Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nNote in particular the neck region, which displays considerable ar-\ntifacts after the editing step which are then eliminated through our\nstitching-tuning approach.\nin inversions that are often inconsistent with the target iden-\ntity. Tov et al. ",
                    "Citation Text": "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik,\nand Daniel Cohen-Or. Designing an encoder for style-\ngan image manipulation, 2021. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.02766",
                        "Citation Paper Title": "Title:Designing an Encoder for StyleGAN Image Manipulation",
                        "Citation Paper Abstract": "Abstract:Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.",
                        "Citation Paper Authors": "Authors:Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "train a temporal LSTM-\nbased generator. Taking a different approach, Fox et al. ",
                    "Citation Text": "Gereon Fox, Ayush Tewari, Mohamed Elgharib, and\nChristian Theobalt. Stylevideogan: A temporal gener-\native model using a pretrained stylegan. arXiv preprint\narXiv:2107.07224 , 2021. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.07224",
                        "Citation Paper Title": "Title:StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN",
                        "Citation Paper Abstract": "Abstract:Generative adversarial models (GANs) continue to produce advances in terms of the visual quality of still images, as well as the learning of temporal correlations. However, few works manage to combine these two interesting capabilities for the synthesis of video content: Most methods require an extensive training dataset to learn temporal correlations, while being rather limited in the resolution and visual quality of their output. We present a novel approach to the video synthesis problem that helps to greatly improve visual quality and drastically reduce the amount of training data and resources necessary for generating videos. Our formulation separates the spatial domain, in which individual frames are synthesized, from the temporal domain, in which motion is generated. For the spatial domain we use a pre-trained StyleGAN network, the latent space of which allows control over the appearance of the objects it was trained for. The expressive power of this model allows us to embed our training videos in the StyleGAN latent space. Our temporal architecture is then trained not on sequences of RGB frames, but on sequences of StyleGAN latent codes. The advantageous properties of the StyleGAN space simplify the discovery of temporal correlations. We demonstrate that it suffices to train our temporal architecture on only 10 minutes of footage of 1 subject for about 6 hours. After training, our model can not only generate new portrait videos for the training subject, but also for any random subject which can be embedded in the StyleGAN space.",
                        "Citation Paper Authors": "Authors:Gereon Fox, Ayush Tewari, Mohamed Elgharib, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.11222v3": {
            "Paper Title": "On Aliased Resizing and Surprising Subtleties in GAN Evaluation",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": ". Achieving robustness to such\nperturbations remains an open problem ",
                    "Citation Text": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini,\nBenjamin Recht, and Ludwig Schmidt. Measuring robustness\nto natural distribution shifts in image classi\ufb01cation. In Ad-\nvances in Neural Information Processing Systems (NeurIPS) ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00644",
                        "Citation Paper Title": "Title:Measuring Robustness to Natural Distribution Shifts in Image Classification",
                        "Citation Paper Abstract": "Abstract:We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 204 ImageNet models in 213 different test conditions, we find that there is often little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets, which in multiple cases increases robustness, but is still far from closing the performance gaps. Our results indicate that distribution shifts arising in real data are currently an open research problem. We provide our testbed and data as a resource for future work at this https URL .",
                        "Citation Paper Authors": "Authors:Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, Ludwig Schmidt"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ",\nrecent works have demonstrated that antialiasing can be\ncompatible and improve performance in convolutional net-\nworks [79,85], transformers ",
                    "Citation Text": "Shengju Qian, Hao Shao, Yi Zhu, Mu Li, and Jiaya Jia. Blend-\ning anti-aliasing into vision transformer. In Thirty-Fifth Con-\nference on Neural Information Processing Systems , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.15156",
                        "Citation Paper Title": "Title:Blending Anti-Aliasing into Vision Transformer",
                        "Citation Paper Abstract": "Abstract:The transformer architectures, based on self-attention mechanism and convolution-free design, recently found superior performance and booming applications in computer vision. However, the discontinuous patch-wise tokenization process implicitly introduces jagged artifacts into attention maps, arising the traditional problem of aliasing for vision transformers. Aliasing effect occurs when discrete patterns are used to produce high frequency or continuous information, resulting in the indistinguishable distortions. Recent researches have found that modern convolution networks still suffer from this phenomenon. In this work, we analyze the uncharted problem of aliasing in vision transformer and explore to incorporate anti-aliasing properties. Specifically, we propose a plug-and-play Aliasing-Reduction Module(ARM) to alleviate the aforementioned issue. We investigate the effectiveness and generalization of the proposed method across multiple tasks and various vision transformer families. This lightweight design consistently attains a clear boost over several famous structures. Furthermore, our module also improves data efficiency and robustness of vision transformers.",
                        "Citation Paper Authors": "Authors:Shengju Qian, Hao Shao, Yi Zhu, Mu Li, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". As a result, it has been used in recent GANs\npapers [9,37,77] as well as large-scale evaluation study ",
                    "Citation Text": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly,\nand Olivier Bousquet. Are gans created equal? a large-scale\nstudy. In Advances in Neural Information Processing Systems ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10337",
                        "Citation Paper Title": "Title:Are GANs Created Equal? A Large-Scale Study",
                        "Citation Paper Abstract": "Abstract:Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \\cite{goodfellow2014generative}.",
                        "Citation Paper Authors": "Authors:Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07022v3": {
            "Paper Title": "Learning Body-Aware 3D Shape Generative Models",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "(eval-\nuated in the feature space of a pre-trained PointNet\nclassi\ufb01er ",
                    "Citation Text": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3D classi\ufb01cation\nand segmentation. In Proceedings of the IEEE Conference\n9on Computer Vision and Pattern Recognition , pages 652\u2013660,\n2017. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "uses a body-centric rep-\nresentation to place it at a static within 3D scenes, while\nSAMPL ",
                    "Citation Text": "Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito,\nJimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-\naware motion prediction. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 11374\u2013\n11384, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08284",
                        "Citation Paper Title": "Title:Stochastic Scene-Aware Motion Prediction",
                        "Citation Paper Abstract": "Abstract:A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as a source of training data. This is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. It is necessary to model this diversity when synthesizing virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our method, called SAMP, for Scene-Aware Motion Prediction, generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train our method, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate our method on complex indoor scenes and achieve superior performance compared to existing solutions. Our code and data are available for research at this https URL.",
                        "Citation Paper Authors": "Authors:Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, Michael Black"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.08440v1": {
            "Paper Title": "A Guide to Particle Advection Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.07786v1": {
            "Paper Title": "Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "to extract latent code\nvolume. We query the latent code f2R88at each point\nby trilinear interpolation1similar to Peng et al. ",
                    "Citation Text": "Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 9054\u20139063, 2021. 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15838",
                        "Citation Paper Title": "Title:Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans",
                        "Citation Paper Abstract": "Abstract:This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "\ufb01rst predict 2D landmarks then\ngenerate faces. Thies et al. ",
                    "Citation Text": "Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian\nTheobalt, and Matthias Nie\u00dfner. Neural voice puppetry:\nAudio-driven facial reenactment. In European Conference\non Computer Vision , pages 716\u2013731. Springer, 2020. 1, 2, 5,\n6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.05566",
                        "Citation Paper Title": "Title:Neural Voice Puppetry: Audio-driven Facial Reenactment",
                        "Citation Paper Abstract": "Abstract:We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study.",
                        "Citation Paper Authors": "Authors:Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "explicitly disentangle identity and word information forbetter feature extraction. Prajwal et al. ",
                    "Citation Text": "KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nambood-\niri, and CV Jawahar. A lip sync expert is all you need for\nspeech to lip generation in the wild. In Proceedings of the\n28th ACM International Conference on Multimedia , pages\n484\u2013492, 2020. 1, 2, 5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10010",
                        "Citation Paper Title": "Title:A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild",
                        "Citation Paper Abstract": "Abstract:In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: \\url{this http URL}. The code and models are released at this GitHub repository: \\url{this http URL}. You can also try out the interactive demo at this link: \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "propose the \ufb01rst end-to-end\nmethod with an encoder-decoder pipeline. Zhou et al. ",
                    "Citation Text": "Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang\nWang. Talking face generation by adversarially disentangled\naudio-visual representation. In AAAI Conference on Arti\ufb01-\ncial Intelligence (AAAI) , 2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.07860",
                        "Citation Paper Title": "Title:Talking Face Generation by Adversarially Disentangled Audio-Visual Representation",
                        "Citation Paper Abstract": "Abstract:Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.",
                        "Citation Paper Authors": "Authors:Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.08354v1": {
            "Paper Title": "HPCGen: Hierarchical K-Means Clustering and Level Based Principal\n  Components for Scan Path Genaration",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Here, the input consists of static images and saliency maps out of which a\nscan path is generated. An approache for scan path manipulation using an autoencoder and reinforcement learning is\ndescribed in ",
                    "Citation Text": "W. Fuhl, E. Bozkir, and E. Kasneci. Reinforcement learning for the privacy preservation and manipulation of eye\ntracking data. In Proceedings of IEEE International Joint Conference on Neural Networks , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06806",
                        "Citation Paper Title": "Title:Reinforcement learning for the privacy preservation and manipulation of eye tracking data",
                        "Citation Paper Abstract": "Abstract:In this paper, we present an approach based on reinforcement learning for eye tracking data manipulation. It is based on two opposing agents, where one tries to classify the data correctly and the second agent looks for patterns in the data, which get manipulated to hide specific information. We show that our approach is successfully applicable to preserve the privacy of the subjects. For this purpose, we evaluate our approach iteratively to showcase the behavior of the reinforcement learning based approach. In addition, we evaluate the importance of temporal, as well as spatial, information of eye tracking data for specific classification goals. In the last part of our evaluation, we apply the procedure to further public data sets without re-training the autoencoder or the data manipulator. The results show that the learned manipulation is generalized and applicable to unseen data as well.",
                        "Citation Paper Authors": "Authors:Wolfgang Fuhl, Efe Bozkir, Enkelejda Kasneci"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.02634v2": {
            "Paper Title": "Light Field Networks: Neural Scene Representations with\n  Single-Evaluation Rendering",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "on the task of single-shot (auto-decoding with a single view), multi-class reconstruction\nof the 13 largest ShapeNet ",
                    "Citation Text": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio\nSavarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.\narXiv preprint arXiv:1512.03012 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.06960v1": {
            "Paper Title": "The Wrought Iron Beauty of Poncelet Loci",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.06256v1": {
            "Paper Title": "A Robust Grid-Based Meshing Algorithm for Embedding Self-Intersecting\n  Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.04623v1": {
            "Paper Title": "Virtual Elastic Objects",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": "( \u00001to+1), where higher is better for both,\nand the learned perceptual metric LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "reconstructs a dynamic scene from multi-view in-\nput only, but does not compute temporal correspondences.\nSee a recent survey on neural rendering ",
                    "Citation Text": "A. Tewari, O. Fried, J. Thies, V . Sitzmann, S. Lombardi,\nK. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M.\nNie\u00dfner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y . Zhu, C.\nTheobalt, M. Agrawala, E. Shechtman, D. B Goldman, and\nM. Zollh \u00a8ofer. State of the Art on Neural Rendering. Com-\nputer Graphics Forum (EG STAR 2020) , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03805",
                        "Citation Paper Title": "Title:State of the Art on Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.04439v1": {
            "Paper Title": "Real-Time Style Modelling of Human Locomotion via Feature-Wise\n  Transformations and Local Motion Phases",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "showed that style could be modelled\nin real-time interactive systems by altering the hidden features\nof neural network layers using residual adapters ",
                    "Citation Text": "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning mul-\ntiple visual domains with residual adapters. In Advances in Neural Information\nProcessing Systems . 506\u2013516.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.08045",
                        "Citation Paper Title": "Title:Learning multiple visual domains with residual adapters",
                        "Citation Paper Abstract": "Abstract:There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.",
                        "Citation Paper Authors": "Authors:Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "before the FiLM\nparameters, creating a form of conditional normalisation ",
                    "Citation Text": "Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. 2017. A learned\nrepresentation for artistic style. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.07629",
                        "Citation Paper Title": "Title:A Learned Representation For Artistic Style",
                        "Citation Paper Abstract": "Abstract:The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
                        "Citation Paper Authors": "Authors:Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". They did\nnot however, try to learn a general style parameterisation function.\nAberman et al . ",
                    "Citation Text": "Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen.\n2020. Unpaired motion style transfer from video to animation. ACM Transactions\non Graphics (TOG) 39, 4 (2020), 64.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.05751",
                        "Citation Paper Title": "Title:Unpaired Motion Style Transfer from Video to Animation",
                        "Citation Paper Abstract": "Abstract:Transferring the motion style from one animation clip to another, while preserving the motion content of the latter, has been a long-standing problem in character animation. Most existing data-driven approaches are supervised and rely on paired data, where motions with the same content are performed in different styles. In addition, these approaches are limited to transfer of styles that were seen during training. In this paper, we present a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training. Furthermore, our framework is able to extract motion styles directly from videos, bypassing 3D reconstruction, and apply them to the 3D input motion. Our style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, we learn a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. Our results are comparable to the state-of-the-art, despite not requiring paired training data, and outperform other methods when transferring previously unseen styles. To our knowledge, we are the first to demonstrate style transfer directly from videos to 3D animations - an ability which enables one to extend the set of style examples far beyond motions captured by MoCap systems.",
                        "Citation Paper Authors": "Authors:Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.04205v1": {
            "Paper Title": "JSOL: JavaScript Open-source Library for Grammar of Graphics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06866v2": {
            "Paper Title": "A Multi-Implicit Neural Representation for Fonts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.02926v1": {
            "Paper Title": "Variational design for a structural family of CAD models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.14870v2": {
            "Paper Title": "Remeshing-Free Graph-Based Finite Element Method for Ductile and Brittle\n  Fracture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.17261v2": {
            "Paper Title": "Video-Specific Autoencoders for Exploring, Editing and Transmitting\n  Videos",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ". Fig-\nure 4 compares this with a reconstructed average of latent\ncodes from a DA VIS video ",
                    "Citation Text": "Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675 , 2017. 4, 5, 7, 8, 9, 10, 16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00675",
                        "Citation Paper Title": "Title:The 2017 DAVIS Challenge on Video Object Segmentation",
                        "Citation Paper Abstract": "Abstract:We present the 2017 DAVIS Challenge on Video Object Segmentation, a public dataset, benchmark, and competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset, define the evaluation metrics of the competition, and present a detailed analysis of the results of the participants to the challenge.",
                        "Citation Paper Authors": "Authors:Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nWe study this behaviour via three controlled experiments:\n(1)multi-view videos : training a video-speci\ufb01c autoencoder\non one stationary camera from a multi-view sequence ",
                    "Citation Text": "Aayush Bansal, Minh V o, Yaser Sheikh, Deva Ramanan, and\nSrinivasa Narasimhan. 4d visualization of dynamic events\nfrom unconstrained multi-view videos. In CVPR , 2020. 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.13532",
                        "Citation Paper Title": "Title:4D Visualization of Dynamic Events from Unconstrained Multi-View Videos",
                        "Citation Paper Abstract": "Abstract:We present a data-driven approach for 4D space-time visualization of dynamic events from videos captured by hand-held multiple cameras. Key to our approach is the use of self-supervised neural networks specific to the scene to compose static and dynamic aspects of an event. Though captured from discrete viewpoints, this model enables us to move around the space-time of the event continuously. This model allows us to create virtual cameras that facilitate: (1) freezing the time and exploring views; (2) freezing a view and moving through time; and (3) simultaneously changing both time and view. We can also edit the videos and reveal occluded objects for a given view if it is visible in any of the other views. We validate our approach on challenging in-the-wild events captured using up to 15 mobile cameras.",
                        "Citation Paper Authors": "Authors:Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", video prediction [50, 51],\ngenerative modeling for associating two videos ",
                    "Citation Text": "Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser\nSheikh. Recycle-gan: Unsupervised video retargeting. In\nECCV , 2018. 1, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05174",
                        "Citation Paper Title": "Title:Recycle-GAN: Unsupervised Video Retargeting",
                        "Citation Paper Abstract": "Abstract:We introduce a data-driven approach for unsupervised video retargeting that translates content from one domain to another while preserving the style native to a domain, i.e., if contents of John Oliver's speech were to be transferred to Stephen Colbert, then the generated content/speech should be in Stephen Colbert's style. Our approach combines both spatial and temporal information along with adversarial losses for content translation and style preservation. In this work, we first study the advantages of using spatiotemporal constraints over spatial constraints for effective retargeting. We then demonstrate the proposed approach for the problems where information in both space and time matters such as face-to-face translation, flower-to-flower, wind and cloud synthesis, sunrise and sunset.",
                        "Citation Paper Authors": "Authors:Aayush Bansal, Shugao Ma, Deva Ramanan, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "\ufb01nd close loops to do video texture. It is not a strict requirement for our work as we are able to interpolate new frames\nbetween x1andx2to smoothly transition between these frames.\nLPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "There is a large body of work on specialized video pro-\ncessing tasks such as video completion ",
                    "Citation Text": "Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In ECCV , 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.01835",
                        "Citation Paper Title": "Title:Flow-edge Guided Video Completion",
                        "Citation Paper Abstract": "Abstract:We present a new flow-based video completion algorithm. Previous flow completion methods are often unable to retain the sharpness of motion boundaries. Our method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges. Existing methods propagate colors among local flow connections between adjacent frames. However, not all missing regions in a video can be reached in this way because the motion boundaries form impenetrable barriers. Our method alleviates this problem by introducing non-local flow connections to temporally distant frames, enabling propagating video content over motion boundaries. We validate our approach on the DAVIS dataset. Both visual and quantitative results show that our method compares favorably against the state-of-the-art algorithms.",
                        "Citation Paper Authors": "Authors:Chen Gao, Ayush Saraf, Jia-Bin Huang, Johannes Kopf"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ", video inpainting [11, 22, 56], video editing [10,\n36], temporal super-resolution [25, 31, 34], spatial super-\nresolution [20, 55], space-time super-resolution [44, 45], re-\nmoving obstructions ",
                    "Citation Text": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu\nChuang, and Jia-Bin Huang. Learning to see through obstruc-\ntions with layered decomposition. In CVPR , 2020. 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.04902",
                        "Citation Paper Title": "Title:Learning to See Through Obstructions with Layered Decomposition",
                        "Citation Paper Abstract": "Abstract:We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",
                        "Citation Paper Authors": "Authors:Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.02369v1": {
            "Paper Title": "Deep Generative Framework for Interactive 3D Terrain Authoring and\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05172v2": {
            "Paper Title": "Projecting Robot Navigation Paths: Hardware and Software for Projected\n  AR",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": "D. Wang, C. Kohler, A. ten Pas, A. Wilkinson, M. Liu, H. Yanco, and\nR. Platt, \u201cTowards assistive robotic pick and place in open world envi-\nronments,\u201d in Proceedings of the International Symposium on Robotics\nResearch (ISRR) , 2019. ",
                    "Citation Text": "Z. Han, A. Wilkinson, J. Parrillo, J. Allspaw, and H. A. Yanco,\n\u201cProjection mapping implementation: Enabling direct externalization of\nperception results and action intent to improve robot explainability,\u201d theAI-HRI Symposium at AAAI-FSS 2020 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02263",
                        "Citation Paper Title": "Title:Projection Mapping Implementation: Enabling Direct Externalization of Perception Results and Action Intent to Improve Robot Explainability",
                        "Citation Paper Abstract": "Abstract:Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: this https URL.",
                        "Citation Paper Authors": "Authors:Zhao Han, Alexander Wilkinson, Jenna Parrillo, Jordan Allspaw, Holly A. Yanco"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "\u201cimage view: A simple viewer for ROS image topics.\u201d https:///wiki.ros.\norg/image view. ",
                    "Citation Text": "D. Wang, C. Kohler, A. ten Pas, A. Wilkinson, M. Liu, H. Yanco, and\nR. Platt, \u201cTowards assistive robotic pick and place in open world envi-\nronments,\u201d in Proceedings of the International Symposium on Robotics\nResearch (ISRR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.09541",
                        "Citation Paper Title": "Title:Towards Assistive Robotic Pick and Place in Open World Environments",
                        "Citation Paper Abstract": "Abstract:Assistive robot manipulators must be able to autonomously pick and place a wide range of novel objects to be truly useful. However, current assistive robots lack this capability. Additionally, assistive systems need to have an interface that is easy to learn, to use, and to understand. This paper takes a step forward in this direction. We present a robot system comprised of a robotic arm and a mobility scooter that provides both pick-and-drop and pick-and-place functionality for open world environments without modeling the objects or environment. The system uses a laser pointer to directly select an object in the world, with feedback to the user via projecting an interface into the world. Our evaluation over several experimental scenarios shows a significant improvement in both runtime and grasp success rate relative to a baseline from the literature [5], and furthermore demonstrates accurate pick and place capabilities for tabletop scenarios.",
                        "Citation Paper Authors": "Authors:Dian Wang, Colin Kohler, Andreas ten Pas, Alexander Wilkinson, Maozhi Liu, Holly Yanco, Robert Platt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.01889v1": {
            "Paper Title": "Stay in Touch! Shape and Shadow Influence Surface Contact in XR Displays",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.01487v1": {
            "Paper Title": "Harmonics Virtual Lights : fast projection of luminance field on\n  spherical harmonics for efficient rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02019v2": {
            "Paper Title": "Neural Actor: Neural Free-view Synthesis of Human Actors with Pose\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00094v1": {
            "Paper Title": "Wavelet Transparency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.15075v1": {
            "Paper Title": "Pose Estimation of Specific Rigid Objects",
            "Sentences": [
                {
                    "Sentence ID": 281,
                    "Sentence": ", Labb\u0013 e et al. regress a\ncontinuous 6D representation from ",
                    "Citation Text": "Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. \\On the conti-\nnuity of rotation representations in neural networks\". In: CVPR (2019).\n122",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07035",
                        "Citation Paper Title": "Title:On the Continuity of Rotation Representations in Neural Networks",
                        "Citation Paper Abstract": "Abstract:In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.",
                        "Citation Paper Authors": "Authors:Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li"
                    }
                },
                {
                    "Sentence ID": 277,
                    "Sentence": "segment object instances to eliminate surrounding clut-\nter and occluders, and predict the 3D object coordinates only for pixels in the instance\nmasks. Zakharov et al. ",
                    "Citation Text": "Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. \\DPOD: 6D Pose Object De-\ntector and Re\fner\". In: ICCV (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.11020",
                        "Citation Paper Title": "Title:DPOD: 6D Pose Object Detector and Refiner",
                        "Citation Paper Abstract": "Abstract:In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.",
                        "Citation Paper Authors": "Authors:Sergey Zakharov, Ivan Shugurov, Slobodan Ilic"
                    }
                },
                {
                    "Sentence ID": 276,
                    "Sentence": "Xenophon Zabulis, Manolis Lourakis, and Panagiotis Koutlemanis. \\3D Object\nPose Re\fnement in Range Images\". In: ICVS (2015).\n121Bibliography ",
                    "Citation Text": "Sergey Zakharov, Wadim Kehl, and Slobodan Ilic. \\DeceptionNet: Network-driven\ndomain randomization\". In: ICCV (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02750",
                        "Citation Paper Title": "Title:DeceptionNet: Network-Driven Domain Randomization",
                        "Citation Paper Abstract": "Abstract:We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing \"blind\" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities.",
                        "Citation Paper Authors": "Authors:Sergey Zakharov, Wadim Kehl, Slobodan Ilic"
                    }
                },
                {
                    "Sentence ID": 268,
                    "Sentence": "for semantic segmentation, normal estimation and boundary\ndetection. Wood et al. ",
                    "Citation Text": "Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson,\nand Andreas Bulling. \\Rendering of eyes for eye-shape registration and gaze esti-\nmation\". In: ICCV (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.05916",
                        "Citation Paper Title": "Title:Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
                        "Citation Paper Abstract": "Abstract:Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.",
                        "Citation Paper Authors": "Authors:Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling"
                    }
                },
                {
                    "Sentence ID": 264,
                    "Sentence": "Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Fed-\nerico Tombari. \\Self6D: Self-Supervised Monocular 6D Object Pose Estimation\".\nIn:ECCV (2020). ",
                    "Citation Text": "He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and\nLeonidas J Guibas. \\Normalized object coordinate space for category-level 6D ob-\nject pose and size estimation\". In: CVPR (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02970",
                        "Citation Paper Title": "Title:Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation",
                        "Citation Paper Abstract": "Abstract:The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to \"instance-level\" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.",
                        "Citation Paper Authors": "Authors:He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 263,
                    "Sentence": "Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart\u0013 \u0010n-Mart\u0013 \u0010n, Cewu Lu, Li Fei-Fei,\nand Silvio Savarese. \\DenseFusion: 6D object pose estimation by iterative dense\nfusion\". In: CVPR (2019). ",
                    "Citation Text": "Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Fed-\nerico Tombari. \\Self6D: Self-Supervised Monocular 6D Object Pose Estimation\".\nIn:ECCV (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06468",
                        "Citation Paper Title": "Title:Self6D: Self-Supervised Monocular 6D Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:6D object pose estimation is a fundamental problem in computer vision. Convolutional Neural Networks (CNNs) have recently proven to be capable of predicting reliable 6D pose estimates even from monocular images. Nonetheless, CNNs are identified as being extremely data-driven, and acquiring adequate annotations is oftentimes very time-consuming and labor intensive. To overcome this shortcoming, we propose the idea of monocular 6D pose estimation by means of self-supervised learning, removing the need for real annotations. After training our proposed network fully supervised with synthetic RGB data, we leverage recent advances in neural rendering to further self-supervise the model on unannotated real RGB-D data, seeking for a visually and geometrically optimal alignment. Extensive evaluations demonstrate that our proposed self-supervision is able to significantly enhance the model's original performance, outperforming all other methods relying on synthetic data or employing elaborate techniques from the domain adaptation realm.",
                        "Citation Paper Authors": "Authors:Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, Federico Tombari"
                    }
                },
                {
                    "Sentence ID": 253,
                    "Sentence": "Philip Torr and Andrew Zisserman. \\MLESAC: A new robust estimator with ap-\nplication to estimating image geometry\". In: CVIU (2000). ",
                    "Citation Text": "Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jam-\npani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birch\feld.\n\\Training deep networks with synthetic data: Bridging the reality gap by domain\nrandomization\". In: CVPRW (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.06516",
                        "Citation Paper Title": "Title:Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                        "Citation Paper Abstract": "Abstract:We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator$-$such as lighting, pose, object textures, etc.$-$are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds$-$both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.",
                        "Citation Paper Authors": "Authors:Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield"
                    }
                },
                {
                    "Sentence ID": 244,
                    "Sentence": "Grit Th\u007f urrner and Charles A W\u007f uthrich. \\Computing vertex normals from polyg-\nonal facets\". In: Journal of Graphics Tools (1998). ",
                    "Citation Text": "Meng Tian, Marcelo H Ang, and Gim Hee Lee. \\Shape Prior Deformation for\nCategorical 6D Object Pose and Size Estimation\". In: ECCV (2020).\n119Bibliography",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08454",
                        "Citation Paper Title": "Title:Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation",
                        "Citation Paper Abstract": "Abstract:We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Meng Tian, Marcelo H Ang Jr, Gim Hee Lee"
                    }
                },
                {
                    "Sentence ID": 237,
                    "Sentence": ". As shown by the failure cases presented in [127,\n237], estimating pose of partially occluded objects remains challenging for the recent\nholistic methods, even when occlusions are simulated at training time ",
                    "Citation Text": "Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, and Rudolph\nTriebel. \\Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Ob-\nject Detection\". In: IJCV (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01275",
                        "Citation Paper Title": "Title:Implicit 3D Orientation Learning for 6D Object Detection from RGB Images",
                        "Citation Paper Abstract": "Abstract:We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.",
                        "Citation Paper Authors": "Authors:Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, Rudolph Triebel"
                    }
                },
                {
                    "Sentence ID": 234,
                    "Sentence": ".\nAn alternative to acquiring real training images is synthesizing images by computer\ngraphics. This approach scales well as it requires only a minimal human e\u000bort, which may\ninclude 3D modeling. Su et al. ",
                    "Citation Text": "Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. \\Render for CNN:\nViewpoint estimation in images using cnns trained with rendered 3D model views\".\nIn:ICCV (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.05641",
                        "Citation Paper Title": "Title:Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views",
                        "Citation Paper Abstract": "Abstract:Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs. We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.",
                        "Citation Paper Authors": "Authors:Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 229,
                    "Sentence": "A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel. \\BigBIRD: A large-scale\n3D database of object instances\". In: ICRA (2014). rll.berkeley.edu/bigbird . ",
                    "Citation Text": "Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae Kyun Kim. \\In-\ntroducing Pose Consistency and Warp-Alignment for Self-Supervised 6D Object\nPose Estimation in Color Images\". In: 3DV (2020).\n118Bibliography",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12344",
                        "Citation Paper Title": "Title:Introducing Pose Consistency and Warp-Alignment for Self-Supervised 6D Object Pose Estimation in Color Images",
                        "Citation Paper Abstract": "Abstract:Most successful approaches to estimate the 6D pose of an object typically train a neural network by supervising the learning with annotated poses in real world images. These annotations are generally expensive to obtain and a common workaround is to generate and train on synthetic scenes, with the drawback of limited generalisation when the model is deployed in the real world. In this work, a two-stage 6D object pose estimator framework that can be applied on top of existing neural-network-based approaches and that does not require pose annotations on real images is proposed. The first self-supervised stage enforces the pose consistency between rendered predictions and real input images, narrowing the gap between the two domains. The second stage fine-tunes the previously trained model by enforcing the photometric consistency between pairs of different object views, where one image is warped and aligned to match the view of the other and thus enabling their comparison. In the absence of both real image annotations and depth information, applying the proposed framework on top of two recent approaches results in state-of-the-art performance when compared to methods trained only on synthetic data, domain adaptation baselines and a concurrent self-supervised approach on LINEMOD, LINEMOD OCCLUSION and HomebrewedDB datasets.",
                        "Citation Paper Authors": "Authors:Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, Tae-Kyun Kim"
                    }
                },
                {
                    "Sentence ID": 226,
                    "Sentence": "Dave Shreiner. OpenGL programming guide: the o\u000ecial guide to learning OpenGL,\nversions 3.0 and 3.1 . Pearson Education, 2009. ",
                    "Citation Text": "Ashish Shrivastava, Tomas P\fster, Oncel Tuzel, Joshua Susskind, Wenda Wang,\nand Russell Webb. \\Learning from Simulated and Unsupervised Images through\nAdversarial Training.\" In: CVPR (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.07828",
                        "Citation Paper Title": "Title:Learning from Simulated and Unsupervised Images through Adversarial Training",
                        "Citation Paper Abstract": "Abstract:With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
                        "Citation Paper Authors": "Authors:Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb"
                    }
                },
                {
                    "Sentence ID": 223,
                    "Sentence": ", or object\ncategories [264, 244, 30, 33], and learning to recognize objects in a model-free [26, 187,\n189], few-shot [187, 189], weakly-supervised ",
                    "Citation Text": "Jianzhun Shao, Yuhang Jiang, Gu Wang, Zhigang Li, and Xiangyang Ji. \\PFRL:\nPose-Free Reinforcement Learning for 6D Pose Estimation\". In: CVPR (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12096",
                        "Citation Paper Title": "Title:PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:6D pose estimation from a single RGB image is a challenging and vital task in computer vision. The current mainstream deep model methods resort to 2D images annotated with real-world ground-truth 6D object poses, whose collection is fairly cumbersome and expensive, even unavailable in many cases. In this work, to get rid of the burden of 6D annotations, we formulate the 6D pose refinement as a Markov Decision Process and impose on the reinforcement learning approach with only 2D image annotations as weakly-supervised 6D pose information, via a delicate reward definition and a composite reinforced optimization method for efficient and effective policy training. Experiments on LINEMOD and T-LESS datasets demonstrate that our Pose-Free approach is able to achieve state-of-the-art performance compared with the methods without using real-world ground-truth 6D pose labels.",
                        "Citation Paper Authors": "Authors:Jianzhun Shao, Yuhang Jiang, Gu Wang, Zhigang Li, Xiangyang Ji"
                    }
                },
                {
                    "Sentence ID": 216,
                    "Sentence": "German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M\nLopez. \\The SYNTHIA dataset: A large collection of synthetic images for semantic\nsegmentation of urban scenes\". In: CVPR (2016). ",
                    "Citation Text": "Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. \\Beyond sharing weights\nfor deep domain adaptation\". In: TPAMI (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06432",
                        "Citation Paper Title": "Title:Beyond Sharing Weights for Deep Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:The performance of a classifier trained on data coming from a specific domain typically degrades when applied to a related but different one. While annotating many samples from the new domain would address this issue, it is often too expensive or impractical. Domain Adaptation has therefore emerged as a solution to this problem; It leverages annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which it is either sparse or even lacking altogether. In this context, the recent trend consists of learning deep architectures whose weights are shared for both domains, which essentially amounts to learning domain invariant features.\nHere, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, where one operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings.",
                        "Citation Paper Authors": "Authors:Artem Rozantsev, Mathieu Salzmann, Pascal Fua"
                    }
                },
                {
                    "Sentence ID": 211,
                    "Sentence": "Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. \\Playing for bench-\nmarks\". In: ICCV (2017). ",
                    "Citation Text": "Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. \\Playing for\ndata: Ground truth from computer games\". In: ECCV (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.02192",
                        "Citation Paper Title": "Title:Playing for Data: Ground Truth from Computer Games",
                        "Citation Paper Abstract": "Abstract:Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 210,
                    "Sentence": "Colin Rennie, Rahul Shome, Kostas E Bekris, and Alberto F De Souza. \\A Dataset\nfor Improved RGBD-Based Object Detection and Pose Estimation for Warehouse\nPick-and-Place\". In: RA-L (2016). www.pracsyslab.org/rutgers_apc_rgbd_\ndataset . ",
                    "Citation Text": "Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. \\Playing for bench-\nmarks\". In: ICCV (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07322",
                        "Citation Paper Title": "Title:Playing for Benchmarks",
                        "Citation Paper Abstract": "Abstract:We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Zeeshan Hayder, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 207,
                    "Sentence": "achieve a\nspeed of 50 frames per second with a method inspired by the YOLO object detector ",
                    "Citation Text": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. \\You only look\nonce: Uni\fed, real-time object detection\". In: CVPR (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02640",
                        "Citation Paper Title": "Title:You Only Look Once: Unified, Real-Time Object Detection",
                        "Citation Paper Abstract": "Abstract:We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\nOur unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
                        "Citation Paper Authors": "Authors:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi"
                    }
                },
                {
                    "Sentence ID": 203,
                    "Sentence": "fuse color and depth features\n(the latter are calculated by PointNet++ ",
                    "Citation Text": "Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. \\Pointnet++:\nDeep hierarchical feature learning on point sets in a metric space\". In: Advances\nin neural information processing systems (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 202,
                    "Sentence": "segment objects in RGB\nchannels, in each mask calculate per-pixel color features by an auto-encoder network and\nper-pixel depth features by PointNet ",
                    "Citation Text": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. \\PointNet: Deep\nlearning on point sets for 3D classi\fcation and segmentation\". In: CVPR (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 196,
                    "Sentence": "assume that the global object\nsymmetries are known and propose a pose normalization applicable to the case when the\nprojection of the axis of symmetry is close to vertical. Pitteri et al. ",
                    "Citation Text": "Giorgia Pitteri, Micha\u007f el Ramamonjisoa, Slobodan Ilic, and Vincent Lepetit. \\On\nObject Symmetries and 6D Pose Estimation from Images\". In: 3DV (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07640",
                        "Citation Paper Title": "Title:On Object Symmetries and 6D Pose Estimation from Images",
                        "Citation Paper Abstract": "Abstract:Objects with symmetries are common in our daily life and in industrial contexts, but are often ignored in the recent literature on 6D pose estimation from images. In this paper, we study in an analytical way the link between the symmetries of a 3D object and its appearance in images. We explain why symmetrical objects can be a challenge when training machine learning algorithms that aim at estimating their 6D pose from images. We propose an efficient and simple solution that relies on the normalization of the pose rotation. Our approach is general and can be used with any 6D pose estimation algorithm. Moreover, our method is also beneficial for objects that are 'almost symmetrical', i.e. objects for which only a detail breaks the symmetry. We validate our approach within a Faster-RCNN framework on a synthetic dataset made with objects from the T-Less dataset, which exhibit various types of symmetries, as well as real sequences from T-Less.",
                        "Citation Paper Authors": "Authors:Giorgia Pitteri, Micha\u00ebl Ramamonjisoa, Slobodan Ilic, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 195,
                    "Sentence": "densely regress 2D unit vectors pointing to the 2D projec-\ntions of the 3D keypoints and \fnd the 2D projections via a RANSAC-based procedure.\nPitteri et al. ",
                    "Citation Text": "Giorgia Pitteri, Slobodan Ilic, and Vincent Lepetit. \\CorNet: Generic 3D Corners\nfor 6D Pose Estimation of New Objects without Retraining\". In: ICCVW . 2019.\n116Bibliography",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.11457",
                        "Citation Paper Title": "Title:CorNet: Generic 3D Corners for 6D Pose Estimation of New Objects without Retraining",
                        "Citation Paper Abstract": "Abstract:We present a novel approach to the detection and 3D pose estimation of objects in color images. Its main contribution is that it does not require any training phases nor data for new objects, while state-of-the-art methods typically require hours of training time and hundreds of training registered images. Instead, our method relies only on the objects' geometries. Our method focuses on objects with prominent corners, which covers a large number of industrial objects. We first learn to detect object corners of various shapes in images and also to predict their 3D poses, by using training images of a small set of objects. To detect a new object in a given image, we first identify its corners from its CAD model; we also detect the corners visible in the image and predict their 3D poses. We then introduce a RANSAC-like algorithm that robustly and efficiently detects and estimates the object's 3D pose by matching its corners on the CAD model with their detected counterparts in the image. Because we also estimate the 3D poses of the corners in the image, detecting only 1 or 2 corners is sufficient to estimate the pose of the object, which makes the approach robust to occlusions. We finally rely on a final check that exploits the full 3D geometry of the objects, in case multiple objects have the same corner spatial arrangement. The advantages of our approach make it particularly attractive for industrial contexts, and we demonstrate our approach on the challenging T-LESS dataset.",
                        "Citation Paper Authors": "Authors:Giorgia Pitteri, Slobodan Ilic, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 194,
                    "Sentence": "estimate the 3D rotation from predicted 2D-3D correspondences and the 3D\ntranslation by directly regressing a scale-invariant 3D translation vector. Instead of the\n3D object coordinates, Pitteri et al. ",
                    "Citation Text": "Giorgia Pitteri, Aur\u0013 elie Bugeau, Slobodan Ilic, and Vincent Lepetit. \\3D Object\nDetection and Pose Estimation of Unseen Objects in Color Images with Local\nSurface Embeddings\". In: ACCV (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04075",
                        "Citation Paper Title": "Title:3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings",
                        "Citation Paper Abstract": "Abstract:We present an approach for detecting and estimating the 3D poses of objects in images that requires only an untextured CAD model and no training phase for new objects. Our approach combines Deep Learning and 3D geometry: It relies on an embedding of local 3D geometry to match the CAD models to the input images. For points at the surface of objects, this embedding can be computed directly from the CAD model; for image locations, we learn to predict it from the image itself. This establishes correspondences between 3D points on the CAD model and 2D locations of the input images. However, many of these correspondences are ambiguous as many points may have similar local geometries. We show that we can use Mask-RCNN in a class-agnostic way to detect the new objects without retraining and thus drastically limit the number of possible correspondences. We can then robustly estimate a 3D pose from these discriminative correspondences using a RANSAC- like algorithm. We demonstrate the performance of this approach on the T-LESS dataset, by using a small number of objects to learn the embedding and testing it on the other objects. Our experiments show that our method is on par or better than previous methods.",
                        "Citation Paper Authors": "Authors:Giorgia Pitteri, Aur\u00e9lie Bugeau, Slobodan Ilic, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 192,
                    "Sentence": "follow a similar strategy but use smaller cells and aggregate the predictions\nover the cells. Peng et al. ",
                    "Citation Text": "Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. \\PVNet:\nPixel-wise Voting Network for 6DoF Pose Estimation\". In: CVPR (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.11788",
                        "Citation Paper Title": "Title:PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation",
                        "Citation Paper Abstract": "Abstract:This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise unit vectors pointing to the keypoints and use these vectors to vote for keypoint locations using RANSAC. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code will be avaliable at this https URL.",
                        "Citation Paper Authors": "Authors:Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 190,
                    "Sentence": "Tom\u0013 a\u0014 s Hoda\u0014 n , Dima Damen, Walterio Mayol-Cuevas, Ji\u0014 r\u0013 \u0010 Matas. E\u000ecient Texture-\nless Object Detection for Augmented Reality Guidance . International Symposium on\nMixed and Augmented Reality Workshops (ISMARW), 2015. ",
                    "Citation Text": "Yash Patel, Tom\u0013 a\u0014 s Hoda\u0014 n, and Ji\u0014 r\u0013 \u0010 Matas. \\Learning Surrogates via Deep Embed-\nding\". In: ECCV (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00799",
                        "Citation Paper Title": "Title:Learning Surrogates via Deep Embedding",
                        "Citation Paper Abstract": "Abstract:This paper proposes a technique for training a neural network by minimizing a surrogate loss that approximates the target evaluation metric, which may be non-differentiable. The surrogate is learned via a deep embedding where the Euclidean distance between the prediction and the ground truth corresponds to the value of the evaluation metric. The effectiveness of the proposed technique is demonstrated in a post-tuning setup, where a trained model is tuned using the learned surrogate. Without a significant computational overhead and any bells and whistles, improvements are demonstrated on challenging and practical tasks of scene-text recognition and detection. In the recognition task, the model is tuned using a surrogate approximating the edit distance metric and achieves up to $39\\%$ relative improvement in the total edit distance. In the detection task, the surrogate approximates the intersection over union metric for rotated bounding boxes and yields up to $4.25\\%$ relative improvement in the $F_{1}$ score.",
                        "Citation Paper Authors": "Authors:Yash Patel, Tomas Hodan, Jiri Matas"
                    }
                },
                {
                    "Sentence ID": 189,
                    "Sentence": "Kiru Park, Timothy Patten, and Markus Vincze. \\Pix2Pose: Pixel-Wise Coordi-\nnate Regression of Objects for 6D Pose Estimation\". In: ICCV (2019). ",
                    "Citation Text": "Kiru Park, Timothy Patten, and Markus Vincze. \\Neural Object Learning for 6D\nPose Estimation Using a Few Cluttered Images\". In: ECCV (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03717",
                        "Citation Paper Title": "Title:Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images",
                        "Citation Paper Abstract": "Abstract:Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 13 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene.",
                        "Citation Paper Authors": "Authors:Kiru Park, Timothy Patten, Markus Vincze"
                    }
                },
                {
                    "Sentence ID": 188,
                    "Sentence": "predict the UV texture coordinates instead of the 3D object\ncoordinates. Park et al. ",
                    "Citation Text": "Kiru Park, Timothy Patten, and Markus Vincze. \\Pix2Pose: Pixel-Wise Coordi-\nnate Regression of Objects for 6D Pose Estimation\". In: ICCV (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07433",
                        "Citation Paper Title": "Title:Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.",
                        "Citation Paper Authors": "Authors:Kiru Park, Timothy Patten, Markus Vincze"
                    }
                },
                {
                    "Sentence ID": 187,
                    "Sentence": "Chavdar Papazov and Darius Burschka. \\An e\u000ecient RANSAC for 3D object\nrecognition in noisy and occluded scenes\". In: ACCV (2010). ",
                    "Citation Text": "Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. \\LatentFusion:\nEnd-to-End Di\u000berentiable Reconstruction and Rendering for Unseen Object Pose\nEstimation\". In: CVPR (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00416",
                        "Citation Paper Title": "Title:LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Current 6D object pose estimation methods usually require a 3D model for each object. These methods also require additional training in order to incorporate new objects. As a result, they are difficult to scale to a large number of objects and cannot be directly applied to unseen objects.\nWe propose a novel framework for 6D pose estimation of unseen objects. We present a network that reconstructs a latent 3D representation of an object using a small number of reference views at inference time. Our network is able to render the latent 3D representation from arbitrary views. Using this neural renderer, we directly optimize for pose given an input image. By training our network with a large number of 3D shapes for reconstruction and rendering, our network generalizes well to unseen objects. We present a new dataset for unseen object pose estimation--MOPED. We evaluate the performance of our method for unseen object pose estimation on MOPED as well as the ModelNet and LINEMOD datasets. Our method performs competitively to supervised methods that are trained on those objects. Code and data is available at this https URL.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox"
                    }
                },
                {
                    "Sentence ID": 183,
                    "Sentence": ". PSO is applied\nfor post-re\fnement in the template-matching method proposed in Chapter 3.\nLearning-based re\fnement methods have emerged as well. Oberweger et al. ",
                    "Citation Text": "Markus Oberweger, Paul Wohlhart, and Vincent Lepetit. \\Training a feedback\nloop for hand pose estimation\". In: ICCV (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09698",
                        "Citation Paper Title": "Title:Training a Feedback Loop for Hand Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.",
                        "Citation Paper Authors": "Authors:Markus Oberweger, Paul Wohlhart, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 182,
                    "Sentence": "de\fne the 3D keypoints manually on the model sur-\nface. Oberweger ",
                    "Citation Text": "Markus Oberweger, Mahdi Rad, and Vincent Lepetit. \\Making deep heatmaps\nrobust to partial occlusions for 3D object pose estimation\". In: ECCV (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.03959",
                        "Citation Paper Title": "Title:Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We introduce a novel method for robust and accurate 3D object pose estimation from a single color image under large occlusions. Following recent approaches, we first predict the 2D projections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training subsequently becomes challenging because patches with similar appearances but different positions on the object correspond to different heatmaps. However, we provide a simple yet effective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded LineMOD dataset and the YCB-Video dataset, both exhibiting cluttered scenes with highly occluded objects. Project website: this https URL",
                        "Citation Paper Authors": "Authors:Markus Oberweger, Mahdi Rad, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 172,
                    "Sentence": "use PBR images of eyes for training gaze estimation models.\nAttias et al. ",
                    "Citation Text": "Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh. \\How useful is photo-\nrealistic rendering for visual learning?\" In: ECCV (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08152",
                        "Citation Paper Title": "Title:How useful is photo-realistic rendering for visual learning?",
                        "Citation Paper Abstract": "Abstract:Data seems cheap to get, and in many ways it is, but the process of creating a high quality labeled dataset from a mass of data is time-consuming and expensive.\nWith the advent of rich 3D repositories, photo-realistic rendering systems offer the opportunity to provide nearly limitless data. Yet, their primary value for visual learning may be the quality of the data they can provide rather than the quantity. Rendering engines offer the promise of perfect labels in addition to the data: what the precise camera pose is; what the precise lighting location, temperature, and distribution is; what the geometry of the object is.\nIn this work we focus on semi-automating dataset creation through use of synthetic data and apply this method to an important task -- object viewpoint estimation. Using state-of-the-art rendering software we generate a large labeled dataset of cars rendered densely in viewpoint space. We investigate the effect of rendering parameters on estimation performance and show realism is important. We show that generalizing from synthetic data is not harder than the domain adaptation required between two real-image datasets and that combining synthetic images with a small amount of real data improves estimation accuracy.",
                        "Citation Paper Authors": "Authors:Yair Movshovitz-Attias, Takeo Kanade, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 163,
                    "Sentence": "model 3D scenes for semantic scene understanding and McCormac et al. ",
                    "Citation Text": "John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew J Davison.\n\\SceneNet RGB-D: Can 5M synthetic images beat generic ImageNet pre-training\non indoor segmentation?\" In: ICCV (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.05079",
                        "Citation Paper Title": "Title:SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth",
                        "Citation Paper Abstract": "Abstract:We introduce SceneNet RGB-D, expanding the previous work of SceneNet to enable large scale photorealistic rendering of indoor scene trajectories. It provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. Random sampling permits virtually unlimited scene configurations, and here we provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system. We host the dataset at this http URL",
                        "Citation Paper Authors": "Authors:John McCormac, Ankur Handa, Stefan Leutenegger, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 151,
                    "Sentence": "as the backbone. The network is pre-trained on the Microsoft COCO dataset ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u0013 ar, and C Lawrence Zitnick. \\Microsoft COCO: Common ob-\njects in context\". In: ECCV (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 147,
                    "Sentence": "and to each detection applies a neural\nnetwork for coarse pose estimation followed by a neural network for iterative re\fnement.\nBoth networks have the same structure inspired by DeepIM ",
                    "Citation Text": "Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. \\DeepIM: Deep iterative\nmatching for 6D pose estimation\". In: ECCV (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00175",
                        "Citation Paper Title": "Title:DeepIM: Deep Iterative Matching for 6D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the observed image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.",
                        "Citation Paper Authors": "Authors:Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox"
                    }
                },
                {
                    "Sentence ID": 146,
                    "Sentence": "predict the\n3D orientation by regressing a Lie algebra representation. Li et al. ",
                    "Citation Text": "Chi Li, Jin Bai, and Gregory D Hager. \\A uni\fed framework for multi-view multi-\nclass object pose estimation\". In: ECCV (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08103",
                        "Citation Paper Title": "Title:A Unified Framework for Multi-View Multi-Class Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:One core challenge in object pose estimation is to ensure accurate and robust performance for large numbers of diverse foreground objects amidst complex background clutter. In this work, we present a scalable framework for accurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of object classes from single or multiple views. To learn discriminative pose features, we integrate three new capabilities into a deep Convolutional Neural Network (CNN): an inference scheme that combines both classification and pose regression based on a uniform tessellation of the Special Euclidean group in three dimensions (SE(3)), the fusion of class priors into the training process via a tiled class map, and an additional regularization using deep supervision with an object mask. Further, an efficient multi-view framework is formulated to address single-view ambiguity. We show that this framework consistently improves the performance of the single-view network. We evaluate our method on three large-scale benchmarks: YCB-Video, JHUScene-50 and ObjectNet-3D. Our approach achieves competitive or superior performance over the current state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Chi Li, Jin Bai, Gregory D. Hager"
                    }
                },
                {
                    "Sentence ID": 130,
                    "Sentence": "directly\nregress the 3D orientation and 3D location of the object from X-ray images and show\nthat the accuracy can be improved if the network is trained by jointly minimizing a pose\nerror, as in ",
                    "Citation Text": "Alex Kendall, Matthew Grimes, and Roberto Cipolla. \\PoseNet: A convolutional\nnetwork for real-time 6-DOF camera relocalization\". In: ICCV (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.07427",
                        "Citation Paper Title": "Title:PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization",
                        "Citation Paper Abstract": "Abstract:We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at this http URL",
                        "Citation Paper Authors": "Authors:Alex Kendall, Matthew Grimes, Roberto Cipolla"
                    }
                },
                {
                    "Sentence ID": 129,
                    "Sentence": "and based on hashing of depth measurements and orientations of surface\nnormals. A similar RGB-D method was concurrently proposed by Kehl et al. ",
                    "Citation Text": "Wadim Kehl, Federico Tombari, Nassir Navab, Slobodan Ilic, and Vincent Lep-\netit. \\Hashmod: A Hashing Method for Scalable 3D Object Detection\". In: BMVC\n(2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06062",
                        "Citation Paper Title": "Title:Hashmod: A Hashing Method for Scalable 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:We present a scalable method for detecting objects and estimating their 3D poses in RGB-D data. To this end, we rely on an efficient representation of object views and employ hashing techniques to match these views against the input frame in a scalable way. While a similar approach already exists for 2D detection, we show how to extend it to estimate the 3D pose of the detected objects. In particular, we explore different hashing strategies and identify the one which is more suitable to our problem. We show empirically that the complexity of our method is sublinear with the number of objects and we enable detection and pose estimation of many 3D objects with high accuracy while outperforming the state-of-the-art in terms of runtime.",
                        "Citation Paper Authors": "Authors:Wadim Kehl, Federico Tombari, Nassir Navab, Slobodan Ilic, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 128,
                    "Sentence": "with features calculated with an auto-encoder neural network. Kehl et al. ",
                    "Citation Text": "Wadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, and Nassir Navab.\n\\Deep learning of local RGB-D patches for 3D object detection and 6D pose esti-\nmation\". In: ECCV (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06038",
                        "Citation Paper Title": "Title:Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.",
                        "Citation Paper Authors": "Authors:Wadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, Nassir Navab"
                    }
                },
                {
                    "Sentence ID": 125,
                    "Sentence": "Troels Bo J\u001crgensen, Anders Glent Buch, and Dirk Kraft. \\Geometric edge descrip-\ntion and classi\fcation in point cloud data with application to 3D object recogni-\ntion\". In: VISAPP (2015). ",
                    "Citation Text": "Philipp Jund, Nichola Abdo, Andreas Eitel, and Wolfram Burgard. \\The Freiburg\ngroceries dataset\". In: arXiv preprint arXiv:1611.05799 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05799",
                        "Citation Paper Title": "Title:The Freiburg Groceries Dataset",
                        "Citation Paper Abstract": "Abstract:With the increasing performance of machine learning techniques in the last few years, the computer vision and robotics communities have created a large number of datasets for benchmarking object recognition tasks. These datasets cover a large spectrum of natural images and object categories, making them not only useful as a testbed for comparing machine learning approaches, but also a great resource for bootstrapping different domain-specific perception and robotic systems. One such domain is domestic environments, where an autonomous robot has to recognize a large variety of everyday objects such as groceries. This is a challenging task due to the large variety of objects and products, and where there is great need for real-world training data that goes beyond product images available online. In this paper, we address this issue and present a dataset consisting of 5,000 images covering 25 different classes of groceries, with at least 97 images per class. We collected all images from real-world settings at different stores and apartments. In contrast to existing groceries datasets, our dataset includes a large variety of perspectives, lighting conditions, and degrees of clutter. Overall, our images contain thousands of different object instances. It is our hope that machine learning and robotics researchers find this dataset of use for training, testing, and bootstrapping their approaches. As a baseline classifier to facilitate comparison, we re-trained the CaffeNet architecture (an adaptation of the well-known AlexNet) on our dataset and achieved a mean accuracy of 78.9%. We release this trained model along with the code and data splits we used in our experiments.",
                        "Citation Paper Authors": "Authors:Philipp Jund, Nichola Abdo, Andreas Eitel, Wolfram Burgard"
                    }
                },
                {
                    "Sentence ID": 115,
                    "Sentence": ".\nThey split the input image into regular cells, and for each cell predict the probability of\neach object's presence and regress the 2D projection coordinates of the 3D keypoints. Hu\net al. ",
                    "Citation Text": "Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salzmann. \\Segmentation-\ndriven 6D object pose estimation\". In: CVPR (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02541",
                        "Citation Paper Title": "Title:Segmentation-driven 6D Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:The most recent trend in estimating the 6D pose of rigid objects has been to train deep networks to either directly regress the pose from the image or to predict the 2D locations of 3D keypoints, from which the pose can be obtained using a PnP algorithm. In both cases, the object is treated as a global entity, and a single pose estimate is computed. As a consequence, the resulting techniques can be vulnerable to large occlusions.\nIn this paper, we introduce a segmentation-driven 6D pose estimation framework where each visible part of the objects contributes a local pose prediction in the form of 2D keypoint locations. We then use a predicted measure of confidence to combine these pose candidates into a robust set of 3D-to-2D correspondences, from which a reliable pose estimate can be obtained. We outperform the state-of-the-art on the challenging Occluded-LINEMOD and YCB-Video datasets, which is evidence that our approach deals well with multiple poorly-textured objects occluding each other. Furthermore, it relies on a simple enough architecture to achieve real-time performance.",
                        "Citation Paper Authors": "Authors:Yinlin Hu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "demonstrate the\nimportance of selecting suitable background images, and Hinterstoisser et al. ",
                    "Citation Text": "Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Marek Martina, and Martin\nBokeloh. \\An annotation saved is an annotation earned: Using fully synthetic train-\ning for object detection\". In: ICCVW (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.09967",
                        "Citation Paper Title": "Title:An Annotation Saved is an Annotation Earned: Using Fully Synthetic Training for Object Instance Detection",
                        "Citation Paper Abstract": "Abstract:Deep learning methods typically require vast amounts of training data to reach their full potential. While some publicly available datasets exists, domain specific data always needs to be collected and manually labeled, an expensive, time consuming and error prone process. Training with synthetic data is therefore very lucrative, as dataset creation and labeling comes for free. We propose a novel method for creating purely synthetic training data for object detection. We leverage a large dataset of 3D background models and densely render them using full domain randomization. This yields background images with realistic shapes and texture on top of which we render the objects of interest. During training, the data generation process follows a curriculum strategy guaranteeing that all foreground models are presented to the network equally under all possible poses and conditions with increasing complexity. As a result, we entirely control the underlying statistics and we create optimal training samples at every stage of training. Using a set of 64 retail objects, we demonstrate that our simple approach enables the training of detectors that outperform models trained with real data on a challenging evaluation dataset.",
                        "Citation Paper Authors": "Authors:Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina Marek, Martin Bokeloh"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "synthesize images of 3D object models for viewpoint\nestimation, Hinterstoisser et al. ",
                    "Citation Text": "Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and Kurt Konolige. \\On\nPre-Trained Image Features and Synthetic Images for Deep Learning\". In: EC-\nCVW (2018).\n110Bibliography",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10710",
                        "Citation Paper Title": "Title:On Pre-Trained Image Features and Synthetic Images for Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep Learning methods usually require huge amounts of training data to perform at their full potential, and often require expensive manual labeling. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free, and several approaches have been proposed to combine synthetic and real images for training.\nIn this paper, we show that a simple trick is sufficient to train very effectively modern object detectors with synthetic images only: We freeze the layers responsible for feature extraction to generic layers pre-trained on real images, and train only the remaining layers with plain OpenGL rendering. Our experiments with very recent deep architectures for object recognition (Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet and Resnet) show this simple approach performs surprisingly well.",
                        "Citation Paper Authors": "Authors:Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, Kurt Konolige"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": "reduce the\nsearch space by a coarse-to-\fne segmentation, favor points with higher expected visibility,\nand \flter pose hypotheses by an occlusion-aware ranking. Hinterstoisser et al. ",
                    "Citation Text": "Stefan Hinterstoisser, Vincent Lepetit, Naresh Rajkumar, and Kurt Konolige. \\Go-\ning further with point pair features\". In: ECCV (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.04061",
                        "Citation Paper Title": "Title:Going Further with Point Pair Features",
                        "Citation Paper Abstract": "Abstract:Point Pair Features is a widely used method to detect 3D objects in point clouds, however they are prone to fail in presence of sensor noise and background clutter. We introduce novel sampling and voting schemes that significantly reduces the influence of clutter and sensor noise. Our experiments show that with our improvements, PPFs become competitive against state-of-the-art methods as it outperforms them on several objects from challenging benchmarks, at a low computational cost.",
                        "Citation Paper Authors": "Authors:Stefan Hinterstoisser, Vincent Lepetit, Naresh Rajkumar, Kurt Konolige"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": ", fuse the two types of features, and regress\nobject poses as described in Section 2.2.5. He et al. ",
                    "Citation Text": "Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.\n\\PVN3D: A deep point-wise 3D keypoints voting network for 6DoF pose estima-\ntion\". In: CVPR (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04231",
                        "Citation Paper Title": "Title:PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation",
                        "Citation Paper Abstract": "Abstract:In this work, we present a novel data-driven method for robust 6DoF object pose estimation from a single RGBD image. Unlike previous methods that directly regressing pose parameters, we tackle this challenging task with a keypoint-based approach. Specifically, we propose a deep Hough voting network to detect 3D keypoints of objects and then estimate the 6D pose parameters within a least-squares fitting manner. Our method is a natural extension of 2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It allows us to fully utilize the geometric constraint of rigid objects with the extra depth information and is easy for a network to learn and optimize. Extensive experiments were conducted to demonstrate the effectiveness of 3D-keypoint detection in the 6D pose estimation task. Experimental results also show our method outperforms the state-of-the-art methods by large margins on several benchmarks. Code and video are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "were gen-\nerated using virtual cities modeled from scratch. Handa et al. ",
                    "Citation Text": "Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto\nCipolla. \\Understanding real world indoor scenes with synthetic data\". In: CVPR\n(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.07041",
                        "Citation Paper Title": "Title:SceneNet: Understanding Real World Indoor Scenes With Synthetic Data",
                        "Citation Paper Abstract": "Abstract:Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route to generating synthesized frame or video data, and understanding of different factors influencing performance gains.",
                        "Citation Paper Authors": "Authors:Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": ". The pose hypotheses\ncan be further re\fned individually by a surface registration method, such as Iterative\nClosest Point (ICP) [13, 217], or globally by optimizing all hypotheses together [186, 2].\nGuo et al. ",
                    "Citation Text": "Yulan Guo, Mohammed Bennamoun, Ferdous Sohel, Min Lu, and Jianwei Wan.\n\\3D object recognition in cluttered scenes with local surface features: a survey\".\nIn:TPAMI (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1304.3192",
                        "Citation Paper Title": "Title:Rotational Projection Statistics for 3D Local Surface Description and Object Recognition",
                        "Citation Paper Abstract": "Abstract:Recognizing 3D objects in the presence of noise, varying mesh resolution, occlusion and clutter is a very challenging task. This paper presents a novel method named Rotational Projection Statistics (RoPS). It has three major modules: Local Reference Frame (LRF) definition, RoPS feature description and 3D object recognition. We propose a novel technique to define the LRF by calculating the scatter matrix of all points lying on the local surface. RoPS feature descriptors are obtained by rotationally projecting the neighboring points of a feature point onto 2D planes and calculating a set of statistics (including low-order central moments and entropy) of the distribution of these projected points. Using the proposed LRF and RoPS descriptor, we present a hierarchical 3D object recognition algorithm. The performance of the proposed LRF, RoPS descriptor and object recognition algorithm was rigorously tested on a number of popular and publicly available datasets. Our proposed techniques exhibited superior performance compared to existing techniques. We also showed that our method is robust with respect to noise and varying mesh resolution. Our RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and 96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari Venezia Datasets.",
                        "Citation Paper Authors": "Authors:Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "follows the same pipeline, with the instance masks predicted by RetinaMask ",
                    "Citation Text": "Cheng-Yang Fu, Mykhailo Shvets, and Alexander C Berg. \\RetinaMask: Learning\nto predict masks improves state-of-the-art single-shot detection for free\". In: arXiv\npreprint arXiv:1901.03353 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.03353",
                        "Citation Paper Title": "Title:RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free",
                        "Citation Paper Abstract": "Abstract:Recently two-stage detectors have surged ahead of single-shot detectors in the accuracy-vs-speed trade-off. Nevertheless single-shot detectors are immensely popular in embedded vision applications. This paper brings single-shot detectors up to the same level as current two-stage techniques. We do this by improving training for the state-of-the-art single-shot detector, RetinaNet, in three ways: integrating instance mask prediction for the first time, making the loss function adaptive and more stable, and including additional hard examples in training. We call the resulting augmented network RetinaMask. The detection component of RetinaMask has the same computational cost as the original RetinaNet, but is more accurate. COCO test-dev results are up to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the runtime is the same during evaluation. Adding Group Normalization increases the performance of RetinaMask-101 to 41.7 mAP. Code is at:this https URL",
                        "Citation Paper Authors": "Authors:Cheng-Yang Fu, Mykhailo Shvets, Alexander C. Berg"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Dmitry Ole-\n\fr, Tom\u0013 a\u0014 s Hoda\u0014 n, Youssef Zidan, Mohamad Elbadrawy, Markus Knauer, Hari-\nnandan Katam, and Ahsan Lodhi. \\BlenderProc: Reducing the Reality Gap with\nPhotorealistic Rendering\". In: RSS Workshops (2020). ",
                    "Citation Text": "Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zi-\ndan, Dmitry Ole\fr, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.\n\\BlenderProc\". In: arXiv preprint arXiv:1911.01911 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01911",
                        "Citation Paper Title": "Title:BlenderProc",
                        "Citation Paper Abstract": "Abstract:BlenderProc is a modular procedural pipeline, which helps in generating real looking images for the training of convolutional neural networks. These can be used in a variety of use cases including segmentation, depth, normal and pose estimation and many others. A key feature of our extension of blender is the simple to use modular pipeline, which was designed to be easily extendable. By offering standard modules, which cover a variety of scenarios, we provide a starting point on which new modules can be created.",
                        "Citation Paper Authors": "Authors:Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "train a classi\fer\nfor only a subset of viewpoints de\fned by global object symmetries. Corona et al. ",
                    "Citation Text": "Enric Corona, Kaustav Kundu, and Sanja Fidler. \\Pose estimation for objects with\nrotational symmetry\". In: IROS (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05780",
                        "Citation Paper Title": "Title:Pose Estimation for Objects with Rotational Symmetry",
                        "Citation Paper Abstract": "Abstract:Pose estimation is a widely explored problem, enabling many robotic tasks such as grasping and manipulation. In this paper, we tackle the problem of pose estimation for objects that exhibit rotational symmetry, which are common in man-made and industrial environments. In particular, our aim is to infer poses for objects not seen at training time, but for which their 3D CAD models are available at test time. Previous work has tackled this problem by learning to compare captured views of real objects with the rendered views of their 3D CAD models, by embedding them in a joint latent space using neural networks. We show that sidestepping the issue of symmetry in this scenario during training leads to poor performance at test time. We propose a model that reasons about rotational symmetry during training by having access to only a small set of symmetry-labeled objects, whereby exploiting a large collection of unlabeled CAD models. We demonstrate that our approach significantly outperforms a naively trained neural network on a new pose dataset containing images of tools and hardware.",
                        "Citation Paper Authors": "Authors:Enric Corona, Kaustav Kundu, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and Ales Leonardis. \\G2L-\nNet: Global to Local Network for Real-time 6D Pose Estimation with Embedding\nVector Features\". In: CVPR (2020).\n106Bibliography ",
                    "Citation Text": "Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, and Otmar Hilliges. \\Cate-\ngory Level Object Pose Estimation via Neural Analysis-by-Synthesis\". In: ECCV\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.08145",
                        "Citation Paper Title": "Title:Category Level Object Pose Estimation via Neural Analysis-by-Synthesis",
                        "Citation Paper Abstract": "Abstract:Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit CAD models per object instance unnecessary. The image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. At inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. Keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2D images alone. When provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6DOF pose successfully.",
                        "Citation Paper Authors": "Authors:Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, Otmar Hilliges"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "process a color point cloud with\nPointNet to \fnd candidate object locations and to establish 3D-3D correspondences at\neach such location. Chen et al. ",
                    "Citation Text": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and Ales Leonardis. \\G2L-\nNet: Global to Local Network for Real-time 6D Pose Estimation with Embedding\nVector Features\". In: CVPR (2020).\n106Bibliography",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11089",
                        "Citation Paper Title": "Title:G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel real-time 6D object pose estimation framework, named G2L-Net. Our network operates on point clouds from RGB-D detection in a divide-and-conquer fashion. Specifically, our network consists of three steps. First, we extract the coarse object point cloud from the RGB-D image by 2D detection. Second, we feed the coarse object point cloud to a translation localization network to perform 3D segmentation and object translation prediction. Third, via the predicted segmentation and translation, we transfer the fine object point cloud into a local canonical coordinate, in which we train a rotation localization network to estimate initial object rotation. In the third step, we define point-wise embedding vector features to capture viewpoint-aware information. To calculate more accurate rotation, we adopt a rotation residual estimator to estimate the residual between initial rotation and ground truth, which can boost initial pose estimation performance. Our proposed G2L-Net is real-time despite the fact multiple steps are stacked via the proposed coarse-to-fine framework. Extensive experiments on two benchmark datasets show that G2L-Net achieves state-of-the-art performance in terms of both accuracy and speed.",
                        "Citation Paper Authors": "Authors:Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Ales Leonardis"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "Owen Carmichael and Martial Hebert. \\Object recognition by a cascade of edge\nprobes\". In: BMVC (2002). ",
                    "Citation Text": "Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu. \\Learning canonical shape\nspace for category-level 6D object pose and size estimation\". In: CVPR (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.09322",
                        "Citation Paper Title": "Title:Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation",
                        "Citation Paper Abstract": "Abstract:We present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a unified representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a pose-independent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Dengsheng Chen, Jun Li, Zheng Wang, Kai Xu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". None of the descriptors utilize color.\nThe Buch-17 method ",
                    "Citation Text": "Anders Glent Buch, Lilita Kiforenko, and Dirk Kraft. \\Rotational Subgroup Voting\nand Pose Clustering for Robust 3D Object Recognition\". In: ICCV (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02142",
                        "Citation Paper Title": "Title:Rotational Subgroup Voting and Pose Clustering for Robust 3D Object Recognition",
                        "Citation Paper Abstract": "Abstract:It is possible to associate a highly constrained subset of relative 6 DoF poses between two 3D shapes, as long as the local surface orientation, the normal vector, is available at every surface point. Local shape features can be used to find putative point correspondences between the models due to their ability to handle noisy and incomplete data. However, this correspondence set is usually contaminated by outliers in practical scenarios, which has led to many past contributions based on robust detectors such as the Hough transform or RANSAC. The key insight of our work is that a single correspondence between oriented points on the two models is constrained to cast votes in a 1 DoF rotational subgroup of the full group of poses, SE(3). Kernel density estimation allows combining the set of votes efficiently to determine a full 6 DoF candidate pose between the models. This modal pose with the highest density is stable under challenging conditions, such as noise, clutter, and occlusions, and provides the output estimate of our method.\nWe first analyze the robustness of our method in relation to noise and show that it handles high outlier rates much better than RANSAC for the task of 6 DoF pose estimation. We then apply our method to four state of the art data sets for 3D object recognition that contain occluded and cluttered scenes. Our method achieves perfect recall on two LIDAR data sets and outperforms competing methods on two RGB-D data sets, thus setting a new standard for general 3D object recognition using point cloud data.",
                        "Citation Paper Authors": "Authors:Anders Glent Buch, Lilita Kiforenko, Dirk Kraft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.06730v2": {
            "Paper Title": "VirtualCube: An Immersive 3D Video Communication System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02270v2": {
            "Paper Title": "Accessible Color Sequences for Data Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13094v2": {
            "Paper Title": "Path Guiding Using Spatio-Directional Mixture Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.03515v2": {
            "Paper Title": "Reconstructing Recognizable 3D Face Shapes based on 3D Morphable Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.08210v1": {
            "Paper Title": "Comparative Study of Cloud and Non-Cloud Gaming Platform: Apercu",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.14934v2": {
            "Paper Title": "Generative Adversarial Networks with Conditional Neural Movement\n  Primitives for An Interactive Generative Drawing Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09509v3": {
            "Paper Title": "Resurrect3D: An Open and Customizable Platform for Visualizing and\n  Analyzing Cultural Heritage Artifacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06904v3": {
            "Paper Title": "HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair\n  Performance Capture",
            "Sentences": [
                {
                    "Sentence ID": 75,
                    "Sentence": "further leverages depth as\nan extra source of supervision. STaR ",
                    "Citation Text": "Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven\nLovegrove. Star: Self-supervised tracking and reconstruc-\ntion of rigid objects in motion with neural rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 13144\u201313152, 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.01602",
                        "Citation Paper Title": "Title:STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering",
                        "Citation Paper Abstract": "Abstract:We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.",
                        "Citation Paper Authors": "Authors:Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "learns a grid of discrete\ncolor and density values via volumetric raymarching. Neu-\nral body ",
                    "Citation Text": "Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 9054\u20139063, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.15838",
                        "Citation Paper Title": "Title:Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans",
                        "Citation Paper Abstract": "Abstract:This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at this https URL.",
                        "Citation Paper Authors": "Authors:Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.09629v1": {
            "Paper Title": "Scalar Spatiotemporal Blue Noise Masks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.00946v2": {
            "Paper Title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07599v1": {
            "Paper Title": "Learning to Deblur and Rotate Motion-Blurred Faces",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "performed face alignment to the input of the net-\nwork. ",
                    "Citation Text": "Grigorios G. Chrysos, Paolo Favaro, and Stefanos Zafeiriou. Motion deblurring of\nfaces. International Journal of Computer Vision , 127(6), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.03330",
                        "Citation Paper Title": "Title:Motion deblurring of faces",
                        "Citation Paper Abstract": "Abstract:Face analysis is a core part of computer vision, in which remarkable progress has been observed in the past decades. Current methods achieve recognition and tracking with invariance to fundamental modes of variation such as illumination, 3D pose, expressions. Notwithstanding, a much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. Therefore, we propose a data-driven method that encourages identity preservation. The proposed model includes two parallel streams (sub-networks): the first deblurs the image, the second implicitly extracts and projects the identity of both the sharp and the blurred image in similar subspaces. We devise a method for creating realistic motion blur by averaging a variable number of frames to train our model. The averaged images originate from a 2MF2 dataset with 10 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we utilize the deblurred outputs to conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification. The experimental evaluation demonstrates the superiority of our method.",
                        "Citation Paper Authors": "Authors:Grigorios G. Chrysos, Paolo Favaro, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.06705v1": {
            "Paper Title": "N-SfC: Robust and Fast Shape Estimation from Caustic Images",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "de\ufb01ne caustic design as a\ntwo step process by \ufb01rst solving an optimal transport problem andthen calculating a height \ufb01eld achieving said transport. Meyron et\nal. ",
                    "Citation Text": "Jocelyn Meyron, Quentin M \u00b4erigot, and Boris Thibert. Light\nin power: a general and parameter-free algorithm for caustic\ndesign. ACM Trans. Graph. , 37(6):1\u201313, Dec. 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04820",
                        "Citation Paper Title": "Title:Light in Power: A General and Parameter-free Algorithm for Caustic Design",
                        "Citation Paper Abstract": "Abstract:We present in this paper a generic and parameter-free algorithm to efficiently build a wide variety of optical components, such as mirrors or lenses, that satisfy some light energy constraints. In all of our problems, one is given a collimated or point light source and a desired illumination after reflection or refraction and the goal is to design the geometry of a mirror or lens which transports exactly the light emitted by the source onto the target. We first propose a general framework and show that eight different optical component design problems amount to solving a light energy conservation equation that involves the computation of visibility diagrams. We then show that these diagrams all have the same structure and can be obtained by intersecting a 3D Power diagram with a planar or spherical domain. This allows us to propose an efficient and fully generic algorithm capable to solve these eight optical component design problems. The support of the prescribed target illumination can be a set of directions or a set of points located at a finite distance. Our solutions satisfy design constraints such as convexity or concavity. We show the effectiveness of our algorithm on simulated and fabricated examples.",
                        "Citation Paper Authors": "Authors:Jocelyn Meyron, Quentin M\u00e9rigot, Boris Thibert"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05793v1": {
            "Paper Title": "The 3D Motorcycle Complex for Structured Volume Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05219v1": {
            "Paper Title": "CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". In order to\nproject CLIP extracted labeled directions, we simply embed\nthe corresponding negative and positive example images in\nX+andX\u0000in the StyleGAN W+space using the pSp ",
                    "Citation Text": "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding\nin style: a stylegan encoder for image-to-image translation.\narXiv preprint arXiv:2008.00951 , 2020. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.00951",
                        "Citation Paper Title": "Title:Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
                        "Citation Paper Abstract": "Abstract:We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.",
                        "Citation Paper Authors": "Authors:Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", extracts non-\nlinear paths in the latent space to enable sequential image\nediting. In the area of text-based image editing, Style-\nCLIP ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery, 2021. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "uses the CLIP embedding vectors to adjust the\nlatent codes of a GAN. Another CLIP based framework,\n2StyleGAN-NADA ",
                    "Citation Text": "Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. Stylegan-nada: Clip-guided domain adap-\ntation of image generators, 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00946",
                        "Citation Paper Title": "Title:StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators",
                        "Citation Paper Abstract": "Abstract:Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",
                        "Citation Paper Authors": "Authors:Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", constructs a riggable 3D\nmodel on top of StyleGAN. StyleFlow ",
                    "Citation Text": "Rameen Abdal, Peihao Zhu, Niloy J. Mitra, and Peter\nWonka. Style\ufb02ow: Attribute-conditioned exploration of\nstylegan-generated images using conditional continuous nor-\nmalizing \ufb02ows. ACM Trans. Graph. , 40(3), May 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02401",
                        "Citation Paper Title": "Title:StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows",
                        "Citation Paper Abstract": "Abstract:High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.",
                        "Citation Paper Authors": "Authors:Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "architecture for learning task-agnostic joint\nrepresentations of image content and natural language. VL-\nBERT ",
                    "Citation Text": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.08530",
                        "Citation Paper Title": "Title:VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "was an early work that showed how language could im-\nprove a model. Such methods combine visual and lan-guage models to perform many interesting downstream\ntasks such as visual question answering and image caption-\ning. ICMLM ",
                    "Citation Text": "Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus.\nLearning visual representations with caption annotations,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01392",
                        "Citation Paper Title": "Title:Learning Visual Representations with Caption Annotations",
                        "Citation Paper Abstract": "Abstract:Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: this https URL.",
                        "Citation Paper Authors": "Authors:Mert Bulent Sariyildiz, Julien Perez, Diane Larlus"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "because it is considered state of the art for face\nimage generation and because as humans we are especially\ntuned to understand and describe images of faces. Note that\nStyleGAN3 ",
                    "Citation Text": "Tero Karras, Miika Aittala, Samuli Laine, Erik H \u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.12423",
                        "Citation Paper Title": "Title:Alias-Free Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
                        "Citation Paper Authors": "Authors:Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "High-Quality GANs. Two state of the art architectures\nfor GANs are BigGAN ",
                    "Citation Text": "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high \ufb01delity natural image synthe-\nsis. In International Conference on Learning Representa-\ntions , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11096",
                        "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                        "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                        "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05131v1": {
            "Paper Title": "Plenoxels: Radiance Fields without Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "restructures the coordinate-\nbased MLP to compute ray integrals exactly, for more than\n10\u0002faster rendering with a small loss in quality. Learned\nInitializations ",
                    "Citation Text": "Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\nSchmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren\nNg. Learned initializations for optimizing coordinate-based\nneural representations, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02189",
                        "Citation Paper Title": "Title:Learned Initializations for Optimizing Coordinate-Based Neural Representations",
                        "Citation Paper Abstract": "Abstract:Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.",
                        "Citation Paper Authors": "Authors:Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "reimplementation of NeRF offers a speedup for\nboth training and rendering via parallelization across many\nGPUs or TPUs. AutoInt ",
                    "Citation Text": "David B. Lindell, Julien N. P. Martel, and Gordon Wetzstein.\nAutoint: Automatic integration for fast neural volume render-\ning, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.01714",
                        "Citation Paper Title": "Title:AutoInt: Automatic Integration for Fast Neural Volume Rendering",
                        "Citation Paper Abstract": "Abstract:Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",
                        "Citation Paper Authors": "Authors:David B. Lindell, Julien N. P. Martel, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.14735v4": {
            "Paper Title": "Neural Relighting and Expression Transfer On Video Portraits",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "take a single image portrait as input to predict OLAT(one-\nlight-at-a-time) as Re\ufb02ectance Fields, which can be relit to\nother lighting via image-based rendering. Sun et al. ",
                    "Citation Text": "Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang\nXu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay\nBusch, Paul E Debevec, and Ravi Ramamoorthi. Single\nimage portrait relighting. ACM Trans. Graph. , 38(4):79\u20131,\n2019. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00824",
                        "Citation Paper Title": "Title:Single Image Portrait Relighting",
                        "Citation Paper Abstract": "Abstract:Lighting plays a central role in conveying the essence and depth of the subject in a portrait photograph. Professional photographers will carefully control the lighting in their studio to manipulate the appearance of their subject, while consumer photographers are usually constrained to the illumination of their environment. Though prior works have explored techniques for relighting an image, their utility is usually limited due to requirements of specialized hardware, multiple images of the subject under controlled or known illuminations, or accurate models of geometry and reflectance. To this end, we present a system for portrait relighting: a neural network that takes as input a single RGB image of a portrait taken with a standard cellphone camera in an unconstrained environment, and from that image produces a relit image of that subject as though it were illuminated according to any provided environment map. Our method is trained on a small database of 18 individuals captured under different directional light sources in a controlled light stage setup consisting of a densely sampled sphere of lights. Our proposed technique produces quantitatively superior results on our dataset's validation set compared to prior works, and produces convincing qualitative relighting results on a dataset of hundreds of real-world cellphone portraits. Because our technique can produce a 640 $\\times$ 640 image in only 160 milliseconds, it may enable interactive user-facing photographic applications in the future.",
                        "Citation Paper Authors": "Authors:Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, Ravi Ramamoorthi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05775v2": {
            "Paper Title": "Fast and Explicit Neural View Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nBefore rendering our RGB \u03b1volume V, we apply a per-\nspective deformation (using intrinsic camera parameters)\non the viewing frustum using inverse warping and trilinearsampling ",
                    "Citation Text": "Max Jaderberg, Karen Simonyan, Andrew Zisserman, and\nKoray Kavukcuoglu. Spatial transformer networks. arXiv\npreprint arXiv:1506.02025 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02025",
                        "Citation Paper Title": "Title:Spatial Transformer Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
                        "Citation Paper Authors": "Authors:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". Another set of\napproaches focuses on learning a free-form 2D flow field\nthat takes pixels from a single ",
                    "Citation Text": "Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-\nlik, and Alexei A Efros. View synthesis by appearance flow.\nInEuropean conference on computer vision , pages 286\u2013301.\nSpringer, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03557",
                        "Citation Paper Title": "Title:View Synthesis by Appearance Flow",
                        "Citation Paper Abstract": "Abstract:We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows -- 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques.",
                        "Citation Paper Authors": "Authors:Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04165v1": {
            "Paper Title": "Shortest Paths in Graphs with Matrix-Valued Edges: Concepts, Algorithm\n  and Application to 3D Multi-Shape Analysis",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "present a method for synchronising graphs with matrices\ninSE(3), and Bernard et al. ",
                    "Citation Text": "Florian Bernard, Johan Thunberg, Peter Gemmar, Frank\nHertel, Andreas Husch, and Jorge Goncalves. A solution\nfor multi-alignment by transformation synchronisation. In\nCVPR , 2015. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.8546",
                        "Citation Paper Title": "Title:A Solution for Multi-Alignment by Transformation Synchronisation",
                        "Citation Paper Abstract": "Abstract:The alignment of a set of objects by means of transformations plays an important role in computer vision. Whilst the case for only two objects can be solved globally, when multiple objects are considered usually iterative methods are used. In practice the iterative methods perform well if the relative transformations between any pair of objects are free of noise. However, if only noisy relative transformations are available (e.g. due to missing data or wrong correspondences) the iterative methods may fail.\nBased on the observation that the underlying noise-free transformations can be retrieved from the null space of a matrix that can directly be obtained from pairwise alignments, this paper presents a novel method for the synchronisation of pairwise transformations such that they are transitively consistent.\nSimulations demonstrate that for noisy transformations, a large proportion of missing data and even for wrong correspondence assignments the method delivers encouraging results.",
                        "Citation Paper Authors": "Authors:Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas Husch, Jorge Goncalves"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "use skeleton graphs to compare 3D meshes us-\ning graph matching techniques. Wang et al. ",
                    "Citation Text": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single rgb images. In ECCV , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.01654",
                        "Citation Paper Title": "Title:Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images",
                        "Citation Paper Abstract": "Abstract:We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.03907v1": {
            "Paper Title": "Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance\n  Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02214v2": {
            "Paper Title": "Joint Audio-Text Model for Expressive Speech-Driven 3D Facial Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03517v1": {
            "Paper Title": "CG-NeRF: Conditional Generative Neural Radiance Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03221v1": {
            "Paper Title": "Text2Mesh: Text-Driven Neural Stylization for Meshes",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "Text-Driven Manipulation. Our work is similar in spirit\nto image manipulation techniques controlled through tex-\ntual descriptions embedded by CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ", here we consider both color and local geome-\ntry changes for the manipulation of style.\nNeural Priors and Neural Fields. A recent line of work\nleverages the inductive bias of neural networks for tasks\nsuch as image denoising ",
                    "Citation Text": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 9446\u20139454,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10925",
                        "Citation Paper Title": "Title:Deep Image Prior",
                        "Citation Paper Abstract": "Abstract:Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.\nApart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at this https URL .",
                        "Citation Paper Authors": "Authors:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "deforms a\ntemplate shape to a target one. The above methods rely on\n3D datasets, while other techniques use a single mesh ex-\nemplar for synthesizing geometric textures ",
                    "Citation Text": "Amir Hertz, Rana Hanocka, Raja Giryes, and Daniel Cohen-\nOr. Deep geometric texture synthesis. ACM Transactions on\nGraphics (TOG) , 39(4):108\u20131, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00074",
                        "Citation Paper Title": "Title:Deep Geometric Texture Synthesis",
                        "Citation Paper Abstract": "Abstract:Recently, deep generative adversarial networks for image generation have advanced rapidly; yet, only a small amount of research has focused on generative models for irregular structures, particularly meshes. Nonetheless, mesh generation and synthesis remains a fundamental topic in computer graphics. In this work, we propose a novel framework for synthesizing geometric textures. It learns geometric texture statistics from local neighborhoods (i.e., local triangular patches) of a single reference 3D model. It learns deep features on the faces of the input triangulation, which is used to subdivide and generate offsets across multiple scales, without parameterization of the reference or target mesh. Our network displaces mesh vertices in any direction (i.e., in the normal and tangential direction), enabling synthesis of geometric textures, which cannot be expressed by a simple 2D displacement map. Learning and synthesizing on local geometric patches enables a genus-oblivious framework, facilitating texture transfer between shapes of different genus.",
                        "Citation Paper Authors": "Authors:Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "stylize a 3D shape by adding geo-\nmetric detail (without color), and ALIGNet ",
                    "Citation Text": "Rana Hanocka, Noa Fish, Zhenhua Wang, Raja Giryes,\nShachar Fleishman, and Daniel Cohen-Or. Alignet: partial-\nshape agnostic alignment via unsupervised learning. ACM\nTransactions on Graphics (TOG) , 38(1):1, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08497",
                        "Citation Paper Title": "Title:ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning",
                        "Citation Paper Abstract": "Abstract:The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. These techniques generally do not incorporate prior knowledge about expected shape characteristics, which can help compensate for any misleading cues left by inaccuracies exhibited in the input shapes. We present an approach based on a deep neural network, leveraging shape datasets to learn a shape-aware prior for source-to-target alignment that is robust to shape incompleteness. In the absence of ground truth alignments for supervision, we train a network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision. Our network, called ALIGNet, is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We aim for the network to develop specialized expertise over the common characteristics of the shapes in each dataset, thereby achieving a higher-level understanding of the expected shape space to which a local approach would be oblivious. We constrain ALIGNet through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications. We demonstrate that ALIGNet learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show that our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.",
                        "Citation Paper Authors": "Authors:Rana Hanocka, Noa Fish, Zhenhua Wang, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "edits shape content with a\npart-aware low-frequency deformation and synthesizes col-\nors in a texture map, guided by a target mesh. Mesh Ren-\nderer ",
                    "Citation Text": "Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-\nral 3d mesh renderer. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 3907\u2013\n3916, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07566",
                        "Citation Paper Title": "Title:Neural 3D Mesh Renderer",
                        "Citation Paper Abstract": "Abstract:For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.",
                        "Citation Paper Authors": "Authors:Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "perform CLIP-guided image editing using a pre-trainedStyleGAN [26,27]. VQGAN-CLIP [8,9,45] leverage CLIP\nfor text-guided image generation. Concurrent work uses\nCLIP to \ufb01ne-tune a pre-trained StyleGAN ",
                    "Citation Text": "Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,\nand Daniel Cohen-Or. Stylegan-nada: Clip-guided do-\nmain adaptation of image generators. arXiv preprint\narXiv:2108.00946 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00946",
                        "Citation Paper Title": "Title:StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators",
                        "Citation Paper Abstract": "Abstract:Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",
                        "Citation Paper Authors": "Authors:Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ". CLIP learns a\njoint embedding space for images and text. StyleCLIP ",
                    "Citation Text": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.17249",
                        "Citation Paper Title": "Title:StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                        "Citation Paper Abstract": "Abstract:Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                        "Citation Paper Authors": "Authors:Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01983v2": {
            "Paper Title": "CoNeRF: Controllable Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "showed that one can edit\nNeRF models by modifying the shape and appearance en-\ncoding, but they require a curated dataset of objects viewed\nunder different views and colors. HyperNeRF ",
                    "Citation Text": "Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, So\ufb01en Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz. HyperNeRF: A Higher-\nDimensional Representation for Topologically Varying Neu-\nral Radiance Fields. ArXiv preprint , 2021. 1, 2, 3, 4, 5, 6, 7,\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.13228",
                        "Citation Paper Title": "Title:HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at this https URL.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ", or linear\nblend skinning weights [4, 10, 15, 27, 29, 38, 53, 54]. Some\ninitial attempts at learned disentangled of shape and poses\nhave also been made in A-SDF ",
                    "Citation Text": "Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\nNuno Vasconcelos, and Xiaolong Wang. A-SDF: Learn-\ning Disentangled Signed Distance Functions for Articulated\nShape Representation. In Int. Conf. on Comput. Vis. , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07645",
                        "Citation Paper Title": "Title:A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation",
                        "Citation Paper Abstract": "Abstract:Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.",
                        "Citation Paper Authors": "Authors:Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "model to generate interpretable pose param-\neters, and Neural Actor ",
                    "Citation Text": "Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt. Neural Actor:\nNeural Free-view Synthesis of Human Actors with Pose\nControl. In ACM SIGGRAPH Asia , 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.02019",
                        "Citation Paper Title": "Title:Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control",
                        "Citation Paper Abstract": "Abstract:We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "with face attribute predicted by pre-trained\nface tracking networks, such as Face2Face ",
                    "Citation Text": "Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-\ntian Theobalt, and Matthias Nie\u00dfner. Face2Face: Real-time\nFace Capture and Reenactment of RGB Videos. In Conf. on\nComput. Vis. Pattern Recognit. , 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.14808",
                        "Citation Paper Title": "Title:Face2Face: Real-time Face Capture and Reenactment of RGB Videos",
                        "Citation Paper Abstract": "Abstract:We present Face2Face, a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.",
                        "Citation Paper Authors": "Authors:Justus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00169v2": {
            "Paper Title": "3D Photo Stylization: Learning to Generate Stylized Novel Views from a\n  Single Image",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "Neural Style Transfer . Neural style transfer has received\nconsiderable attention. Image style transfer [12,13] renders\nthe content of one image in the style of another. Video style\ntransfer ",
                    "Citation Text": "Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.\nArtistic style transfer for videos and spherical images. IJCV ,\n2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04538",
                        "Citation Paper Title": "Title:Artistic style transfer for videos and spherical images",
                        "Citation Paper Abstract": "Abstract:Manually re-drawing an image in a certain artistic style takes a professional artist a long time. Doing this for a video sequence single-handedly is beyond imagination. We present two computational approaches that transfer the style from one image (for example, a painting) to a whole video sequence. In our first approach, we adapt to videos the original image style transfer technique by Gatys et al. based on energy minimization. We introduce new ways of initialization and new loss functions to generate consistent and stable stylized video sequences even in cases with large motion and strong occlusion. Our second approach formulates video stylization as a learning problem. We propose a deep network architecture and training procedures that allow us to stylize arbitrary-length videos in a consistent and stable way, and nearly in real time. We show that the proposed methods clearly outperform simpler baselines both qualitatively and quantitatively. Finally, we propose a way to adapt these approaches also to 360 degree images and videos as they emerge with recent virtual reality hardware.",
                        "Citation Paper Authors": "Authors:Manuel Ruder, Alexey Dosovitskiy, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02091v1": {
            "Paper Title": "Class-agnostic Reconstruction of Dynamic Objects from Videos",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "template.\nTo compute depth, we use Consistent Video Depth (CVD) ",
                    "Citation Text": "X. Luo, J. Huang, R. Szeliski, K. Matzen, and J. Kopf. Consistent video depth estimation. TOG , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.15021",
                        "Citation Paper Title": "Title:Consistent Video Depth Estimation",
                        "Citation Paper Abstract": "Abstract:We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.",
                        "Citation Paper Authors": "Authors:Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01504v1": {
            "Paper Title": "Neural Weight Step Video Compression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00958v1": {
            "Paper Title": "Hierarchical Neural Implicit Pose Network for Animation and Motion\n  Retargeting",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "occ.,\nskinning weights, posespose occ. N Y N Y\nSNARF ",
                    "Citation Text": "Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skin-\nning for animating non-rigid neural implicit shapes. arXiv\npreprint arXiv:2104.03953 , 2021. 3, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03953",
                        "Citation Paper Title": "Title:SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes",
                        "Citation Paper Abstract": "Abstract:Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.",
                        "Citation Paper Authors": "Authors:Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "for the con-\nsistent topology and the skinning weights, our approach is\ncompletely agnostic to both.\n4.2. Evaluation\nMetrics: We report the mean Intersection over Union\n(mIoU) and the Chamfer-L1 distance ",
                    "Citation Text": "Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 605\u2013613, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00603",
                        "Citation Paper Title": "Title:A Point Set Generation Network for 3D Object Reconstruction from a Single Image",
                        "Citation Paper Abstract": "Abstract:Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.",
                        "Citation Paper Authors": "Authors:Haoqiang Fan, Hao Su, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "rely on LBS within their model, inheriting the artifacts of\ntraditional skinning. This was also noted in the follow-up\nwork Neural-GIF ",
                    "Citation Text": "Garvita Tiwari, Nikolaos Sara\ufb01anos, Tony Tung, and Gerard\nPons-Moll. Neural-gif: Neural generalized implicit func-\ntions for animating people in clothing. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 11708\u201311718, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08807",
                        "Citation Paper Title": "Title:Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing",
                        "Citation Paper Abstract": "Abstract:We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: this https URL",
                        "Citation Paper Authors": "Authors:Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "oriented pt. clouds,\noff-srf. pts., GHUM enc.GHUM enc.SDF,\nsemanticsY N Y\u0003Y\nLEAP ",
                    "Citation Text": "Marko Mihajlovic, Yan Zhang, Michael J Black, and Siyu\nTang. Leap: Learning articulated occupancy of people. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 10461\u201310471, 2021. 3,\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.06849",
                        "Citation Paper Title": "Title:LEAP: Learning Articulated Occupancy of People",
                        "Citation Paper Abstract": "Abstract:Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.",
                        "Citation Paper Authors": "Authors:Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00724v1": {
            "Paper Title": "RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from\n  Sparse Inputs",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "14.22 0.683 0.251 0.193\nDensity Surface Reg. 14.71 0.687 0.247 0.184\nSparsity Reg. ",
                    "Citation Text": "Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance \ufb01elds for real-time view synthesis. In Proc. of the\nIEEE International Conf. on Computer Vision (ICCV) , 2021.\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14645",
                        "Citation Paper Title": "Title:Baking Neural Radiance Fields for Real-Time View Synthesis",
                        "Citation Paper Abstract": "Abstract:Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",
                        "Citation Paper Authors": "Authors:Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "11.07 0.617 0.309 0.268\nRay Density Entropy Reg. 13.93 0.680 0.254 0.198\nNormal Smooth. Reg. ",
                    "Citation Text": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProc. IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2020. 2, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.07372",
                        "Citation Paper Title": "Title:Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "use local CNN features ex-\ntracted from the input images, whereas MVSNeRF ",
                    "Citation Text": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance \ufb01eld reconstruction from multi-view stereo.\nInProc. of the IEEE International Conf. on Computer Vision\n(ICCV) , 2021. 1, 2, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15595",
                        "Citation Paper Title": "Title:MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo",
                        "Citation Paper Abstract": "Abstract:We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",
                        "Citation Paper Authors": "Authors:Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "have emerged as a powerful method for\nnovel-view synthesis due to its simplicity and state-of-the-\nart performance. In mip-NeRF ",
                    "Citation Text": "Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance \ufb01elds. In Proc. of the IEEE International Conf.\non Computer Vision (ICCV) , 2021. 2, 3, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00532v1": {
            "Paper Title": "FaceTuneGAN: Face Autoencoder for Convolutional Expression Transfer\n  Using Neural Generative Adversarial Networks",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ", with the added ability to decompose the input face\ninto identity and expression latent representations ",
                    "Citation Text": "Jiang, Z.H., Wu, Q., Chen, K., Zhang, J., 2019. Disentangled\nrepresentation learning for 3D face shape, in: Proceedings of the\nIEEEComputerSocietyConferenceonComputerVisionandPattern\nRecognition, pp. 11949\u201311958. URL: http://arxiv.org/abs/1902.\n09887, doi: 10.1109/CVPR.2019.01223 ,arXiv:1902.09887 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.09887",
                        "Citation Paper Title": "Title:Disentangled Representation Learning for 3D Face Shape",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a novel strategy to design disentangled 3D face shape representation. Specifically, a given 3D face shape is decomposed into identity part and expression part, which are both encoded and decoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.",
                        "Citation Paper Authors": "Authors:Zi-Hang Jiang, Qianyi Wu, Keyu Chen, Juyong Zhang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "to 3D faces [21, 54, 44]. Others map geometries to the\n2D domain and apply standard 2D convolution operators\n[46, 1, 38].\nIn 2018, Lim et al. introduced SpiralNet, a new convo-\nlution operator specialized for 3D meshes ",
                    "Citation Text": "Lim, I., Dielen, A., Campen, M., Kobbelt, L., 2019. A simple\napproach to intrinsic correspondence learning on unstructured 3D\nmeshes, in: Lecture Notes in Computer Science (including subseries\nLecture Notes in Arti\ufb01cial Intelligence and Lecture Notes in Bioin-\nformatics), pp. 349\u2013362. URL: http://arxiv.org/abs/1809.06664 ,\ndoi:10.1007/978-3-030-11015-4_26 ,arXiv:1809.06664 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.06664",
                        "Citation Paper Title": "Title:A Simple Approach to Intrinsic Correspondence Learning on Unstructured 3D Meshes",
                        "Citation Paper Abstract": "Abstract:The question of representation of 3D geometry is of vital importance when it comes to leveraging the recent advances in the field of machine learning for geometry processing tasks. For common unstructured surface meshes state-of-the-art methods rely on patch-based or mapping-based techniques that introduce resampling operations in order to encode neighborhood information in a structured and regular manner. We investigate whether such resampling can be avoided, and propose a simple and direct encoding approach. It does not only increase processing efficiency due to its simplicity - its direct nature also avoids any loss in data fidelity. To evaluate the proposed method, we perform a number of experiments in the challenging domain of intrinsic, non-rigid shape correspondence estimation. In comparisons to current methods we observe that our approach is able to achieve highly competitive results.",
                        "Citation Paper Authors": "Authors:Isaak Lim, Alexander Dielen, Marcel Campen, Leif Kobbelt"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "to model\nnon-linearvariationsof3Dfacegeometrywhiledecoupling\nOlivier et al.: Preprint submitted to Elsevier Page 2 of 13FaceTuneGAN: Face Autoencoder for Convolutional Expression Transfer Using Neural Generative Adversarial Networks\nidentityandexpressionfactors ",
                    "Citation Text": "Abrevaya, V.F., Boukhayma, A., Wuhrer, S., Boyer, E., 2019. A\ndecoupled 3D facial shape model by adversarial training, in: Pro-\nceedings of the IEEE International Conference on Computer Vision,\nIEEE. pp. 9418\u20139427. URL: https://hal.archives-ouvertes.fr/\nhal-02064711 , doi: 10.1109/ICCV.2019.00951 ,arXiv:1902.03619 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.03619",
                        "Citation Paper Title": "Title:A Decoupled 3D Facial Shape Model by Adversarial Training",
                        "Citation Paper Abstract": "Abstract:Data-driven generative 3D face models are used to compactly encode facial shape data into meaningful parametric representations. A desirable property of these models is their ability to effectively decouple natural sources of variation, in particular identity and expression. While factorized representations have been proposed for that purpose, they are still limited in the variability they can capture and may present modeling artifacts when applied to tasks such as expression transfer. In this work, we explore a new direction with Generative Adversarial Networks and show that they contribute to better face modeling performances, especially in decoupling natural factors, while also achieving more diverse samples. To train the model we introduce a novel architecture that combines a 3D generator with a 2D discriminator that leverages conventional CNNs, where the two components are bridged by a geometry mapping layer. We further present a training scheme, based on auxiliary classifiers, to explicitly disentangle identity and expression attributes. Through quantitative and qualitative results on standard face datasets, we illustrate the benefits of our model and demonstrate that it outperforms competing state of the art methods in terms of decoupling and diversity.",
                        "Citation Paper Authors": "Authors:Victoria Fernandez Abrevaya, Adnane Boukhayma, Stefanie Wuhrer, Edmond Boyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00180v1": {
            "Paper Title": "SpaceEdit: Learning a Unified Editing Space for Open-Domain Image\n  Editing",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Hence, the data domain that the style-\nGAN is pretrained on will limit the editing domain. Al-\nthough ",
                    "Citation Text": "Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan\nTran, Xiaogang Wang, and Hongsheng Li. Open-edit: Open-\ndomain image manipulation with open-vocabulary instruc-\ntions. In ECCV , 2020. 1, 3, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01576",
                        "Citation Paper Title": "Title:Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions",
                        "Citation Paper Abstract": "Abstract:We propose a novel algorithm, named Open-Edit, which is the first attempt on open-domain image manipulation with open-vocabulary instructions. It is a challenging task considering the large variation of image domains and the lack of training supervision. Our approach takes advantage of the unified visual-semantic embedding space pretrained on a general image-caption dataset, and manipulates the embedded visual features by applying text-guided vector arithmetic on the image feature maps. A structure-preserving image decoder then generates the manipulated images from the manipulated feature maps. We further propose an on-the-fly sample-specific optimization approach with cycle-consistency constraints to regularize the manipulated images and force them to preserve details of the source images. Our approach shows promising results in manipulating open-vocabulary color, texture, and high-level attributes for various scenarios of open-domain images.",
                        "Citation Paper Authors": "Authors:Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, Hongsheng Li"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ", disentangling of image content and style [15,22],\nor explicitly enforcing the image diversity with distance-\nbased loss term [24, 27]. However, the enforcement of di-\nversity deteriorates the image quality. Inspired by the recent\nmodulation approach ",
                    "Citation Text": "Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image comple-\ntion via co-modulated generative adversarial networks. arXiv\npreprint arXiv:2103.10428 , 2021. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10428",
                        "Citation Paper Title": "Title:Large Scale Image Completion via Co-Modulated Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". A lower value indicates\nhigher visual quality and diversity. LPIPS ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "to solve this problem, as formulated in Eq. (2)\nw;n= arg min\nw;nLLPIPS (Itgt;G(Iin;w;n)) +\u0015nLn(n);\n(2)\nwhere wandnare the inverted latent code and stochas-\ntic noise inputs to different layers of the decoder, respec-\ntively.LLPIPS is the LPIPS perceptual loss ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR , 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.15552v1": {
            "Paper Title": "NeuSample: Neural Sample Field for Efficient View Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "represents a scene with thousands of tiny\nMLPs, each of which is only responsible for a cell in the\nspace grid. SNeRG ",
                    "Citation Text": "Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance \ufb01elds for real-time view synthesis. In ICCV , 2021. 1,\n2, 3, 5, 6, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14645",
                        "Citation Paper Title": "Title:Baking Neural Radiance Fields for Real-Time View Synthesis",
                        "Citation Paper Abstract": "Abstract:Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",
                        "Citation Paper Authors": "Authors:Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ".\n*Inference cost for NeRF-ID includes the additional proposal network.\nTable 3. Comparison with state-of-the-art methods on Real Forward-Facing scenes.\nSRN ",
                    "Citation Text": "Vincent Sitzmann, Michael Zollhoefer, and Gordon Wet-\nzstein. Scene representation networks: Continuous 3d-\nstructure-aware neural scene representations. NeurIPS ,\n2019. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01618",
                        "Citation Paper Title": "Title:Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations",
                        "Citation Paper Abstract": "Abstract:Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "proposes to automatically integrate out-\nput colors for rendering with neural networks, which ap-\nproximates the integral along rays in piecewise sections.\nDONeRF ",
                    "Citation Text": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum , 2021. 2, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03231",
                        "Citation Paper Title": "Title:DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks",
                        "Citation Paper Abstract": "Abstract:The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",
                        "Citation Paper Authors": "Authors:Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "et al. adopt a similar methodology by pre-computing prop-\nerties/features with sparse voxel grid-like structures using\na learned NeRF. Directly querying these properties during\ninference can effectively save cost. NeX ",
                    "Citation Text": "Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In CVPR , 2021.\n2, 5, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.05606",
                        "Citation Paper Title": "Title:NeX: Real-time View Synthesis with Neural Basis Expansion",
                        "Citation Paper Abstract": "Abstract:We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\\times$ faster rendering time than the state of the art. For real-time demos, visit this https URL",
                        "Citation Paper Authors": "Authors:Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "allows for empty space skipping and\nearly ray termination by constructing an octree structure.\nKiloNeRF ",
                    "Citation Text": "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance \ufb01elds with\nthousands of tiny mlps. In ICCV , 2021. 1, 2, 3, 5, 9\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13744",
                        "Citation Paper Title": "Title:KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
                        "Citation Paper Abstract": "Abstract:NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
                        "Citation Paper Authors": "Authors:Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.00007v1": {
            "Paper Title": "Sound-Guided Semantic Image Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "uti-\nlize the latent space of the pre-trained StyleGAN and the\nprior knowledge from CLIP ",
                    "Citation Text": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.\nLearning transferable visual models from natural language\nsupervision. Image , 2:T2. 2, 3, 4, 5, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "as the backbone. Secondly,\nwe compare the zero-shot audio classi\ufb01cation accuracy with\nWav2clip ",
                    "Citation Text": "H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello.\nWav2clip: Learning robust audio representations from clip,\n2021. 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.11499",
                        "Citation Paper Title": "Title:Wav2CLIP: Learning Robust Audio Representations From CLIP",
                        "Citation Paper Abstract": "Abstract:We propose Wav2CLIP, a robust audio representation learning method by distilling from Contrastive Language-Image Pre-training (CLIP). We systematically evaluate Wav2CLIP on a variety of audio tasks including classification, retrieval, and generation, and show that Wav2CLIP can outperform several publicly available pre-trained audio representation algorithms. Wav2CLIP projects audio into a shared embedding space with images and text, which enables multimodal applications such as zero-shot classification, and cross-modal retrieval. Furthermore, Wav2CLIP needs just ~10% of the data to achieve competitive performance on downstream tasks compared with fully supervised models, and is more efficient to pre-train than competing methods as it does not require learning a visual model in concert with an auditory model. Finally, we demonstrate image generation from Wav2CLIP as qualitative assessment of the shared embedding space. Our code and model weights are open sourced and made available for further applications.",
                        "Citation Paper Authors": "Authors:Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "supervised by random initialization of weights as a base-\nline model, and AudioCLIP ",
                    "Citation Text": "A. Guzhov, F. Raue, J. Hees, and A. Dengel. Audioclip: Ex-\ntending clip to image, text and audio, 2021. 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.13043",
                        "Citation Paper Title": "Title:AudioCLIP: Extending CLIP to Image, Text and Audio",
                        "Citation Paper Abstract": "Abstract:In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models.\nIn this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion.\nAudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets (68.78% and 69.40%, respectively).\nFinally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.",
                        "Citation Paper Authors": "Authors:Andrey Guzhov, Federico Raue, J\u00f6rn Hees, Andreas Dengel"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "and a neural music visualizer by map-\nping music embeddings to visual embeddings from Style-\nGAN ",
                    "Citation Text": "D. Jeong, S. Doh, and T. Kwon. Tr \u007faumerai: Dreaming music\nwith stylegan. arXiv preprint arXiv:2102.04680 , 2021. 2, 3,\n5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.04680",
                        "Citation Paper Title": "Title:Tr\u00e4umerAI: Dreaming Music with StyleGAN",
                        "Citation Paper Abstract": "Abstract:The goal of this paper to generate a visually appealing video that responds to music with a neural network so that each frame of the video reflects the musical characteristics of the corresponding audio clip. To achieve the goal, we propose a neural music visualizer directly mapping deep music embeddings to style embeddings of StyleGAN, named Tr\u00e4umerAI, which consists of a music auto-tagging model using short-chunk CNN and StyleGAN2 pre-trained on WikiArt dataset. Rather than establishing an objective metric between musical and visual semantics, we manually labeled the pairs in a subjective manner. An annotator listened to 100 music clips of 10 seconds long and selected an image that suits the music among the 200 StyleGAN-generated examples. Based on the collected data, we trained a simple transfer function that converts an audio embedding to a style embedding. The generated examples show that the mapping between audio and video makes a certain level of intra-segment similarity and inter-segment dissimilarity.",
                        "Citation Paper Authors": "Authors:Dasaem Jeong, Seungheon Doh, Taegyun Kwon"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "in a\nsupplemental document. By subtracting the vectors of the\nlatent code guided by each modality and the source latent\ncode, we show the distribution of manipulating direction.\nWe select the attributes in VGG-Sound ",
                    "Citation Text": "H. Chen, W. Xie, A. Vedaldi, and A. Zisserman. Vggsound:\nA large-scale audio-visual dataset. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pages 721\u2013725. IEEE, 2020.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.14368",
                        "Citation Paper Title": "Title:VGGSound: A Large-scale Audio-Visual Dataset",
                        "Citation Paper Abstract": "Abstract:Our goal is to collect a large-scale audio-visual dataset with low label noise from videos in the wild using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 210k videos for 310 audio classes. Third, we investigate various Convolutional Neural Network~(CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at this http URL",
                        "Citation Paper Authors": "Authors:Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.14825v1": {
            "Paper Title": "Latent Transformations via NeuralODEs for GAN-based Image Editing",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". This is a standard bench-\nmark for image editing as it contains a rich variation in\nage, ethnicity, lighting, and background.\n\u2022Places365 consists of 1,803,460training images with\n400+ unique scene categories ",
                    "Citation Text": "Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence , 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02055",
                        "Citation Paper Title": "Title:Places: An Image Database for Deep Scene Understanding",
                        "Citation Paper Abstract": "Abstract:The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.",
                        "Citation Paper Authors": "Authors:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, Aude Oliva"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "learns a set of directions that can be easily distin-\nguished by a separate classification model based on two\nsamples, produced from the original latent codes and shifted\nalong the particular direction. ",
                    "Citation Text": "William Peebles, John Peebles, Jun-Yan Zhu, Alexei A.\nEfros, and Antonio Torralba. The hessian penalty: A weak\nprior for unsupervised disentanglement. In Proceedings of\nEuropean Conference on Computer Vision (ECCV) , 2020. 1,\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10599",
                        "Citation Paper Title": "Title:The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement",
                        "Citation Paper Abstract": "Abstract:Existing disentanglement methods for deep generative models rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization term that encourages the Hessian of a generative model with respect to its input to be diagonal. We introduce a model-agnostic, unbiased stochastic approximation of this term based on Hutchinson's estimator to compute it efficiently during training. Our method can be applied to a wide range of deep generators with just a few lines of code. We show that training with the Hessian Penalty often causes axis-aligned disentanglement to emerge in latent space when applied to ProGAN on several datasets. Additionally, we use our regularization term to identify interpretable directions in BigGAN's latent space in an unsupervised fashion. Finally, we provide empirical evidence that the Hessian Penalty encourages substantial shrinkage when applied to over-parameterized latent spaces.",
                        "Citation Paper Authors": "Authors:William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio Torralba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.14643v1": {
            "Paper Title": "Urban Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "perform novel\nview synthesis and semantic segmentation from RGB-D\nvideos of the ScanNet dataset ",
                    "Citation Text": "Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. CVPR , 2017.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04405",
                        "Citation Paper Title": "Title:ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at this http URL.",
                        "Citation Paper Authors": "Authors:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "investigates the\nparameterization of unbounded scenes. Other works have\naddressed short video inputs ",
                    "Citation Text": "Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video.\nCVPR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.06468",
                        "Citation Paper Title": "Title:Dynamic View Synthesis from Dynamic Monocular Video",
                        "Citation Paper Abstract": "Abstract:We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.",
                        "Citation Paper Authors": "Authors:Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "proposed a scale-aware scene representation\nbased on conical frustrums instead of rays to compensate\nfor blurring and aliasing artifacts. NSVF ",
                    "Citation Text": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Sua, and\nChristian Theobalt. Neural Sparse V oxel Fields. In Adv.\nNeural Inform. Process. Syst. , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", and provide only a sampled, partial representation\nfrom which it is dif\ufb01cult to render high quality novel views.\nTraditional surface reconstruction methods aggregate the raw\ndata into explicit 3D scene representations, such as textured\nmeshes ",
                    "Citation Text": "Andrea Romanoni, Daniele Fiorenti, and Matteo Matteucci.\nMesh-based 3d textured urban mapping. IROS , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.05543",
                        "Citation Paper Title": "Title:Mesh-based 3D Textured Urban Mapping",
                        "Citation Paper Abstract": "Abstract:In the era of autonomous driving, urban mapping represents a core step to let vehicles interact with the urban context. Successful mapping algorithms have been proposed in the last decade building the map leveraging on data from a single sensor. The focus of the system presented in this paper is twofold: the joint estimation of a 3D map from lidar data and images, based on a 3D mesh, and its texturing. Indeed, even if most surveying vehicles for mapping are endowed by cameras and lidar, existing mapping algorithms usually rely on either images or lidar data; moreover both image-based and lidar-based systems often represent the map as a point cloud, while a continuous textured mesh representation would be useful for visualization and navigation purposes. In the proposed framework, we join the accuracy of the 3D lidar data, and the dense information and appearance carried by the images, in estimating a visibility consistent map upon the lidar measurements, and refining it photometrically through the acquired images. We evaluate the proposed framework against the KITTI dataset and we show the performance improvement with respect to two state of the art urban mapping algorithms, and two widely used surface reconstruction algorithms in Computer Graphics.",
                        "Citation Paper Authors": "Authors:Andrea Romanoni, Daniele Fiorenti, Matteo Matteucci"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13679v1": {
            "Paper Title": "NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw\n  Images",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ", though they typically cannot afford to employ\ndeep networks due to speed and power limitations.\nAnother line of research investigated whether denoisers\ncould be trained using only noisy data when no correspond-\ning clean ground truth exists. Noise2Noise ",
                    "Citation Text": "Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli\nLaine, Tero Karras, Miika Aittala, and Timo Aila.\nNoise2noise: Learning image restoration without clean data.\nICML , 2018. 3, 4, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04189",
                        "Citation Paper Title": "Title:Noise2Noise: Learning Image Restoration without Clean Data",
                        "Citation Paper Abstract": "Abstract:We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.",
                        "Citation Paper Authors": "Authors:Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "LDR inputs, either recovering\nor hallucinating detail in clipped highlights.\nSynthetic defocus Many modern cellphones include a\npostprocessing option to add synthetic defocus blur after\ncapture ",
                    "Citation Text": "Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E.\nFeldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-\nAttias, Jonathan T. Barron, Yael Pritch, and Marc Levoy.\nSynthetic depth-of-\ufb01eld with a single-camera mobile phone.\nACM Trans. Graph. , 2018. 3, 8, 17",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04171",
                        "Citation Paper Title": "Title:Synthetic Depth-of-Field with a Single-Camera Mobile Phone",
                        "Citation Paper Abstract": "Abstract:Shallow depth-of-field is commonly used by photographers to isolate a subject from a distracting background. However, standard cell phone cameras cannot produce such images optically, as their short focal lengths and small apertures capture nearly all-in-focus images. We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and a single button press. If the image is of a person, we use a person segmentation network to separate the person and their accessories from the background. If available, we also use dense dual-pixel auto-focus hardware, effectively a 2-sample light field with an approximately 1 millimeter baseline, to compute a dense depth map. These two signals are combined and used to render a defocused image. Our system can process a 5.4 megapixel image in 4 seconds on a mobile phone, is fully automatic, and is robust enough to be used by non-experts. The modular nature of our system allows it to degrade naturally in the absence of a dual-pixel sensor or a human subject.",
                        "Citation Paper Authors": "Authors:Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T. Barron, Yael Pritch, Marc Levoy"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", who used a stack of aligned LDR images\ntaken at different exposures to recover and invert the cam-\nera\u2019s nonlinear response curve. Current approaches apply\nmachine learning to produce HDR outputs from single ",
                    "Citation Text": "Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa\u0142\nMantiuk, and Jonas Unger. Hdr image reconstruction from\na single exposure using deep cnns. ACM Transactions on\nGraphics (TOG) , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07480",
                        "Citation Paper Title": "Title:HDR image reconstruction from a single exposure using deep CNNs",
                        "Citation Paper Abstract": "Abstract:Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods.",
                        "Citation Paper Authors": "Authors:Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa\u0142 K. Mantiuk, Jonas Unger"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "demon-\nstrated this was possible given a dataset of pairs of indepen-\ndent noisy observations of the same image, an insight Ehret\net al. ",
                    "Citation Text": "Thibaud Ehret, Axel Davy, Jean-Michel Morel, Gabriele\nFacciolo, and Pablo Arias. Model-blind video denoising via\nframe-to-frame training. CVPR , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12766",
                        "Citation Paper Title": "Title:Model-blind Video Denoising Via Frame-to-frame Training",
                        "Citation Paper Abstract": "Abstract:Modeling the processing chain that has produced a video is a difficult reverse engineering task, even when the camera is available. This makes model based video processing a still more complex task. In this paper we propose a fully blind video denoising method, with two versions off-line and on-line. This is achieved by fine-tuning a pre-trained AWGN denoising network to the video with a novel frame-to-frame training strategy. Our denoiser can be used without knowledge of the origin of the video or burst and the post processing steps applied from the camera sensor. The on-line process only requires a couple of frames before achieving visually-pleasing results for a wide range of perturbations. It nonetheless reaches state of the art performance for standard Gaussian noise, and can be used off-line with still better performance.",
                        "Citation Paper Authors": "Authors:Thibaud Ehret, Axel Davy, Jean-Michel Morel, Gabriele Facciolo, Pablo Arias"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "or added supervision with\ndepth [24, 47, 48], time-of-\ufb02ight data ",
                    "Citation Text": "Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil\nKim, Christian Richardt, James Tompkin, and Matthew\nO\u2019Toole. T \u00a8orf: Time-of-\ufb02ight radiance \ufb01elds for dynamic\nscene view synthesis, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.15271",
                        "Citation Paper Title": "Title:T\u00f6RF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis",
                        "Citation Paper Abstract": "Abstract:Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.",
                        "Citation Paper Authors": "Authors:Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O'Toole"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "directly op-\ntimizes a neural volumetric scene representation to match\nall input images using gradient descent on a rendering loss.\nVarious extensions have improved NeRF\u2019s robustness to\nvarying lighting conditions ",
                    "Citation Text": "Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,\nJonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. NeRF in the Wild: Neural Radiance Fields for Un-\nconstrained Photo Collections. CVPR , 2021. 2\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02268",
                        "Citation Paper Title": "Title:NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",
                        "Citation Paper Authors": "Authors:Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13674v1": {
            "Paper Title": "Neural Fields as Learnable Kernels for 3D Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "0.060 0.876\nOurs 0.032 0.873\nTable 4: Scene-level 3D reconstruction from sparse point\nclouds on ScanNet ",
                    "Citation Text": "Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-\nannotated 3d reconstructions of indoor scenes. In Proc. Com-\nputer Vision and Pattern Recognition (CVPR), IEEE , 2017.\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04405",
                        "Citation Paper Title": "Title:ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at this http URL.",
                        "Citation Paper Authors": "Authors:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "meta-learns a network which\ncan be rapidly trained to predict SDFs. Neural Kernel Fields\ncan also be viewed as a form of meta-learning since they\npredict a kernel machine from data. Shape as Points ",
                    "Citation Text": "Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer,\nMarc Pollefeys, Andreas Geiger, et al. Shape as points: A dif-\nferentiable poisson solver. arXiv preprint arXiv:2106.03452 ,\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03452",
                        "Citation Paper Title": "Title:Shape As Points: A Differentiable Poisson Solver",
                        "Citation Paper Abstract": "Abstract:In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference time and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.",
                        "Citation Paper Authors": "Authors:Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "proposes a convolutional\narchitecture that maps 3D points to features. We use a simi-\nlar feature network for our Neural Kernel Field architecture.\nLIG ",
                    "Citation Text": "Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\nand Thomas Funkhouser. Local deep implicit functions for\n3d shape. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 4857\u20134866,\n2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06126",
                        "Citation Paper Title": "Title:Local Deep Implicit Functions for 3D Shape",
                        "Citation Paper Abstract": "Abstract:The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",
                        "Citation Paper Authors": "Authors:Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.13545v1": {
            "Paper Title": "$\u03bc$NCA: Texture Generation with Ultra-Compact Neural Cellular Automata",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "on using pre-trained convolutional net-\nworks as feature-extractors to differentiably train a generator net-\nwork, d)(approx. 65k params) Houdard et al\u2019s work ",
                    "Citation Text": "Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, and\nJulien Rabin. A generative model for texture synthesis based\non optimal transport between feature distributions. June\n2020. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.03408",
                        "Citation Paper Title": "Title:A Generative Model for Texture Synthesis based on Optimal Transport between Feature Distributions",
                        "Citation Paper Abstract": "Abstract:We propose GOTEX, a general framework for texture synthesis by optimization that constrains the statistical distribution of local features. While our model encompasses several existing texture models, we focus on the case where the comparison between feature distributions relies on optimal transport distances. We show that the semi-dual formulation of optimal transport allows to control the distribution of various possible features, even if these features live in a high-dimensional space. We then study the resulting minimax optimization problem, which corresponds to a Wasserstein generative model, for which the inner concave maximization problem can be solved with standard stochastic gradient methods. The alternate optimization algorithm is shown to be versatile in terms of applications, features and architecture; in particular it allows to produce high-quality synthesized textures with different sets of features. We analyze the results obtained by constraining the distribution of patches or the distribution of responses to a pre-learned VGG neural network. We show that the patch representation can retrieve the desired textural aspect in a more precise manner. We also provide a detailed comparison with state-of-the-art texture synthesis methods. The GOTEX model based on patch features is also adapted to texture inpainting and texture interpolation. Finally, we show how to use our framework to learn a feed-forward neural network that can synthesize on-the-fly new textures of arbitrary size in a very fast manner. Experimental results and comparisons with the mainstream methods from the literature illustrate the relevance of the generative models learned with GOTEX.",
                        "Citation Paper Authors": "Authors:Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, Julien Rabin"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". The problem setting of com-\nputing divergence between two distributions, and differenti-\nating through such a divergence appears in many sub-\ufb01elds\nof machine learning ",
                    "Citation Text": "Derek Onken, Samy Wu Fung, Xingjian Li, and Lars\nRuthotto. OT-Flow: Fast and accurate continuous normal-\nizing \ufb02ows via optimal transport. May 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.00104",
                        "Citation Paper Title": "Title:OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport",
                        "Citation Paper Abstract": "Abstract:A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.",
                        "Citation Paper Authors": "Authors:Derek Onken, Samy Wu Fung, Xingjian Li, Lars Ruthotto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12905v1": {
            "Paper Title": "CIRCLE: Convolutional Implicit Reconstruction and Completion for\n  Large-scale Indoor Scene",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "using representations of either\nlocal implicit grid or a neural signed-distance volume. We\nfurther consider methods operating on fully-fused geome-\ntry,i.e., the convolutional occupancy network ",
                    "Citation Text": "Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger. Convolutional occupancy\nnetworks. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part III 16 , pages 523\u2013540. Springer, 2020. 1, 2, 5,\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.04618",
                        "Citation Paper Title": "Title:Convolutional Occupancy Networks",
                        "Citation Paper Abstract": "Abstract:Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",
                        "Citation Paper Authors": "Authors:Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "(denoted\nConvON) is the state-of-art local implicit network for sur-\nface reconstruction considering global information, while\nSPSG ",
                    "Citation Text": "Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin,\nand Matthias Nie\u00dfner. Spsg: Self-supervised photomet-\nric scene generation from rgb-d scans. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 1747\u20131756, 2021. 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.14660",
                        "Citation Paper Title": "Title:SPSG: Self-Supervised Photometric Scene Generation from RGB-D Scans",
                        "Citation Paper Abstract": "Abstract:We present SPSG, a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.",
                        "Citation Paper Authors": "Authors:Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", this dataset\ncontains 1788 + 394 (for training / validation and testing\nrespectively) scans of rooms from 90 buildings captured by\na Matterport Pro Camera. We follow the self-supervised\nsetting from ",
                    "Citation Text": "Angela Dai, Christian Diller, and Matthias Nie\u00dfner. Sg-nn:\nSparse generative neural networks for self-supervised scene\ncompletion of rgb-d scans. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 849\u2013858, 2020. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00036",
                        "Citation Paper Title": "Title:SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans",
                        "Citation Paper Abstract": "Abstract:We present a novel approach that converts partial and noisy RGB-D scans into high-quality 3D scene reconstructions by inferring unobserved scene geometry. Our approach is fully self-supervised and can hence be trained solely on real-world, incomplete scans. To achieve self-supervision, we remove frames from a given (incomplete) 3D scan in order to make it even more incomplete; self-supervision is then formulated by correlating the two levels of partialness of the same scan while masking out regions that have never been observed. Through generalization across a large training set, we can then predict 3D scene completion without ever seeing any 3D scan of entirely complete geometry. Combined with a new 3D sparse generative neural network architecture, our method is able to predict highly-detailed surfaces in a coarse-to-fine hierarchical fashion, generating 3D scenes at 2cm resolution, more than twice the resolution of existing state-of-the-art methods as well as outperforming them by a significant margin in reconstruction quality.",
                        "Citation Paper Authors": "Authors:Angela Dai, Christian Diller, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "for our convolution layer.\nFor the decoder branch, inspired by ",
                    "Citation Text": "Peng-Shuai Wang, Yang Liu, and Xin Tong. Deep octree-\nbased cnns with output-guided skip connections for 3d shape\nand scene completion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition Work-\nshops , pages 266\u2013267, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.03762",
                        "Citation Paper Title": "Title:Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape and Scene Completion",
                        "Citation Paper Abstract": "Abstract:Acquiring complete and clean 3D shape and scene data is challenging due to geometric occlusion and insufficient views during 3D capturing. We present a simple yet effective deep learning approach for completing the input noisy and incomplete shapes or scenes. Our network is built upon the octree-based CNNs (O-CNN) with U-Net like structures, which enjoys high computational and memory efficiency and supports to construct a very deep network structure for 3D CNNs. A novel output-guided skip-connection is introduced to the network structure for better preserving the input geometry and learning geometry prior from data effectively. We show that with these simple adaptions -- output-guided skip-connection and deeper O-CNN (up to 70 layers), our network achieves state-of-the-art results in 3D shape completion and semantic scene computation.",
                        "Citation Paper Authors": "Authors:Peng-Shuai Wang, Yang Liu, Xin Tong"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "motivates many studies to improve rendering ef\ufb01ciency\nand \ufb01tting speed, either through localized structures ",
                    "Citation Text": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel \ufb01elds. NeurIPS ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11571",
                        "Citation Paper Title": "Title:Neural Sparse Voxel Fields",
                        "Citation Paper Abstract": "Abstract:Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: this https URL.",
                        "Citation Paper Authors": "Authors:Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "casts the problem\nin terms of panoramic image completion but important ge-\nometric details are signi\ufb01cantly missing. ",
                    "Citation Text": "Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed,\nJ\u00a8urgen Sturm, and Matthias Nie\u00dfner. Scancomplete: Large-\nscale scene completion and semantic segmentation for 3d\nscans. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 4578\u20134587, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.10215",
                        "Citation Paper Title": "Title:ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans",
                        "Citation Paper Abstract": "Abstract:We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.",
                        "Citation Paper Authors": "Authors:Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, Matthias Nie\u00dfner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12480v1": {
            "Paper Title": "Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically\n  Structured Sequences",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "introduce\nconvolutions for this compression. However, these are only\nemployed on the encoding side and not in a generative con-\ntext. Closest to our approach come and van den Oord et\nal. ",
                    "Citation Text": "A \u00a8aron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural discrete representation learning.\nInNeurIPS , pages 6306\u20136315, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00937",
                        "Citation Paper Title": "Title:Neural Discrete Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "summarize blocks\nto reduce the sequence length and Wu et al. ",
                    "Citation Text": "Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers. CoRR , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15808",
                        "Citation Paper Title": "Title:CvT: Introducing Convolutions to Vision Transformers",
                        "Citation Paper Abstract": "Abstract:We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.11529v4": {
            "Paper Title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based\n  Recognition of Subtle Human Actions",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "comprises of 60action categories,\nperformed by 40subjects. Its extension NTURGB-D 120 ",
                    "Citation Text": "Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C.\nKot. 2019. NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity\nUnderstanding. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(2019). https://doi.org/10.1109/TPAMI.2019.2916873",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04757",
                        "Citation Paper Title": "Title:NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding",
                        "Citation Paper Abstract": "Abstract:Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding. [The dataset is available at: this http URL]",
                        "Citation Paper Authors": "Authors:Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C. Kot"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "is one\nof the largest and most diverse skeleton action dataset comprising\nof120actions performed by 106subjects from 155viewpoints.\nVarying-view RGB-D Action Dataset (VAD) ",
                    "Citation Text": "Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, and Wei-Shi Zheng.\n2019. A large-scale varying-view RGB-D action dataset for arbitrary-view human\naction recognition. arXiv preprint arXiv:1904.10681 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10681",
                        "Citation Paper Title": "Title:A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition",
                        "Citation Paper Abstract": "Abstract:Current researches of action recognition mainly focus on single-view and multi-view recognition, which can hardly satisfies the requirements of human-robot interaction (HRI) applications to recognize actions from arbitrary views. The lack of datasets also sets up barriers. To provide data for arbitrary-view action recognition, we newly collect a large-scale RGB-D action dataset for arbitrary-view action analysis, including RGB videos, depth and skeleton sequences. The dataset includes action samples captured in 8 fixed viewpoints and varying-view sequences which covers the entire 360 degree view angles. In total, 118 persons are invited to act 40 action categories, and 25,600 video samples are collected. Our dataset involves more participants, more viewpoints and a large number of samples. More importantly, it is the first dataset containing the entire 360 degree varying-view sequences. The dataset provides sufficient data for multi-view, cross-view and arbitrary-view action analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to tackle the problem of arbitrary-view action recognition. Experiment results show that the VS-CNN achieves superior performance.",
                        "Citation Paper Authors": "Authors:Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, Wei-Shi Zheng"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "was one of the first action recognition datasets which\nprovided depth and skeleton joint modalities, albeit from a sin-\ngle viewpoint. However, it only covered a limited set of gaming\nactions (e.g. forward punching, side boxing). The Northwestern-\nUCLA dataset ",
                    "Citation Text": "Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. 2014. Cross-\nview Action Modeling, Learning and Recognition. arXiv:1405.2941 [cs.CV]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.2941",
                        "Citation Paper Title": "Title:Cross-view Action Modeling, Learning and Recognition",
                        "Citation Paper Abstract": "Abstract:Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.",
                        "Citation Paper Authors": "Authors:Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "provides RGB videos along with 3D\nKinect skeleton data. We first extract the RGB frames from the\nvideos at the frame rate of 30 FPS. We estimate 3D poses from RGBframes using SMPL-X ",
                    "Citation Text": "Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed\nA. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body\nCapture: 3D Hands, Face, and Body from a Single Image. In Proceedings IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) .NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions ICVGIP\u201921, December 2021, Jodhpur, India",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05866",
                        "Citation Paper Title": "Title:Expressive Body Capture: 3D Hands, Face, and Body from a Single Image",
                        "Citation Paper Abstract": "Abstract:To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at this https URL.",
                        "Citation Paper Authors": "Authors:Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". However, the\nresulting skeleton dataset contains many invalid sequences ",
                    "Citation Text": "Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari, Neel\nTrivedi, Sourav Das, and Ravi Kiran Sarvadevabhatla. 2021. Quo Vadis, Skeleton\nAction Recognition? International Journal of Computer Vision (05 May 2021).\nhttps://doi.org/10.1007/s11263-021-01470-y",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02072",
                        "Citation Paper Title": "Title:Quo Vadis, Skeleton Action Recognition ?",
                        "Citation Paper Abstract": "Abstract:In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition.",
                        "Citation Paper Authors": "Authors:Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari, Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12018v1": {
            "Paper Title": "Distortion Reduction for Off-Center Perspective Projection of Panoramas",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11759v1": {
            "Paper Title": "ReGroup: Recursive Neural Networks for Hierarchical Grouping of Vector\n  Graphic Primitives",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "group elements as part\nof a generative model while Tagger ",
                    "Citation Text": "Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and J\u00fcrgen\nSchmidhuber. 2016. Tagger: Deep unsupervised perceptual grouping. In Advances\nin Neural Information Processing Systems . 4484\u20134492.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.06724",
                        "Citation Paper Title": "Title:Tagger: Deep Unsupervised Perceptual Grouping",
                        "Citation Paper Abstract": "Abstract:We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. By enriching the representations of a neural network, we enable it to group the representations of different objects in an iterative manner. By allowing the system to amortize the iterative inference of the groupings, we achieve very fast convergence. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. For multi-digit classification of very cluttered images that require texture segmentation, our method offers improved classification performance over convolutional networks despite being fully connected. Furthermore, we observe that our system greatly improves on the semi-supervised result of a baseline Ladder network on our dataset, indicating that segmentation can also improve sample efficiency.",
                        "Citation Paper Authors": "Authors:Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, J\u00fcrgen Schmidhuber, Harri Valpola"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.09244v1": {
            "Paper Title": "Evaluations of The Hierarchical Subspace Iteration Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08988v1": {
            "Paper Title": "LVAC: Learned Volumetric Attribute Compression for Point Clouds using\n  Coordinate Based Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03797v2": {
            "Paper Title": "Neural BRDFs: Representation and Operations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.06906v1": {
            "Paper Title": "Path Verification for Dynamic Indirect Illumination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.06517v1": {
            "Paper Title": "Neuromuscular Control of the Face-Head-Neck Biomechanical Complex With\n  Learning-Based Expression Transfer From Images and Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.08051v2": {
            "Paper Title": "A Light Stage on Every Desk",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "Dwhich\nclassi\ufb01es every 70\u000270 patches as real or fake. We use the\nLS-GAN ",
                    "Citation Text": "Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen\nWang, and Stephen Paul Smolley. Least squares genera-\ntive adversarial networks. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 2794\u20132802,\n2017. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.04076",
                        "Citation Paper Title": "Title:Least Squares Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",
                        "Citation Paper Authors": "Authors:Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.05800v1": {
            "Paper Title": "Arbitrary order principal directions and how to compute them",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05778v1": {
            "Paper Title": "Theoretical and empirical analysis of a fast algorithm for extracting\n  polygons from signed distance bounds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05562v1": {
            "Paper Title": "TomoSLAM: factor graph optimization for rotation angle refinement in\n  microtomography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04945v1": {
            "Paper Title": "PREMA: Part-based REcurrent Multi-view Aggregation Network for 3D Shape\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "View-based deep learning models.  Due to the success of deep learning in a large number of vision tasks, discriminative 3D deep representations have been extended to play an important role in view-based 3D recognition. Bai et al. ",
                    "Citation Text": "Song  Bai,   Xiang  Bai,   Zhichao  Zhou,   ZhaoxiangZhang, and Longin Jan Latecki. Gift: A real-time andscalable  3d  shape  search  engine.   InProceedings  ofthe IEEE conference on computer vision and patternrecognition, pages 5023\u2013503.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01879",
                        "Citation Paper Title": "Title:GIFT: A Real-time and Scalable 3D Shape Search Engine",
                        "Citation Paper Abstract": "Abstract:Projective analysis is an important solution for 3D shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different view points. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, thus cannot satisfy the basic requirement of scalability for search engines. In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: (1) efficient projection and view feature extraction using GPU acceleration; (2) the first inverted file, referred as F-IF, is utilized to speed up the procedure of multi-view matching; (3) the second inverted file (S-IF), which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based re-ranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and Inverted File Twice, as GIFT. Besides its high efficiency, GIFT also outperforms the state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks and competitions.",
                        "Citation Paper Authors": "Authors:Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "develop a method to train CNN to learn the corresponding relationship between views and view points in a weakly supervised manner, which can urge CNN to learning more spatial information. He et al. ",
                    "Citation Text": "Xinwei  He,  Tengteng  Huang,  Song  Bai,  and  XiangBai.  View n-gram network for 3d object retrieval.  InProceedings of the IEEE International Conference onComputer Vision, pages 7515\u2013752.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01958",
                        "Citation Paper Title": "Title:View N-gram Network for 3D Object Retrieval",
                        "Citation Paper Abstract": "Abstract:How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.",
                        "Citation Paper Authors": "Authors:Xinwei He, Tengteng Huang, Song Bai, Xiang Bai"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", using the full dataset for classification and 80/20 split for retrieval. Results for retrieval. We compare PREMA with state-of-the-art methods, such as MVCNN, TCL ",
                    "Citation Text": "Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, andXiang Bai.  Triplet-center loss for multi-view 3d ob-ject retrieval.  InProceedings of the IEEE Conferenceon  Computer  Vision  and  Pattern  Recognition,  pages1945\u20131954, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06189",
                        "Citation Paper Title": "Title:Triplet-Center Loss for Multi-View 3D Object Retrieval",
                        "Citation Paper Abstract": "Abstract:Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then, we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared with the state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, Xiang Bai"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "presented a real-time and unified 3D recognition framework, where single view feature is firstly extracted by a pre-trained CNN and an off-line re-ranking technique is proposed to merge complementary information of different level CNN features. Su et al. ",
                    "Citation Text": "Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutionalneural networks for 3d shape recognition. InProceed-ings of the IEEE international conference on computervision, pages 945\u2013955, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00880",
                        "Citation Paper Title": "Title:Multi-view Convolutional Neural Networks for 3D Shape Recognition",
                        "Citation Paper Abstract": "Abstract:A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
                        "Citation Paper Authors": "Authors:Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "propose a view-group-shape framework (GVCNN) to explore the content correlation among views for efficiently feature aggregation. Kanezaki et al. ",
                    "Citation Text": "Asako Kanezaki, Yasuyuki Matsushita, and YoshifumiNishida.  Rotationnet:  Joint object categorization andpose estimation using multiviews from unsupervisedviewpoints.   InProceedings of the IEEE Conferenceon  Computer  Vision  and  Pattern  Recognition,  pages5010\u20135019, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.06208",
                        "Citation Paper Title": "Title:RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints",
                        "Citation Paper Abstract": "Abstract:We propose a Convolutional Neural Network (CNN)-based model \"RotationNet,\" which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. The code is available on this https URL",
                        "Citation Paper Authors": "Authors:Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "proposed a multi-view convolutional network (MVCNN), which used a set of CNN to extract each view\u2019s deep representation and then aggregated multiple view features with the element-wise maximum operation. Following that, Johns et al. ",
                    "Citation Text": "Edward  Johns,  Stefan  Leutenegger,  and  Andrew  JDavison. Pairwise decomposition of image sequencesfor active multi-view recognition.   InProceedings ofthe IEEE Conference on Computer Vision and PatternRecognition, pages 3813\u20133822, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08359",
                        "Citation Paper Title": "Title:Pairwise Decomposition of Image Sequences for Active Multi-View Recognition",
                        "Citation Paper Abstract": "Abstract:A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.",
                        "Citation Paper Authors": "Authors:Edward Johns, Stefan Leutenegger, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "develop a sophisticated self-attention mechanism to capture long-range dependency in view sequence. The proposed non-local block can be inserted into backbone network and improve the performance for several tasks. Cao et al. ",
                    "Citation Text": "Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, andHan Hu.   Gcnet:  Non-local networks meet squeeze-excitation networks and beyond.  In2019 IEEE/CVFInternational Conference on Computer Vision Work-shops, ICCV Workshops 2019, Seoul, Korea (South),October 27-28, 2019, pages 1971\u20131980.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11492",
                        "Citation Paper Title": "Title:GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond",
                        "Citation Paper Abstract": "Abstract:The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at this https URL.",
                        "Citation Paper Authors": "Authors:Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.04212v2": {
            "Paper Title": "Dense Representative Tooth Landmark/axis Detection Network on 3D Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04396v1": {
            "Paper Title": "Content-aware media retargeting based on deep importance map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03452v2": {
            "Paper Title": "Shape As Points: A Differentiable Poisson Solver",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07991v3": {
            "Paper Title": "ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and\n  Tactile Representations",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "for details and a comparison of the simulated and real tactile readings. Particu-\nlarly, we use DIGIT ",
                    "Citation Text": "M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V . R. Most, D. Stroud, R. Santos,\nA. Byagowi, G. Kammerer, et al. Digit: A novel design for a low-cost compact high-resolution\ntactile sensor with application to in-hand manipulation. RA-L , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14679",
                        "Citation Paper Title": "Title:DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation",
                        "Citation Paper Abstract": "Abstract:Despite decades of research, general purpose in-hand manipulation remains one of the unsolved challenges of robotics. One of the contributing factors that limit current robotic manipulation systems is the difficulty of precisely sensing contact forces -- sensing and reasoning about contact forces are crucial to accurately control interactions with the environment. As a step towards enabling better robotic manipulation, we introduce DIGIT, an inexpensive, compact, and high-resolution tactile sensor geared towards in-hand manipulation. DIGIT improves upon past vision-based tactile sensors by miniaturizing the form factor to be mountable on multi-fingered hands, and by providing several design improvements that result in an easier, more repeatable manufacturing process, and enhanced reliability. We demonstrate the capabilities of the DIGIT sensor by training deep neural network model-based controllers to manipulate glass marbles in-hand with a multi-finger robotic hand. To provide the robotic community access to reliable and low-cost tactile sensors, we open-source the DIGIT design at this https URL.",
                        "Citation Paper Authors": "Authors:Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, Dinesh Jayaraman, Roberto Calandra"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "; bene\ufb01t robotic grasping [42, 43, 44]; augment vision to enhance 3D shape recon-\nstruction ",
                    "Citation Text": "E. J. Smith, R. Calandra, A. Romero, G. Gkioxari, D. Meger, J. Malik, and M. Drozdzal. 3d\nshape reconstruction from vision and touch. In NeurIPS , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.09584",
                        "Citation Paper Title": "Title:Active 3D Shape Reconstruction from Vision and Touch",
                        "Citation Paper Abstract": "Abstract:Humans build 3D understandings of the world through active object exploration, using jointly their senses of vision and touch. However, in 3D shape reconstruction, most recent progress has relied on static datasets of limited sensory data such as RGB images, depth maps or haptic readings, leaving the active exploration of the shape largely unexplored. Inactive touch sensing for 3D reconstruction, the goal is to actively select the tactile readings that maximize the improvement in shape reconstruction accuracy. However, the development of deep learning-based active touch models is largely limited by the lack of frameworks for shape exploration. In this paper, we focus on this problem and introduce a system composed of: 1) a haptic simulator leveraging high spatial resolution vision-based tactile sensors for active touching of 3D objects; 2)a mesh-based 3D shape reconstruction model that relies on tactile or visuotactile signals; and 3) a set of data-driven solutions with either tactile or visuotactile priors to guide the shape exploration. Our framework enables the development of the first fully data-driven solutions to active touch on top of learned models for object understanding. Our experiments show the benefits of such solutions in the task of 3D shape understanding where our models consistently outperform natural baselines. We provide our framework as a tool to foster future research in this direction.",
                        "Citation Paper Authors": "Authors:Edward J. Smith, David Meger, Luis Pineda, Roberto Calandra, Jitendra Malik, Adriana Romero, Michal Drozdzal"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". Tactile sensing\nis more local compared to vision and audio, and it is shown to improve sample ef\ufb01ciency on a peg\ninsertion task ",
                    "Citation Text": "M. A. Lee, Y . Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg.\nMaking sense of vision and touch: Self-supervised learning of multimodal representations for\ncontact-rich tasks. In ICRA , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.10191",
                        "Citation Paper Title": "Title:Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks",
                        "Citation Paper Abstract": "Abstract:Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.",
                        "Citation Paper Authors": "Authors:Michelle A. Lee, Yuke Zhu, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "; and\nbridge the simulation-to-reality gap for tasks that involve stochastic dynamics ",
                    "Citation Text": "C. Matl, Y . Narang, D. Fox, R. Bajcsy, and F. Ramos. Stressd: Sim-to-real from sound for\nstochastic dynamics. In CoRL , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03136",
                        "Citation Paper Title": "Title:STReSSD: Sim-To-Real from Sound for Stochastic Dynamics",
                        "Citation Paper Abstract": "Abstract:Sound is an information-rich medium that captures dynamic physical events. This work presents STReSSD, a framework that uses sound to bridge the simulation-to-reality gap for stochastic dynamics, demonstrated for the canonical case of a bouncing ball. A physically-motivated noise model is presented to capture stochastic behavior of the balls upon collision with the environment. A likelihood-free Bayesian inference framework is used to infer the parameters of the noise model, as well as a material property called the coefficient of restitution, from audio observations. The same inference framework and the calibrated stochastic simulator are then used to learn a probabilistic model of ball dynamics. The predictive capabilities of the dynamics model are tested in two robotic experiments. First, open-loop predictions anticipate probabilistic success of bouncing a ball into a cup. The second experiment integrates audio perception with a robotic arm to track and deflect a bouncing ball in real-time. We envision that this work is a step towards integrating audio-based inference for dynamic robotic tasks. Experimental results can be viewed at this https URL.",
                        "Citation Paper Authors": "Authors:Carolyn Matl, Yashraj Narang, Dieter Fox, Ruzena Bajcsy, Fabio Ramos"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "in robotic scooping and pouring tasks; study its synergies with the motion of a robot ",
                    "Citation Text": "D. Gandhi, A. Gupta, and L. Pinto. Swoosh! rattle! thump!\u2013actions that sound. In RSS, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.01851",
                        "Citation Paper Title": "Title:Swoosh! Rattle! Thump! -- Actions that Sound",
                        "Citation Paper Abstract": "Abstract:Truly intelligent agents need to capture the interplay of all their senses to build a rich physical understanding of their world. In robotics, we have seen tremendous progress in using visual and tactile perception; however, we have often ignored a key sense: sound. This is primarily due to the lack of data that captures the interplay of action and sound. In this work, we perform the first large-scale study of the interactions between sound and robotic action. To do this, we create the largest available sound-action-vision dataset with 15,000 interactions on 60 objects using our robotic platform Tilt-Bot. By tilting objects and allowing them to crash into the walls of a robotic tray, we collect rich four-channel audio information. Using this data, we explore the synergies between sound and action and present three key insights. First, sound is indicative of fine-grained object class information, e.g., sound can differentiate a metal screwdriver from a metal wrench. Second, sound also contains information about the causal effects of an action, i.e. given the sound produced, we can predict what action was applied to the object. Finally, object representations derived from audio embeddings are indicative of implicit physical properties. We demonstrate that on previously unseen objects, audio embeddings generated through interactions can predict forward models 24% better than passive visual embeddings. Project videos and data are at this https URL",
                        "Citation Paper Authors": "Authors:Dhiraj Gandhi, Abhinav Gupta, Lerrel Pinto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.04017v1": {
            "Paper Title": "Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online\n  Adaptation",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "is a widely used parametric model for 3D human\nmesh reconstruction, which is also used in this work. Early\nmethods [ 18,29,30,31,32,33] gradually \ufb01t a standard T-Pose\nSMPL model to an input image based on the silhouettes ",
                    "Citation Text": "C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, and\nP. V . Gehler, \u201cUnite the people: Closing the loop between 3D\nand 2D human representations,\u201d in CVPR , 2017, pp. 6050\u2013\n6059.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.02468",
                        "Citation Paper Title": "Title:Unite the People: Closing the Loop Between 3D and 2D Human Representations",
                        "Citation Paper Abstract": "Abstract:3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits \"in-the- wild\". However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes.",
                        "Citation Paper Authors": "Authors:Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03643v1": {
            "Paper Title": "TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "factorizes NeRF into two MLPs and\nspeeds up the rendering process via caching. ",
                    "Citation Text": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance \ufb01elds. In arXiv , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14024",
                        "Citation Paper Title": "Title:PlenOctrees for Real-time Rendering of Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "decomposes the\nspace into a number of regions and uses a smaller MLP for\nlearning the appearance of each region, which signi\ufb01cantly\nreduces the rendering time. KiloNeRF ",
                    "Citation Text": "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance \ufb01elds with\nthousands of tiny mlps. CoRR , abs/2103.13744, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13744",
                        "Citation Paper Title": "Title:KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
                        "Citation Paper Abstract": "Abstract:NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
                        "Citation Paper Authors": "Authors:Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "utilizes depth supervision for\nmore ef\ufb01cient sample placement during the training. DON-\neRF ",
                    "Citation Text": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum , 40, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03231",
                        "Citation Paper Title": "Title:DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks",
                        "Citation Paper Abstract": "Abstract:The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",
                        "Citation Paper Authors": "Authors:Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "takes this idea to\nthe extreme and uses a very large collection of many small\nMLPs. FastNeRF ",
                    "Citation Text": "Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-\ufb01delity neural\nrendering at 200fps. arXiv preprint arXiv:2103.10380 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10380",
                        "Citation Paper Title": "Title:FastNeRF: High-Fidelity Neural Rendering at 200FPS",
                        "Citation Paper Abstract": "Abstract:Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
                        "Citation Paper Authors": "Authors:Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". Other works have\nlooked at how the scene can be decomposed into compo-\nnents such as re\ufb02ectance and incoming radiance so that the\nscene can be re-lit with new lighting.\nPixelNeRF ",
                    "Citation Text": "Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance \ufb01elds from one or few images.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 4578\u20134587, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02190",
                        "Citation Paper Title": "Title:pixelNeRF: Neural Radiance Fields from One or Few Images",
                        "Citation Paper Abstract": "Abstract:We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: this https URL",
                        "Citation Paper Authors": "Authors:Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". These approaches are generally trained\nusing a sampled 3D point cloud of the shape as supervi-\nsion. DVR ",
                    "Citation Text": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProc. IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.07372",
                        "Citation Paper Title": "Title:Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision",
                        "Citation Paper Abstract": "Abstract:Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",
                        "Citation Paper Authors": "Authors:Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nThe \ufb01rst approaches in this direction were focused on\nrepresenting geometric shapes as distance \ufb01elds and include\nmethods such as DeepSDF ",
                    "Citation Text": "Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nInThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , June 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03545v1": {
            "Paper Title": "ActFloor-GAN: Activity-Guided Adversarial Networks for Human-Centric\n  Floorplan Design",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "by default\ncan only generate one single \ufb02oorplan for an input buildingboundary. To support the generation of diverse \ufb02oorplans,\nthey incorporate a probability distribution of predicted\nrooms, which, however, may not produce optimal \ufb02oor-\nplans. Similar to our approach, Graph2Plan ",
                    "Citation Text": "R. Hu, Z. Huang, Y. Tang, O. van Kaick, H. Zhang, and H. Huang.\nGraph2plan: Learning \ufb02oorplan generation from layout graphs.\nACM Transactions on Graphics , 39(4):Article 118, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13204",
                        "Citation Paper Title": "Title:Graph2Plan: Learning Floorplan Generation from Layout Graphs",
                        "Citation Paper Abstract": "Abstract:We introduce a learning framework for automated floorplan generation which combines generative modeling using deep neural networks and user-in-the-loop designs to enable human users to provide sparse design constraints. Such constraints are represented by a layout graph. The core component of our learning framework is a deep neural network, Graph2Plan, which converts a layout graph, along with a building boundary, into a floorplan that fulfills both the layout and boundary constraints. Given an input building boundary, we allow a user to specify room counts and other layout constraints, which are used to retrieve a set of floorplans, with their associated layout graphs, from a database. For each retrieved layout graph, along with the input boundary, Graph2Plan first generates a corresponding raster floorplan image, and then a refined set of boxes representing the rooms. Graph2Plan is trained on RPLAN, a large-scale dataset consisting of 80K annotated floorplans. The network is mainly based on convolutional processing over both the layout graph, via a graph neural network (GNN), and the input building boundary, as well as the raster floorplan images, via conventional image convolution.",
                        "Citation Paper Authors": "Authors:Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver van Kaick, Hao Zhang, Hui Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03384v1": {
            "Paper Title": "Seamless Satellite-image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.02703v1": {
            "Paper Title": "Towards Smart Monitored AM: Open Source in-Situ Layer-wise 3D Printing\n  Image Anomaly Detection Using Histograms of Oriented Gradients and a\n  Physics-Based Rendering Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12057v1": {
            "Paper Title": "Field-Based Toolpath Generation for 3D Printing Continuous Fibre\n  Reinforced Thermoplastic Composites",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01455v1": {
            "Paper Title": "Learning a perceptual manifold with deep features for animation video\n  resequencing",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nIn the proposed method, we use the activations of deep convolutional neural networks for feature extraction and a metric\ninspired by the Learned Perceptual Image Patch Similarity (LPIPS) metric proposed by ",
                    "Citation Text": "R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as\na perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n586\u2013595, 2018.\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03657v1": {
            "Paper Title": "Holodeck: Immersive 3D Displays Using Swarms of Flying Light Specks",
            "Sentences": [
                {
                    "Sentence ID": 93,
                    "Sentence": ".\n5.1 Wave-based sound propagation\nThis section presents a wave-based propagation technique [ 66,77]\nfor immersive FLS displays. Subsequently, it describes the role of\nmachine learning ",
                    "Citation Text": "Zhenyu Tang, Hsien-Yu Meng, and Dinesh Manocha. 2021. Learning Acoustic\nScattering Fields for Highly Dynamic Interactive Sound Propagation. In IEEE\nVirtual Reality and 3D User Interfaces (VR) . IEEE, 826\u2013836.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04865",
                        "Citation Paper Title": "Title:Learning Acoustic Scattering Fields for Dynamic Interactive Sound Propagation",
                        "Citation Paper Abstract": "Abstract:We present a novel hybrid sound propagation algorithm for interactive applications. Our approach is designed for dynamic scenes and uses a neural network-based learned scattered field representation along with ray tracing to generate specular, diffuse, diffraction, and occlusion effects efficiently. We use geometric deep learning to approximate the acoustic scattering field using spherical harmonics. We use a large 3D dataset for training, and compare its accuracy with the ground truth generated using an accurate wave-based solver. The additional overhead of computing the learned scattered field at runtime is small and we demonstrate its interactive performance by generating plausible sound effects in dynamic scenes with diffraction and occlusion effects. We demonstrate the perceptual benefits of our approach based on an audio-visual user study.",
                        "Citation Paper Authors": "Authors:Zhenyu Tang, Hsien-Yu Meng, Dinesh Manocha"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01067v1": {
            "Paper Title": "OctField: Hierarchical Implicit Functions for 3D Modeling",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ".\n4.5 Shape Completion\n(a) Input\n (b) IF-Net\n (c) O-CNN\n (d) Ours\n (e) GT\nFigure 8: Shape Completion and Comparison with IF-\nNet ",
                    "Citation Text": "Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d\nshape reconstruction and completion. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) . IEEE, jun 2020. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.01456",
                        "Citation Paper Title": "Title:Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion",
                        "Citation Paper Abstract": "Abstract:While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",
                        "Citation Paper Authors": "Authors:Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "proposes a\nauto-decoder structure to train latent space and decoder without using a traditional shape encoder.\nRecently, the local implicit methods [27, 4] combine regular space decomposition with local implicit\ngenerators for modeling 3D scenes with \ufb01ne geometric details. ACORN ",
                    "Citation Text": "Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wet-\nzstein. Acorn: Adaptive coordinate networks for neural scene representation. ACM Transactions on\nGraphics (TOG) , 40(4), 2021. 3, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.02788",
                        "Citation Paper Title": "Title:ACORN: Adaptive Coordinate Networks for Neural Scene Representation",
                        "Citation Paper Abstract": "Abstract:Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",
                        "Citation Paper Authors": "Authors:Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "for quantitative evaluation and comparison with prior methods.\n4.2 Shape Reconstruction\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\n (g)\n (h)\nFigure 4: Shape reconstruction comparison with\nthe baseline methods ((a) Input, (b) AOCNN ",
                    "Citation Text": "Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong. Adaptive o-cnn: A patch-based deep repre-\nsentation of 3d shapes. ACM Transactions on Graphics (TOG) , 37(6):1\u201311, 2018. 3, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.07917",
                        "Citation Paper Title": "Title:Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes",
                        "Citation Paper Abstract": "Abstract:We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding. Different from volumetric-based or octree-based CNN methods that represent a 3D shape with voxels in the same resolution, our method represents a 3D shape adaptively with octants at different levels and models the 3D shape within each octant with a planar patch. Based on this adaptive patch-based representation, we propose an Adaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The Adaptive O-CNN encoder takes the planar patch normal and displacement as input and performs 3D convolutions only at the octants at each level, while the Adaptive O-CNN decoder infers the shape occupancy and subdivision status of octants at each level and estimates the best plane normal and displacement for each leaf octant. As a general framework for 3D shape analysis and generation, the Adaptive O-CNN not only reduces the memory and computational cost, but also offers better shape generation capability than the existing 3D-CNN approaches. We validate Adaptive O-CNN in terms of efficiency and effectiveness on different shape analysis and generation tasks, including shape classification, 3D autoencoding, shape prediction from a single image, and shape completion for noisy and incomplete point clouds.",
                        "Citation Paper Authors": "Authors:Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, Xin Tong"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ": chair, table, airplane, car, and sofa. For fair comparison, we use the\nof\ufb01cially released training and testing data splits. All the shapes are normalized to \ufb01t a unit sphere\nand converted into watertight meshes ",
                    "Citation Text": "Jingwei Huang, Yichao Zhou, and Leonidas Guibas. Manifoldplus: A robust and scalable watertight\nmanifold surface generation method for triangle soups. arXiv preprint arXiv:2005.11621 , 2020. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.11621",
                        "Citation Paper Title": "Title:ManifoldPlus: A Robust and Scalable Watertight Manifold Surface Generation Method for Triangle Soups",
                        "Citation Paper Abstract": "Abstract:We present ManifoldPlus, a method for robust and scalable conversion of triangle soups to watertight manifolds. While many algorithms in computer graphics require the input mesh to be a watertight manifold, in practice many meshes designed by artists are often for visualization purposes, and thus have non-manifold structures such as incorrect connectivity, ambiguous face orientation, double surfaces, open boundaries, self-intersections, etc. Existing methods suffer from problems in the inputs with face orientation and zero-volume structures. Additionally most methods do not scale to meshes of high complexity. In this paper, we propose a method that extracts exterior faces between occupied voxels and empty voxels, and uses a projection-based optimization method to accurately recover a watertight manifold that resembles the reference mesh. Compared to previous methods, our methodology is simpler. It does not rely on face normals of the input triangle soups and can accurately recover zero-volume structures. Our algorithm is scalable, because it employs an adaptive Gauss-Seidel method for shape optimization, in which each step is an easy-to-solve convex problem. We test ManifoldPlus on ModelNet10 and AccuCity datasets to verify that our methods can generate watertight meshes ranging from object-level shapes to city-level models. Furthermore, through our experimental evaluations, we show that our method is more robust, efficient and accurate than the state-of-the-art. Our implementation is publicly available.",
                        "Citation Paper Authors": "Authors:Jingwei Huang, Yichao Zhou, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "has experimented with both V AE\nand GAN models to learn stronger shape priors. In a concurrent work, DeepSDF ",
                    "Citation Text": "Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF:\nLearning continuous signed distance functions for shape representation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pages 165\u2013174, 2019. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05103",
                        "Citation Paper Title": "Title:DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
                        "Citation Paper Abstract": "Abstract:Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
                        "Citation Paper Authors": "Authors:Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": ", have shown promising ability of synthesizing realistic images in 2D domain. 3D learning\napproaches have strived to duplicate the success of 2D generative models into 3D shape generation.\n3D-GAN ",
                    "Citation Text": "Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic\nlatent space of object shapes via 3D generative-adversarial modeling. In NIPS , pages 82\u201390, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.07584",
                        "Citation Paper Title": "Title:Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
                        "Citation Paper Abstract": "Abstract:We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
                        "Citation Paper Authors": "Authors:Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01048v1": {
            "Paper Title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02909v2": {
            "Paper Title": "Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "pro-\nposed Point2Mesh, which reconstructs an entire surface\nfrom an input point cloud. Unlike DGP, Point2Mesh uses\nMeshCNN ",
                    "Citation Text": "Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar\nFleishman, and Daniel Cohen-Or. Meshcnn: A network with\nan edge. ACM Transactions on Graphics , 38(4), 2019. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.05910",
                        "Citation Paper Title": "Title:MeshCNN: A Network with an Edge",
                        "Citation Paper Abstract": "Abstract:Polygonal meshes provide an efficient representation for 3D shapes. They explicitly capture both shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of our task-driven pooling on various learning tasks applied to 3D meshes.",
                        "Citation Paper Authors": "Authors:Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "that corrects noisy surface normals using a deep\nneural network and recovers a denoised mesh using the sur-\nface normals. For completion, Wang et al. ",
                    "Citation Text": "Peng-Shuai Wang, Yang Liu, and Xin Tong. Deep octree-\nbased cnns with output-guided skip connections for 3d shape\nand scene completion. In Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n266\u2013267, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.03762",
                        "Citation Paper Title": "Title:Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape and Scene Completion",
                        "Citation Paper Abstract": "Abstract:Acquiring complete and clean 3D shape and scene data is challenging due to geometric occlusion and insufficient views during 3D capturing. We present a simple yet effective deep learning approach for completing the input noisy and incomplete shapes or scenes. Our network is built upon the octree-based CNNs (O-CNN) with U-Net like structures, which enjoys high computational and memory efficiency and supports to construct a very deep network structure for 3D CNNs. A novel output-guided skip-connection is introduced to the network structure for better preserving the input geometry and learning geometry prior from data effectively. We show that with these simple adaptions -- output-guided skip-connection and deeper O-CNN (up to 70 layers), our network achieves state-of-the-art results in 3D shape completion and semantic scene computation.",
                        "Citation Paper Authors": "Authors:Peng-Shuai Wang, Yang Liu, Xin Tong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00699v1": {
            "Paper Title": "Principles towards Real-Time Simulation of Material Point Method on\n  Modern GPUs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00231v1": {
            "Paper Title": "Two Heads are Better than One: Geometric-Latent Attention for Point\n  Cloud Classification and Segmentation",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "addresses this issue by sampling\nthe points, grouping them in clusters, and applying PointNet on the clusters. SO-Net ",
                    "Citation Text": "Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point\ncloud analysis. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 9397\u20139406, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04249",
                        "Citation Paper Title": "Title:SO-Net: Self-Organizing Network for Point Cloud Analysis",
                        "Citation Paper Abstract": "Abstract:This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website. this https URL",
                        "Citation Paper Authors": "Authors:Jiaxin Li, Ben M. Chen, Gim Hee Lee"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "- 62.1 73.0 89.9 95.1 76.4 62.8 47.1 55.3 68.4 73.5 69.2 63.2 45.9 8.7 52.9\nRandLA-Net ",
                    "Citation Text": "Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki\nTrigoni, and Andrew Markham. Randla-net: Ef\ufb01cient semantic segmentation of large-\nscale point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 11108\u201311117, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.11236",
                        "Citation Paper Title": "Title:RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds",
                        "Citation Paper Abstract": "Abstract:We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.",
                        "Citation Paper Authors": "Authors:Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "62.7 68.9 94.6 98.5 80.9 0.0 19.1 60.1 48.9 80.6 88.0 53.2 68.4 68.2 54.9\nMinkowskiNet ",
                    "Citation Text": "Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets:\nMinkowski convolutional neural networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages 3075\u20133084, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08755",
                        "Citation Paper Title": "Title:4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, those 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors and propose the generalized sparse convolution that encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for high-dimensional convolutional neural networks. We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases.",
                        "Citation Paper Authors": "Authors:Christopher Choy, JunYoung Gwak, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "41.1 49.0 88.8 97.3 69.8 0.1 3.9 46.3 10.8 59.0 52.6 5.9 40.3 26.4 33.2\nSegCloud ",
                    "Citation Text": "Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese.\nSegcloud: Semantic segmentation of 3d point clouds. In 2017 international conference\non 3D vision (3DV) , pages 537\u2013547. IEEE, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07563",
                        "Citation Paper Title": "Title:SEGCloud: Semantic Segmentation of 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks (NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, this http URL), and show performance comparable or superior to the state-of-the-art on all datasets.",
                        "Citation Paper Authors": "Authors:Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "construct kernels based on the input coordinates to be used as convolution weights.\nProjection-based approaches. Some works project local neighborhoods into tangent planes\nand process them with 2D convolutions. The tangent plane parameters can be found using\npoint tangent estimation ",
                    "Citation Text": "Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convo-\nlutions for dense prediction in 3d. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pages 3887\u20133896, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.02443",
                        "Citation Paper Title": "Title:Tangent Convolutions for Dense Prediction in 3D",
                        "Citation Paper Abstract": "Abstract:We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes.",
                        "Citation Paper Authors": "Authors:Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "uses a similar hierarchical structure adding self-organizing maps (SOMs) to capture better\nlocal structures. Other approaches like PointConv ",
                    "Citation Text": "Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks\non 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 9621\u20139630, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.07246",
                        "Citation Paper Title": "Title:PointConv: Deep Convolutional Networks on 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and density functions through kernel density estimation. The most important contribution of this work is a novel reformulation proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.",
                        "Citation Paper Authors": "Authors:Wenxuan Wu, Zhongang Qi, Li Fuxin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00140v1": {
            "Paper Title": "DIB-R++: Learning to Predict Lighting and Material with a Hybrid\n  Differentiable Renderer",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ". Thanks to this powerful generative model, the distribution of GAN\nimages is similar to the distribution of real images, allowing our model to generalize well. We show\nreconstruction results on real images from LSUN ",
                    "Citation Text": "Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.03365",
                        "Citation Paper Title": "Title:LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop",
                        "Citation Paper Abstract": "Abstract:While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.",
                        "Citation Paper Authors": "Authors:Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "tackles unsupervised inverse rendering in the\ncontext of differentiable rendering. Zhang et al. ",
                    "Citation Text": "Yuxuan Zhang, Wenzheng Chen, Jun Gao, Huan Ling, Yinan Zhang, Antonio Torralba, and\nSanja Fidler. Image GANs meet differentiable rendering for inverse graphics and interpretable\n3D neural rendering. In International Conference on Learning Representations (ICLR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09125",
                        "Citation Paper Title": "Title:Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Differentiable rendering has paved the way to training neural networks to perform \"inverse graphics\" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \"neural renderer\", complementing traditional graphics renderers.",
                        "Citation Paper Authors": "Authors:Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". In our work, we propose a hybrid differentiable renderer and learn to disentangle complex\nspecular effects given a single image. Similar to the recent NeRD ",
                    "Citation Text": "Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik PA Lensch.\nNeRD: Neural re\ufb02ectance decomposition from image collections, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03918",
                        "Citation Paper Title": "Title:NeRD: Neural Reflectance Decomposition from Image Collections",
                        "Citation Paper Abstract": "Abstract:Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections. The datasets and code is available on the project page: this https URL",
                        "Citation Paper Authors": "Authors:Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P.A. Lensch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00943v1": {
            "Paper Title": "SVBRDF Recovery From a Single Image With Highlights using a Pretrained\n  Generative Adversarial Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06199v3": {
            "Paper Title": "A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape,\n  Appearance, and Pose",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": "This dataset is a standard benchmark for human pose estimation. It\nconsists of 4 indoor and 2 outdoor subjects with challenging human poses. The number of\nframes per subject range from 276 to 603.\n\u2022MonoPerfCap ",
                    "Citation Text": "W. Xu, A. Chatterjee, M. Zollh\u00f6fer, H. Rhodin, D. Mehta, H.-P. Seidel, and C. Theobalt.\nMonoperfcap: Human performance capture from monocular video. TOG , 37(2):27, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.02136",
                        "Citation Paper Title": "Title:MonoPerfCap: Human Performance Capture from Monocular Video",
                        "Citation Paper Abstract": "Abstract:We present the first marker-less approach for temporally coherent 3D performance capture of a human with general clothing from monocular video. Our approach reconstructs articulated human skeleton motion as well as medium-scale non-rigid surface deformations in general scenes. Human performance capture is a challenging problem due to the large range of articulation, potentially fast motion, and considerable non-rigid deformations, even from multi-view data. Reconstruction from monocular video alone is drastically more challenging, since strong occlusions and the inherent depth ambiguity lead to a highly ill-posed reconstruction problem. We tackle these challenges by a novel approach that employs sparse 2D and 3D human pose detections from a convolutional neural network using a batch-based pose estimation strategy. Joint recovery of per-batch motion allows to resolve the ambiguities of the monocular reconstruction problem based on a low dimensional trajectory subspace. In addition, we propose refinement of the surface geometry based on fully automatically extracted silhouettes to enable medium-scale non-rigid alignment. We demonstrate state-of-the-art performance capture results that enable exciting applications such as video editing and free viewpoint video, previously infeasible from monocular video. Our qualitative and quantitative evaluation demonstrates that our approach significantly outperforms previous monocular methods in terms of accuracy, robustness and scene complexity that can be handled.",
                        "Citation Paper Authors": "Authors:Weipeng Xu, Avishek Chatterjee, Michael Zollh\u00f6fer, Helge Rhodin, Dushyant Mehta, Hans-Peter Seidel, Christian Theobalt"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". Our test-time\npose re\ufb01nement improves consistently upon the SPIN baseline, with largest improvements for\nextremities (PA-Wrist).\nHuman 3.6M MPI-INF-3DHP\nProtocol I Protocol II Prot. II Wrist Prot. II Multi-view\nMethod PA-MPJPE #PA-MPJPE#PA-Wrist# PA-MPJPE# PA-MPJPE#PCK\"\nMotioNet ",
                    "Citation Text": "M. Shi, K. Aberman, A. Aristidou, T. Komura, D. Lischinski, D. Cohen-Or, and B. Chen.\nMotionet: 3d human motion reconstruction from monocular video with skeleton consistency.\nACM TOG (Proc. SIGGRAPH) , 40(1):1\u201315, 2020.\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.12075",
                        "Citation Paper Title": "Title:MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency",
                        "Citation Paper Abstract": "Abstract:We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video.While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data, rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.",
                        "Citation Paper Authors": "Authors:Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", and facial\nmodels [ 15,16]. Orthogonal to these works, our A-NeRF learns an articulated body model from\nestimated poses and uncalibrated cameras.\nClosely related to ours is the NASA surface body model ",
                    "Citation Text": "B. Deng, J. Lewis, T. Jeruzalski, G. Pons-Moll, G. Hinton, M. Norouzi, and A. Tagliasacchi.\nNasa: neural articulated shape approximation. arXiv preprint arXiv:1912.03207 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.03207",
                        "Citation Paper Title": "Title:NASA: Neural Articulated Shape Approximation",
                        "Citation Paper Abstract": "Abstract:Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.",
                        "Citation Paper Authors": "Authors:Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "that textures and geometrically\nre\ufb01nes an untextured parametric quadruped model to zebra images and to ",
                    "Citation Text": "D. Xiang, H. Joo, and Y . Sheikh. Monocular total capture: Posing face, body, and hands in the\nwild. In CVPR , pages 10965\u201310974, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.01598",
                        "Citation Paper Title": "Title:Monocular Total Capture: Posing Face, Body, and Hands in the Wild",
                        "Citation Paper Abstract": "Abstract:We present the first method to capture the 3D total motion of a target person from a monocular view input. Given an image or a monocular video, our method reconstructs the motion from body, face, and fingers represented by a 3D deformable mesh model. We use an efficient representation called 3D Part Orientation Fields (POFs), to encode the 3D orientations of all body parts in the common 2D image space. POFs are predicted by a Fully Convolutional Network (FCN), along with the joint confidence maps. To train our network, we collect a new 3D human motion dataset capturing diverse total body motion of 40 subjects in a multiview system. We leverage a 3D deformable human model to reconstruct total body pose from the CNN outputs by exploiting the pose and shape prior in the model. We also present a texture-based tracking method to obtain temporally coherent motion capture output. We perform thorough quantitative evaluations including comparison with the existing body-specific and hand-specific methods, and performance analysis on camera viewpoint and human pose changes. Finally, we demonstrate the results of our total body motion capture on various challenging in-the-wild videos. Our code and newly collected human motion dataset will be publicly shared.",
                        "Citation Paper Authors": "Authors:Donglai Xiang, Hanbyul Joo, Yaser Sheikh"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". It also\nenables optimization within the bounds of the learned prior [ 14,19,28] and weak-supervision when\nintegrated in a differentiable form ",
                    "Citation Text": "S. Liu, T. Li, W. Chen, and H. Li. Soft rasterizer: A differentiable renderer for image-based 3d\nreasoning. In ICCV , pages 7708\u20137717, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01786",
                        "Citation Paper Title": "Title:Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning",
                        "Citation Paper Abstract": "Abstract:Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers.",
                        "Citation Paper Authors": "Authors:Shichen Liu, Tianye Li, Weikai Chen, Hao Li"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "Our approach builds upon and is related to the following work on human pose and shape estimation,\nhuman modeling, and neural scene representations ",
                    "Citation Text": "A. Tewari, O. Fried, J. Thies, V . Sitzmann, S. Lombardi, K. Sunkavalli, R. Martin-Brualla,\nT. Simon, J. Saragih, M. Nie\u00dfner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y . Zhu, C. Theobalt,\nM. Agrawala, E. Shechtman, D. B. Goldman, and M. Zollh\u00f6fer. State of the Art on Neural\nRendering. Computer Graphics Forum (EG STAR 2020) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03805",
                        "Citation Paper Title": "Title:State of the Art on Neural Rendering",
                        "Citation Paper Abstract": "Abstract:Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
                        "Citation Paper Authors": "Authors:Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.01553v2": {
            "Paper Title": "Spline Positional Encoding for Learning 3D Implicit Signed Distance\n  Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14833v1": {
            "Paper Title": "Modeling 3D geometry using 1D laser distance measurements with\n  application to cylinder for visualization and evaluating surface quality",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "Hai Jin, Xun Wang, Zichun  Zhong, Jing Hu. Robust 3D face modeling and reconstruction from frontal and \nside images. Computer Aided Geometric Design 50: 1 -13, 2017. ",
                    "Citation Text": "Stephan R. Richter, Stefan Roth. Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers . \nComputer Vision and Pattern Recognition (CVPR 2018) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10975",
                        "Citation Paper Title": "Title:Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers",
                        "Citation Paper Abstract": "Abstract:In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Stefan Roth"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.14373v1": {
            "Paper Title": "Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "to enable photorealistic results\non novel view synthesis, well-exempli\ufb01ed by NeRF ",
                    "Citation Text": "Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,\nand Ren Ng. NeRF: Representing scenes as neural radiance \ufb01elds for view synthesis. In\nEuropean Conference on Computer Vision (ECCV) , 2020.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.14942v2": {
            "Paper Title": "Fast Training of Neural Lumigraph Representations using Meta Learning",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "use\nexplicit, implicit, or hybrid scene representations. Explicit representations, for example those using\nproxy geometry (see above), object-speci\ufb01c shape templates ",
                    "Citation Text": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning category-speci\ufb01c\nmesh reconstruction from image collections. In ECCV , pages 371\u2013386, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07549",
                        "Citation Paper Title": "Title:Learning Category-Specific Mesh Reconstruction from Image Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.13272v1": {
            "Paper Title": "Learning Neural Transmittance for Efficient Rendering of Reflectance\n  Fields",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "propose\nto use an implicit representation for the volume where an MLP is used to predict the volume\ndensity and radiance of an arbitrary point by taking its 3D location and view direction as\ninput. Bi et al. ",
                    "Citation Text": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance \ufb01elds for view\nsynthesis. In ECCV , 2020.6 M. SHAFIEI, S. BI, ET AL.: NEURAL TRANSMITTANCE FOR EFFICIENT RENDERING\n(a)\n (b)\n (c)\n(d)\n (e)\nFigure 4: We can relight the scene with our method (e) faster than NRF (b) and with less\nartifacts compared to NeRV visibility function (d). Our augmentation approach removes the\nartifacts caused by over\ufb01tting of neural transmittance on input images (c).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "precompute a transmittance volume at\nthe light by calculating the transmittance of each sampled point on the rays corresponding to\nthe pixels on the virtual image plane placed at the light.\nThe work concurrent to ours by Srinivasan et al. ",
                    "Citation Text": "Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall,\nand Jonathan T. Barron. Nerv: Neural re\ufb02ectance and visibility \ufb01elds for relighting and\nview synthesis. In arXiv , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03927",
                        "Citation Paper Title": "Title:NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",
                        "Citation Paper Authors": "Authors:Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "precompute a deep shadow map for ef\ufb01cient visibility lookup and\nhigh-quality shadows. Kallweit et al. ",
                    "Citation Text": "Simon Kallweit, Thomas M\u00fcller, Brian Mcwilliams, Markus Gross, and Jan Nov\u00e1k.\nDeep scattering: Rendering atmospheric clouds with radiance-predicting neural net-\nworks. ACM Transactions on Graphics (TOG) , 36(6):1\u201311, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.05418",
                        "Citation Paper Title": "Title:Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting Neural Networks",
                        "Citation Paper Abstract": "Abstract:We present a technique for efficiently synthesizing images of atmospheric clouds using a combination of Monte Carlo integration and neural networks. The intricacies of Lorenz-Mie scattering and the high albedo of cloud-forming aerosols make rendering of clouds---e.g. the characteristic silverlining and the \"whiteness\" of the inner body---challenging for methods based solely on Monte Carlo integration or diffusion theory. We approach the problem differently. Instead of simulating all light transport during rendering, we pre-learn the spatial and directional distribution of radiant flux from tens of cloud exemplars. To render a new scene, we sample visible points of the cloud and, for each, extract a hierarchical 3D descriptor of the cloud geometry with respect to the shading location and the light source. The descriptor is input to a deep neural network that predicts the radiance function for each shading configuration. We make the key observation that progressively feeding the hierarchical descriptor into the network enhances the network's ability to learn faster and predict with high accuracy while using few coefficients. We also employ a block design with residual connections to further improve performance. A GPU implementation of our method synthesizes images of clouds that are nearly indistinguishable from the reference solution within seconds interactively. Our method thus represents a viable solution for applications such as cloud design and, thanks to its temporal stability, also for high-quality production of animated content.",
                        "Citation Paper Authors": "Authors:Simon Kallweit, Thomas M\u00fcller, Brian McWilliams, Markus Gross, Jan Nov\u00e1k"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12993v1": {
            "Paper Title": "Neural Relightable Participating Media Rendering",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "aim to infer the 3D context of scenes from images. Classic explicit 3D representations, such as\nvoxels [ 15,16,6], multiplane images [ 17,18] and proxy geometry ",
                    "Citation Text": "Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio Orts-\nEscolano, Philip Davidson, Christoph Rhemann, Paul Debevec, et al. Neural light transport for relighting\nand view synthesis. ACM Transactions on Graphics (TOG) , 40(1):1\u201317, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.03806",
                        "Citation Paper Title": "Title:Neural Light Transport for Relighting and View Synthesis",
                        "Citation Paper Abstract": "Abstract:The light transport (LT) of a scene describes how it appears under different lighting and viewing directions, and complete knowledge of a scene's LT enables the synthesis of novel views under arbitrary lighting. In this paper, we focus on image-based LT acquisition, primarily for human bodies within a light stage setup. We propose a semi-parametric approach to learn a neural representation of LT that is embedded in the space of a texture atlas of known geometric properties, and model all non-diffuse and global LT as residuals added to a physically-accurate diffuse base rendering. In particular, we show how to fuse previously seen observations of illuminants and views to synthesize a new image of the same scene under a desired lighting condition from a chosen viewpoint. This strategy allows the network to learn complex material effects (such as subsurface scattering) and global illumination, while guaranteeing the physical correctness of the diffuse LT (such as hard shadows). With this learned LT, one can relight the scene photorealistically with a directional light or an HDRI map, synthesize novel views with view-dependent effects, or do both simultaneously, all in a unified framework using a set of sparse, previously seen observations. Qualitative and quantitative experiments demonstrate that our neural LT (NLT) outperforms state-of-the-art solutions for relighting and view synthesis, without separate treatment for both problems that prior work requires.",
                        "Citation Paper Authors": "Authors:Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, Jonathan T. Barron, Ravi Ramamoorthi, William T. Freeman"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "learns an embedding manifold from 2D images and scene representation networks ",
                    "Citation Text": "Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. Scene representation networks: Continuous\n3D-structure-aware neural scene representations. In Advances in Neural Information Processing Systems ,\npages 1121\u20131132, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01618",
                        "Citation Paper Title": "Title:Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations",
                        "Citation Paper Abstract": "Abstract:Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",
                        "Citation Paper Authors": "Authors:Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12601v1": {
            "Paper Title": "Semantic Resizing of Charts Through Generalization:A Case Study with\n  Line Charts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11460v1": {
            "Paper Title": "MUGL: Large Scale Multi Person Conditional Action Generation with\n  Locomotion",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "propose a novel self-\nattention based GCN method with category conditioning of a GAN\nnetwork. However, the focus is on generation of 2-D skeletons and\nonly 10action categories are considered. Guo et al. ",
                    "Citation Text": "Chuan Guo, Xinxin Zuo, et al .2020. Action2Motion: Conditioned Generation of\n3D Human Motions. ACMMM (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.15240",
                        "Citation Paper Title": "Title:Action2Motion: Conditioned Generation of 3D Human Motions",
                        "Citation Paper Abstract": "Abstract:Action recognition is a relatively established task, where givenan input sequence of human motion, the goal is to predict its ac-tion category. This paper, on the other hand, considers a relativelynew problem, which could be thought of as an inverse of actionrecognition: given a prescribed action type, we aim to generateplausible human motion sequences in 3D. Importantly, the set ofgenerated motions are expected to maintain itsdiversityto be ableto explore the entire action-conditioned motion space; meanwhile,each sampled sequence faithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by these objectives, we followthe physics law of human kinematics by adopting the Lie Algebratheory to represent thenaturalhuman motions; we also propose atemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of the motion space. A new 3D human motion dataset, HumanAct12, is also constructed. Empirical experiments overthree distinct human motion datasets (including ours) demonstratethe effectiveness of our approach.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "propose to generate the action sequence using a graph convolution\nbased GAN model with generation conditioned on a latent vector\nsampled from a Gaussian process. Yu et al. ",
                    "Citation Text": "Ping Yu, Yang Zhao, et al .2020. Structure-aware human-action generation. In\nECCV . 18\u201334.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.01971",
                        "Citation Paper Title": "Title:Structure-Aware Human-Action Generation",
                        "Citation Paper Abstract": "Abstract:Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, well-capturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods.",
                        "Citation Paper Authors": "Authors:Ping Yu, Yang Zhao, Chunyuan Li, Junsong Yuan, Changyou Chen"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "contains Kinect-based 3D skeleton\ndata with temporally incoherent, noisy and uncurated joint data.\nNa\u00efvely using this data for training causes models to generate poor\nquality action sequences. Therefore, we use VIBE ",
                    "Citation Text": "Muhammed Kocabas, Nikos Athanasiou, et al .2020. VIBE: Video Inference for\nHuman Body Pose and Shape Estimation. In CVPR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.05656",
                        "Citation Paper Title": "Title:VIBE: Video Inference for Human Body Pose and Shape Estimation",
                        "Citation Paper Abstract": "Abstract:Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose Video Inference for Body Pose and Shape Estimation (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Muhammed Kocabas, Nikos Athanasiou, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "adopt a similar approach\nbut within a GAN-based setting which enables stochastic sequence\ngeneration. Hyemin et al. ",
                    "Citation Text": "Hyemin Ahn, Timothy Ha, et al .2018. Text2Action: Generative Adversarial\nSynthesis from Language to Action. In ICRA . 5915\u20135920.Ablation Type Pipeline Component Ablation Details MMD-A\u2193MMD-S\u2193\nArchitecturalVAE (Sec. 4.1)Reduce VAE latent dimensions (0.5x) 0.47 0.47\nIncrease VAE latent dimensions (2x) 0.48 0.55\nReduce GMM components ( \ud835\udc3e= 60) 0.41 0.39\nIncrease GMM components ( \ud835\udc3e= 240 ) 0.47 0.39\nVanilla VAE: Unimodal Gaussian ( \ud835\udc3e= 1) 0.45 0.41\nEncoder & Decoder (Sec. 4.2,4.4)MLP in Spatial Encoder 0.46 0.39\nTranspose 2D conv in Spatial Decoder 0.65 0.64\nNo class, viewpoint conditioning in Sequence Encoder 0.48 0.41\n3-D joint representation as input, output 0.46 0.45\nViewpoint conditioning (Sec. 4.3)Using one-hot vector/remove viewpoint transformer 0.49 0.41\nNo viewpoint conditioning 0.45 0.39\nNo viewpoint conditioning in latent 0.79 0.75\nVariable sequence length Remove variable sequence length encoder-decoder 1.77 0.79\nOptimization Sequence Decoder (Sec. 4.4)No 3D Loss 0.74 0.37\nNo Rotation Loss 0.56 0.39\nMUGL (multi-person) 0.45 0.36\nTable 4: Performance scores for MUGL ablative variants.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.05298",
                        "Citation Paper Title": "Title:Text2Action: Generative Adversarial Synthesis from Language to Action",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.",
                        "Citation Paper Authors": "Authors:Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, Songhwai Oh"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "use reinforcement\nlearning to create realistic motion clips imitating a broad range\nincluding locomotion acrobatics. Peng et al. ",
                    "Citation Text": "Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey\nLevine. 2018. SFV: Reinforcement Learning of Physical Skills from Videos. ACM\nSIGGRAPH , Article 178 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.03599",
                        "Citation Paper Title": "Title:SFV: Reinforcement Learning of Physical Skills from Videos",
                        "Citation Paper Abstract": "Abstract:Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts.",
                        "Citation Paper Authors": "Authors:Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "also use an autoregressive setup involving disjoint part grouping\nof skeleton joints. However, they train separate models for each\naction category. To produce sequences, Battan et al. ",
                    "Citation Text": "Neeraj Battan, Yudhik Agrawal, Sai Soorya Rao, Aman Goel, and Avinash Sharma.\n2021. GlocalNet: Class-Aware Long-Term Human Motion Synthesis. In WACV .\n879\u2013888.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.10744",
                        "Citation Paper Title": "Title:GlocalNet: Class-aware Long-term Human Motion Synthesis",
                        "Citation Paper Abstract": "Abstract:Synthesis of long-term human motion skeleton sequences is essential to aid human-centric video generation with potential applications in Augmented Reality, 3D character animations, pedestrian trajectory prediction, etc. Long-term human motion synthesis is a challenging task due to multiple factors like, long-term temporal dependencies among poses, cyclic repetition across poses, bi-directional and multi-scale dependencies among poses, variable speed of actions, and a large as well as partially overlapping space of temporal pose variations across multiple class/types of human activities. This paper aims to address these challenges to synthesize a long-term (> 6000 ms) human motion trajectory across a large variety of human activity classes (>50). We propose a two-stage activity generation method to achieve this goal, where the first stage deals with learning the long-term global pose dependencies in activity sequences by learning to synthesize a sparse motion trajectory while the second stage addresses the generation of dense motion trajectories taking the output of the first stage. We demonstrate the superiority of the proposed method over SOTA methods using various quantitative evaluation metrics on publicly available datasets.",
                        "Citation Paper Authors": "Authors:Neeraj Battan, Yudhik Agrawal, Veeravalli Saisooryarao, Aman Goel, Avinash Sharma"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Action prediction: A small initial action sequence is used to con-\ndition the generation of the full version in action prediction task.\nOne set of approaches employ a sequence-to-sequence paradigm to\npredict joints [ 6,11] or joint velocities ",
                    "Citation Text": "Julieta Martinez, Michael J Black, et al .2017. On human motion prediction using\nrecurrent neural networks. In CVPR . 2891\u20132900.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02445",
                        "Citation Paper Title": "Title:On human motion prediction using recurrent neural networks",
                        "Citation Paper Abstract": "Abstract:Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",
                        "Citation Paper Authors": "Authors:Julieta Martinez, Michael J. Black, Javier Romero"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.10501v1": {
            "Paper Title": "STALP: Style Transfer with Auxiliary Limited Pairing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.10494v1": {
            "Paper Title": "Deep Point Cloud Normal Estimation via Triplet Learning",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "adopted an attentional module that softly\nselects neighbouring points for normal estimation. This\nnetwork, known as NINormal, can thereby decide which\nneighbouring points are useful for preserving features in the\nshape. To further preserve feature normals, Lu et al. ",
                    "Citation Text": "Dening Lu, Xuequan Lu, Yangxing Sun, and Jun Wang.\nDeep feature-preserving normal estimation for point cloud\n\ufb01ltering. Computer-Aided Design , 125:102860, 2020. 1, 2,\n5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11563",
                        "Citation Paper Title": "Title:Deep Feature-preserving Normal Estimation for Point Cloud Filtering",
                        "Citation Paper Abstract": "Abstract:Point cloud filtering, the main bottleneck of which is removing noise (outliers) while preserving geometric features, is a fundamental problem in 3D field. The two-step schemes involving normal estimation and position update have been shown to produce promising results. Nevertheless, the current normal estimation methods including optimization ones and deep learning ones, often either have limited automation or cannot preserve sharp features. In this paper, we propose a novel feature-preserving normal estimation method for point cloud filtering with preserving geometric features. It is a learning method and thus achieves automatic prediction for normals. For training phase, we first generate patch based samples which are then fed to a classification network to classify feature and non-feature points. We finally train the samples of feature and non-feature points separately, to achieve decent results. Regarding testing, given a noisy point cloud, its normals can be automatically estimated. For further point cloud filtering, we iterate the above normal estimation and a current position update algorithm for a few times. Various experiments demonstrate that our method outperforms state-of-the-art normal estimation methods and point cloud filtering techniques, in terms of both quality and quantity.",
                        "Citation Paper Authors": "Authors:Dening Lu, Xuequan Lu, Yangxing Sun, Jun Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.09936v1": {
            "Paper Title": "NeuralDiff: Segmenting 3D objects that move in egocentric videos",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ", NeuralDiff, NeuralDiff+C, and NeuralDiff+C+A.\nFrame + GT NeuralDiff+A MG ",
                    "Citation Text": "Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman,\nand Weidi Xie. Self-supervised video object segmentation\nby motion grouping. In Proc. ICCV , 2021. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07658",
                        "Citation Paper Title": "Title:Self-supervised Video Object Segmentation by Motion Grouping",
                        "Citation Paper Abstract": "Abstract:Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.",
                        "Citation Paper Authors": "Authors:Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, Weidi Xie"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ", still starting from a canonical\nvolume. Closer to our work, ",
                    "Citation Text": "Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene \ufb02ow \ufb01elds for space-time view synthesis of dy-\nnamic scenes. In Proc. CVPR , 2021. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13084",
                        "Citation Paper Title": "Title:Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes",
                        "Citation Paper Abstract": "Abstract:We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",
                        "Citation Paper Authors": "Authors:Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.01137v2": {
            "Paper Title": "The Power of Points for Modeling Humans in Clothing",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ", and implicit\nfunctions [10, 37, 48, 65, 72, 75].\nOur shape decoder is a coordinate-based multi-layer per-\nceptron (MLP), reminiscent of the recent line of work on\nneural implicit surfaces [11, 44, 50] and neural radiance\nfields ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) , pages 405\u2013421. Springer, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "), resulting\nin 24 outfits including challenging cases such as skirts and\njackets. We then use physics simulation to drape the cloth-\ning on the 3D bodies from the AGORA dataset ",
                    "Citation Text": "Priyanka Patel, Chun-Hao Huang Paul, Joachim Tesch,\nDavid Hoffmann, Shashank Tripathi, and Michael J. Black.\nAGORA: Avatars in geography optimized for regression\nanalysis. In Proceedings IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR) , pages 13468\u201313478, June\n2021. 6, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.14643",
                        "Citation Paper Title": "Title:AGORA: Avatars in Geography Optimized for Regression Analysis",
                        "Citation Paper Abstract": "Abstract:While the accuracy of 3D human pose estimation from images has steadily improved on benchmark datasets, the best methods still fail in many real-world scenarios. This suggests that there is a domain gap between current datasets and common scenes containing people. To obtain ground-truth 3D pose, current datasets limit the complexity of clothing, environmental conditions, number of subjects, and occlusion. Moreover, current datasets evaluate sparse 3D joint locations corresponding to the major joints of the body, ignoring the hand pose and the face shape. To evaluate the current state-of-the-art methods on more challenging images, and to drive the field to address new problems, we introduce AGORA, a synthetic dataset with high realism and highly accurate ground truth. Here we use 4240 commercially-available, high-quality, textured human scans in diverse poses and natural clothing; this includes 257 scans of children. We create reference 3D poses and body shapes by fitting the SMPL-X body model (with face and hands) to the 3D scans, taking into account clothing. We create around 14K training and 3K test images by rendering between 5 and 15 people per image using either image-based lighting or rendered 3D environments, taking care to make the images physically plausible and photoreal. In total, AGORA consists of 173K individual person crops. We evaluate existing state-of-the-art methods for 3D human pose estimation on this dataset and find that most methods perform poorly on images of children. Hence, we extend the SMPL-X model to better capture the shape of children. Additionally, we fine-tune methods on AGORA and show improved performance on both AGORA and 3DPW, confirming the realism of the dataset. We provide all the registered 3D reference training data, rendered images, and a web-based evaluation site at this https URL.",
                        "Citation Paper Authors": "Authors:Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann, Shashank Tripathi, Michael J. Black"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". We evaluate over 6,000 unseen test examples\nin the CAPE and ReSynth datasets, from different subjects,\nperforming different poses. For each example, the point\ncloud output from POP and SCALE are both rendered with\na surfel-based renderer by Open3D ",
                    "Citation Text": "Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D:\nA modern library for 3D data processing. arXiv preprint:\n1801.09847 , 2018. 1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.09847",
                        "Citation Paper Title": "Title:Open3D: A Modern Library for 3D Data Processing",
                        "Citation Paper Abstract": "Abstract:Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.",
                        "Citation Paper Authors": "Authors:Qian-Yi Zhou, Jaesik Park, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "in prior work. 2) We\npredict the clothing deformation field on top of the canoni-\ncally posed body, instead of absolute Euclidean point coor-\ndinates ",
                    "Citation Text": "Thibault Groueix, Matthew Fisher, Vladimir G Kim,\nBryan C Russell, and Mathieu Aubry. 3D-CODED: 3D cor-\nrespondences by deep deformation. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n230\u2013246, 2018. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05228",
                        "Citation Paper Title": "Title:3D-CODED : 3D Correspondences by Deep Deformation",
                        "Citation Paper Abstract": "Abstract:We present a new deep learning approach for matching deformable shapes by introducing {\\it Shape Deformation Networks} which jointly encode 3D shapes and correspondences. This is achieved by factoring the surface representation into (i) a template, that parameterizes the surface, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input surface. By predicting this feature for a new shape, we implicitly predict correspondences between this shape and the template. We show that these correspondences can be improved by an additional step which improves the shape feature by minimizing the Chamfer distance between the input and transformed template. We demonstrate that our simple approach improves on state-of-the-art results on the difficult FAUST-inter challenge, with an average correspondence error of 2.88cm. We show, on the TOSCA dataset, that our method is robust to many types of perturbations, and generalizes to non-human shapes. This robustness allows it to perform well on real unclean, meshes from the the SCAPE dataset.",
                        "Citation Paper Authors": "Authors:Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "proposes an articulated dense point cloud representation\nto model clothing deformation. However, the patch-based\npoint clouds often suffer from overlap ",
                    "Citation Text": "Thibault Groueix, Matthew Fisher, Vladimir G. Kim,\nBryan C. Russell, and Mathieu Aubry. A papier-m \u02c6ach\u00b4e ap-\nproach to learning 3D surface generation. Proceedings IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) ,\npages 216\u2013224, 2018. 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05384",
                        "Citation Paper Title": "Title:AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation",
                        "Citation Paper Abstract": "Abstract:We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",
                        "Citation Paper Authors": "Authors:Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.07272v2": {
            "Paper Title": "Relighting Humans in the Wild: Monocular Full-Body Human Relighting with\n  Domain Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07253v1": {
            "Paper Title": "Rethinking Point Cloud Filtering: A Non-Local Position Based Approach",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "\ufb01rst manually labeled sharp edges on 3D mod-\nels and designed a network for consolidating point cloud data.\nPointCleanNet ",
                    "Citation Text": "M. Rakotosaona, V . L. Barbera, P. Guerrero, N. J. Mitra, M. Ovsjanikov,\nPointcleannet: Learning to denoise and remove outliers from dense point\nclouds, Computer Graphics Forum 39 (1).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.01060",
                        "Citation Paper Title": "Title:PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds obtained with 3D scanners or by image-based reconstruction techniques are often corrupted with significant amount of noise and outliers. Traditional methods for point cloud denoising largely rely on local surface fitting (e.g., jets or MLS surfaces), local or non-local averaging, or on statistical assumptions about the underlying noise model. In contrast, we develop a simple data-driven method for removing outliers and reducing noise in unordered point clouds. We base our approach on a deep learning architecture adapted from PCPNet, which was recently proposed for estimating local 3D shape properties in point clouds. Our method first classifies and discards outlier samples, and then estimates correction vectors that project noisy points onto the original clean surfaces. The approach is efficient and robust to varying amounts of noise and outliers, while being able to handle large densely-sampled point clouds. In our extensive evaluation, both on synthesic and real data, we show an increased robustness to strong noise levels compared to various state-of-the-art methods, enabling accurate surface reconstruction from extremely noisy real data obtained by range scans. Finally, the simplicity and universality of our approach makes it very easy to integrate in any existing geometry processing pipeline.",
                        "Citation Paper Authors": "Authors:Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J. Mitra, Maks Ovsjanikov"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "proposed a two-stage method to separately re-\nmove outliers and noise, without the consideration of normal in-\nformation. DNP ",
                    "Citation Text": "D. Lu, X. Lu, Y . Sun, J. Wang, Deep feature-preserving normal estimation\nfor point cloud \ufb01ltering, Computer-Aided Design 125 (2020) 102860.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.11563",
                        "Citation Paper Title": "Title:Deep Feature-preserving Normal Estimation for Point Cloud Filtering",
                        "Citation Paper Abstract": "Abstract:Point cloud filtering, the main bottleneck of which is removing noise (outliers) while preserving geometric features, is a fundamental problem in 3D field. The two-step schemes involving normal estimation and position update have been shown to produce promising results. Nevertheless, the current normal estimation methods including optimization ones and deep learning ones, often either have limited automation or cannot preserve sharp features. In this paper, we propose a novel feature-preserving normal estimation method for point cloud filtering with preserving geometric features. It is a learning method and thus achieves automatic prediction for normals. For training phase, we first generate patch based samples which are then fed to a classification network to classify feature and non-feature points. We finally train the samples of feature and non-feature points separately, to achieve decent results. Regarding testing, given a noisy point cloud, its normals can be automatically estimated. For further point cloud filtering, we iterate the above normal estimation and a current position update algorithm for a few times. Various experiments demonstrate that our method outperforms state-of-the-art normal estimation methods and point cloud filtering techniques, in terms of both quality and quantity.",
                        "Citation Paper Authors": "Authors:Dening Lu, Xuequan Lu, Yangxing Sun, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", we choose the Alternating Lagrange Multiplier\n(ALM) based algorithm introduced by ",
                    "Citation Text": "Z. Lin, M. Chen, Y . Ma, The augmented lagrange multiplier method for\nexact recovery of corrupted low-rank matrices, Mathematical Programming\n9.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1009.5055",
                        "Citation Paper Title": "Title:The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices",
                        "Citation Paper Abstract": "Abstract:This paper proposes scalable and fast algorithms for solving the Robust PCA problem, namely recovering a low-rank matrix with an unknown fraction of its entries being arbitrarily corrupted. This problem arises in many applications, such as image processing, web data ranking, and bioinformatic data analysis. It was recently shown that under surprisingly broad conditions, the Robust PCA problem can be exactly solved via convex optimization that minimizes a combination of the nuclear norm and the $\\ell^1$-norm . In this paper, we apply the method of augmented Lagrange multipliers (ALM) to solve this convex program. As the objective function is non-smooth, we show how to extend the classical analysis of ALM to such new objective functions and prove the optimality of the proposed algorithms and characterize their convergence rate. Empirically, the proposed new algorithms can be more than five times faster than the previous state-of-the-art algorithms for Robust PCA, such as the accelerated proximal gradient (APG) algorithm. Moreover, the new algorithms achieve higher precision, yet being less storage/memory demanding. We also show that the ALM technique can be used to solve the (related but somewhat simpler) matrix completion problem and obtain rather promising results too. We further prove the necessary and sufficient condition for the inexact ALM to converge globally. Matlab code of all algorithms discussed are available at this http URL",
                        "Citation Paper Authors": "Authors:Zhouchen Lin, Minming Chen, Yi Ma"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "designed a graph-convolutional\nlayers-based neural network to build hierarchies of features from\nsimilarity among the high-dimensional feature representations of\nthe point cloud. ",
                    "Citation Text": "S. Luo, W. Hu, Di \u000berentiable manifold reconstruction for point cloud de-\nnoising, in: Proceedings of the 28th ACM International Conference on\nMultimedia, 2020, pp. 1330\u20131338.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.13551",
                        "Citation Paper Title": "Title:Differentiable Manifold Reconstruction for Point Cloud Denoising",
                        "Citation Paper Abstract": "Abstract:3D point clouds are often perturbed by noise due to the inherent limitation of acquisition equipments, which obstructs downstream tasks such as surface reconstruction, rendering and so on. Previous works mostly infer the displacement of noisy points from the underlying surface, which however are not designated to recover the surface explicitly and may lead to sub-optimal denoising results. To this end, we propose to learn the underlying manifold of a noisy point cloud from differentiably subsampled points with trivial noise perturbation and their embedded neighborhood feature, aiming to capture intrinsic structures in point clouds. Specifically, we present an autoencoder-like neural network. The encoder learns both local and non-local feature representations of each point, and then samples points with low noise via an adaptive differentiable pooling operation. Afterwards, the decoder infers the underlying manifold by transforming each sampled point along with the embedded feature of its neighborhood to a local surface centered around the point. By resampling on the reconstructed manifold, we obtain a denoised point cloud. Further, we design an unsupervised training loss, so that our network can be trained in either an unsupervised or supervised fashion. Experiments show that our method significantly outperforms state-of-the-art denoising methods under both synthetic noise and real world noise. The code and data are available at this https URL",
                        "Citation Paper Authors": "Authors:Shitong Luo, Wei Hu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "proposed an unsupervised \ufb01ltering\nmethod that only trains on noise data without ground truth. Due\nto the missing ground-truth information, they cannot well pre-\nserve sharp-edge features. ",
                    "Citation Text": "F. Pistilli, G. Fracastoro, D. Valsesia, E. Magli, Learning graph-\nconvolutional representations for point cloud denoising, in: European Con-\nference on Computer Vision, Springer, 2020, pp. 103\u2013118.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02578",
                        "Citation Paper Title": "Title:Learning Graph-Convolutional Representations for Point Cloud Denoising",
                        "Citation Paper Abstract": "Abstract:Point clouds are an increasingly relevant data type but they are often corrupted by noise. We propose a deep neural network based on graph-convolutional layers that can elegantly deal with the permutation-invariance problem encountered by learning-based point cloud processing methods. The network is fully-convolutional and can build complex hierarchies of features by dynamically constructing neighborhood graphs from similarity among the high-dimensional feature representations of the points. When coupled with a loss promoting proximity to the ideal surface, the proposed approach significantly outperforms state-of-the-art methods on a variety of metrics. In particular, it is able to improve in terms of Chamfer measure and of quality of the surface normals that can be estimated from the denoised data. We also show that it is especially robust both at high noise levels and in presence of structured noise such as the one encountered in real LiDAR scans.",
                        "Citation Paper Authors": "Authors:Francesca Pistilli, Giulia Fracastoro, Diego Valsesia, Enrico Magli"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "pro-\nposed a high-\ufb01delity di \u000berentiable renderer based on point posi-\ntions and normals, which could also be used in point cloud \ufb01lter-\ning. ",
                    "Citation Text": "X. Lu, S. Schaefer, J. Luo, L. Ma, Y . He, Low rank matrix approximation\nfor 3d geometry \ufb01ltering, IEEE Transactions on Visualization and Com-\nputer Graphics.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06783",
                        "Citation Paper Title": "Title:Low Rank Matrix Approximation for Geometry Filtering",
                        "Citation Paper Abstract": "Abstract:We propose a robust normal estimation method for both point clouds and meshes using a low rank matrix approximation algorithm. First, we compute a local feature descriptor for each point and find similar, non-local neighbors that we organize into a matrix. We then show that a low rank matrix approximation algorithm can robustly estimate normals for both point clouds and meshes. Furthermore, we provide a new filtering method for point cloud data to smooth the position data to fit the estimated normals. We show applications of our method to point cloud filtering, point set upsampling, surface reconstruction, mesh denoising, and geometric texture removal. Our experiments show that our method outperforms current methods in both visual quality and accuracy.",
                        "Citation Paper Authors": "Authors:Xuequan Lu, Scott Schaefer, Jun Luo, Lizhuang Ma, Ying He"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "transformed\n3D points into 2D height maps, which are fed into CNNs for\ntraining. ",
                    "Citation Text": "L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, P.-A. Heng, Ec-net: an edge-aware\npoint set consolidation network, in: Proceedings of the European Confer-\nence on Computer Vision (ECCV), 2018, pp. 386\u2013402.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06010",
                        "Citation Paper Title": "Title:EC-Net: an Edge-aware Point set Consolidation Network",
                        "Citation Paper Abstract": "Abstract:Point clouds obtained from 3D scans are typically sparse, irregular, and noisy, and required to be consolidated. In this paper, we present the first deep learning based edge-aware technique to facilitate the consolidation of point clouds. We design our network to process points grouped in local patches, and train it to learn and help consolidate points, deliberately for edges. To achieve this, we formulate a regression component to simultaneously recover 3D point coordinates and point-to-edge distances from upsampled features, and an edge-aware joint loss function to directly minimize distances from output points to 3D meshes and to edges. Compared with previous neural network based works, our consolidation is edge-aware. During the synthesis, our network can attend to the detected sharp edges and enable more accurate 3D reconstructions. Also, we trained our network on virtual scanned point clouds, demonstrated the performance of our method on both synthetic and real point clouds, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06688v1": {
            "Paper Title": "DeepVecFont: Synthesizing High-quality Vector Fonts via Dual-modality\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05805v1": {
            "Paper Title": "Real-time Skeletonization for Sketch-based Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05433v1": {
            "Paper Title": "Mesh Draping: Parametrization-Free Neural Mesh Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.11008v2": {
            "Paper Title": "SuperCaustics: Real-time, open-source simulation of transparent objects\n  for deep learning applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04994v1": {
            "Paper Title": "Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision\n  Datasets from 3D Scans",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "83.00 2 91.11 1 62.8 3 43.8 3 0.0426 1 0.4 3\nCross-stitch ",
                    "Citation Text": "Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\ntial Hebert. Cross-stitch networks for multi-task learning.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 3994\u20134003, 2016. 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.03539",
                        "Citation Paper Title": "Title:Cross-stitch Networks for Multi-task Learning",
                        "Citation Paper Abstract": "Abstract:Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
                        "Citation Paper Authors": "Authors:Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", we evaluate zero-shot cross-dataset transfer\nwith test predictions and GT aligned in scale and shift in\n1MiDaS v3.0 also uses MTAN ",
                    "Citation Text": "Shikun Liu, Edward Johns, and Andrew J Davison. End-to-\nend multi-task learning with attention. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1871\u20131880, 2019. 3, 6, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.10704",
                        "Citation Paper Title": "Title:End-to-End Multi-Task Learning with Attention",
                        "Citation Paper Abstract": "Abstract:We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of task-specific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-to-image predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shikun Liu, Edward Johns, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "introduced a Blender-based approach for doing\ndomain randomization and creating static datasets of RGB,\ndepth, and surface normals from SunCG ",
                    "Citation Text": "Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose es-\ntimation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 5693\u2013\n5703, 2019. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.09212",
                        "Citation Paper Title": "Title:Deep High-Resolution Representation Learning for Human Pose Estimation",
                        "Citation Paper Abstract": "Abstract:This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". Further details and results are available in the\nsupplementary.\nPanoptic segmentation: To demonstrate the pipeline\u2019s\nability to train models for non-geometric tasks, we train\na PanopticFPN ",
                    "Citation Text": "Alexander Kirillov, Ross B. Girshick, Kaiming He, and Piotr\nDoll\u00b4ar. Panoptic feature pyramid networks. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 6392\u20136401, 2019. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02446",
                        "Citation Paper Title": "Title:Panoptic Feature Pyramid Networks",
                        "Citation Paper Abstract": "Abstract:The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ". Full experimental details\nand more results are available in the supplementary.\nMethod Test Data L1 Error ( \u2193)\u03b4 >1.25(\u2193)\u03b4 >1.252(\u2193)\u03b4 >1.253(\u2193)\nXTC ",
                    "Citation Text": "Amir Zamir, Alexander Sax, Teresa Yeo, O \u02d8guzhan Kar,\nNikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik,\nand Leonidas Guibas. Robust learning through cross-task\nconsistency. arXiv , 2020. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.04096",
                        "Citation Paper Title": "Title:Robust Learning Through Cross-Task Consistency",
                        "Citation Paper Abstract": "Abstract:Visual perception entails solving a wide set of tasks, e.g., object detection, depth estimation, etc. The predictions made for multiple tasks from the same image are not independent, and therefore, are expected to be consistent. We propose a broadly applicable and fully computational method for augmenting learning with Cross-Task Consistency. The proposed formulation is based on inference-path invariance over a graph of arbitrary tasks. We observe that learning with cross-task consistency leads to more accurate predictions and better generalization to out-of-distribution inputs. This framework also leads to an informative unsupervised quantity, called Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy correlates well with the supervised error (r=0.67), thus it can be employed as an unsupervised confidence metric as well as for detection of out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and they benchmark cross-task consistency versus various baselines including conventional multi-task learning, cycle consistency, and analytical consistency.",
                        "Citation Paper Authors": "Authors:Amir Zamir, Alexander Sax, Teresa Yeo, O\u011fuzhan Kar, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, Leonidas Guibas"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". Other datasets con-\ntain primarily outdoor scenes, usually driving\u2013such as\nCARLA ",
                    "Citation Text": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Proceedings of the 1st Annual Conference on\nRobot Learning , pages 1\u201316, 2017. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", and common augmentations include tex-\nture and background randomization on the scene mesh. Re-\ncently, ",
                    "Citation Text": "Maximilian Denninger, Martin Sundermeyer, Dominik\nWinkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad El-\nbadrawy, Ahsan Lodhi, and Harinandan Katam. Blender-\nproc. arXiv preprint arXiv:1911.01911 , 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01911",
                        "Citation Paper Title": "Title:BlenderProc",
                        "Citation Paper Abstract": "Abstract:BlenderProc is a modular procedural pipeline, which helps in generating real looking images for the training of convolutional neural networks. These can be used in a variety of use cases including segmentation, depth, normal and pose estimation and many others. A key feature of our extension of blender is the simple to use modular pipeline, which was designed to be easily extendable. By offering standard modules, which cover a variety of scenarios, we provide a starting point on which new modules can be created.",
                        "Citation Paper Authors": "Authors:Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.05917v3": {
            "Paper Title": "DiffPD: Differentiable Projective Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10682v2": {
            "Paper Title": "DiffStyler: Controllable Dual Diffusion for Text-Driven Image\n  Stylization",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "Inference time (s/image) \u2193 18 1 20 45 40 20\nCLIP score ",
                    "Citation Text": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al. , \u201cLearning transferable\nvisual models from natural language supervision,\u201d in International\nConference on Machine Learning (ICML) , 2021, pp. 8748\u20138763. 2,\n5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", respectively), which is much smaller than the\ngeneral diffusion model ",
                    "Citation Text": "N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman,\n\u201cDreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation,\u201d in IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , 2023, pp. 22 500\u201322 510. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.12242",
                        "Citation Paper Title": "Title:DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
                        "Citation Paper Abstract": "Abstract:Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.02397v3": {
            "Paper Title": "Textured Mesh Quality Assessment: Large-Scale Dataset and Deep\n  Learning-based Quality Metric",
            "Sentences": []
        }
    }
}